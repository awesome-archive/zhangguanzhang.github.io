<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhangguanzhang</title>
  
  <subtitle>站在巨人的肩膀上</subtitle>
  <link href="http://zhangguanzhang.github.io/atom.xml" rel="self"/>
  
  <link href="http://zhangguanzhang.github.io/"/>
  <updated>2021-12-19T21:28:30.000Z</updated>
  <id>http://zhangguanzhang.github.io/</id>
  
  <author>
    <name>Zhangguanzhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>openwrt 的在线升级固件和扩容的研究</title>
    <link href="http://zhangguanzhang.github.io/2021/12/19/openwrt-update/"/>
    <id>http://zhangguanzhang.github.io/2021/12/19/openwrt-update/</id>
    <published>2021-12-19T21:28:30.000Z</published>
    <updated>2021-12-19T21:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>手上有 r2s、N1 和 x86_64 的固件维护，r2s 的参照别人的脚本搞了在线升级固件的脚本，别人的脚本只支持 ext4 升级，而后面我也把 squashfs 格式的固件升级搞出来了。恩山上有的人的固件我也看 x86_64 也可以在线升级，后面我也会去测下 x86_64 的，理论上是通用的。</p><h2 id="升级过程"><a href="#升级过程" class="headerlink" title="升级过程"></a>升级过程</h2><p>以 r2s 为例讲解。参照目前看到的的 <a href="https://github.com/klever1988/nanopi-openwrt/raw/master/scripts/autoupdate.sh">1988 的升级脚本</a> ，最初的人不知道是谁搞的在线升级，因为很久之前就看到有些人的固件能在线升级了。</p><h3 id="升级前准备"><a href="#升级前准备" class="headerlink" title="升级前准备"></a>升级前准备</h3><h4 id="相关命令"><a href="#相关命令" class="headerlink" title="相关命令"></a>相关命令</h4><p>确保固件有下面命令：</p><table><thead><tr><th align="left">command</th><th align="left">package name</th><th align="left">用途</th></tr></thead><tbody><tr><td align="left">parted</td><td align="left">parted</td><td align="left">修改分区和获取分区信息</td></tr><tr><td align="left">losetup</td><td align="left">losetup</td><td align="left">loop device 命令，用于挂载固件里的文件分区</td></tr><tr><td align="left">resize2fs</td><td align="left">resize2fs</td><td align="left">resize ext4 需要</td></tr><tr><td align="left">truncate</td><td align="left">coreutils-truncate</td><td align="left">填充和清空文件，这里是填充扩容</td></tr><tr><td align="left">curl</td><td align="left">curl</td><td align="left">下载，以及http 调用一些 api</td></tr><tr><td align="left">wget</td><td align="left">wget</td><td align="left">下载命令</td></tr><tr><td align="left">mksquashfs</td><td align="left">squashfs-tools-mksquashfs</td><td align="left">squashfs格式需要</td></tr><tr><td align="left">unsquashfs</td><td align="left">squashfs-tools-unsquashfs</td><td align="left">squashfs格式需要</td></tr></tbody></table><h4 id="KERNEL-PARTSIZE-和-ROOTFS-PARTSIZE"><a href="#KERNEL-PARTSIZE-和-ROOTFS-PARTSIZE" class="headerlink" title="KERNEL_PARTSIZE 和 ROOTFS_PARTSIZE"></a>KERNEL_PARTSIZE 和 ROOTFS_PARTSIZE</h4><p><code>CONFIG_TARGET_KERNEL_PARTSIZE</code> 和 <code>CONFIG_TARGET_ROOTFS_PARTSIZE</code> 是 <code>.config</code> 文件里的，单位是 <code>M</code>，前者是类似常规大型 linux os 里的 <code>/boot</code> 分区，openwrt 默认就只有这两个分区。</p><p>r2s 的话 <code>KERNEL_PARTSIZE</code> 一般 <code>12M</code> 就够用了，但是很多网上互相抄的人在 r2s 的 <code>.config</code> 里给 32、64 之类的非常浪费。<code>ROOTFS_PARTSIZE</code> 是最终的根分区大小，给小了因为编译带很多插件，导致最终的打包镜像容量不够，我的固件是 <code>635</code>。然后 r2s 是内存卡，一般现在内存大大小都是 4G 以上，也就是刷完固件后，根分区就是 635M ，卡的剩下空间都没使用，当然，x86_64 也是一样的问题。所以就有了这个升级顺带扩容的步骤。</p><h4 id="提前的容量存储新固件"><a href="#提前的容量存储新固件" class="headerlink" title="提前的容量存储新固件"></a>提前的容量存储新固件</h4><p>1988 的固件非常小，下载 300M，从一个新手初次尝试来说，很可能尴尬的情况就是卡刷后 rootfs 是 600M，然后可用就200M，固件压缩后350M，所以我固件在初次扩容升级会暂时新建一个分区，用于存储下载升级的固件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 一般很多固件 /opt 单独挂载，或者属于 / ，所以如果已经在升级阶段扩容了，固件就存 /opt下，没扩容过，就存挂载点 /tmp/update/download</span></span><br><span class="line">if [ $(df  -m /opt | awk &#x27;NR==2&#123;print $4&#125;&#x27;) -lt 2400 ];then</span><br><span class="line">    NEED_GROW=1</span><br><span class="line">    mkdir -p /tmp/update/download</span><br><span class="line">    warning &#x27;检测到当前未扩容，先借用初版固件扩容，后续请再执行升级脚本&#x27;</span><br><span class="line">    df -h</span><br><span class="line">    parted /dev/$block_device p</span><br><span class="line">    # 该文件存 part_num ，防止机器重启后重复新建了分区表</span><br><span class="line">    if [ ! -f &#x27;/opt/.parted&#x27; ];then</span><br><span class="line">        start_sec=$(parted /dev/$block_device unit s print free | awk &#x27;$1~&quot;s&quot;&#123;a=$1&#125;END&#123;print a&#125;&#x27;)</span><br><span class="line">        parted /dev/$block_device mkpart p ext4 $&#123;start_sec&#125; 4G</span><br><span class="line">        part_num=$( parted /dev/$block_device p | awk &#x27;$5==&quot;primary&quot;&#123;a=$1&#125;END&#123;print a&#125;&#x27; )</span><br><span class="line">        sleep 3 # 此处会自动挂载造成蛋疼</span><br><span class="line">        if grep -E /dev/$&#123;block_device&#125;p$&#123;part_num&#125; /proc/mounts;then</span><br><span class="line">            if mountpoint -q  /mnt/$&#123;block_device&#125;p$&#123;part_num&#125;;then</span><br><span class="line">                touch /mnt/$&#123;block_device&#125;p$&#123;part_num&#125;/test &amp;&gt;/dev/null || NEED_MKFS=1</span><br><span class="line">                umount /mnt/$&#123;block_device&#125;p$&#123;part_num&#125;</span><br><span class="line">            fi</span><br><span class="line">            [ -n &quot;$NEED_MKFS&quot; ] &amp;&amp; mkfs.ext4 -F /dev/$&#123;block_device&#125;p$&#123;part_num&#125;</span><br><span class="line">        else</span><br><span class="line">            mkfs.ext4 -F /dev/$&#123;block_device&#125;p$&#123;part_num&#125;</span><br><span class="line">        fi</span><br><span class="line">        echo $&#123;part_num&#125; &gt; /opt/.parted</span><br><span class="line">    else</span><br><span class="line">        part_num=$(cat /opt/.parted)</span><br><span class="line">    fi</span><br><span class="line">    mountpoint -q  /tmp/update/download || mount /dev/$&#123;block_device&#125;p$&#123;part_num&#125; /tmp/update/download</span><br><span class="line">    USER_FILE=/tmp/update/download/openwrt.img.gz</span><br><span class="line">    rm -f $&#123;USER_FILE&#125;</span><br><span class="line">    # 因为初次没扩容，我的固件是存放在 docker 镜像里的，可能拉取 docker 镜像就容量满了，所以我单独有个release 存放编译好的固件，直接下载，用于初次</span><br><span class="line">    wget https://ghproxy.com/https://github.com/zhangguanzhang/Actions-OpenWrt/releases/download/fs/openwrt-rockchip-armv8-friendlyarm_nanopi-r2s-ext4-sysupgrade.img.gz -O $&#123;USER_FILE&#125;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>对于后面的下载新版本固件，1988 的脚本我看他是 github action 每天定时编译发布存 release，感觉后面他可能会被 github 给 ban了。 我脚本里是存 docker hub 的镜像里，我的固件都自带 docker，docker 拉取镜像后提取镜像文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;$&#123;BUILD_DIR&#125;/Dockerfile &lt;&lt; EOF</span><br><span class="line">FROM alpine</span><br><span class="line">LABEL FILE=$file</span><br><span class="line">LABEL NUM=$&#123;GITHUB_RUN_NUMBER&#125;</span><br><span class="line">COPY * /</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>github action 上利用 buildx 构建存储这个镜像，用 LABEL 指定文件路径名，直接 copy 出来：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker pull zhangguanzhang/r2s:$&#123;VER&#125;</span><br><span class="line">CTR_PATH=$( docker inspect zhangguanzhang/r2s:$&#123;VER&#125; --format &#x27;&#123;&#123; .Config.Labels &#125;&#125;&#x27; | grep -Eo &#x27;openwrt-.+img.gz&#x27; )</span><br><span class="line">docker create --name update zhangguanzhang/r2s:$&#123;VER&#125;</span><br><span class="line">docker cp update:/$&#123;CTR_PATH&#125; $&#123;USER_FILE&#125;</span><br><span class="line">docker rm update</span><br><span class="line">docker rmi zhangguanzhang/r2s:$&#123;VER&#125;</span><br></pre></td></tr></table></figure><h3 id="扩容和升级"><a href="#扩容和升级" class="headerlink" title="扩容和升级"></a>扩容和升级</h3><p>固件分为两个文件系统，SquashFS 和 Ext4。</p><p>SquashFS（推荐）：固件文件名带有 “squashfs”，SquashFS 为只读文件系统，支持系统还原（支持物理 Reset按钮 还原），支持后台固件升级，更能避免 SD 卡文件系统触发写保护，适合绝大部分用户使用。</p><p>Ext4：固件文件名带有 “ext4”，Ext4 文件系统具备整个分区可读写性质，更适合熟悉 Linux 系统的用户使用，但意外断电有几率造成分区写入保护。</p><h4 id="ext4"><a href="#ext4" class="headerlink" title="ext4"></a>ext4</h4><p>前面两个章节是下载和存放固件 img.gz ，现在开始扩容和升级，扩容就是利用 truncate 下固件文件的大小，然后修复固件文件里的第二个分区。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 因为最终会把修改后的固件写入到根分区所在的块设备，所以固件需要存放在 /tmp 目录下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 大小和内存挂钩，所以不要size太大</span></span><br><span class="line">mount -t tmpfs -o remount,size=870m tmpfs /tmp</span><br><span class="line"><span class="meta">#</span><span class="bash"> 后面的 <span class="literal">true</span> 是因为 github action 的打包会影响解压，虽然最终报错，但是解压的固件还是能用的</span></span><br><span class="line">gzip -dc openwrt.img.gz &gt; /tmp/update/openwrt.img || true</span><br><span class="line"></span><br><span class="line">block_device=&#x27;mmcblk0&#x27;</span><br><span class="line">[ ! -d /sys/block/$block_device ] &amp;&amp; block_device=&#x27;mmcblk1&#x27;</span><br><span class="line">bs=`expr $(cat /sys/block/$block_device/size) \* 512`</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件大小</span></span><br><span class="line">truncate -s $bs /tmp/update/openwrt.img</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改第二个分区大小，1988用的是 <span class="built_in">echo</span> <span class="string">&quot;, +&quot;</span> | sfdisk -N 2 /tmp/update/openwrt.img 可读性不好</span></span><br><span class="line">parted /tmp/update/openwrt.img resizepart 2 100%</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将镜像文件虚拟成块设备，类似于 windows 的那种双击 iso 后的装载 iso ，对于块设备的操作都会时刻写入到 img 文件里</span></span><br><span class="line">lodev=$(losetup -f)</span><br><span class="line">losetup -P $lodev /tmp/update/openwrt.img</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 挂载 rootfs 解压备份文件</span></span><br><span class="line">mkdir -p /mnt/img</span><br><span class="line">mount -t ext4 $&#123;lodev&#125;p2 /mnt/img</span><br><span class="line"><span class="meta">#</span><span class="bash"> op 的备份命令</span></span><br><span class="line">sysupgrade -b back.tar.gz</span><br><span class="line">tar zxf back.tar.gz -C /mnt/img</span><br><span class="line">    if ! grep -q macaddr /etc/config/network; then</span><br><span class="line">        warning &#x27;注意：由于已知的问题，“网络接口”配置无法继承，重启后需要重新设置WAN拨号和LAN网段信息&#x27;</span><br><span class="line">        rm /mnt/img/etc/config/network;</span><br><span class="line">    fi</span><br><span class="line">mountpoint -q  /mnt/img &amp;&amp; umount /mnt/img</span><br><span class="line"><span class="meta">#</span><span class="bash"> openwrt 存在 auto mount，此处取消挂载</span></span><br><span class="line">grep -q $&#123;lodev&#125;p1 /proc/mounts &amp;&amp; umount $&#123;lodev&#125;p1</span><br><span class="line">grep -q $&#123;lodev&#125;p2 /proc/mounts &amp;&amp; umount $&#123;lodev&#125;p2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修复固件里扩容的分区</span></span><br><span class="line">e2fsck -yf $&#123;lodev&#125;p2 || true</span><br><span class="line">resize2fs $&#123;lodev&#125;p2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 取消 img 文件的挂载</span></span><br><span class="line">losetup -d $lodev</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /proc/sys/kernel/sysrq</span><br><span class="line">echo u &gt; /proc/sysrq-trigger &amp;&amp; umount / || true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这个 ddnz 命令从他那里复制的</span></span><br><span class="line">/tmp/ddnz /tmp/update/openwrt.img /dev/$block_device</span><br><span class="line">printf &#x27;%b\n&#x27; &quot;\033[1;32m[SUCCESS] 刷机完毕，正在重启...\033[0m&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重启</span></span><br><span class="line">echo b &gt; /proc/sysrq-trigger</span><br></pre></td></tr></table></figure><h4 id="squashfs"><a href="#squashfs" class="headerlink" title="squashfs"></a>squashfs</h4><p>openwrt 的另一种文件系统固件，就是一个只可读写的压缩的 rootfs 解压开作为 <code>overlay</code> 的 lower dir 只读，提供给用户的是 overlay 的 upper dir 去写入，长按设备上的 reset 按钮恢复出厂设置就是把 overlay 的上层丢弃掉，所以 squashfs 类型的固件带快照功能。当然市面上搜了下也没找到 squashfs 类型的固件在升级的时候扩容的步骤，自己研究了下搞出来了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 挂载 rootfs 解压备份文件</span></span><br><span class="line">mkdir -p /mnt/img</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里会报错 wrong fs <span class="built_in">type</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mount -t ext4 <span class="variable">$&#123;lodev&#125;</span>p2 /mnt/img</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 被我改成这样</span></span><br><span class="line">mount $&#123;lodev&#125;p2 /mnt/img</span><br><span class="line">IMG_FSTYPE=$(df -T /mnt/img | awk &#x27;NR==2&#123;print $2&#125;&#x27;)</span><br></pre></td></tr></table></figure><p>取到了 <code>IMG_FSTYPE</code> 后走不同的逻辑，这里它的值是 <code>squashfs</code> ，而挂载后的 <code>/mnt/img</code> 是无法写入任何文件的。然后搜了下 <code>squashfs</code> 相关，自己折腾的话需要 <code>mksquashfs</code> 和 <code>unsquashfs</code> 的两个命令玩。一开始是尝试解压 <code>$&#123;lodev&#125;p2</code> ，结果经常 oom ，去找 squashfs-tools 源码作者询问如何限制内存得到下面信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># https://github.com/plougher/squashfs-tools/issues/139#issuecomment-991779738</span><br><span class="line">unsquashfs -da 10 -fr 10 $&#123;lodev&#125;p2</span><br></pre></td></tr></table></figure><p>基本一直卡着，毕竟最后肯定要重新 <code>mksquashfs</code> 打包的，然后直接 cp 得了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;$IMG_FSTYPE&quot; = &#x27;squashfs&#x27; ];then</span><br><span class="line">    info &quot;检测到使用 squashfs 固件，开始导出文件系统&quot;</span><br><span class="line">    # https://github.com/plougher/squashfs-tools/issues/139#issuecomment-991779738</span><br><span class="line">    # unsquashfs -da 10 -fr 10 /dev/loop0p2</span><br><span class="line">    # 这个解压太耗时了，只能拷贝整了</span><br><span class="line">    mkdir -p /mnt/img_sq</span><br><span class="line">    cp -a /mnt/img/* /mnt/img_sq</span><br><span class="line">    umount /mnt/img/</span><br><span class="line">    rm -rf /mnt/img</span><br><span class="line">    mv /mnt/img_sq /mnt/img</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>然后这个目录写入备份文件，然后就打包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mksquashfs /mnt/img /opt/op.squashfs</span><br></pre></td></tr></table></figure><p>结果打包也经常 oom ，看了下命令的帮助，发现有内存限制的，加上也偶尔 oom</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mksquashfs /mnt/img /opt/op.squashfs -mem 20M </span><br></pre></td></tr></table></figure><p>最后逼我用 <code>oom_score_adj</code> 调整 oom 优先级了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo -998 &gt; /proc/$$/oom_score_adj 2&gt;/dev/null || true</span><br></pre></td></tr></table></figure><p>当然，实际打包很多选项的，可以先利用 <code>unsquashfs</code> 看下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">unsquashfs -s $&#123;lodev&#125;p2 &gt; squashfs.info</span><br><span class="line"></span><br><span class="line">comp=$(awk &#x27;$1==&quot;Compression&quot;&#123;print $2&#125;&#x27; squashfs.info)</span><br><span class="line">sq_block_size=$(awk &#x27;$1==&quot;Block&quot;&#123;print $NF&#125;&#x27; squashfs.info)</span><br><span class="line">xattrs=&#x27;-xattrs&#x27; # CONFIG_SELINUX=y</span><br><span class="line">grep -Eq &#x27;Xattrs.+?not&#x27; squashfs.info &amp;&amp; xattrs=&#x27;-no-xattrs&#x27;</span><br><span class="line"></span><br><span class="line">echo -998 &gt; /proc/$$/oom_score_adj 2&gt;/dev/null || true</span><br><span class="line"></span><br><span class="line">mksquashfs /mnt/img /opt/op.squashfs -comp $&#123;comp&#125; \</span><br><span class="line">    -b $[sq_block_size/1024]k $xattrs -mem 20M</span><br></pre></td></tr></table></figure><p>然后写入到块设备上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd if=/opt/op.squashfs of=$&#123;lodev&#125;p2</span><br></pre></td></tr></table></figure><p>然后卸载 ${lodev} 刷入固件发现无法开机，在 lede 的源码里 find grep 后找到了 mksquashfs 参数来源于源码下 <code>./include/image.mk</code> 的 <code>SQUASHFSOPT</code> 和 <code>define Image/mkfs/squashfs-common</code></p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CONFIG_TARGET_SQUASHFS_BLOCK_SIZE=1024k</span><br><span class="line">SQUASHFS_BLOCKSIZE := <span class="variable">$(CONFIG_TARGET_SQUASHFS_BLOCK_SIZE)</span>k</span><br><span class="line">SQUASHFSOPT := -b <span class="variable">$(SQUASHFS_BLOCKSIZE)</span></span><br><span class="line">SQUASHFSOPT += -p &#x27;/dev d 755 0 0&#x27; -p &#x27;/dev/console c 600 0 0 5 1&#x27;</span><br><span class="line">SQUASHFSOPT += <span class="variable">$(<span class="built_in">if</span> <span class="variable">$(CONFIG_SELINUX)</span>,-xattrs,-no-xattrs)</span></span><br><span class="line">SQUASHFSCOMP := gzip</span><br><span class="line">LZMA_XZ_OPTIONS := -Xpreset 9 -Xe -Xlc 0 -Xlp 2 -Xpb 2</span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(CONFIG_SQUASHFS_XZ)</span>,y)</span><br><span class="line">  <span class="keyword">ifneq</span> (<span class="variable">$(<span class="built_in">filter</span> arm x86 powerpc sparc,<span class="variable">$(LINUX_KARCH)</span>)</span>,)</span><br><span class="line">    BCJ_FILTER:=-Xbcj <span class="variable">$(LINUX_KARCH)</span>   <span class="comment"># 例如此处  -Xbcj x86</span></span><br><span class="line">  <span class="keyword">endif</span></span><br><span class="line">  SQUASHFSCOMP := xz <span class="variable">$(LZMA_XZ_OPTIONS)</span> <span class="variable">$(BCJ_FILTER)</span></span><br><span class="line"><span class="keyword">endif</span></span><br></pre></td></tr></table></figure><p>本地搞个编译 openwrt 的时候 make 带上 <code>-V=s</code> 开详细信息看到下面信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> /home/guanzhang/lede/staging_dir/host/bin/mksquashfs4 /home/guanzhang/lede/build_dir/target-aarch64_generic_musl/root-rockchip /home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/root.squashfs -nopad -noappend -root-owned -comp xz -Xpreset 9 -Xe -Xlc 0 -Xlp 2 -Xpb 2  -b 1024k -p <span class="string">&#x27;/dev d 755 0 0&#x27;</span> -p <span class="string">&#x27;/dev/console c 600 0 0 5 1&#x27;</span> -no-xattrs -processors 6</span></span><br><span class="line">Pseudo file &quot;/dev&quot; exists in source filesystem &quot;/home/guanzhang/lede/build_dir/target-aarch64_generic_musl/root-rockchip/dev&quot;.</span><br><span class="line">Ignoring, exclude it (-e/-ef) to override.</span><br><span class="line">Parallel mksquashfs: Using 6 processors</span><br><span class="line">Creating 4.0 filesystem on /home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/root.squashfs, block size 1048576.</span><br><span class="line">[=============================================================-] 8430/8430 100%</span><br><span class="line"></span><br><span class="line">Exportable Squashfs 4.0 filesystem, xz compressed, data block size 1048576</span><br><span class="line">compressed data, compressed metadata, compressed fragments,</span><br><span class="line">no xattrs, compressed ids</span><br><span class="line">duplicates are removed</span><br><span class="line">Filesystem size 135525.99 Kbytes (132.35 Mbytes)</span><br><span class="line">25.39% of uncompressed filesystem size (533693.75 Kbytes)</span><br><span class="line">Inode table size 60908 bytes (59.48 Kbytes)</span><br><span class="line">20.00% of uncompressed inode table size (304503 bytes)</span><br><span class="line">Directory table size 85796 bytes (83.79 Kbytes)</span><br><span class="line">38.42% of uncompressed directory table size (223339 bytes)</span><br><span class="line">Number of duplicate files found 1164</span><br><span class="line">Number of inodes 9212</span><br><span class="line">Number of files 8077</span><br><span class="line">Number of fragments 123</span><br><span class="line">Number of symbolic links  647</span><br><span class="line">Number of device nodes 1</span><br><span class="line">Number of fifo nodes 0</span><br><span class="line">Number of socket nodes 0</span><br><span class="line">Number of directories 487</span><br><span class="line">Number of ids (unique uids + gids) 1</span><br><span class="line">Number of uids 1</span><br><span class="line">root (0)</span><br><span class="line">Number of gids 1</span><br><span class="line">root (0)</span><br></pre></td></tr></table></figure><p>大致参数就是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-nopad -noappend -root-owned -comp xz -Xpreset 9 -Xe -Xlc 0 -Xlp 2 -Xpb 2  -b 1024k -p &#x27;/dev d 755 0 0&#x27; -p &#x27;/dev/console c 600 0 0 5 1&#x27; -no-xattrs -processors 6</span><br></pre></td></tr></table></figure><p>但是 openwrt 和 Centos 上安装的 <code>squashfs-tools</code> 的 mksquashfs xz 压缩时候都没有 <code>-Xpreset 9 -Xe -Xlc 0 -Xlp 2 -Xpb 2</code> 这些参数，后面发现了是 openwrt 编译的时候下载 squashfs-tools 后打了 patch 编译后才有的，不过后面测试这几个选项不影响。同时看到了打包的脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PADDING=1 /home/guanzhang/lede/scripts/gen_image_generic.sh /home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/tmp/openwrt-rockchip-armv8-friendlyarm_nanopi-r2s-squashfs-sysupgrade.img.gz 18 /home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/tmp/openwrt-rockchip-armv8-friendlyarm_nanopi-r2s-squashfs-sysupgrade.img.gz.boot 635 /home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/root.squashfs 32768</span><br></pre></td></tr></table></figure><p>得到了参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">+ dd if=/dev/zero of=/home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/tmp/openwrt-rockchip-armv8-friendlyarm_nanopi-r2s-squashfs-sysupgrade.img.gz bs=512 seek=131072 conv=notrunc count=1300480</span><br><span class="line">1300480+0 records in</span><br><span class="line">1300480+0 records out</span><br><span class="line">665845760 bytes (666 MB, 635 MiB) copied, 2.12896 s, 313 MB/s</span><br><span class="line">+ dd if=/home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/root.squashfs of=/home/guanzhang/lede/build_dir/target-aarch64_generic_musl/linux-rockchip_armv8/tmp/openwrt-rockchip-armv8-friendlyarm_nanopi-r2s-squashfs-sysupgrade.img.gz bs=512 seek=131072 conv=notrunc</span><br></pre></td></tr></table></figure><p>可以看到是直接写 img 文件的，这里虽然显示的是 img.gz ，但是如果压缩后的文件的话，那 seek 大小就不对。实际上后面才压缩的，所以我的参数为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">part2_seek=$(parted /tmp/update/openwrt.img u s p | awk &#x27;$1==2&#123;print +$2&#125;&#x27;)</span><br><span class="line">mksquashfs /mnt/img /opt/op.squashfs -nopad -noappend -root-owned \</span><br><span class="line">    -comp $&#123;comp&#125; $&#123;LZMA_XZ_OPTIONS&#125; \</span><br><span class="line">    -b $[sq_block_size/1024]k \</span><br><span class="line">    -p &#x27;/dev d 755 0 0&#x27; -p &#x27;/dev/console c 600 0 0 5 1&#x27; \</span><br><span class="line">    $xattrs -mem 20M </span><br><span class="line"></span><br><span class="line">losetup -l -O NAME -n | grep -Eqw $lodev &amp;&amp; losetup -d $lodev</span><br><span class="line">dd if=/opt/op.squashfs of=/tmp/update/openwrt.img bs=512 seek=$&#123;part2_seek&#125; conv=notrunc</span><br></pre></td></tr></table></figure><p>然后写入即可</p><h2 id="最终参考"><a href="#最终参考" class="headerlink" title="最终参考"></a>最终参考</h2><p>我的脚本当前存放在 test 分支，可能后续切到 main 分支：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">https://github.com/zhangguanzhang/Actions-OpenWrt/blob/test/build/scripts/update.sh</span><br><span class="line"></span><br><span class="line">https://github.com/zhangguanzhang/Actions-OpenWrt/blob/main/build/scripts/update.sh</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/klever1988/nanopi-openwrt/raw/master/scripts/autoupdate.sh">1988 的升级脚本</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;手上有 r2s、N1 和 x86_64 的固件维护，r2s 的参照别人的脚本搞了在线升级固件的脚本，别人的脚本只支持 ext4 升级，而后面</summary>
      
    
    
    
    
    <category term="openwrt" scheme="http://zhangguanzhang.github.io/tags/openwrt/"/>
    
    <category term="squashfs" scheme="http://zhangguanzhang.github.io/tags/squashfs/"/>
    
  </entry>
  
  <entry>
    <title>docker数据盘损坏后启动报错 Error starting daemon: Error initializing network controller: Error creating default &quot;bridge&quot; network: package not installed</title>
    <link href="http://zhangguanzhang.github.io/2021/12/12/mod-rejected-by-service/"/>
    <id>http://zhangguanzhang.github.io/2021/12/12/mod-rejected-by-service/</id>
    <published>2021-12-12T23:28:30.000Z</published>
    <updated>2021-12-12T23:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>客户现场的数据盘损坏了，修复启动机器后 docker 无法启动</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@db1 docker]# /data/kube/bin/dockerd</span><br><span class="line">WARN[0000] The &quot;graph&quot; config file option is deprecated. Please use &quot;data-root&quot; instead. </span><br><span class="line">WARN[2021-12-11T21:16:07.917969366+08:00] could not change group /var/run/docker.sock to docker: group docker not found </span><br><span class="line">WARN[2021-12-11T21:16:07.942745757+08:00] failed to load plugin io.containerd.snapshotter.v1.btrfs  error=&quot;path /data/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">WARN[2021-12-11T21:16:07.944020734+08:00] failed to load plugin io.containerd.snapshotter.v1.aufs  error=&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">WARN[2021-12-11T21:16:07.944275670+08:00] failed to load plugin io.containerd.snapshotter.v1.zfs  error=&quot;path /data/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">WARN[2021-12-11T21:16:07.944314186+08:00] could not use snapshotter btrfs in metadata plugin  error=&quot;path /data/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">WARN[2021-12-11T21:16:07.944324941+08:00] could not use snapshotter aufs in metadata plugin  error=&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">WARN[2021-12-11T21:16:07.944333098+08:00] could not use snapshotter zfs in metadata plugin  error=&quot;path /data/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">WARN[2021-12-11T21:16:09.131994686+08:00] Running modprobe bridge br_netfilter failed with message: modprobe: ERROR: could not insert &#x27;bridge&#x27;: Key was rejected by service</span><br><span class="line">modprobe: ERROR: could not insert &#x27;br_netfilter&#x27;: Key was rejected by service</span><br><span class="line">insmod /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko </span><br><span class="line">insmod /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko </span><br><span class="line">, error: exit status 1 </span><br><span class="line">Error starting daemon: Error initializing network controller: Error creating default &quot;bridge&quot; network: package not installed</span><br></pre></td></tr></table></figure><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><h3 id="相关信息"><a href="#相关信息" class="headerlink" title="相关信息"></a>相关信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ dockerd --version</span><br><span class="line">Docker version 18.09.3, build 774a1f4</span><br><span class="line">$ uname -a </span><br><span class="line">Linux db1 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">$ cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.3.1611 (Core) </span><br></pre></td></tr></table></figure><h3 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h3><p>先看下是不是把内核模块禁止了，发现没禁止，手动加载也报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ grep -r black /etc/modprobe.d/*.conf</span><br><span class="line">$ modprobe overlay</span><br><span class="line">$ modprobe bridge</span><br><span class="line">modprobe: ERROR: could not insert &#x27;bridge&#x27;: Key was rejected by service</span><br></pre></td></tr></table></figure><p>查看下也没开启 <code>enforcemodulesig</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ dmesg | grep enforcemodulesig=1</span><br><span class="line">$ cat /proc/cmdline </span><br><span class="line">BOOT_IMAGE=/vmlinuz-3.10.0-514.el7.x86_64 root=UUID=5ab681a0-7e5c-4ab7-9c88-27d788f725b3 ro crashkernel=auto rhgb quiet LANG=en_US.UTF-8</span><br></pre></td></tr></table></figure><p>但是能查看到内核模块信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ modinfo bridge</span><br><span class="line">filename:       /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko</span><br><span class="line">alias:          rtnl-link-bridge</span><br><span class="line">version:        2.3</span><br><span class="line">license:        GPL</span><br><span class="line">rhelversion:    7.3</span><br><span class="line">srcversion:     FF0448CD85C271287DE1963</span><br><span class="line">depends:        stp,llc</span><br><span class="line">intree:         Y</span><br><span class="line">vermagic:       3.10.0-514.el7.x86_64 SMP mod_unload modversions </span><br><span class="line">signer:         CentOS Linux kernel signing key</span><br><span class="line">sig_key:        D4:88:63:A7:C1:6F:CC:27:41:23:E6:29:8F:74:F0:57:AF:19:FC:54</span><br><span class="line">sig_hashalgo:   sha256</span><br></pre></td></tr></table></figure><p>感觉是内核签名对不上，查看下模块哈希</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ md5sum /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko</span><br><span class="line">62001928100a30bace9bc6493b956e2f  /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko</span><br></pre></td></tr></table></figure><p>找了另一台机器对比下，发现模块损坏了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux db2 3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">$ modinfo bridge</span><br><span class="line">filename:       /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko</span><br><span class="line">alias:          rtnl-link-bridge</span><br><span class="line">version:        2.3</span><br><span class="line">license:        GPL</span><br><span class="line">rhelversion:    7.3</span><br><span class="line">srcversion:     FF0448CD85C271287DE1963</span><br><span class="line">depends:        stp,llc</span><br><span class="line">intree:         Y</span><br><span class="line">vermagic:       3.10.0-514.el7.x86_64 SMP mod_unload modversions </span><br><span class="line">signer:         CentOS Linux kernel signing key</span><br><span class="line">sig_key:        D4:88:63:A7:C1:6F:CC:27:41:23:E6:29:8F:74:F0:57:AF:19:FC:54</span><br><span class="line">sig_hashalgo:   sha256</span><br><span class="line">$ md5sum /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko</span><br><span class="line">41c62afa67e66d107cc2a9e471910726  /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko</span><br></pre></td></tr></table></figure><p>修复</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/</span><br><span class="line">cp bridge.ko bridge.ko.bak</span><br><span class="line">scp root@xxxx:/lib/modules/3.10.0-514.el7.x86_64/kernel/net/bridge/bridge.ko .</span><br></pre></td></tr></table></figure><p>然后能启动了</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.oracle.com/technical-resources/articles/linux/signed-kernel-modules.html">https://www.oracle.com/technical-resources/articles/linux/signed-kernel-modules.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;客户现场的数据盘损坏了，修复启动机器后 docker 无法启动&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>1.15 kubelet 在 nodefs 容量富裕下循环 reclaim ephemeral-storage</title>
    <link href="http://zhangguanzhang.github.io/2021/10/29/kubelet-ephemeral-storage-loop-evicted/"/>
    <id>http://zhangguanzhang.github.io/2021/10/29/kubelet-ephemeral-storage-loop-evicted/</id>
    <published>2021-10-29T14:08:06.000Z</published>
    <updated>2021-10-29T14:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障"><a href="#故障" class="headerlink" title="故障"></a>故障</h2><p>现场 k8s node 很多 pod 都被硬性驱逐显示 <code>Evicted</code> ，现场人员查看分区容量和 inode 都正常，但是一直 <code>reclaim ephemeral-storage</code>。</p><h2 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux xxx-2 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">$ cat /etc/os-release</span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line">$ kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">$ docker info</span><br><span class="line">Containers: 5</span><br><span class="line"> Running: 4</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 1</span><br><span class="line">Images: 40</span><br><span class="line">Server Version: 18.09.3</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: e6b3f5632f50dbc4e9cb6288d911bf4f5e95b18e</span><br><span class="line">runc version: 6635b4f0c6af3810594d2770f662f34ddc15b40d</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 3.10.0-693.el7.x86_64</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 32</span><br><span class="line">Total Memory: 62.91GiB</span><br><span class="line">Name: SCJY-2</span><br><span class="line">ID: XZ33:PHUQ:U2CI:7PXH:SYFG:Y6LK:3K3U:XXM6:QJWP:U3B3:MW4M:XPJS</span><br><span class="line">Docker Root Dir: /data/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><p>向日葵远程上去看了下，根分区容量都是正常的，inode 也是。看了下 <code>uptime -s</code> 重启过，现场说重启过还是没用。重启 kubelet 的话，看了下还是一直 <code>reclaim ephemeral-storage</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">$ du -h</span><br><span class="line">Filesystem                 Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/rootvg-lvroot   30G  5.2G   25G  18% /</span><br><span class="line">devtmpfs                    32G     0   32G   0% /dev</span><br><span class="line">tmpfs                       32G  160K   32G   1% /dev/shm</span><br><span class="line">tmpfs                       32G   26M   32G   1% /run</span><br><span class="line">tmpfs                       32G     0   32G   0% /sys/fs/cgroup</span><br><span class="line">/dev/sdb                   600G   36G  565G   6% /data</span><br><span class="line">/dev/sda1                 1014M  160M  855M  16% /boot</span><br><span class="line">/dev/mapper/rootvg-lvopt    10G   33M   10G   1% /opt</span><br><span class="line">/dev/mapper/rootvg-lvhome 1014M   39M  976M   4% /home</span><br><span class="line">/dev/mapper/rootvg-lvvar   2.0G  1.2G  888M  57% /var</span><br><span class="line">overlay                    600G   36G  565G   6% /data/kube/docker/overlay2/788ee4620da0a3f76ef5f4b24755a68de0e66c8f2425d8332d5a792116d7659f/merged</span><br><span class="line">overlay                    600G   36G  565G   6% /data/kube/docker/overlay2/d2b5f08e9873f5c9365aaf57eeca492734631a3842ccb2f379aa89998b0c7304/merged</span><br><span class="line">overlay                    600G   36G  565G   6% /data/kube/docker/overlay2/c4793b6c3f774cc960ef23e18b61405040698be698306ee993d4d501bdcf485a/merged</span><br><span class="line">overlay                    600G   36G  565G   6% /data/kube/docker/overlay2/b5a0fc544935db77c92bd978db9c1c7018e5e09bba9d2bf53bd300e96c656cec/merged</span><br><span class="line">shm                         64M     0   64M   0% /data/kube/docker/containers/ad86ab9b01e1ce0d62e1f98249274d9bfe75eca6efd8ce0e8f1c591d5570d75f/mounts/shm</span><br><span class="line">shm                         64M     0   64M   0% /data/kube/docker/containers/e3ebeac9a82264869429f44ea6834bcbc94b79013621490c071ef002b4b8e90e/mounts/shm</span><br><span class="line">shm                         64M     0   64M   0% /data/kube/docker/containers/a917bd3b8006198a58900efb5c82c6e162cfc4e732c7e588eaadfb59294ea22b/mounts/shm</span><br><span class="line">shm                         64M     0   64M   0% /data/kube/docker/containers/aa52df1894ad495f4f269d77ddd90954fdc7bbd0fbf25d9d4aa0674a76ff6a6c/mounts/shm</span><br><span class="line">tmpfs                      6.3G   12K  6.3G   1% /run/user/42</span><br><span class="line">tmpfs                      6.3G     0  6.3G   0% /run/user/1003</span><br><span class="line">tmpfs                      6.3G     0  6.3G   0% /run/user/1000</span><br><span class="line"></span><br><span class="line">$ dh -i</span><br><span class="line">Filesystem                   Inodes  IUsed     IFree IUse% Mounted on</span><br><span class="line">/dev/mapper/rootvg-lvroot  15726592 184878  15541714    2% /</span><br><span class="line">devtmpfs                    8242230    527   8241703    1% /dev</span><br><span class="line">tmpfs                       8246150     41   8246109    1% /dev/shm</span><br><span class="line">tmpfs                       8246150    735   8245415    1% /run</span><br><span class="line">tmpfs                       8246150     16   8246134    1% /sys/fs/cgroup</span><br><span class="line">/dev/sdb                  314572800 303816 314268984    1% /data</span><br><span class="line">/dev/sda1                    524288    327    523961    1% /boot</span><br><span class="line">/dev/mapper/rootvg-lvopt    5242880      7   5242873    1% /opt</span><br><span class="line">/dev/mapper/rootvg-lvhome    524288    397    523891    1% /home</span><br><span class="line">/dev/mapper/rootvg-lvvar    1048576  10179   1038397    1% /var</span><br><span class="line">overlay                   314572800 303816 314268984    1% /data/kube/docker/overlay2/788ee4620da0a3f76ef5f4b24755a68de0e66c8f2425d8332d5a792116d7659f/merged</span><br><span class="line">overlay                   314572800 303816 314268984    1% /data/kube/docker/overlay2/d2b5f08e9873f5c9365aaf57eeca492734631a3842ccb2f379aa89998b0c7304/merged</span><br><span class="line">overlay                   314572800 303816 314268984    1% /data/kube/docker/overlay2/c4793b6c3f774cc960ef23e18b61405040698be698306ee993d4d501bdcf485a/merged</span><br><span class="line">overlay                   314572800 303816 314268984    1% /data/kube/docker/overlay2/b5a0fc544935db77c92bd978db9c1c7018e5e09bba9d2bf53bd300e96c656cec/merged</span><br><span class="line">shm                         8246150      1   8246149    1% /data/kube/docker/containers/ad86ab9b01e1ce0d62e1f98249274d9bfe75eca6efd8ce0e8f1c591d5570d75f/mounts/shm</span><br><span class="line">shm                         8246150      1   8246149    1% /data/kube/docker/containers/e3ebeac9a82264869429f44ea6834bcbc94b79013621490c071ef002b4b8e90e/mounts/shm</span><br><span class="line">shm                         8246150      1   8246149    1% /data/kube/docker/containers/a917bd3b8006198a58900efb5c82c6e162cfc4e732c7e588eaadfb59294ea22b/mounts/shm</span><br><span class="line">shm                         8246150      1   8246149    1% /data/kube/docker/containers/aa52df1894ad495f4f269d77ddd90954fdc7bbd0fbf25d9d4aa0674a76ff6a6c/mounts/shm</span><br><span class="line">tmpfs                       8246150      9   8246141    1% /run/user/42</span><br><span class="line">tmpfs                       8246150      1   8246149    1% /run/user/1003</span><br><span class="line">tmpfs                       8246150      1   8246149    1% /run/user/1000</span><br><span class="line"></span><br><span class="line">$ kubectl describe node xx.xx.112.135</span><br><span class="line">...</span><br><span class="line">Capacity:</span><br><span class="line"> cpu:                32</span><br><span class="line"> ephemeral-storage:  2038Mi</span><br><span class="line"> hugepages-2Mi:      0</span><br><span class="line"> memory:             65969200Ki</span><br><span class="line"> pods:               110</span><br><span class="line">Allocatable:</span><br><span class="line"> cpu:                31800m</span><br><span class="line"> ephemeral-storage:  1014Mi</span><br><span class="line"> hugepages-2Mi:      0</span><br><span class="line"> memory:             65469200Ki</span><br><span class="line"> pods:               110</span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                   Age                      From                    Message</span><br><span class="line">  ----     ------                   ----                     ----                    -------</span><br><span class="line">  Warning  EvictionThresholdMet     3m57s (x1434 over 4h3m)  kubelet, xx.xx.112.135  Attempting to reclaim ephemeral-storage</span><br><span class="line">  Normal   Starting                 37s                      kubelet, xx.xx.112.135  Starting kubelet.</span><br><span class="line">  Normal   NodeHasSufficientMemory  37s                      kubelet, xx.xx.112.135  Node xx.xx.112.135 status is now: NodeHasSufficientMemory</span><br><span class="line">  Normal   NodeHasNoDiskPressure    37s (x2 over 37s)        kubelet, xx.xx.112.135  Node xx.xx.112.135 status is now: NodeHasNoDiskPressure</span><br><span class="line">  Normal   NodeHasSufficientPID     37s                      kubelet, xx.xx.112.135  Node xx.xx.112.135 status is now: NodeHasSufficientPID</span><br><span class="line">  Normal   NodeNotReady             37s                      kubelet, xx.xx.112.135  Node xx.xx.112.135 status is now: NodeNotReady</span><br><span class="line">  Normal   NodeAllocatableEnforced  37s                      kubelet, xx.xx.112.135  Updated Node Allocatable <span class="built_in">limit</span> across pods</span><br><span class="line">  Normal   NodeReady                37s                      kubelet, xx.xx.112.135  Node xx.xx.112.135 status is now: NodeReady</span><br><span class="line">  Normal   NodeHasDiskPressure      27s                      kubelet, xx.xx.112.135  Node xx.xx.112.135 status is now: NodeHasDiskPressure</span><br><span class="line">  Warning  EvictionThresholdMet     7s (x4 over 37s)         kubelet, xx.xx.112.135  Attempting to reclaim ephemeral-storage</span><br></pre></td></tr></table></figure><p>看了一会儿后发现上面的 <code>ephemeral-storage</code> 不对，<code>Capacity</code> 居然是 <code>2038Mi</code> 。</p><h3 id="源码的一些探索"><a href="#源码的一些探索" class="headerlink" title="源码的一些探索"></a>源码的一些探索</h3><p>本地开发环境起了下 kubelet 调试了下，一些信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">./build/run.sh make kubelet GOFLAGS=&quot;-v -tags=nokmem&quot; GOGCFLAGS=&quot;all=-N -l&quot;  KUBE_BUILD_PLATFORMS=linux/amd64</span><br><span class="line"></span><br><span class="line">cp  _output/dockerized/bin/linux/amd64/kubelet .</span><br><span class="line"></span><br><span class="line">dlv exec --check-go-version=false ./kubelet  -- --cgroup-driver=systemd</span><br><span class="line"></span><br><span class="line"># 推荐下面两个断点</span><br><span class="line">vendor/github.com/google/cadvisor/container/docker/handler.go#L421</span><br><span class="line"></span><br><span class="line">vendor/github.com/google/cadvisor/container/docker/handler.go:364</span><br><span class="line"></span><br><span class="line">   724:func (self *manager) GetFsInfo(label string) ([]v2.FsInfo, error) &#123;</span><br><span class="line">=&gt; 725:var empty time.Time</span><br><span class="line">   726:// Get latest data from filesystems hanging off root container.</span><br><span class="line">   727:stats, err := self.memoryCache.RecentStats(&quot;/&quot;, empty, empty, 1)</span><br><span class="line">   728:if err != nil &#123;</span><br><span class="line">   729:return nil, err</span><br><span class="line">   730:&#125;</span><br><span class="line">(dlv) so</span><br><span class="line">&gt; k8s.io/kubernetes/vendor/github.com/google/cadvisor/manager.(*manager).getFsInfoByDeviceName() _output/dockerized/go/src/k8s.io/kubernetes/vendor/github.com/google/cadvisor/manager/manager.go:1311 (PC: 0x1fc7180)</span><br><span class="line">Values returned:</span><br><span class="line">~r1: []k8s.io/kubernetes/vendor/github.com/google/cadvisor/info/v2.FsInfo len: 2, cap: 2, [</span><br><span class="line">&#123;</span><br><span class="line">Timestamp: (*time.Time)(0xc0002262d0),</span><br><span class="line">Device: &quot;/dev/sda1&quot;,</span><br><span class="line">Mountpoint: &quot;/&quot;,</span><br><span class="line">Capacity: 75150372864,</span><br><span class="line">Available: 36613033984,</span><br><span class="line">Usage: 38537338880,</span><br><span class="line">Labels: []string len: 2, cap: 2, [</span><br><span class="line">&quot;docker-images&quot;,</span><br><span class="line">&quot;root&quot;,</span><br><span class="line">],</span><br><span class="line">Inodes: *36699584,</span><br><span class="line">InodesFree: *35850609,&#125;,</span><br><span class="line">&#123;</span><br><span class="line">Timestamp: (*time.Time)(0xc000226348),</span><br><span class="line">Device: &quot;tmpfs&quot;,</span><br><span class="line">Mountpoint: &quot;/dev/shm&quot;,</span><br><span class="line">Capacity: 1986203648,</span><br><span class="line">Available: 1986203648,</span><br><span class="line">Usage: 0,</span><br><span class="line">Labels: []string len: 0, cap: 0, [],</span><br><span class="line">Inodes: *484913,</span><br><span class="line">InodesFree: *484912,&#125;,</span><br><span class="line">]</span><br><span class="line">~r2: error nil</span><br></pre></td></tr></table></figure><p>容量这部分我现场通过特性 <code>--feature-gates=LocalStorageCapacityIsolation=false</code> 后删掉 node restart 后 describe 看不到 <code>ephemeral-storage</code> 了，但是还是问题还在，看了下源码，这个容量大小是 <code>vendor/github.com/google/cadvisor/container/docker</code> 下从 docker 获取的，嵌套的 interface 太多了，查看麻烦。现场是已经重启过机器了，docker 我重启和查看日志也没啥有用的地方。</p><h3 id="最终解决"><a href="#最终解决" class="headerlink" title="最终解决"></a>最终解决</h3><p><code>ephemeral-storage</code> 这个 limit 是 1.15 alpha 的，暂时不想折腾了。 尝试换下 kubelet 的 root 目录。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl cat kubelet</span><br><span class="line"># /etc/systemd/system/kubelet.service</span><br><span class="line">[Unit]</span><br><span class="line">...</span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line">ExecStart=/data/kube/bin/kubelet \</span><br><span class="line">  ...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>主要修改 <code>WorkingDirectory</code> 和给 kubelet 增加参数 <code>--root-dir</code> 以及 <code>--docker-root</code> ，现场 <code>/data</code> 是单独分区的，切到 <code>/data/kube/kubelet</code> 下，<code>--docker-root</code> 则是 docker 的 <code>data-root</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/systemd/system/kubelet.service</span><br><span class="line">...</span><br><span class="line">WorkingDirectory=/data/kube/kubelet</span><br><span class="line">ExecStart=/data/kube/bin/kubelet \</span><br><span class="line">  --root-dir=/data/kube/kubelet \</span><br><span class="line">  --docker-root=/data/kube/docker \</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><p>问题解决。后面才发现 /var 是单独分区的，客户现场动过分区表，之前是 /var 没有单独分区，后面他们创建了个 lv 并写在 /etc/fstab 里，并没有挂载和重启。一周前他们重启了下，而且有一些服务在 /var/log 输出日志，所以造成了这次故障。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/kubernetes/kubernetes/tree/v1.15.5/vendor/github.com/google/cadvisor/manager/manager.go#L724">https://github.com/kubernetes/kubernetes/tree/v1.15.5/vendor/github.com/google/cadvisor/manager/manager.go#L724</a></li><li><a href="https://github.com/kubernetes/kubernetes/tree/v1.15.5/vendor/github.com/google/cadvisor/container/libcontainer/handler.go">https://github.com/kubernetes/kubernetes/tree/v1.15.5/vendor/github.com/google/cadvisor/container/libcontainer/handler.go</a></li><li><a href="https://github.com/kubernetes/kubernetes/tree/v1.15.5/vendor/github.com/docker/docker/pkg/mount/mountinfo_linux.go">https://github.com/kubernetes/kubernetes/tree/v1.15.5/vendor/github.com/docker/docker/pkg/mount/mountinfo_linux.go</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;故障&quot;&gt;&lt;a href=&quot;#故障&quot; class=&quot;headerlink&quot; title=&quot;故障&quot;&gt;&lt;/a&gt;故障&lt;/h2&gt;&lt;p&gt;现场 k8s node 很多 pod 都被硬性驱逐显示 &lt;code&gt;Evicted&lt;/code&gt; ，现场人员查看分区容量和 inode 都正</summary>
      
    
    
    
    
    <category term="kubelet" scheme="http://zhangguanzhang.github.io/tags/kubelet/"/>
    
  </entry>
  
  <entry>
    <title>在非容器环境上实现散装的 IPVS SVC</title>
    <link href="http://zhangguanzhang.github.io/2021/09/28/ipvs-svc/"/>
    <id>http://zhangguanzhang.github.io/2021/09/28/ipvs-svc/</id>
    <published>2021-09-28T19:28:30.000Z</published>
    <updated>2021-09-28T19:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>内部有非 K8S 环境上需要类似 SVC 的负载实现，一开始是用 NGINX 做的，所有 SVC 域名都解析成一个 dummy IP ，然后 NGINX 根据 <code>server_name</code> 去 proxy 不同的 upstream 。 开始还是能用的，结果后面很多服务依赖 <code>host</code> 这个 header ，报错签名错误，而且毕竟这样是在用户态，效率不如内核态高。于是打算搞下之前的打算：把 IPVS 的 <code>ClusterIP</code> 的 SVC 扣到非 K8S 环境上使用。</p><p>kube-proxy 的 SVC 简单讲就是 node 上任何进程访问 <code>SVC IP:SVC PORT</code> 会被 dnat 成 <code>endpoint</code> ，是工作在内核态的四层负载，不会在机器上看到端口监听，而默认非集群的机器是无法访问 SVC IP 。在 K8S 里，endpoint 的 ip 无非就是 <code>POD IP</code>，<code>host IP</code>。前者就是 SVC 选中 POD ，后者例如 <code>kubernetes</code> 这个 SVC ，会 DNAT 成每个 <code>kube-apiserver</code> 的 <code>host IP:6443</code> 端口，也可能是 <code>ExternalName</code> 或者手动创建的 endpoint 。既然 <code>kubernetes</code> 这个 SVC 可以。那我的打算应该也是可以实现的。但是一开始实际按照思路试了下发现不行，网上的文章基本都是在单机 docker 或者现有的 K8S 环境上搞的，漏掉了很多精华和核心思想，这里记录下我的思路和实现过程。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>前面说的 SVC 现象是和 <code>kube-proxy</code> 的模式无关的。<code>iptables</code> 模式排查不直观，我更倾向于 <code>IPVS</code> 去搞，它更直观，而且支持更多的调度算法。管理 IPVS 规则的话我们需要安装 <code>ipvsadm</code> ，这里我是两台干净的 CentOS 7.8 来做环境。</p><table><thead><tr><th align="left">IP</th></tr></thead><tbody><tr><td align="left">192.168.2.111</td></tr><tr><td align="left">192.168.2.222</td></tr></tbody></table><p>先安装下基础需要的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ipvsadm curl wget tcpdump ipset conntrack-tools</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启转发</span></span><br><span class="line"></span><br><span class="line">sysctl -w net.ipv4.ip_forward=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认 iptables 规则清空</span></span><br><span class="line">$ iptables -S</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P FORWARD ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">$ iptables -t nat -S</span><br><span class="line">-P PREROUTING ACCEPT</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-P POSTROUTING ACCEPT</span><br></pre></td></tr></table></figure><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>先思考下 kube-proxy 的 IPVS ，因为 SVC 端口和 POD 的端口不一样，所以 kube-proxy 使用的 <code>nat</code> 模式。暂且打算添加一个下面类似的 SVC ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">IP:                169.254.11.2</span><br><span class="line">Port:              https  80/TCP</span><br><span class="line">TargetPort:        8080/TCP</span><br><span class="line">Endpoints:         192.168.2.111:8080,192.168.2.222:8080</span><br><span class="line">Session Affinity:  None</span><br></pre></td></tr></table></figure><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>web 的话我是使用的 golang 的一个简单 web 二进制起的 <a href="https://github.com/m3ng9i/ran">ran</a> :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/m3ng9i/ran/releases/download/v0.1.6/ran_linux_amd64.zip</span><br><span class="line">unzip -x ran_linux_amd64.zip</span><br><span class="line">mkdir www</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两个机器创建不同的 index 文件</span></span><br><span class="line"><span class="built_in">echo</span> 192.168.2.111 &gt; www/<span class="built_in">test</span></span><br><span class="line"><span class="built_in">echo</span> 192.168.2.222 &gt; www/<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">./ran_linux_amd64 -port 8080 -listdir www</span><br></pre></td></tr></table></figure><p>两个机器的这个 web 都起来后我们开个窗口去 <code>192.168.2.111</code> 上继续后面的操作。</p><h3 id="lvs-nat"><a href="#lvs-nat" class="headerlink" title="lvs nat"></a>lvs nat</h3><p>kube-proxy 并没有像 lvs nat 那样有单独的机器做 <code>NAT GW</code>，或者认为每个 node 都是自己的 <code>NAT GW</code>。现在来添加 <code>169.254.11.2:80</code> 这个 SVC ，使用 ipvsadm 添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm --add-service --tcp-service 169.254.11.2:80 --scheduler rr</span><br></pre></td></tr></table></figure><p>先添加本地的 web 作为 real server ，下面含义是添加为一个 nat 类型的 real server ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm --add-server --tcp-service 169.254.11.2:80 \</span><br><span class="line">  --real-server 192.168.2.111:8080 --masquerading --weight 1</span><br></pre></td></tr></table></figure><p>查看下当前列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ipvsadm -ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  169.254.11.2:80 rr</span><br><span class="line">  -&gt; 192.168.2.111:8080           Masq    1      0          0 </span><br></pre></td></tr></table></figure><p>因为是自己的 <code>NAT GW</code>，所以 VIP 配置在自己身上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip addr add 169.254.11.2/32 dev eth0</span><br></pre></td></tr></table></figure><p>测试下访问看看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br></pre></td></tr></table></figure><p>添加上另一个节点的 8080：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ipvsadm --add-server --tcp-service 169.254.11.2:80 \</span><br><span class="line">  --real-server 192.168.2.222:8080 --masquerading --weight 1</span><br><span class="line"></span><br><span class="line">$ ipvsadm -ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  169.254.11.2:80 rr</span><br><span class="line">  -&gt; 192.168.2.111:8080           Masq    1      0          0         </span><br><span class="line">  -&gt; 192.168.2.222:8080           Masq    1      0          0</span><br></pre></td></tr></table></figure><p>测试下访问看看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>发现 curl 在卡住和能访问返回 <code>192.168.2.111</code> 之间切换，没有返回 <code>192.168.2.222</code> 的。查看下 IPVS 的 connection ，发现调度到非本机才会卡住：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ipvsadm -lnc</span><br><span class="line">IPVS connection entries</span><br><span class="line">pro expire state       <span class="built_in">source</span>             virtual            destination</span><br><span class="line">TCP 00:48  SYN_RECV    169.254.11.2:50698 169.254.11.2:80    192.168.2.222:8080</span><br></pre></td></tr></table></figure><p>在 <code>192.168.2.222</code> 上抓包看看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth0 port 8080</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">07:38:26.360716 IP 169.254.11.2.50710 &gt; 192.168.2.222.8080: Flags [S], seq 768065283, win 43690, options [mss 65495,sackOK,TS val 12276183 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:38:26.360762 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676518144 ecr 12276183,nop,wscale 7], length 0</span><br><span class="line">07:38:27.362848 IP 169.254.11.2.50710 &gt; 192.168.2.222.8080: Flags [S], seq 768065283, win 43690, options [mss 65495,sackOK,TS val 12277186 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:38:27.362884 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676519146 ecr 12276183,nop,wscale 7], length 0</span><br><span class="line">07:38:28.562629 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676520346 ecr 12276183,nop,wscale 7], length 0</span><br><span class="line">07:38:29.368811 IP 169.254.11.2.50710 &gt; 192.168.2.222.8080: Flags [S], seq 768065283, win 43690, options [mss 65495,sackOK,TS val 12279192 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:38:29.368853 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676521152 ecr 12276183,nop,wscale 7], length 0</span><br><span class="line">07:38:31.562633 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676523346 ecr 12276183,nop,wscale 7], length 0</span><br><span class="line">07:38:33.376829 IP 169.254.11.2.50710 &gt; 192.168.2.222.8080: Flags [S], seq 768065283, win 43690, options [mss 65495,sackOK,TS val 12283200 ecr 0,nop,wscale 7], length 0</span><br><span class="line">07:38:33.376869 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676525160 ecr 12276183,nop,wscale 7], length 0</span><br><span class="line">07:38:37.562632 IP 192.168.2.222.8080 &gt; 169.254.11.2.50710: Flags [S.], seq 2142784980, ack 768065284, win 28960, options [mss 1460,sackOK,TS val 676529346 ecr 12276183,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>从 <code>Flags</code> 看，就是 tcp 重传，并且 <code>SRC IP</code> 是 VIP 。节点 <code>192.168.2.222.8080</code> 给 <code>169.254.11.2.50710</code> 回包会走到网关上去。网关上抓包也看到确实如此：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth0 host 169.254.11.2</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">19:39:47.487362 IP 192.168.2.222.8080 &gt; 169.254.11.2.50714: Flags [S.], seq 4149799699, ack 251479303, win 28960, options [mss 1460,sackOK,TS val 676599263 ecr 12357303,nop,wscale 7], length 0</span><br><span class="line">19:39:47.487405 IP 192.168.2.222.8080 &gt; 169.254.11.2.50714: Flags [S.], seq 4149799699, ack 251479303, win 28960, options [mss 1460,sackOK,TS val 676599263 ecr 12357303,nop,wscale 7], length 0</span><br><span class="line">19:39:48.487838 IP 192.168.2.222.8080 &gt; 169.254.11.2.50714: Flags [S.], seq 4149799699, ack 251479303, win 28960, options [mss 1460,sackOK,TS val 676600264 ecr 12357303,nop,wscale 7], length 0</span><br><span class="line">19:39:48.487868 IP 192.168.2.222.8080 &gt; 169.254.11.2.50714: Flags [S.], seq 4149799699, ack 251479303, win 28960, options [mss 1460,sackOK,TS val 676600264 ecr 12357303,nop,wscale 7], length 0</span><br><span class="line">19:39:49.569667 IP 192.168.2.222.8080 &gt; 169.254.11.2.50714: Flags [S.], seq 4149799699, ack 251479303, win 28960, options [mss 1460,sackOK,TS val 676601346 ecr 12357303,nop,wscale 7], length 0</span><br><span class="line">19:39:49.569699 IP 192.168.2.222.8080 &gt; 169.254.11.2.50714: Flags [S.], seq 4149799699, ack 251479303, win 28960, options [mss 1460,sackOK,TS val 676601346 ecr 12357303,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><h4 id="lvs-和-netfilter"><a href="#lvs-和-netfilter" class="headerlink" title="lvs 和 netfilter"></a>lvs 和 netfilter</h4><p>在介绍 lvs 的实现之前，我们需要了解 netfilter ，Linux 的所有数据包都会经过它，而我们使用的 iptables 是用户态提供的操作工具之一。Linux 内核处理进出的数据包分为了 5 个阶段。netfilter 在这 5 个阶段提供了 hook 点，来让注册的 hook 函数来实现对包的过滤和修改。下图的 local process 就是上层的协议栈。</p><p>下面是 IPVS 在 netfilter 里的模型图，IPVS 也是基于 netfilter 框架的，但只工作在 <code>INPUT</code> 链上，通过注册 <code>ip_vs_in</code> 钩子函数来处理请求。因为 VIP 我们配置在机器上（常规的 lvs nat 的 VIP 是在 NAT GW 上，我们这里是自己），我们 curl 的时候就会进到 <code>INPUT</code> 链，<code>ip_vs_in</code> 会匹配然后直接跳转触发 <code>POSTROUTING</code> 链，跳过 iptables 规则。</p><p><img src="https://raw.githubusercontent.com/zhangguanzhang/Image-Hosting/master/picgo/lvs-netfilter.png" alt="lvs-netfilter"></p><p>所以请求流程是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># CIP: client IP    # RIP: real server IP</span><br><span class="line"></span><br><span class="line">CLIENT</span><br><span class="line">   | CIP:CPORT -&gt; VIP:VPORT</span><br><span class="line">   |||</span><br><span class="line">   |\/</span><br><span class="line">       | CIP:CPORT -&gt; VIP:VPORT</span><br><span class="line">   LVS DNAT</span><br><span class="line">      | CIP:CPORT -&gt; RIP:RPORT</span><br><span class="line">      |||</span><br><span class="line">   |\/</span><br><span class="line">   | CIP:CPORT -&gt; RIP:RPORT</span><br><span class="line">   +</span><br><span class="line">REAL SERVER</span><br></pre></td></tr></table></figure><p>lvs 做了 DNAT 并没有做 SNAT ，所以我们利用 iptables 做 SNAT ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -A POSTROUTING -m ipvs --vaddr 169.254.11.2 --vport 80 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>访问看看还是不通，抓包看还是没生效，nat 是依赖 <code>conntrack</code> 的，而 IPVS 默认不会记录 conntrack，我们需要开启 IPVS 的 conntrack 才可以让 MASQUERADE 生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让 Netfilter 的 conntrack 状态管理功能也能应用于 IPVS 模块</span></span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt;  /proc/sys/net/ipv4/vs/conntrack</span><br><span class="line">$  curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.222</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.222</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.222</span><br></pre></td></tr></table></figure><p>现在实现了单个 SVC 的，但是仔细思考下还是有问题，如果后续增加另一个 SVC 又得增加一个 iptables 规则了，那就又回到 iptables 的匹配复杂度耗时长上去了。所以我们可以利用 iptables 的 mark 和 ipset 配合减少 iptables 规则。</p><h3 id="利用-ipset-和-iptable-的-mark"><a href="#利用-ipset-和-iptable-的-mark" class="headerlink" title="利用 ipset 和 iptable 的 mark"></a>利用 ipset 和 iptable 的 mark</h3><p><img src="https://raw.githubusercontent.com/zhangguanzhang/Image-Hosting/master/picgo/iptables_netfilter.png" alt="iptables_netfilter"></p><p>iptables 的五链四表如上图所示，我们先删掉原有的规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -D POSTROUTING -m ipvs --vaddr 169.254.11.2 --vport 80 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>平时自己家里使用了 openwrt ，之前看了下上面的 iptables 规则设计挺好的，特别是预留了很多链专门给用户在合适的位置插入规则，比如下面的 <code>INPUT</code> 规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-A INPUT -i eth0 -m comment --comment <span class="string">&quot;!fw3&quot;</span> -j zone_lan_input</span><br><span class="line">...</span><br><span class="line">-A zone_lan_input -m comment --comment <span class="string">&quot;!fw3: Custom lan input rule chain&quot;</span> -j input_lan_rule</span><br><span class="line">-A zone_lan_input -m conntrack --ctstate DNAT -m comment --comment <span class="string">&quot;!fw3: Accept port redirections&quot;</span> -j ACCEPT</span><br><span class="line">-A zone_lan_input -m comment --comment <span class="string">&quot;!fw3&quot;</span> -j zone_lan_src_ACCEPT</span><br></pre></td></tr></table></figure><p><code>zone_lan_src_ACCEPT</code> 是最后面，<code>zone_lan_input</code> 是最开始，那用户向 <code>input_lan_rule</code> 链里插入规则即可，利用多个链来设计也方便别人。<br>规则设计我们先逆着来思考下，最后肯定是 <code>MASQUERADE</code> 的，得在 nat 表的 <code>POSTROUTING</code> 链创建 <code>MASQUERADE</code> 的规则。</p><p>但是添加之前先思考下，lvs 做了 DNAT 后，最后包走向了 <code>POSTROUTING</code> 链，而且后面我们是有多个 SVC 的。此刻包的 <code>SRC IP</code> 会是 <code>VIP</code>，而 <code>DEST IP</code> 是做了 DNAT 后的 <code>real server</code> ，而且后续可能是在在 docker 环境上部署，可能默认桥接网络的容器也会去访问 <code>SVC</code>，此刻的 <code>SRC IP</code> 就不会是网卡上的 <code>VIP</code> 了，但是在 <code>PREROUTING</code> 链的时候，<code>DEST IP</code> 还没变，我们可以在此刻利用一个 ipset 存储所有的 <code>SVC_IP:SVC_PORT</code> 匹配，然后打上 mark，然后在 <code>POSTROUTING</code> 链去根据 mark 去做 <code>MASQUERADE</code> 。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PREROUTING 阶段处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提供一个入口链，而不是直接添加在 PREROUTING 链上</span></span><br><span class="line">iptables -t nat -N ZGZ-SERVICES</span><br><span class="line">iptables -t nat -A PREROUTING -m comment --comment <span class="string">&quot;zgz service portals&quot;</span> -j ZGZ-SERVICES</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 PREROUTING 子链里去 ipset 匹配，跳转到我们 mark 的链</span></span><br><span class="line">iptables -t nat -N ZGZ-MARK-MASQ</span><br><span class="line"><span class="comment"># 创建存储所有 `SVC_IP:SVC_PORT` 的 ipset </span></span><br><span class="line">ipset create ZGZ-CLUSTER-IP <span class="built_in">hash</span>:ip,port -exist</span><br><span class="line">iptables -t nat -A ZGZ-SERVICES -m comment --comment <span class="string">&quot;zgz service cluster ip + port for masquerade purpose&quot;</span> -m <span class="built_in">set</span> --match-set ZGZ-CLUSTER-IP dst,dst -j ZGZ-MARK-MASQ</span><br><span class="line"></span><br><span class="line"><span class="comment"># 专门 mark 的链</span></span><br><span class="line">iptables -t nat -A ZGZ-MARK-MASQ -j MARK --set-xmark 0x2000/0x2000</span><br><span class="line"></span><br><span class="line"><span class="comment"># POSTROUTING 阶段处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提供一个入口链，而不是直接添加在 POSTROUTING 链上</span></span><br><span class="line">iptables -t nat -N ZGZ-POSTROUTING</span><br><span class="line">iptables -t nat -A POSTROUTING -m comment --comment <span class="string">&quot;zgz postrouting rules&quot;</span> -j ZGZ-POSTROUTING</span><br><span class="line"><span class="comment"># 在 POSTROUTING 阶段做 snat</span></span><br><span class="line">iptables -t nat -A ZGZ-POSTROUTING -m comment --comment <span class="string">&quot;zgz service traffic requiring SNAT&quot;</span> -m mark --mark 0x2000/0x2000 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>然后添加下 <code>SVC_IP:SVC_PORT</code> 到我们的 ipset 里：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipset add ZGZ-CLUSTER-IP 169.254.11.2,tcp:80 -exist</span><br></pre></td></tr></table></figure><p>上面我们创建的 ipset 里 <code>ip,port</code> 和 iptables 里 <code>--match-set</code> 后面的 <code>dst,dst</code> 组合在一起就是 <code>DEST IP</code> 和 <code>DEST PORT</code> 同时匹配，下面是一些举例：</p><table><thead><tr><th align="left">ipset type</th><th align="left">iptables match-set</th><th align="left">Packet fields</th></tr></thead><tbody><tr><td align="left">hash:net,port,net</td><td align="left">src,dst,dst</td><td align="left">src IP CIDR address, dst port, dst IP CIDR address</td></tr><tr><td align="left">hash:net,port,net</td><td align="left">dst,src,src</td><td align="left">dst IP CIDR address, src port, src IP CIDR address</td></tr><tr><td align="left">hash:ip,port,ip</td><td align="left">src,dst,dst</td><td align="left">src IP address, dst port, dst IP address</td></tr><tr><td align="left">hash:ip,port,ip</td><td align="left">dst,src,src</td><td align="left">dst IP address, src port, src ip address</td></tr><tr><td align="left">hash:mac</td><td align="left">src</td><td align="left">src mac address</td></tr><tr><td align="left">hash:mac</td><td align="left">dst</td><td align="left">dst mac address</td></tr><tr><td align="left">hash:ip,mac</td><td align="left">src,src</td><td align="left">src IP address, src mac address</td></tr><tr><td align="left">hash:ip,mac</td><td align="left">dst,dst</td><td align="left">dst IP address, dst mac address</td></tr><tr><td align="left">hash:ip,mac</td><td align="left">dst,src</td><td align="left">dst IP address, src mac address</td></tr></tbody></table><p>然后访问下还是不通，突然想起来在机器上访问本机的 IP 端口的时候是内核直接转发给本地进程，这种数据包会只经过 <code>OUTPUT</code> 链，不会过 <code>PREROUTING</code> ，不过我们上面的 <code>PREROUTING</code> 链路可以处理后续的 docker 容器。 调试了下发现确实会走 <code>OUTPUT</code> 链：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;kern.warning /var/log/iptables.log&#x27;</span> &gt;&gt; /etc/rsyslog.conf</span><br><span class="line">$ systemctl restart rsyslog</span><br><span class="line">$ iptables -t nat -I OUTPUT -m <span class="built_in">set</span> --match-set ZGZ-CLUSTER-IP dst,dst  -j LOG --log-prefix <span class="string">&#x27;**log-test**&#x27;</span></span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">$ cat /var/<span class="built_in">log</span>/iptables.log</span><br><span class="line">Sep 27 23:17:51 centos7 kernel: **log-test**IN= OUT=lo SRC=169.254.11.2 DST=169.254.11.2 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=44864 DF PROTO=TCP SPT=50794 DPT=80 WINDOW=43690 RES=0x00 SYN URGP=0 </span><br><span class="line">Sep 27 23:17:52 centos7 kernel: **log-test**IN= OUT=lo SRC=169.254.11.2 DST=169.254.11.2 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=2010 DF PROTO=TCP SPT=50796 DPT=80 WINDOW=43690 RES=0x00 SYN URGP=0 </span><br></pre></td></tr></table></figure><p>需要添加下面规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A OUTPUT -m comment --comment <span class="string">&quot;zgz service portals&quot;</span> -j ZGZ-SERVICES</span><br></pre></td></tr></table></figure><h3 id="keepalived-的自动化实现"><a href="#keepalived-的自动化实现" class="headerlink" title="keepalived 的自动化实现"></a>keepalived 的自动化实现</h3><p>到目前为止都是手动挡，而且没健康检查，其实我们可以利用 keepalived 做个自动挡的。</p><h4 id="安装-keepalived-2"><a href="#安装-keepalived-2" class="headerlink" title="安装 keepalived 2"></a>安装 keepalived 2</h4><p>CentOS7 自带的源里 <code>keepalived</code> 版本很低，我们安装下比自带新的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y http://www.nosuchhost.net/~cheese/fedora/packages/epel-7/x86_64/cheese-release-7-1.noarch.rpm</span><br><span class="line">yum install -y keepalived</span><br><span class="line"><span class="comment"># 备份下自带的配置文件</span></span><br><span class="line">cp /etc/keepalived/keepalived.conf&#123;,.bak&#125;</span><br></pre></td></tr></table></figure><h4 id="配置-keepalived"><a href="#配置-keepalived" class="headerlink" title="配置 keepalived"></a>配置 keepalived</h4><p>我们需要配置下 keepalived ，修改之前先看下默认相关的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl cat keepalived</span><br><span class="line"><span class="comment"># /usr/lib/systemd/system/keepalived.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=LVS and VRRP High Availability Monitor</span><br><span class="line">After=syslog.target network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">KillMode=process</span><br><span class="line">EnvironmentFile=-/etc/sysconfig/keepalived</span><br><span class="line">ExecStart=/usr/sbin/keepalived <span class="variable">$KEEPALIVED_OPTIONS</span></span><br><span class="line">ExecReload=/bin/<span class="built_in">kill</span> -HUP <span class="variable">$MAINPID</span></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">$ cat /etc/sysconfig/keepalived</span><br><span class="line"><span class="comment"># Options for keepalived. See `keepalived --help&#x27; output and keepalived(8) and</span></span><br><span class="line"><span class="comment"># keepalived.conf(5) man pages for a list of all options. Here are the most</span></span><br><span class="line"><span class="comment"># common ones :</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># --vrrp               -P    Only run with VRRP subsystem.</span></span><br><span class="line"><span class="comment"># --check              -C    Only run with Health-checker subsystem.</span></span><br><span class="line"><span class="comment"># --dont-release-vrrp  -V    Dont remove VRRP VIPs &amp; VROUTEs on daemon stop.</span></span><br><span class="line"><span class="comment"># --dont-release-ipvs  -I    Dont remove IPVS topology on daemon stop.</span></span><br><span class="line"><span class="comment"># --dump-conf          -d    Dump the configuration data.</span></span><br><span class="line"><span class="comment"># --log-detail         -D    Detailed log messages.</span></span><br><span class="line"><span class="comment"># --log-facility       -S    0-7 Set local syslog facility (default=LOG_DAEMON)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line">KEEPALIVED_OPTIONS=<span class="string">&quot;-D&quot;</span></span><br></pre></td></tr></table></figure><p><code>/etc/sysconfig/keepalived</code> 里修改为下面：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KEEPALIVED_OPTIONS=&quot;-D --log-console --log-detail --use-file=/etc/keepalived/keepalived.conf&quot;</span><br></pre></td></tr></table></figure><p>我们选择在主配置文件里去 include 子配置文件，keepalivd 接收 <code>kill -HUP</code> 信号触发 reload ，后续自动化添加 SVC 的时候添加子配置文件后发送信号即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/keepalived/keepalived.conf &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">! Configuration File for keepalived</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">global_defs &#123;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"># 记住 keepalived 的任何配置文件不能有 x 权限</span></span><br><span class="line"><span class="string">include /etc/keepalived/conf.d/*.conf</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">mkdir -p /etc/keepalived/conf.d/</span><br></pre></td></tr></table></figure><p>我们写一个脚本，一个是用来添加一个子配置文件里的相关信息到 ipset 里，另一方面也让它在重启或者启动 keepalived 的时候每次能初始化，先添加 systemd 的部分：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /usr/lib/systemd/system/keepalived.service.d</span><br><span class="line">cat &gt; /usr/lib/systemd/system/keepalived.service.d/10.keepalived.conf &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">[Service]</span></span><br><span class="line"><span class="string">ExecStartPre=/etc/keepalived/ipvs.sh</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后编写脚本 <code>/etc/keepalived/ipvs.sh</code> :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line"></span><br><span class="line">dummy_if=svc</span><br><span class="line">CONF_DIR=/etc/keepalived/conf.d/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">ipset_init</span></span>()&#123;</span><br><span class="line">    ipset create ZGZ-CLUSTER-IP <span class="built_in">hash</span>:ip,port -exist</span><br><span class="line">    ipset flush ZGZ-CLUSTER-IP</span><br><span class="line">    <span class="built_in">local</span> f ip port protocol</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> $(find  <span class="variable">$&#123;CONF_DIR&#125;</span> -maxdepth 1 -<span class="built_in">type</span> f -name <span class="string">&#x27;*.conf&#x27;</span>);<span class="keyword">do</span></span><br><span class="line">        awk <span class="string">&#x27;&#123;if($1==&quot;virtual_server&quot;)&#123;printf $2&quot; &quot;$3&quot; &quot;;flag=1;&#125;;if(flag==1 &amp;&amp; $1==&quot;protocol&quot;)&#123;print $2;flag=0&#125;&#125;&#x27;</span> <span class="string">&quot;<span class="variable">$f</span>&quot;</span> | <span class="keyword">while</span> <span class="built_in">read</span> ip port protocol;<span class="keyword">do</span></span><br><span class="line">            <span class="comment"># SVC IP port 插入 ipset 里</span></span><br><span class="line">            ipset add ZGZ-CLUSTER-IP <span class="variable">$&#123;ip&#125;</span>,<span class="variable">$&#123;protocol,,&#125;</span>:<span class="variable">$&#123;port&#125;</span> -exist</span><br><span class="line">            <span class="comment"># 添加 SVC IP 到 dummy 接口上</span></span><br><span class="line">            <span class="keyword">if</span> ! ip r g <span class="variable">$&#123;ip&#125;</span> | grep -qw lo;<span class="keyword">then</span></span><br><span class="line">                ip addr add <span class="variable">$&#123;ip&#125;</span>/32 dev <span class="variable">$&#123;dummy_if&#125;</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">create_Chain_in_nat</span></span>()&#123;</span><br><span class="line">    <span class="comment"># delete use -X</span></span><br><span class="line">    <span class="built_in">local</span> Chain option</span><br><span class="line">    option=<span class="string">&quot;-t nat --wait&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> Chain <span class="keyword">in</span> <span class="variable">$@</span>;<span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> ! iptables <span class="variable">$option</span> -S | grep -Eq -- <span class="string">&quot;-N\s+<span class="variable">$&#123;Chain&#125;</span>&quot;</span>;<span class="keyword">then</span></span><br><span class="line">        iptables <span class="variable">$option</span> -N <span class="variable">$&#123;Chain&#125;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">create_Rule_in_nat</span></span>()&#123;</span><br><span class="line">    <span class="built_in">local</span> cmd=<span class="string">&#x27;iptables -t nat --wait &#x27;</span></span><br><span class="line">    <span class="keyword">if</span> ! <span class="variable">$&#123;cmd&#125;</span>  --check <span class="string">&quot;<span class="variable">$@</span>&quot;</span> 2&gt;/dev/null;<span class="keyword">then</span></span><br><span class="line">        <span class="variable">$&#123;cmd&#125;</span> -A <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">iptables_init</span></span>()&#123;</span><br><span class="line">    create_Chain_in_nat ZGZ-SERVICES  ZGZ-SERVICES-POSTROUTING ZGZ-SERVICES-MARK-MASQ</span><br><span class="line"></span><br><span class="line">    create_Rule_in_nat ZGZ-SERVICES-MARK-MASQ -j MARK --set-xmark 0x2000/0x2000</span><br><span class="line"></span><br><span class="line">    create_Rule_in_nat ZGZ-SERVICES -m comment --comment <span class="string">&quot;zgz service cluster ip + port for masquerade purpose&quot;</span> -m <span class="built_in">set</span> --match-set ZGZ-CLUSTER-IP dst,dst -j ZGZ-SERVICES-MARK-MASQ</span><br><span class="line"></span><br><span class="line">    create_Rule_in_nat PREROUTING -m comment --comment <span class="string">&quot;zgz service portals&quot;</span> -j ZGZ-SERVICES</span><br><span class="line">    create_Rule_in_nat OUTPUT -m comment --comment <span class="string">&quot;zgz service portals&quot;</span> -j ZGZ-SERVICES</span><br><span class="line"></span><br><span class="line">    create_Rule_in_nat ZGZ-SERVICES-POSTROUTING -m comment --comment <span class="string">&quot;zgz service traffic requiring SNAT&quot;</span> -m mark --mark 0x2000/0x2000 -j MASQUERADE</span><br><span class="line">    create_Rule_in_nat POSTROUTING -m comment --comment <span class="string">&quot;zgz postrouting rules&quot;</span> -j ZGZ-SERVICES-POSTROUTING</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">ipvs_svc_run</span></span>()&#123;</span><br><span class="line">  ip addr flush dev <span class="variable">$&#123;dummy_if&#125;</span></span><br><span class="line">  ipset_init</span><br><span class="line">  iptables_init</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 无参数则是 keepalived 启动，也可以接收单个配置文件参数</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>()&#123;</span><br><span class="line">  <span class="keyword">if</span> [ ! -d /proc/sys/net/ipv4/conf/<span class="variable">$&#123;dummy_if&#125;</span> ];<span class="keyword">then</span></span><br><span class="line">    ip link add <span class="variable">$&#123;dummy_if&#125;</span> <span class="built_in">type</span> dummy</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$#</span>&quot;</span> -eq 0 ];<span class="keyword">then</span></span><br><span class="line">    ipvs_svc_run</span><br><span class="line">    <span class="built_in">return</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">local</span> file fullFile ip port protocol</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> <span class="variable">$@</span>;<span class="keyword">do</span></span><br><span class="line">    fullFile=<span class="variable">$&#123;CONF_DIR&#125;</span>/<span class="variable">$file</span></span><br><span class="line">      awk <span class="string">&#x27;&#123;if($1==&quot;virtual_server&quot;)&#123;printf $2&quot; &quot;$3&quot; &quot;;flag=1;&#125;;if(flag==1 &amp;&amp; $1==&quot;protocol&quot;)&#123;print $2;flag=0&#125;&#125;&#x27;</span> <span class="string">&quot;<span class="variable">$f</span>&quot;</span> | <span class="keyword">while</span> <span class="built_in">read</span> ip port protocol;<span class="keyword">do</span></span><br><span class="line">          <span class="comment"># SVC IP port 插入 ipset 里</span></span><br><span class="line">          ipset add ZGZ-CLUSTER-IP <span class="variable">$&#123;ip&#125;</span>,<span class="variable">$&#123;protocol,,&#125;</span>:<span class="variable">$&#123;port&#125;</span> -exist</span><br><span class="line">          <span class="comment"># 添加 SVC IP 到 dummy 接口上</span></span><br><span class="line">          <span class="keyword">if</span> ! ip r g <span class="variable">$&#123;ip&#125;</span> | grep -qw lo;<span class="keyword">then</span></span><br><span class="line">              ip addr add <span class="variable">$&#123;ip&#125;</span>/32 dev <span class="variable">$&#123;dummy_if&#125;</span></span><br><span class="line">          <span class="keyword">fi</span></span><br><span class="line">      <span class="keyword">done</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">  <span class="comment"># 重新 reload </span></span><br><span class="line">  pkill --signal HUP keepalived</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">main <span class="variable">$@</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>脚本就如上面所示，读取 keepalived 的 lvs 文件，把 <code>VIP:PORT</code> 加到 ipset 里，VIP 加到 <code>dummy</code> 接口上，之前是加到 eth0 上，但是业务网卡可能会重启影响，dummy 接口和 loopback 类似，它总是 up 的，除非你 down 掉它，SVC 地址配置在它上面不会随着物理接口状态变化而受到影响。删除掉之前 eth0 上的 VIP <code>ip addr del 169.254.11.2/32 dev eth0</code>，然后把前面的转成 keepalived 的配置文件测试下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x /etc/keepalived/ipvs.sh</span><br><span class="line">cat &gt; /etc/keepalived/conf.d/test.conf &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">virtual_server 169.254.11.2 80 &#123;</span></span><br><span class="line"><span class="string">    delay_loop 3</span></span><br><span class="line"><span class="string">    lb_algo rr</span></span><br><span class="line"><span class="string">    lb_kind NAT</span></span><br><span class="line"><span class="string">    protocol TCP</span></span><br><span class="line"><span class="string">    alpha #默认是禁用，会导致在启动daemon时，所有rs都会上来，开启此选项下则是所有的RS在daemon启动的时候是down状态，healthcheck健康检查failed。这有助于其启动时误报错误</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    real_server  192.168.2.111 8080 &#123;</span></span><br><span class="line"><span class="string">        weight 1</span></span><br><span class="line"><span class="string">        HTTP_GET  &#123;</span></span><br><span class="line"><span class="string">            url &#123;</span></span><br><span class="line"><span class="string">              path /404</span></span><br><span class="line"><span class="string">              status_code 404</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">            connect_port    8080</span></span><br><span class="line"><span class="string">            connect_timeout 2</span></span><br><span class="line"><span class="string">            retry 2</span></span><br><span class="line"><span class="string">            delay_before_retry 2</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    real_server  192.168.2.222 8080 &#123;</span></span><br><span class="line"><span class="string">        weight 1</span></span><br><span class="line"><span class="string">        HTTP_GET  &#123;</span></span><br><span class="line"><span class="string">            url &#123;</span></span><br><span class="line"><span class="string">              path /404</span></span><br><span class="line"><span class="string">              status_code 404</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">            connect_port    8080</span></span><br><span class="line"><span class="string">            connect_timeout 2</span></span><br><span class="line"><span class="string">            retry 2</span></span><br><span class="line"><span class="string">            delay_before_retry 2</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>测试下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 先清理掉之前手动添加的</span><br><span class="line">ipvsadm --clear</span><br><span class="line"></span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart keepalived</span><br><span class="line"></span><br><span class="line">$ curl 169.254.11.2/www/test</span><br><span class="line">192.168.2.222</span><br><span class="line">$ curl 169.254.11.2/www/test</span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/test</span><br><span class="line">192.168.2.222</span><br><span class="line">$ curl 169.254.11.2/www/test</span><br><span class="line">192.168.2.111</span><br><span class="line">$ ip a s svc</span><br><span class="line">4: svc: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether e6:a3:29:07:fa:57 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 169.254.11.2/32 scope global svc</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>停掉一个 web 后在我们配置的健康检查几秒也剔除了 rs ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">curl: (7) Failed connect to 169.254.11.2:80; Connection refused</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br><span class="line">$ curl 169.254.11.2/www/<span class="built_in">test</span></span><br><span class="line">192.168.2.111</span><br></pre></td></tr></table></figure><h4 id="系统的相关配置"><a href="#系统的相关配置" class="headerlink" title="系统的相关配置"></a>系统的相关配置</h4><p>后面重启后发现不通，发现内核模块没加载，使用 <code>systemd-modules-load</code> 去开机加载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat  &gt; /etc/modules-load.d/ipvs.conf &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">ip_vs</span></span><br><span class="line"><span class="string">ip_vs_rr</span></span><br><span class="line"><span class="string">ip_vs_wrr</span></span><br><span class="line"><span class="string">ip_vs_sh</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line">cat &gt; /etc/sysctl.d/90.ipvs.conf &lt;&lt; <span class="string">EOF </span></span><br><span class="line"><span class="string"># https://github.com/moby/moby/issues/31208 </span></span><br><span class="line"><span class="string"># ipvsadm -l --timout</span></span><br><span class="line"><span class="string"># 修复ipvs模式下长连接timeout问题 小于900即可</span></span><br><span class="line"><span class="string">net.ipv4.tcp_keepalive_time=600</span></span><br><span class="line"><span class="string">net.ipv4.tcp_keepalive_intvl=30</span></span><br><span class="line"><span class="string">net.ipv4.vs.conntrack=1</span></span><br><span class="line"><span class="string"># https://github.com/kubernetes/kubernetes/issues/70747 https://github.com/kubernetes/kubernetes/pull/71114</span></span><br><span class="line"><span class="string">net.ipv4.vs.conn_reuse_mode=0</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h3 id="docker-运行的方案"><a href="#docker-运行的方案" class="headerlink" title="docker 运行的方案"></a>docker 运行的方案</h3><p><code>docker-compose</code> 文件如下，自己把脚本挂载进去即可：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.5&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">keepalived:</span> </span><br><span class="line">    <span class="attr">image:</span> <span class="string">&#x27;registry.aliyuncs.com/zhangguanzhang/keepalived:v2.2.0&#x27;</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">&#x27;keepalived-ipvs&#x27;</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">&quot;keepalived-ipvs&quot;</span></span><br><span class="line">    <span class="attr">labels:</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">app=keepalived</span></span><br><span class="line">    <span class="attr">network_mode:</span> <span class="string">host</span></span><br><span class="line">    <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">cap_drop:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ALL</span></span><br><span class="line">    <span class="attr">cap_add:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">NET_BIND_SERVICE</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/lib/modules:/lib/modules</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/run/xtables.lock:/run/xtables.lock</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./conf.d/:/etc/keepalived/conf.d/</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./keepalived.conf:/etc/keepalived/keepalived.conf</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./always-initsh.d:/always-initsh.d</span></span><br><span class="line">    <span class="attr">command:</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">--dont-fork</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--log-console</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--log-detail</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">--use-file=/etc/keepalived/keepalived.conf</span></span><br><span class="line">    <span class="attr">logging:</span></span><br><span class="line">      <span class="attr">driver:</span> <span class="string">json-file</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">max-file:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line">        <span class="attr">max-size:</span> <span class="string">20m</span></span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html">Interaction between LVS and netfilter</a></li><li><a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.LVS-NAT.html#lvs_nat_intro">lvs nat</a></li><li><a href="https://github.com/liexusong/linux-source-code-analyze/blob/master/lvs-principle-and-source-analysis-part2.md">lvs-principle-and-source-analysis</a></li><li><a href="https://www.zsythink.net/archives/1199">朱双印大佬的 iptables 技术系列</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;内部有非 K8S 环境上需要类似 SVC 的负载实现，一开始是用 NGINX 做的，所有 SVC 域名都解析成一个 dummy IP ，然后</summary>
      
    
    
    
    
    <category term="lvs" scheme="http://zhangguanzhang.github.io/tags/lvs/"/>
    
    <category term="ipvsadm" scheme="http://zhangguanzhang.github.io/tags/ipvsadm/"/>
    
    <category term="kube-proxy" scheme="http://zhangguanzhang.github.io/tags/kube-proxy/"/>
    
  </entry>
  
  <entry>
    <title>解决 docker 的 read unix @-&gt;/run/containerd/s/xxx read: connection reset by peer: unknown</title>
    <link href="http://zhangguanzhang.github.io/2021/09/16/read-containerd-con-reset/"/>
    <id>http://zhangguanzhang.github.io/2021/09/16/read-containerd-con-reset/</id>
    <published>2021-09-16T18:14:06.000Z</published>
    <updated>2021-09-16T18:14:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>为了测试关机对集群的影响，关机了几台机器后很多 pod 一直 <code>CrashLoopBackOff</code> 和 <code>RunContainerError</code> 或者一直无法就绪</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS76 ~]# docker info</span><br><span class="line">Client:</span><br><span class="line"> Debug Mode: false</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 404</span><br><span class="line">  Running: 258</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 146</span><br><span class="line"> Images: 110</span><br><span class="line"> Server Version: 19.03.14</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: true</span><br><span class="line">  Native Overlay Diff: true</span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: cgroupfs</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: local</span><br><span class="line">  Network: bridge host ipvlan macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: ea765aba0d05254012b0b9e595e995c09186427f</span><br><span class="line"> runc version: v1.0.0-0-g84113eef6fc2</span><br><span class="line"> init version: fec3683</span><br><span class="line"> Security Options:</span><br><span class="line">  seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 3.10.0-1160.36.2.el7.x86_64</span><br><span class="line"> Operating System: CentOS Linux 7 (Core)</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: x86_64</span><br><span class="line"> CPUs: 16</span><br><span class="line"> Total Memory: 62.76GiB</span><br><span class="line"> Name: CentOS76</span><br><span class="line"> ID: BJ2X:EX7H:SCME:Q3AD:IP2M:IB2D:E4RL:XA4C:EOMQ:7S3F:DIA6:WQ2C</span><br><span class="line"> Docker Root Dir: /data/kube/docker</span><br><span class="line"> Debug Mode: false</span><br><span class="line"> Registry: https://index.docker.io/v1/</span><br><span class="line"> Labels:</span><br><span class="line"> Experimental: false</span><br><span class="line"> Insecure Registries:</span><br><span class="line">  reg.xxx.lan:5000</span><br><span class="line">  treg.yun.xxx.cn</span><br><span class="line">  127.0.0.0/8</span><br><span class="line"> Registry Mirrors:</span><br><span class="line">  https://registry.docker-cn.com/</span><br><span class="line">  https://docker.mirrors.ustc.edu.cn/</span><br><span class="line"> Live Restore Enabled: false</span><br><span class="line"> Product License: Community Engine</span><br></pre></td></tr></table></figure><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><p>日志查看如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RunContainerError: failed to start container &quot;90353b19ae6c7209ba1785286c292f2362fa069b578f2e2731e93747c5ba1912&quot;: Error response from daemon: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/90353b19ae6c7209ba1785286c292f2362fa069b578f2e2731e93747c5ba1912/log.json: no such file or directory): runc did not terminate sucessfully: unknown</span><br></pre></td></tr></table></figure><p>还有下面日志：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">runc did not terminate sucessfully: runtime/cgo: pthread_create failed: Resource temporarily unavailable</span><br><span class="line"></span><br><span class="line">container 9853a196008b92033a299e098d73d4268a76ce58faecfe40ca3411857d44a776: unknown error after kill: fork/exec /data/kube/bin/runc: resource temporarily unavailable: : unknown&quot;</span><br></pre></td></tr></table></figure><p>应该资源限制了，看了下默认的 <code>kernel.pid_max</code> 太小：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -n kernel.pid_max</span><br><span class="line">32768</span><br></pre></td></tr></table></figure><p>后面陆陆续续调整了一些下面的参数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/security/limits.d/21-custom.conf&lt;&lt;EOF</span><br><span class="line">*       soft    nproc   131072</span><br><span class="line">*       hard    nproc   131072</span><br><span class="line">*       soft    nofile  131072</span><br><span class="line">*       hard    nofile  131072</span><br><span class="line">root    soft    nproc   131072</span><br><span class="line">root    hard    nproc   131072</span><br><span class="line">root    soft    nofile  131072</span><br><span class="line">root    hard    nofile  131072</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -ri &#x27;s/^#(DefaultLimitCORE)=/\1=100000/&#x27; /etc/systemd/system.conf</span><br><span class="line">sed -ri &#x27;s/^#(DefaultLimitNOFILE)=/\1=100000/&#x27; /etc/systemd/system.conf</span><br></pre></td></tr></table></figure><p>然后重启后 pod 还没有好转，启动一直处于 Create 的容器会有下面错误：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS76 ~]# docker start 034f</span><br><span class="line">Error response from daemon: read unix @-&gt;/run/containerd/s/2ac09cf054eb19b79336b25efe1aeeaf22bcf0d9559ca79b8459c3490cd6034f: read: connection reset by peer: unknown</span><br><span class="line">Error: failed to start containers: 034f</span><br></pre></td></tr></table></figure><p>手动起容器报错下面的，调整参数后更多是上面的报错。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm nginx:1.19-alpine</span><br><span class="line">docker: Errpr response from daemon: failed to start shim: fork/exec /usr/bin/containerd-shim: resource temporarily unavailable: unknown.</span><br></pre></td></tr></table></figure><p><code> read unix @-&gt;/run/containerd/s</code> 这个按照流程走就是 contained 的问题了，可以从 <a href="https://github.com/docker/docker-ce/blob/d7080c17a580919f5340a15a8e5e013133089680/components/engine/libcontainerd/remote_daemon.go#L244">源码</a> 得知，如果没启动 containerd ，docker 则会 os.Exec 起一个 <code>containerd</code> ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep &#x27;\scontainerd\s&#x27;</span><br><span class="line">root     147580  2.4  0.1 10375568 104588 ?     Ssl  17:06   3:15 containerd --config /var/run/docker/containerd/containerd.toml --log-level warn</span><br></pre></td></tr></table></figure><p>我们的 docker 是官方的 static 二进制安装的，去看了下 rpm 安装的话会分离开，也就是有个 containerd 的 rpm，有一个 <code>containerd.service</code> 服务。 想着看下我们环境上的 containerd 的输出日志，但是源码看的话命令的输出都是绑定到 docker 的输出的。而且命令行参数固定的、无法改为 debug level。</p><p>手动杀掉启动下试试：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -9 147580 &amp;&amp; containerd --config /var/run/docker/containerd/containerd.toml --log-level debug</span><br></pre></td></tr></table></figure><p>另外开个 ssh 窗口发现 pod 状态都正常了。说明了 systemd 启动的 docker 有限制，去 dockerd 的 proc 目录啥的查找了下看没达到文件啥的限制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS76 ~]# pgrep dockerd</span><br><span class="line">113233</span><br><span class="line">[root@CentOS76 ~]# lsof -p 113233  | wc -l</span><br><span class="line">956</span><br></pre></td></tr></table></figure><p>最后找到问题所在，下面的<code>Tasks: 2043 (limit: 2048)</code> 限制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS76 ~]# systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since 四 2021-09-16 16:53:21 CST; 4min 16s ago</span><br><span class="line">     Docs: http://docs.docker.io</span><br><span class="line">  Process: 113228 ExecStopPost=/bin/sh -c /sbin/iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; /dev/null || : (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 113225 ExecStopPost=/bin/sh -c /sbin/iptables --wait -D FORWARD -s 0.0.0.0/0 -j ACCEPT &amp;&gt; /dev/null || : (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 113236 ExecStartPost=/sbin/iptables --wait -I INPUT -i cni0 -j ACCEPT (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 113234 ExecStartPost=/sbin/iptables --wait -I FORWARD -s 0.0.0.0/0 -j ACCEPT (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 113231 ExecStartPre=/bin/bash -c test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 113233 (dockerd)</span><br><span class="line">    Tasks: 2043 (limit: 2048)</span><br><span class="line">   Memory: 1.1G</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           ├─ 89710 containerd-shim -namespace</span><br></pre></td></tr></table></figure><p>systemd 的 <code>DefaultTasksMax</code> 是 <code>2048</code> ，另外对比了官方的 <code>docker.service</code> 是不限制 Tasks 的，我们没加：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl cat docker</span><br><span class="line">..</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>加了后重启 docker 就好了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/systemd/system/docker.service</span><br><span class="line">TasksMax=infinity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://plpan.github.io/docker-exec-%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E4%B9%8B%E6%97%85/">https://plpan.github.io/docker-exec-%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E4%B9%8B%E6%97%85/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;为了测试关机对集群的影响，关机了几台机器后很多 pod 一直 &lt;code&gt;CrashLoopBackOff&lt;/code&gt; 和 &lt;code&gt;R</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="containerd" scheme="http://zhangguanzhang.github.io/tags/containerd/"/>
    
  </entry>
  
  <entry>
    <title>[持续更新] - Openwrt USB 网络</title>
    <link href="http://zhangguanzhang.github.io/2021/09/03/openwrt-usb-net/"/>
    <id>http://zhangguanzhang.github.io/2021/09/03/openwrt-usb-net/</id>
    <published>2021-09-03T22:29:08.000Z</published>
    <updated>2021-09-03T22:29:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="about"><a href="#about" class="headerlink" title="about"></a>about</h2><p>记录下 openwrt 下 usb 网络的折腾，后续折腾这块内容的话也在这个文章内更新</p><h3 id="N1-上的-usb-网络共享折腾"><a href="#N1-上的-usb-网络共享折腾" class="headerlink" title="N1 上的 usb 网络共享折腾"></a>N1 上的 usb 网络共享折腾</h3><h4 id="固件依赖"><a href="#固件依赖" class="headerlink" title="固件依赖"></a>固件依赖</h4><p>暂时没完全区分 usb 网络共享和 <code>usb-cdc</code> 的关系，所以我编译的时候把很多 <code>usb-net-xxx</code> 都编译进去了</p><p>听其他大佬说编译的时候主要有下面的包:</p><ul><li>安卓: <code>kmod-usb-net kmod-usb-net-rndis</code></li><li>苹果: <code>kmod-usb-net-ipheth usbmuxd</code></li></ul><p>建议下面这些也安装上方便调试:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CONFIG_PACKAGE_luci-proto-3g=y</span><br><span class="line">CONFIG_PACKAGE_luci-proto-ncm=y</span><br><span class="line">CONFIG_PACKAGE_luci-proto-qmi=y</span><br><span class="line"></span><br><span class="line">CONFIG_BUSYBOX_DEFAULT_LSPCI=y</span><br><span class="line">CONFIG_BUSYBOX_DEFAULT_LSUSB=y</span><br><span class="line">CONFIG_BUSYBOX_DEFAULT_IPROUTE=y</span><br><span class="line"></span><br><span class="line">CONFIG_PACKAGE_usb-modeswitch=y</span><br><span class="line">CONFIG_PACKAGE_usbutils=y</span><br><span class="line">CONFIG_PACKAGE_usbreset=y</span><br><span class="line">CONFIG_PACKAGE_qmi-utils=y</span><br><span class="line">CONFIG_PACKAGE_libqmi=y</span><br><span class="line"></span><br><span class="line">CONFIG_PACKAGE_bind-dig=y</span><br><span class="line">CONFIG_PACKAGE_tcpdump=y</span><br><span class="line"></span><br><span class="line">CONFIG_PACKAGE_pciutils=y</span><br></pre></td></tr></table></figure><p>这些好像是 usb 4g 网卡的包，一般是 <code>E3372</code>(联通4G) 和 <code>E8372</code>(全网通4G版本) ，</p><p>缺省应该都可以用hilink模式，把下面这些驱动装上即可:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hilink mode: hilink mode 的网卡名 是  ethX 也可以说 usb0，ncm模式的则是wwan0， lsusb 或者 usb-mode -l 查看设备识别否</span><br><span class="line">kmod-usb-net-rndis kmod-usb-net kmod-usb2 usb-modeswitch</span><br><span class="line"></span><br><span class="line">NCM mode:</span><br><span class="line">gcom kmod-usb-net-huawei-cdc-ncm  kmod-usb2 usb-modeswitch kmod-usb-serial-wwan</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONFIG_PACKAGE_uqmi=y</span><br><span class="line">kmod-usb-acm kmod-usb-net kmod-usb-net-qmi-wwan kmod-usb-ohci kmod-usb-serial kmod-usb-serial-option</span><br></pre></td></tr></table></figure><h4 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h4><p>N1 建议开下无线，虽然辣鸡，但是它只有一个网口，方便我们操作。我是我的 <code>redmi k30s</code> usb 网络共享插给 N1，另一个手机连 N1 无线后 ssh 上去敲命令的。</p><p>先备份下网络配置文件，防止后面搞崩了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/config/network /etc/config/network.bak</span><br></pre></td></tr></table></figure><p>插上去后有个 <code>usb0</code> 网卡</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# ip a s</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: sit0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/sit 0.0.0.0 brd 0.0.0.0</span><br><span class="line">3: ip6tnl0@NONE: &lt;NOARP&gt; mtu 1452 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/tunnel6 :: brd :: permaddr d636:b99c:d56d::</span><br><span class="line">4: eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq master br-lan state DOWN group default qlen 1000</span><br><span class="line">    link/ether 0e:02:db:93:b8:20 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">5: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000    link/ipip 0.0.0.0 brd 0.0.0.0</span><br><span class="line">6: dummy0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether f2:09:65:33:97:88 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">7: bond0: &lt;BROADCAST,MULTICAST,MASTER&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether 7e:ad:dd:05:72:66 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: wlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master br-lan state UP group default qlen 1000</span><br><span class="line">    link/ether 0e:02:db:93:b8:1f brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::c02:dbff:fe93:b81f/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9: br-lan: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 0e:02:db:93:b8:20 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.1.1/24 brd 192.168.1.255 scope global br-lan</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fd4e:8614:5748::1/60 scope global noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::c02:dbff:fe93:b820/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">10: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default</span><br><span class="line">    link/ether 02:42:14:ab:d8:da brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.31.0.1/24 brd 172.31.0.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">11: usb0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether 82:e0:7e:db:2c:25 brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure><p>使用 <code>udhcpc</code> 发送 dhcp 请求</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# udhcpc -i usb0</span><br><span class="line">udhcpc: started, v1.33.1</span><br><span class="line">udhcpc: sending discover</span><br><span class="line">udhcpc: sendto: Network is down</span><br><span class="line">udhcpc: read error: Network is down, reopening socket</span><br><span class="line">udhcpc: sending discover</span><br></pre></td></tr></table></figure><p>查看下 usb，确实有我的 手机，居然要手动 up 它</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# lsusb</span><br><span class="line">Bus 001 Device 004: ID 2717:ff80 Xiaomi M2007J3SC</span><br><span class="line">Bus 002 Device 001: ID 1d6b:0003 Linux 5.13.13-flippy-63+ xhci-hcd xHCI Host Controller</span><br><span class="line">Bus 001 Device 001: ID 1d6b:0002 Linux 5.13.13-flippy-63+ xhci-hcd xHCI Host Controller</span><br><span class="line"></span><br><span class="line">root@OpenWrt:~# ip link set usb0 up</span><br><span class="line">root@OpenWrt:~# ip a s usb0</span><br><span class="line"></span><br><span class="line">13: usb0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether 0a:01:0f:46:3a:e6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::801:fff:fe46:3ae6/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">root@OpenWrt:~# udhcpc -i usb0</span><br><span class="line">udhcpc: started, v1.33.1</span><br><span class="line">udhcpc: sending discover</span><br><span class="line">udhcpc: sending select for 192.168.42.128</span><br><span class="line">udhcpc: lease of 192.168.42.128 obtained, lease time 3599</span><br><span class="line">udhcpc: ip addr add 192.168.42.128/255.255.255.0 broadcast 192.168.42.255 dev usb0</span><br><span class="line">udhcpc: setting default routers: 192.168.42.129</span><br></pre></td></tr></table></figure><p>请求过程中也把默认路由设置为它了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@OpenWrt:~# ip r g 1</span><br><span class="line">1.0.0.0 via 192.168.42.129 dev usb0 src 192.168.42.128 uid 0</span><br><span class="line">    cache</span><br><span class="line">root@OpenWrt:~# ping baudu.com</span><br><span class="line">^C</span><br><span class="line">root@OpenWrt:~# ping 114.114.114.114</span><br><span class="line">PING 114.114.114.114 (114.114.114.114): 56 data bytes</span><br><span class="line">64 bytes from 114.114.114.114: seq=0 ttl=64 time=73.218 ms</span><br><span class="line">64 bytes from 114.114.114.114: seq=1 ttl=70 time=45.460 ms</span><br><span class="line">64 bytes from 114.114.114.114: seq=2 ttl=71 time=43.823 ms</span><br><span class="line">64 bytes from 114.114.114.114: seq=3 ttl=81 time=40.963 ms</span><br><span class="line">^C</span><br><span class="line">--- 114.114.114.114 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 40.963/50.866/73.218 ms</span><br><span class="line">root@OpenWrt:~#</span><br></pre></td></tr></table></figure><p>去 web 上，网络接口，把 wan 的口子(如果没有就添加)，<code>dhcp客户端</code>，接口设选 <code>usb0</code>。添加后设置下新添加接口的 <code>防火墙设置</code> ，将其设置为 <code>wan: </code></p><h4 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h4><ul><li><a href="https://www.right.com.cn/forum/thread-220887-1-1.html">https://www.right.com.cn/forum/thread-220887-1-1.html</a></li><li><a href="https://iyzm.net/openwrt/775.html">https://iyzm.net/openwrt/775.html</a></li><li><a href="https://blog.umoe.vip/2020/12/31/bb3180dd6d20/">https://blog.umoe.vip/2020/12/31/bb3180dd6d20/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;about&quot;&gt;&lt;a href=&quot;#about&quot; class=&quot;headerlink&quot; title=&quot;about&quot;&gt;&lt;/a&gt;about&lt;/h2&gt;&lt;p&gt;记录下 openwrt 下 usb 网络的折腾，后续折腾这块内容的话也在这个文章内更新&lt;/p&gt;
&lt;h3 id=&quot;N1</summary>
      
    
    
    
    <category term="openwrt" scheme="http://zhangguanzhang.github.io/categories/openwrt/"/>
    
    
    <category term="usb-net" scheme="http://zhangguanzhang.github.io/tags/usb-net/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu18下io调度算法是cfq导致mysql非常慢</title>
    <link href="http://zhangguanzhang.github.io/2021/09/01/ubuntu18-and-cfq/"/>
    <id>http://zhangguanzhang.github.io/2021/09/01/ubuntu18-and-cfq/</id>
    <published>2021-09-01T20:04:01.000Z</published>
    <updated>2021-09-01T20:04:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录线上一次 io 调度算法导致的 mysql 读写慢问题</p><span id="more"></span><h2 id="问题由来"><a href="#问题由来" class="headerlink" title="问题由来"></a>问题由来</h2><p>同样的部署包，都是容器化的，配置和宿主机也是一样在同一台机器上，虚机的盘存储也是同一个里，mysql 的 docker 镜像是一样，配置文件也是完全一样。<br>在 ubuntu16 上我们初始化 mysql 表时间为 x ，而在 ubuntu 18 上则是 2倍以上的时间，甚至 10 倍。<br>ubuntu 的 <code>systemd-resolved</code> 我们 k8s 已经处理了的。不存在啥反向解析问题。一开始 NUMA 绑定关系不一样，调整一样后还是存在，最后查到相关资料是和 io 调度算法有关系：</p><p><a href="https://codeistry.wordpress.com/2020/01/16/ubuntu-18-04-poor-disk-read-performance/">https://codeistry.wordpress.com/2020/01/16/ubuntu-18-04-poor-disk-read-performance/</a></p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>长话短说，ubuntu 16 是 <code>deadline</code> ，18 则是 <code>cfq</code> ，后者太平均了，不适合 mysql 这种 io 密集型的业务。<code>noop</code> 则适合 ssd 和 flash 之类的。</p><p>可以下面快照查看某个路径所在的 io 调度算法:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DB_DATA_PATH=/data/mysql_data</span><br><span class="line"></span><br><span class="line">DEVICE=$( findmnt -T $&#123;DB_DATA_PATH&#125; -o SOURCE --noheadings )</span><br><span class="line">DEVICE=$&#123;DEVICE##*/&#125;</span><br><span class="line">DEVICE=$( tr -d &#x27;0-9&#x27; &lt;&lt;&lt; &quot;$&#123;DEVICE&#125;&quot;)</span><br><span class="line"># 查看 io 调度算法</span><br><span class="line">cat /sys/block/$&#123;DEVICE&#125;/queue/scheduler</span><br></pre></td></tr></table></figure><p>直接 <code>echo deadline &gt; /sys/block/sdb/queue/scheduler</code> 可以调整，不需要重启，但是重启失效，而调整内核参数 <code>elevator=deadline</code> 的话则是全局，如果机器上其他块设备是 ssd 则不合适，所以我们可以用 systemd 的 oneshot 调整，<code>/etc/fstab</code> 是 <code>systemd-remount-fs.service</code> 负责挂载的。我们的分区肯定是它挂载的，所以在它后面去执行，大概下面这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/systemd/system/mysql-block.service &lt;&lt; EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=set block scheduler for mysql</span><br><span class="line">After=systemd-remount-fs.service</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot;</span><br><span class="line">Type=oneshot</span><br><span class="line">ConditionPathExists=/sys/block/sdb</span><br><span class="line">ExecStart=/bin/bash -c &#x27;echo deadline &gt; /sys/block/sdb/queue/scheduler&#x27;</span><br><span class="line">RemainAfterExit=yes</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl enable mysql-block.service</span><br></pre></td></tr></table></figure><p>如果底层是 ssd ，但是做了 raid，无法通过 <code>rotational</code> 来判断是不是 ssd，直插使用的话为 <code>0</code> 就是 ssd。可以下面查看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/block/sda/queue/rotational</span><br><span class="line"># or</span><br><span class="line">lsblk -d -o name,rota</span><br></pre></td></tr></table></figure><h2 id="一些数据对比"><a href="#一些数据对比" class="headerlink" title="一些数据对比"></a>一些数据对比</h2><p>我们机器系统盘和数据盘，系统盘没改，测试 io 速度：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1804:~# fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat \</span><br><span class="line">   --rw=randwrite --size=500m --io_size=2g --blocksize=4k \</span><br><span class="line">   --ioengine=libaio --fsync=1 --iodepth=1 --direct=1 --numjobs=1 --runtime=60 --group_reporting</span><br><span class="line"></span><br><span class="line">TEST: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1</span><br><span class="line">fio-3.1</span><br><span class="line">Starting 1 process</span><br><span class="line">TEST: Laying out IO file (1 file / 500MiB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)][13.3%][r=0KiB/s,w=306KiB/s][r=0,w=76 IOPS][eta 00m:52s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][23.3%][r=0KiB/s,w=361KiB/s][r=0,w=90 IOPS][eta 00m:46s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][51.7%][r=0KiB/s,w=362KiB/s][r=0,w=90 IOPS][eta 00m:29s] </span><br><span class="line">Jobs: 1 (f=1): [w(1)][61.7%][r=0KiB/s,w=172KiB/s][r=0,w=43 IOPS][eta 00m:23s] </span><br><span class="line">Jobs: 1 (f=1): [w(1)][71.7%][r=0KiB/s,w=396KiB/s][r=0,w=99 IOPS][eta 00m:17s] </span><br><span class="line">Jobs: 1 (f=1): [w(1)][81.7%][r=0KiB/s,w=436KiB/s][r=0,w=109 IOPS][eta 00m:11s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][90.0%][r=0KiB/s,w=420KiB/s][r=0,w=105 IOPS][eta 00m:06s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][100.0%][r=0KiB/s,w=416KiB/s][r=0,w=104 IOPS][eta 00m:00s]</span><br><span class="line">TEST: (groupid=0, jobs=1): err= 0: pid=24257: Thu Sep  2 11:30:06 2021</span><br><span class="line">  write: IOPS=92, BW=370KiB/s (379kB/s)(21.7MiB/60006msec)</span><br><span class="line">    slat (usec): min=18, max=10744, avg=50.73, stdev=175.78</span><br><span class="line">    clat (usec): min=4, max=187002, avg=10094.71, stdev=7746.13</span><br><span class="line">     lat (usec): min=244, max=187062, avg=10146.12, stdev=7742.92</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    9],  5.00th=[    9], 10.00th=[    9], 20.00th=[    9],</span><br><span class="line">     | 30.00th=[    9], 40.00th=[    9], 50.00th=[    9], 60.00th=[    9],</span><br><span class="line">     | 70.00th=[   10], 80.00th=[   10], 90.00th=[   12], 95.00th=[   18],</span><br><span class="line">     | 99.00th=[   42], 99.50th=[   64], 99.90th=[  107], 99.95th=[  133],</span><br><span class="line">     | 99.99th=[  188]</span><br><span class="line">   bw (  KiB/s): min=    3, max=  404, per=36.51%, avg=135.07, stdev=151.88, samples=97</span><br><span class="line">   iops        : min=    0, max=  101, avg=33.30, stdev=38.04, samples=97</span><br><span class="line">  lat (usec)   : 10=0.04%, 100=0.11%, 250=0.11%, 500=0.02%</span><br><span class="line">  lat (msec)   : 10=88.37%, 20=7.97%, 50=2.56%, 100=0.68%, 250=0.14%</span><br><span class="line">  cpu          : usr=0.33%, sys=0.79%, ctx=11118, majf=0, minf=10</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued rwt: total=0,5556,0, short=0,0,0, dropped=0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=1</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=370KiB/s (379kB/s), 370KiB/s-370KiB/s (379kB/s-379kB/s), io=21.7MiB (22.8MB), run=60006-60006msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sda: ios=0/16781, merge=0/11771, ticks=0/63856, in_queue=63744, util=94.50%</span><br></pre></td></tr></table></figure><p>iops avg 是 33.30 , <code>/data</code> 目录是已经调整成 <code>deadline</code> 了，测下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1804:~# cd /data/</span><br><span class="line">root@ubuntu1804:/data# fio --name TEST --eta-newline=5s --filename=fio-tempfile.dat   --rw=randwrite --size=500m --io_size=2g --blocksize=4k   --ioengine=libaio --fsync=1 --iodepth=1 --direct=1 --numjobs=1 --runtime=60 --group_reporting</span><br><span class="line"></span><br><span class="line">TEST: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=1</span><br><span class="line">fio-3.1</span><br><span class="line">Starting 1 process</span><br><span class="line">TEST: Laying out IO file (1 file / 500MiB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)][11.7%][r=0KiB/s,w=4681KiB/s][r=0,w=1170 IOPS][eta 00m:53s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][21.7%][r=0KiB/s,w=6553KiB/s][r=0,w=1638 IOPS][eta 00m:47s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][31.1%][r=0KiB/s,w=4536KiB/s][r=0,w=1134 IOPS][eta 00m:42s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][41.0%][r=0KiB/s,w=2516KiB/s][r=0,w=629 IOPS][eta 00m:36s] </span><br><span class="line">Jobs: 1 (f=1): [w(1)][50.8%][r=0KiB/s,w=3772KiB/s][r=0,w=943 IOPS][eta 00m:30s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][60.7%][r=0KiB/s,w=8476KiB/s][r=0,w=2119 IOPS][eta 00m:24s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][85.0%][r=0KiB/s,w=7158KiB/s][r=0,w=1789 IOPS][eta 00m:09s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][95.0%][r=0KiB/s,w=6984KiB/s][r=0,w=1746 IOPS][eta 00m:03s]</span><br><span class="line">Jobs: 1 (f=1): [w(1)][100.0%][r=0KiB/s,w=3449KiB/s][r=0,w=862 IOPS][eta 00m:00s]</span><br><span class="line">TEST: (groupid=0, jobs=1): err= 0: pid=26679: Thu Sep  2 11:32:17 2021</span><br><span class="line">  write: IOPS=1462, BW=5851KiB/s (5992kB/s)(343MiB/60010msec)</span><br><span class="line">    slat (usec): min=25, max=270564, avg=275.98, stdev=3972.28</span><br><span class="line">    clat (nsec): min=1240, max=190800k, avg=140324.02, stdev=2041880.18</span><br><span class="line">     lat (usec): min=71, max=283124, avg=416.73, stdev=4716.37</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[    44],  5.00th=[    50], 10.00th=[    53], 20.00th=[    59],</span><br><span class="line">     | 30.00th=[    73], 40.00th=[    82], 50.00th=[    87], 60.00th=[    92],</span><br><span class="line">     | 70.00th=[    99], 80.00th=[   109], 90.00th=[   129], 95.00th=[   157],</span><br><span class="line">     | 99.00th=[   334], 99.50th=[   465], 99.90th=[  4555], 99.95th=[ 31327],</span><br><span class="line">     | 99.99th=[105382]</span><br><span class="line">   bw (  KiB/s): min=   93, max=11799, per=72.15%, avg=4221.47, stdev=3043.93, samples=93</span><br><span class="line">   iops        : min=   23, max= 2949, avg=1055.01, stdev=760.91, samples=93</span><br><span class="line">  lat (usec)   : 2=0.04%, 4=0.11%, 10=0.19%, 20=0.07%, 50=5.46%</span><br><span class="line">  lat (usec)   : 100=65.80%, 250=26.62%, 500=1.27%, 750=0.18%, 1000=0.06%</span><br><span class="line">  lat (msec)   : 2=0.07%, 4=0.03%, 10=0.03%, 20=0.01%, 50=0.03%</span><br><span class="line">  lat (msec)   : 100=0.02%, 250=0.01%</span><br><span class="line">  cpu          : usr=1.95%, sys=43.40%, ctx=175616, majf=0, minf=39</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued rwt: total=0,87784,0, short=0,0,0, dropped=0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=1</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=5851KiB/s (5992kB/s), 5851KiB/s-5851KiB/s (5992kB/s-5992kB/s), io=343MiB (360MB), run=60010-60010msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sdb: ios=30/263733, merge=0/186890, ticks=948/55212, in_queue=55912, util=79.04%</span><br></pre></td></tr></table></figure><p>iops avg 是 1055.01</p><h2 id="blktrace"><a href="#blktrace" class="headerlink" title="blktrace"></a>blktrace</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apt-get install blktrace</span><br><span class="line"># 生成采集文件</span><br><span class="line">blktrace -d /dev/sdb</span><br><span class="line"># 合并成一个二进制文件</span><br><span class="line">blkparse -i sdb -d sdb.blktrace.bin</span><br><span class="line"># btt 协助统计分析</span><br><span class="line">btt -i sdb.blktrace.bin -l sdb.d2c_latencybtt -i sdb.blktrace.bin -q sdb.q2c_latency</span><br><span class="line"></span><br><span class="line">Q2G – 生成IO请求所消耗的时间，包括remap和split的时间</span><br><span class="line">G2I – IO请求进入IO Scheduler所消耗的时间，包括merge的时间 </span><br><span class="line">I2D – IO请求在IO Scheduler中等待的时间</span><br><span class="line">D2C – IO请求在driver和硬件上所消耗的时间</span><br><span class="line">Q2C – 整个IO请求所消耗的时间(G2I + I2D + D2C = Q2C)，相当于iostat的await</span><br><span class="line">其中D2C可以作为硬件性能的指标，I2D可以作为IO Scheduler性能的指标</span><br><span class="line">上述命令其实还会产生一些.dat文件，可以看到iops信息</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://codeistry.wordpress.com/2020/01/16/ubuntu-18-04-poor-disk-read-performance/">Ubuntu 18.04 : Poor disk read performance</a></li><li><a href="https://developer.aliyun.com/article/698568">blktrace 用法</a></li><li><a href="https://www.cnblogs.com/cobbliu/p/5389556.html">ubuntu io 调度算法介绍</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录线上一次 io 调度算法导致的 mysql 读写慢问题&lt;/p&gt;</summary>
    
    
    
    <category term="ubuntu18" scheme="http://zhangguanzhang.github.io/categories/ubuntu18/"/>
    
    
    <category term="ubuntu18" scheme="http://zhangguanzhang.github.io/tags/ubuntu18/"/>
    
  </entry>
  
  <entry>
    <title>干掉烦人的 open /run/xtables.lock: is a directory</title>
    <link href="http://zhangguanzhang.github.io/2021/08/27/k8s-runxtables.lock/"/>
    <id>http://zhangguanzhang.github.io/2021/08/27/k8s-runxtables.lock/</id>
    <published>2021-08-27T14:28:30.000Z</published>
    <updated>2021-08-27T14:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>suse 这辣鸡系统，使用官方文档 docker-static 的二进制安装的话会无法起来，所以我们在 suse 上用的是很久之前的 rpm 安装的 docker。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/os-release </span><br><span class="line">NAME=&quot;SLES&quot;</span><br><span class="line">VERSION=&quot;12-SP5&quot;</span><br><span class="line">VERSION_ID=&quot;12.5&quot;</span><br><span class="line">PRETTY_NAME=&quot;SUSE Linux Enterprise Server 12 SP5&quot;</span><br><span class="line">ID=&quot;sles&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;32&quot;</span><br><span class="line">CPE_NAME=&quot;cpe:/o:suse:sles:12:sp5&quot;</span><br><span class="line"></span><br><span class="line">$ rpm -qa | grep docker</span><br><span class="line">docker-17.09.1_ce-98.18.1.x86_64</span><br><span class="line">docker-libnetwork-0.7.0.1+gitr2066_7b2b1feb1de4-10.1.x86_64</span><br><span class="line">docker-runc-1.0.0rc4+gitr3338_3f2f8b84a77f-1.3.1.x86_64</span><br><span class="line"></span><br><span class="line">$ docker info</span><br><span class="line">Containers: 62</span><br><span class="line"> Running: 35</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 27</span><br><span class="line">Images: 89</span><br><span class="line">Server Version: 17.09.1-ce</span><br><span class="line">Storage Driver: btrfs</span><br><span class="line"> Build Version: Btrfs v3.18.2+20150430</span><br><span class="line"> Library Version: 101</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: oci runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 06b9cb35161009dcb7123345749fef02f7cea8e0</span><br><span class="line">runc version: 3f2f8b84a77f73d38244dd690525642a72156c64</span><br><span class="line">init version: v0.1.3_catatonit (expected: 949e6facb77383876aeff8a6944dde66b3089574)</span><br><span class="line">Security Options:</span><br><span class="line"> apparmor</span><br><span class="line">Kernel Version: 4.12.14-120-default</span><br><span class="line">Operating System: SUSE Linux Enterprise Server 12 SP5</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 16</span><br><span class="line">Total Memory: 62.67GiB</span><br><span class="line">Name: SUSESP5</span><br><span class="line">ID: GZWM:UWPN:INDR:SICH:UD6H:RKPG:WRYK:5YDY:6723:I2HR:UPWI:KI6W</span><br><span class="line">Docker Root Dir: /data/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line"></span><br><span class="line">WARNING: No swap limit support</span><br></pre></td></tr></table></figure><p>问题是部署后业务无法解析域名，排查了下是 svc 的 iptables 规则没生成，最后在 kube-proxy （二进制部署的）的日志发现如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Aug 27 10:41:58 SUSESP5 kube-proxy[1666]: E0827 10:41:58.480578    1666 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:41:58 SUSESP5 kube-proxy[1666]: I0827 10:41:58.480614    1666 proxier.go:876] Sync failed; retrying in 18s</span><br><span class="line">Aug 27 10:42:16 SUSESP5 kube-proxy[1666]: E0827 10:42:16.502207    1666 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:42:16 SUSESP5 kube-proxy[1666]: I0827 10:42:16.502266    1666 proxier.go:876] Sync failed; retrying in 18s</span><br><span class="line">Aug 27 10:42:24 SUSESP5 kube-proxy[1666]: E0827 10:42:24.806734    1666 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:42:24 SUSESP5 kube-proxy[1666]: I0827 10:42:24.806768    1666 proxier.go:876] Sync failed; retrying in 18s</span><br></pre></td></tr></table></figure><p>我们的 ipset 容器 <code>-v</code> 挂载了这个 iptables 的锁文件，众所周知 -v 的 src 如果不存在就会被 <code>mkdir -p</code> 。找了个干净的环境，确认了下 iptables 命令（即使啥选项都不带都）会生成该锁文件:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ rm -f /run/xtables.lock</span><br><span class="line">$ ll /run/xtables.lock</span><br><span class="line">ls: cannot access &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">$ iptables</span><br><span class="line">iptables v1.8.2 (legacy): no command specified</span><br><span class="line">Try `iptables -h&#x27; or &#x27;iptables --help&#x27; for more information.</span><br><span class="line">$ ll /run/xtables.lock</span><br><span class="line">-rw------- 1 root root 0 Aug 27 10:27 /run/xtables.lock</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>说个题外话，我们其他 os 上没有这个问题</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>其实昨天就在 kube-proxy 的 systemd 里加了下面，但是今天重启测试了下还是发生了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /run/xtables.lock &amp;&amp; rmdir /run/xtables.lock || true&#x27;</span><br></pre></td></tr></table></figure><p>手动删除目录的话，因为 kube-proxy 时刻调用 iptables ，删除后该锁文件就会被创建。但是总不能每次开机弄下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ rmdir /run/xtables.lock</span><br><span class="line">$ ll /run/xtables.lock</span><br><span class="line">ls: cannot access &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">$ ll /run/xtables.lock</span><br><span class="line">-rw------- 1 root root 0 Aug 27 10:43 /run/xtables.lock</span><br></pre></td></tr></table></figure><p>我们在 suse 的安装下给 systemd 加了个子配置文件，这个 <code>ExecStartPre</code> 是解决另一个问题的，为啥这样自己去猜。所以一开始是我的想法是再加个 <code>ExecStartPre</code> 解决</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line">$ vi /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /run/xtables.lock &amp;&amp; rmdir /run/xtables.lock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;iptables -V &amp;&gt;/dev/null || true&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">$ reboot</span><br></pre></td></tr></table></figure><p>重启后开机还是这样。。。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Aug 27 10:47:31 SUSESP5 kube-proxy[1680]: I0827 10:47:31.265099    1680 shared_informer.go:247] Caches are synced for node config</span><br><span class="line">Aug 27 10:47:31 SUSESP5 kube-proxy[1680]: I0827 10:47:31.865070    1680 shared_informer.go:247] Caches are synced for service config</span><br><span class="line">Aug 27 10:47:31 SUSESP5 kube-proxy[1680]: E0827 10:47:31.893976    1680 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:47:31 SUSESP5 kube-proxy[1680]: I0827 10:47:31.894015    1680 proxier.go:876] Sync failed; retrying in 18s</span><br><span class="line">Aug 27 10:47:32 SUSESP5 kube-proxy[1680]: E0827 10:47:32.632956    1680 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:47:32 SUSESP5 kube-proxy[1680]: I0827 10:47:32.632986    1680 proxier.go:876] Sync failed; retrying in 18s</span><br><span class="line">Aug 27 10:47:39 SUSESP5 kube-proxy[1680]: E0827 10:47:39.109972    1680 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:47:39 SUSESP5 kube-proxy[1680]: I0827 10:47:39.110010    1680 proxier.go:876] Sync failed; retrying in 18s</span><br><span class="line">Aug 27 10:47:39 SUSESP5 kube-proxy[1680]: E0827 10:47:39.128309    1680 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:47:39 SUSESP5 kube-proxy[1680]: I0827 10:47:39.128340    1680 proxier.go:876] Sync failed; retrying in 18s</span><br><span class="line">Aug 27 10:47:40 SUSESP5 kube-proxy[1680]: E0827 10:47:40.150827    1680 proxier.go:1595] Failed to execute iptables-restore: failed to open iptables lock /run/xtables.lock: open /run/xtables.lock: is a directory</span><br><span class="line">Aug 27 10:47:40 SUSESP5 kube-proxy[1680]: I0827 10:47:40.150873    1680 proxier.go:876] Sync failed; retrying in 18s</span><br></pre></td></tr></table></figure><p>确认了下子配置文件是生效了的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl cat docker</span><br><span class="line"># /etc/systemd/system/docker.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=http://docs.docker.com</span><br><span class="line">After=network.target containerd.socket containerd.service lvm2-monitor.service SuSEfirewall2.service</span><br><span class="line">Requires=containerd.socket containerd.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=/etc/sysconfig/docker</span><br><span class="line"></span><br><span class="line"># While Docker has support for socket activation (-H fd://), this is not</span><br><span class="line"># enabled by default because enabling socket activation means that on boot your</span><br><span class="line"># containers won&#x27;t start until someone tries to administer the Docker daemon.</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/usr/bin/dockerd --containerd /run/containerd/containerd.sock --add-runtime oci=/usr/sbin/docker-runc $DOCKER_NETWORK_OPTIONS $DOCKER_OPTS</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line"></span><br><span class="line"># Having non-zero Limit*s causes performance problems due to accounting overhead</span><br><span class="line"># in the kernel. We recommend using cgroups to do container-local accounting.</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line"></span><br><span class="line"># Uncomment TasksMax if your systemd version supports it.</span><br><span class="line"># Only systemd 226 and above support this property.</span><br><span class="line">TasksMax=infinity</span><br><span class="line"></span><br><span class="line"># Set delegate yes so that systemd does not reset the cgroups of docker containers</span><br><span class="line"># Only systemd 218 and above support this property.</span><br><span class="line">Delegate=yes</span><br><span class="line"></span><br><span class="line"># This is not necessary because of how we set up containerd.</span><br><span class="line">#KillMode=process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /run/xtables.lock &amp;&amp; rmdir /run/xtables.lock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;iptables -V &amp;&gt;/dev/null || true&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br></pre></td></tr></table></figure><p>看下状态，确定是执行了的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/docker.service.d</span><br><span class="line">           └─10.docker.conf</span><br><span class="line">   Active: active (running) since Fri 2021-08-27 10:47:31 CST; 50s ago</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line">  Process: 1685 ExecStartPre=/bin/bash -c iptables -V &amp;&gt;/dev/null || true (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1681 ExecStartPre=/bin/bash -c test -d /run/xtables.lock &amp;&amp; rmdir /run/xtables.lock || true (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1675 ExecStartPre=/bin/bash -c test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1740 (dockerd)</span><br><span class="line">    Tasks: 81</span><br><span class="line">   Memory: 318.9M</span><br><span class="line">      CPU: 11.734s</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           └─1740 /usr/bin/dockerd --containerd /run/containerd/containerd.sock --add-runtime oci=/usr/sbin/docker-runc</span><br></pre></td></tr></table></figure><p>一开始怀疑是不是顺序问题，测试了下是对的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /run/xtables.lock &amp;&amp; rmdir /run/xtables.lock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;iptables -V &amp;&gt;/dev/null || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;echo 1 &gt;&gt; /root/test.file&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;echo 2 &gt;&gt; /root/test.file&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;echo 3 &gt;&gt; /root/test.file&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;echo 4 &gt;&gt; /root/test.file&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;echo 5 &gt;&gt; /root/test.file&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$ cat /root/test.file </span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/docker.service.d</span><br><span class="line">           └─10.docker.conf</span><br><span class="line">   Active: active (running) since Fri 2021-08-27 11:03:18 CST; 28s ago</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line">  Process: 1777 ExecStartPre=/bin/bash -c echo 5 &gt;&gt; /root/test.file (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1769 ExecStartPre=/bin/bash -c echo 4 &gt;&gt; /root/test.file (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1766 ExecStartPre=/bin/bash -c echo 3 &gt;&gt; /root/test.file (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1758 ExecStartPre=/bin/bash -c echo 2 &gt;&gt; /root/test.file (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1750 ExecStartPre=/bin/bash -c echo 1 &gt;&gt; /root/test.file (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1684 ExecStartPre=/bin/bash -c iptables -V &amp;&gt;/dev/null || true (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1681 ExecStartPre=/bin/bash -c test -d /run/xtables.lock &amp;&amp; rmdir /run/xtables.lock || true (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1676 ExecStartPre=/bin/bash -c test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1783 (dockerd)</span><br><span class="line">    Tasks: 77</span><br><span class="line">   Memory: 304.5M</span><br><span class="line">      CPU: 10.183s</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           └─1783 /usr/bin/dockerd --containerd /run/containerd/containerd.sock --add-runtime oci=/usr/sbin/docker-runc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ ll /run/xtables.lock</span><br><span class="line">total 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>去掉 iptables 的输出重定向看看执行了没</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;rmdir /run/xtables.lock &amp;&gt;/dev/null &amp;&amp; iptables -V || true&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">$ reboot</span><br></pre></td></tr></table></figure><p>确实执行了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ journalctl -xe -u docker |&amp; grep iptables</span><br><span class="line">Aug 27 11:08:20 SUSESP5 bash[1705]: iptables v1.4.21</span><br></pre></td></tr></table></figure><p>有点绝了，然后试了下 audit 审计，但是查不出啥信息，只能继续查了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;-w /run/xtables.lock -prw &#x27; &gt; /etc/audit/rules.d/zgz.rules</span><br></pre></td></tr></table></figure><p>加个 <code>stat</code> 看看</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;stat /run/xtables.lock&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;rmdir /run/xtables.lock &amp;&gt;/dev/null &amp;&amp; iptables -V || true&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$ journalctl -xe -u docker |&amp; grep /run/xta</span><br><span class="line">Aug 27 11:08:25 SUSESP5 dockerd[1765]: time=&quot;2021-08-27T11:08:25.564282010+08:00&quot; level=error msg=&quot;Failed to start container e74f20c98617fa60b13981513b434cdb3139f2e3b406ec521b48ec81958cebe0: oci runtime error: container_linux.go:265: starting container process caused \&quot;process_linux.go:368: container init caused \\&quot;rootfs_linux.go:57: mounting \\\\&quot;/run/xtables.lock\\\\&quot; to rootfs \\\\&quot;/data/kube/docker/btrfs/subvolumes/7035bbf1baba2ef7cee552cd2fdda6841a22d4bdb62bd66b81f4b62c78b65735\\\\&quot; at \\\\&quot;/data/kube/docker/btrfs/subvolumes/7035bbf1baba2ef7cee552cd2fdda6841a22d4bdb62bd66b81f4b62c78b65735/run/xtables.lock\\\\&quot; caused \\\\&quot;not a directory\\\\&quot;\\&quot;\&quot;</span><br><span class="line">Aug 27 11:11:32 SUSESP5 dockerd[1679]: time=&quot;2021-08-27T11:11:32.556235556+08:00&quot; level=error msg=&quot;Failed to start container e74f20c98617fa60b13981513b434cdb3139f2e3b406ec521b48ec81958cebe0: oci runtime error: container_linux.go:265: starting container process caused \&quot;process_linux.go:368: container init caused \\&quot;rootfs_linux.go:57: mounting \\\\&quot;/run/xtables.lock\\\\&quot; to rootfs \\\\&quot;/data/kube/docker/btrfs/subvolumes/7035bbf1baba2ef7cee552cd2fdda6841a22d4bdb62bd66b81f4b62c78b65735\\\\&quot; at \\\\&quot;/data/kube/docker/btrfs/subvolumes/7035bbf1baba2ef7cee552cd2fdda6841a22d4bdb62bd66b81f4b62c78b65735/run/xtables.lock\\\\&quot; caused \\\\&quot;not a directory\\\\&quot;\\&quot;\&quot;</span><br><span class="line">Aug 27 11:33:03 SUSESP5 dockerd[1705]: time=&quot;2021-08-27T11:33:03.048807247+08:00&quot; level=error msg=&quot;Failed to start container e74f20c98617fa60b13981513b434cdb3139f2e3b406ec521b48ec81958cebe0: oci runtime error: container_linux.go:265: starting container process caused \&quot;process_linux.go:368: container init caused \\&quot;rootfs_linux.go:57: mounting \\\\&quot;/run/xtables.lock\\\\&quot; to rootfs \\\\&quot;/data/kube/docker/btrfs/subvolumes/7035bbf1baba2ef7cee552cd2fdda6841a22d4bdb62bd66b81f4b62c78b65735\\\\&quot; at \\\\&quot;/data/kube/docker/btrfs/subvolumes/7035bbf1baba2ef7cee552cd2fdda6841a22d4bdb62bd66b81f4b62c78b65735/run/xtables.lock\\\\&quot; caused \\\\&quot;not a directory\\\\&quot;\\&quot;\&quot;</span><br><span class="line">Aug 27 11:36:28 SUSESP5 bash[1684]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:36:38 SUSESP5 bash[2265]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:36:48 SUSESP5 bash[2272]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:36:59 SUSESP5 bash[2295]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:37:09 SUSESP5 bash[2327]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:37:19 SUSESP5 bash[2356]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:37:29 SUSESP5 bash[2385]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br><span class="line">Aug 27 11:37:40 SUSESP5 bash[2475]: stat: cannot stat &#x27;/run/xtables.lock&#x27;: No such file or directory</span><br></pre></td></tr></table></figure><p>从日志看出 rmdir 之前路径 <code>/run/xtables.lock</code> 确实不存在，那就不用逻辑或了，直接下面这样始终执行 iptables </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;rmdir /run/xtables.lock &amp;&gt;/dev/null || true; iptables -V 1&gt;/dev/null&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line">$ reboot</span><br></pre></td></tr></table></figure><p>结果还是变成目录，猜测了下应该试 suse 上的 iptables 某些情况不会生成锁文件。所以后面加个 touch 得了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ stat /run/xtables.lock</span><br><span class="line">  File: &#x27;/run/xtables.lock&#x27;</span><br><span class="line">  Size: 40        Blocks: 0          IO Block: 4096   directory</span><br><span class="line">Device: 16h/22dInode: 14292       Links: 2</span><br><span class="line">Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Access: 2021-08-27 11:41:08.309257362 +0800</span><br><span class="line">Modify: 2021-08-27 11:41:08.309257362 +0800</span><br><span class="line">Change: 2021-08-27 11:41:08.309257362 +0800</span><br><span class="line"> Birth: -</span><br><span class="line"></span><br><span class="line">$ vi /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;rmdir /run/xtables.lock &amp;&gt;/dev/null || true; iptables -V 1&gt;/dev/null; touch -m 0600 /run/xtables.lock&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">$ reboot</span><br></pre></td></tr></table></figure><p>结果可以了，但是 touch 的权限不生效</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ stat /run/xtables.lock </span><br><span class="line">  File: &#x27;/run/xtables.lock&#x27;</span><br><span class="line">  Size: 0         Blocks: 0          IO Block: 4096   regular empty file</span><br><span class="line">Device: 16h/22dInode: 12019       Links: 1</span><br><span class="line">Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Access: 2021-08-27 11:50:30.056416389 +0800</span><br><span class="line">Modify: 2021-08-27 11:50:30.056416389 +0800</span><br><span class="line">Change: 2021-08-27 11:50:30.056416389 +0800</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure><p>最终的解决办法，加个 chmod ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/systemd/system/docker.service.d/10.docker.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;test -d /var/run/docker.sock &amp;&amp; rmdir /var/run/docker.sock || true&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;rmdir /run/xtables.lock &amp;&gt;/dev/null || true; iptables -V 1&gt;/dev/null; touch /run/xtables.lock&#x27;</span><br><span class="line">ExecStartPre=/bin/bash -c &#x27;chmod 0600 /run/xtables.lock&#x27;</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=10</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$ stat /run/xtables.lock</span><br><span class="line">  File: &#x27;/run/xtables.lock&#x27;</span><br><span class="line">  Size: 0         Blocks: 0          IO Block: 4096   regular empty file</span><br><span class="line">Device: 16h/22dInode: 26741       Links: 1</span><br><span class="line">Access: (0600/-rw-------)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Access: 2021-08-27 12:18:53.097143334 +0800</span><br><span class="line">Modify: 2021-08-27 12:18:53.097143334 +0800</span><br><span class="line">Change: 2021-08-27 12:18:53.101143334 +0800</span><br><span class="line"> Birth: -</span><br></pre></td></tr></table></figure><p>理论上可以用<code>ConditionPathIsDirectory=!/run/xtables.lock</code>，但是我就不去试了。主要是排错思路分享下。</p><h2 id="2021、08、30"><a href="#2021、08、30" class="headerlink" title="2021、08、30"></a>2021、08、30</h2><p>suse 这个 iptables 锁文件没生成可能是因为 iptables 是 1.40 版本开始加入了锁文件，suse sp12 和 Centos7.8 这个上面的 iptables 的版本是 v1.4.21 。测试了下 iptables -V 不会生成，而 1.8 版本的 iptables 带不带任何选项下都会没锁则创建锁文件。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;p&gt;suse 这辣鸡系统，使用官方文档 docker-static 的二进制安装的话会无法起来，所以我们在 suse 上用的是很</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>flannel下集群有个节点网络不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2021/08/25/flannel-a-host-net-tmout/"/>
    <id>http://zhangguanzhang.github.io/2021/08/25/flannel-a-host-net-tmout/</id>
    <published>2021-08-25T15:08:06.000Z</published>
    <updated>2021-08-25T15:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障"><a href="#故障" class="headerlink" title="故障"></a>故障</h2><p>问题和版本没关系，客户的 node 信息啥的后面排错里有。有个节点通信有问题，其余节点都没问题。</p><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><h3 id="惯例信息"><a href="#惯例信息" class="headerlink" title="惯例信息"></a>惯例信息</h3><p>先看下 <code>flannel</code> 的 <code>vxlan</code> 的 <code>vtep</code> 信息，客户是双网卡的，但是默认路由是这个网卡，不用管另外的网卡了。下面信息看了下 <code>VtepMAC</code> 和 <code>public-ip</code> 都正常。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node -o yaml | grep -B4 public</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;76:21:69:41:de:fe&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.51</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;b6:61:5c:8d:d9:eb&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.52</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;1e:8c:3e:12:fc:0f&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.53</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;ba:fe:64:36:6e:a1&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.54</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;8e:c1:4d:18:e5:d6&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.55</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;fe:95:e6:bf:a0:62&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.56</span><br></pre></td></tr></table></figure><p>coredns 的 pod ip 和 node 分布情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get pod -o wide</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5757945748-cr67w              1/1     Running   0          19h   172.27.2.7   10.25.1.56   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-krwfd              1/1     Running   0          19h   172.27.1.4   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-zf4zm              1/1     Running   0          19h   172.27.3.7   10.25.1.54   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="排查-1"><a href="#排查-1" class="headerlink" title="排查"></a>排查</h3><p>curl 下 coredns 的 metrics 接口试试，只有 <code>10.25.1.51</code> 和其他节点无法通信。会导致下面的 curl 卡住。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl  172.27.1.4:9153</span><br></pre></td></tr></table></figure><p>目标机器 <code>10.25.1.55</code> 上通过 flannel.1 接口抓我们的 curl 包:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i flannel.1 host 172.27.1.4 and port 9153 -vv</span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:07:46.203094 IP (tos 0x0, ttl 64, id 56025, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57888 &gt; 172.27.1.4.9153: Flags [S], cksum 0x6804 (correct), seq 879302783, win 28200, options [mss 1410,sackOK,TS val 56279718 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:07:46.203173 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.1.4.9153 &gt; 172.27.0.0.57888: Flags [S.], cksum 0x5969 (incorrect -&gt; 0x163b), seq 4197245653, ack 879302784, win 27960, options [mss 1410,sackOK,TS val 431774697 ecr 56279718,nop,wscale 7], length 0</span><br><span class="line">10:07:47.204797 IP (tos 0x0, ttl 64, id 56026, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57888 &gt; 172.27.1.4.9153: Flags [S], cksum 0x641a (correct), seq 879302783, win 28200, options [mss 1410,sackOK,TS val 56280720 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:07:47.204880 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.1.4.9153 &gt; 172.27.0.0.57888: Flags [S.], cksum 0x5969 (incorrect -&gt; 0x1251), seq 4197245653, ack 879302784, win 27960, options [mss 1410,sackOK,TS val 431775699 ecr 56279718,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>看着是回复了报文 <code>172.27.1.4.9153 &gt; 172.27.0.0.57888</code>，在我们 curl 的机器 <code>10.25.1.51</code>上 <code>lsof -nPi :57888</code> 看到的确实是卡住的 curl 命令 pid 。<code>10.25.1.51</code> 上也同时抓包看下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i flannel.1 host 172.27.1.4 and port 9153 -vv</span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:08:57.241129 IP (tos 0x0, ttl 64, id 34444, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57966 &gt; 172.27.1.4.9153: Flags [S], cksum 0x5969 (incorrect -&gt; 0x2fb2), seq 276913734, win 28200, options [mss 1410,sackOK,TS val 56350922 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:08:58.242423 IP (tos 0x0, ttl 64, id 34445, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57966 &gt; 172.27.1.4.9153: Flags [S], cksum 0x5969 (incorrect -&gt; 0x2bc8), seq 276913734, win 28200, options [mss 1410,sackOK,TS val 56351924 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:09:00.246423 IP (tos 0x0, ttl 64, id 34446, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57966 &gt; 172.27.1.4.9153: Flags [S], cksum 0x5969 (incorrect -&gt; 0x23f4), seq 276913734, win 28200, options [mss 1410,sackOK,TS val 56353928 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>没收到包，从 <code>eth1</code> 抓下 <code>flannel</code> 的 <code>8475</code> 端口(配置里我们改了 flannel 的端口)试试:</p><p>目标机器 <code>10.25.1.55</code> 上抓包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth1 host 10.25.1.51 and port 8475 -vvv</span><br><span class="line">tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:09:40.966705 IP (tos 0x0, ttl 64, id 50110, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.42770 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:40.966869 IP (tos 0x0, ttl 64, id 46192, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:41.968322 IP (tos 0x0, ttl 64, id 50327, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.42770 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:41.968440 IP (tos 0x0, ttl 64, id 46957, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:43.099646 IP (tos 0x0, ttl 64, id 47316, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:43.972322 IP (tos 0x0, ttl 64, id 51119, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.42770 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:43.972454 IP (tos 0x0, ttl 64, id 47934, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>目标机器 <code>10.25.1.51</code> 上抓包:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth1 host 10.25.1.55 and port 8475 -vvv</span><br><span class="line">tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:10:21.702308 IP (tos 0x0, ttl 64, id 6079, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.59558 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:10:22.702441 IP (tos 0x0, ttl 64, id 6117, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.59558 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:10:24.706444 IP (tos 0x0, ttl 64, id 7699, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.59558 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br></pre></td></tr></table></figure><p>完全没报文过来，看了下 <code>flannel</code> 的接口流量压根就没收到任何包:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ifconfig flannel.1</span><br><span class="line">flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br><span class="line">        inet 172.27.0.0  netmask 255.255.255.255  broadcast 0.0.0.0</span><br><span class="line">        inet6 fe80::7421:69ff:fe41:defe  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 76:21:69:41:de:fe  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 28900  bytes 2113052 (2.0 MiB)</span><br><span class="line">        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><p>说明报文从 <code>10.25.1.55</code> 发出后没到 51 上，让客户开通 <code>udp 8475 10.25.1.0/24</code> 整个段的东西向安全组后就正常了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl  172.27.1.4:9153</span><br><span class="line">^C</span><br><span class="line">$ curl  172.27.1.4:9153</span><br><span class="line">404 page not found</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;故障&quot;&gt;&lt;a href=&quot;#故障&quot; class=&quot;headerlink&quot; title=&quot;故障&quot;&gt;&lt;/a&gt;故障&lt;/h2&gt;&lt;p&gt;问题和版本没关系，客户的 node 信息啥的后面排错里有。有个节点通信有问题，其余节点都没问题。&lt;/p&gt;
&lt;h2 id=&quot;排查&quot;&gt;&lt;a hr</summary>
      
    
    
    
    
    <category term="flannel" scheme="http://zhangguanzhang.github.io/tags/flannel/"/>
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>一次 cni-plugins 导致集群 dns 无法解析的排错</title>
    <link href="http://zhangguanzhang.github.io/2021/08/24/cni-plugins-bridge-err/"/>
    <id>http://zhangguanzhang.github.io/2021/08/24/cni-plugins-bridge-err/</id>
    <published>2021-08-24T13:08:06.000Z</published>
    <updated>2021-08-24T13:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>环境是 1.15.5 的 x86_64 的 k8s 。命令输出被我查看日志给冲掉了，大致描述下。<br>中间件 kafka 无法连上 zookeeper ，看了下日志报错域名无法解析。看了下 coredns 都挂了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span><br><span class="line">NAME                       READY   STATUS             RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5757945748-l2d2g   0/1     CrashLoopBackOff   254        3d11h   172.27.0.2   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-w5pfx   0/1     CrashLoopBackOff   254        3d11h   172.27.0.5   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-wfndd   0/1     CrashLoopBackOff   254        3d11h   172.27.0.3   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>查看了日志是报错无法连 kubernetes svc <code>https://172.26.0.1:443/xxxx</code> ，报错 <code>No route to host</code></p><h2 id="排错"><a href="#排错" class="headerlink" title="排错"></a>排错</h2><h3 id="基本排查"><a href="#基本排查" class="headerlink" title="基本排查"></a>基本排查</h3><p>去节点 <code>10.25.1.55</code> 上 <code>docker ps -a | grep coredns</code> 找 pause 的容器 id ，<code>docker inspect xxxxx | grep -m1 -i pid</code> 取进程 pid。然后 nsenter 进去</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ nsenter --net --target 14659 </span><br><span class="line">$ ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 42:7d:b0:83:a9:aa brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.27.0.5/16 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>curl 了下 svc ip <code>https://172.26.0.1</code> 发现报错 <code>No route to host</code>。然后直接用 ep 也就是 kube-apiserver 的真实 ip和端口访问下，发现不通</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curk -kvL https://10.25.1.51:6443</span><br></pre></td></tr></table></figure><p>然后 ping 下宿主机发现也不通。看了下转发都开了。然后也看了下也没安全软件。</p><h3 id="桥接"><a href="#桥接" class="headerlink" title="桥接"></a>桥接</h3><p>然后看了下桥接发现了问题所在，先写下正常环境桥接信息。我们 flannel，不是二进制，容器都是在 cni-plugins 下桥接在 cni0 上的。下面找个正常环境的 coredns 做下信息展示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 取容器 pid</span><br><span class="line">$ docker inspect d30 | grep -m1 -i pid</span><br><span class="line">            &quot;Pid&quot;: 9079,</span><br><span class="line"></span><br><span class="line">$ nsenter --net --target 9079 ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if210: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 06:3e:42:00:91:33 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.27.0.74/24 brd 172.27.0.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>注意看 <code>if</code> 后面的数字，宿主机上查看下是哪个 <code>veth</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ip link | grep -E &#x27;^210&#x27;</span><br><span class="line">210: vetha1ca1d55@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default</span><br></pre></td></tr></table></figure><p>使用 brctl 看下 cni0 下是有这个的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brctl show cni0 | grep vetha1ca1d55</span><br><span class="line">vetha1ca1d55</span><br></pre></td></tr></table></figure><h4 id="机器异常的桥接信息"><a href="#机器异常的桥接信息" class="headerlink" title="机器异常的桥接信息"></a>机器异常的桥接信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ nsenter --net --target 14659 ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 42:7d:b0:83:a9:aa brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.27.0.5/16 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">$ ip -o link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT qlen 1\    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc pfifo_fast state UP mode DEFAULT qlen 1000\    link/ether fa:16:3e:35:09:13 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000\    link/ether fa:16:3e:e2:4d:8d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT \    link/ether 02:42:3c:a2:d7:3b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">6: vethda399ca1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether a2:95:74:65:29:6d brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">7: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT \    link/ether 0a:e5:a5:9a:66:8b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: veth96d8f326@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether 92:a5:9c:7c:dd:cb brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">9: vethdf8eb371@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether 4a:fb:37:c3:f2:7e brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line">10: veth44ce32f4@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether 6e:96:73:2b:4e:52 brd ff:ff:ff:ff:ff:ff link-netnsid 3</span><br><span class="line">11: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT qlen 1000\    link/ether 2a:f9:74:b7:11:b8 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">12: veth0f91b3a3@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT \    link/ether 0a:36:90:dc:0a:97 brd ff:ff:ff:ff:ff:ff link-netnsid 4</span><br></pre></td></tr></table></figure><p>查看了下 cni0 压根对不上 <code>veth44ce32f4</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ip a s cni0</span><br><span class="line">11: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP qlen 1000</span><br><span class="line">    link/ether 2a:f9:74:b7:11:b8 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.27.1.1/24 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::28f9:74ff:feb7:11b8/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">$ brctl show</span><br><span class="line">bridge name     bridge id               STP enabled     interfaces</span><br><span class="line">cni0            8000.2af974b711b8       no              veth0f91b3a3</span><br><span class="line">docker0         8000.02423ca2d73b       no</span><br></pre></td></tr></table></figure><p>应该是桥接错了，重新创建下发现好了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system delete pod -l k8s-app=kube-dns</span><br><span class="line">pod &quot;coredns-5757945748-l2d2g&quot; deleted</span><br><span class="line">pod &quot;coredns-5757945748-wfndd&quot; deleted</span><br><span class="line">pod &quot;coredns-5757945748-wjrdf&quot; deleted</span><br><span class="line">$ kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5757945748-4smll   1/1     Running   0          29s   172.27.1.3   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-d9wqk   1/1     Running   0          29s   172.27.3.6   10.25.1.54   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-dtvfl   1/1     Running   0          29s   172.27.2.6   10.25.1.56   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现场拿了下 cni-plugins 的校验值去查了下，我们使用的是 <code>v0.7.0</code> 版本。有必要升级下了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h2&gt;&lt;p&gt;环境是 1.15.5 的 x86_64 的 k8s 。命令输出被我查看日志给冲掉了，大致描述下。&lt;br&gt;中间件 kafka 无法连上 zoo</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/tags/kubernetes/"/>
    
    <category term="cni-plugins" scheme="http://zhangguanzhang.github.io/tags/cni-plugins/"/>
    
  </entry>
  
  <entry>
    <title>kubelet 为系统配置预留资源</title>
    <link href="http://zhangguanzhang.github.io/2021/08/16/reserve-compute-resources/"/>
    <id>http://zhangguanzhang.github.io/2021/08/16/reserve-compute-resources/</id>
    <published>2021-08-16T10:08:06.000Z</published>
    <updated>2021-08-16T10:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>我们环境有部分 pod 特殊，单独节点部署，oom 的时候会搞挂一些系统进程，这几天折腾了下配置了下 kubelet 相关的 <code>reserved</code>。主要是 kubelet 的配置文件一些参数，不写 systemd 里，全部写配置文件里。版本是如下，因为我们不单单是 <code>x86_64</code> ，由于还有其他的架构以及会部署在客户的现场，为了减少维护，所以我们都是除了 <code>flanneld</code> 和 <code>coredns</code> 以外。k8s 相关的二进制的形式部署的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;20&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.20.6&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;8a62859e515889f07e3e3be6a1080413f17cf2c3&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2021-04-15T03:28:42Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.15.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;20&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.20.6&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;8a62859e515889f07e3e3be6a1080413f17cf2c3&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2021-04-15T03:19:55Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.15.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>阅读本篇文章之前，推荐先浏览器同时打开这两篇官方文档后稍微看完再看本篇文章:</p><ul><li><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/">官方文档</a> 和 </li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">最初的设计文档</a></li></ul><h3 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h3><p>相关术语就是 <code>enforceNodeAllocatable</code> ，它的默认值是 <code>[&quot;pods&quot;]</code> ，也就是 pod 能够使用节点上所有资源。但是节点上除了自己以外还有 kubelet ，kube 的三个组件，container runtime engine，以及 systemd 纳管的一些系统进程。如果有个 node 达到资源满了被驱逐，可能会漂移到其他节点上，把其他节点也搞挂了，形成连锁雪崩的情况。根据 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">官方文档最开始的设计</a> 一个 node 的 allocate 为下面的情况，<code>Allocatable</code> 为 pod 的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Allocatable] = [Node Capacity] - [Kube-Reserved] - [System-Reserved] - [Hard-Eviction-Threshold]</span><br></pre></td></tr></table></figure><p>转换下就是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Node Capacity] = [Allocatable] + [Kube-Reserved] + [System-Reserved] + [Hard-Eviction-Threshold]</span><br></pre></td></tr></table></figure><p>节点上的 <code>Allocatable</code> 被定义为 pod 的可用计算资源量。 调度器不会超额申请 Allocatable。 目前支持 <code>CPU</code>, <code>memory</code> 和 <code>ephemeral-storage</code> 这几个参数。上面的 <code>Hard-Eviction</code> 是有默认值的。而由于下面默认值，我们需要加上 kube 和 system 的 reserved 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br></pre></td></tr></table></figure><h2 id="尝试"><a href="#尝试" class="headerlink" title="尝试"></a>尝试</h2><p>加了上面俩后发现不生效，最后去看 yaml 里相关设置的参考后以及部分源码后摸索出来了。但是其实官方这块是有文档的: <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/">官方文档</a> 和 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">最初的设计文档</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;15%&quot;</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">inventory_hostname</span> <span class="string">in</span> <span class="string">groups</span>[<span class="string">&#x27;kube_master&#x27;</span>] <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">400m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">896Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">256Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">500Mi</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">2Gi</span></span><br></pre></td></tr></table></figure><p>这个模板判断的灵感是来源于 kubespray ，<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml">defaults/main.yml</a> 和 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/templates/kubelet-config.v1beta1.yaml.j2">templates/kubelet-config.v1beta1.yaml.j2</a><br>我们环境都是二进制，所以 master 上 kube 会多配置些。但是这样配置了看了下无法生效，看了下必须要配置 cgroup path。也就是下面的:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;15%&quot;</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">inventory_hostname</span> <span class="string">in</span> <span class="string">groups</span>[<span class="string">&#x27;kube_master&#x27;</span>] <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">400m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">896Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">256Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">500Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kubeReservedCgroup:</span> <span class="string">/kube.slice</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">2Gi</span></span><br><span class="line"><span class="attr">systemReservedCgroup:</span> <span class="string">/system.slice</span></span><br></pre></td></tr></table></figure><p>根据官方文档的示例值是俩不同的 path，但是市面上有不少人这方面的文章互相抄袭，他们会把 <code>kubeReservedCgroup: /system.slice/kube.slice</code> 嵌套下。配置了上面的后会发现依然无法启动报错下面的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on &quot;/kube.slice&quot;: [&quot;kubelet&quot;] cgroup does not exist</span><br></pre></td></tr></table></figure><p>最后找了下相关源码 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/cgroup_manager_linux.go#L257">pkg/kubelet/cm/cgroup_manager_linux.go 的 func (m *cgroupManagerImpl) Exists(name CgroupName) bool </a> 方法，我们只关心下面的几个 cgroup 就行了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">allowlistControllers := sets.NewString(&quot;cpu&quot;, &quot;cpuacct&quot;, &quot;cpuset&quot;, &quot;memory&quot;, &quot;systemd&quot;, &quot;pids&quot;)</span><br><span class="line"></span><br><span class="line">if _, ok := m.subsystems.MountPoints[&quot;hugetlb&quot;]; ok &#123;</span><br><span class="line">allowlistControllers.Insert(&quot;hugetlb&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>市面上都是手动创建的不推荐，推荐在 kubelet 的 service 加个 <code>ExecStartPre</code> 和脚本判断处理。</p><h2 id="最终配置"><a href="#最终配置" class="headerlink" title="最终配置"></a>最终配置</h2><p>kubelet 的 service 文件参考:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=&#123;&#123; data_dir &#125;&#125;/kube/kubelet</span><br><span class="line">ExecStartPre=/bin/bash &#123;&#123; data_dir &#125;&#125;/kube/kubelet/kubelet-cg.sh</span><br><span class="line">ExecStart=&#123;&#123; bin_dir &#125;&#125;/kubelet \</span><br><span class="line">  --config=&#123;&#123; data_dir &#125;&#125;/kube/kubelet/kubelet-config.yaml \</span><br><span class="line">  --root-dir=&#123;&#123; data_dir &#125;&#125;/kube/kubelet \</span><br><span class="line">  --docker-root=&#123;&#123; data_dir &#125;&#125;/kube/docker \</span><br><span class="line">  --cni-bin-dir=&#123;&#123; bin_dir &#125;&#125; \</span><br><span class="line">  --cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">  --hostname-override=&#123;&#123; inventory_hostname &#125;&#125; \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">  --network-plugin=cni \</span><br><span class="line">  --experimental-dockershim-root-directory=&#123;&#123; data_dir &#125;&#125;/kube/dockershim \</span><br><span class="line">  --pod-infra-container-image=registry.aliyuncs.com/k8sxio/pause:3.5 \</span><br><span class="line">  --register-node=true \</span><br><span class="line">  --v=2 \</span><br><span class="line">  --node-ip=&#123;&#123; inventory_hostname &#125;&#125;</span><br><span class="line"></span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>我们的环境目前还是 cgroupfs ， systemd 的可能需要你自己去摸索了。下面是 <code>kubelet-cg.sh</code> 和 <code>kubelet-config.yaml</code>:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="comment"># Default: []</span></span><br><span class="line"><span class="attr">allowedUnsafeSysctls:</span> []</span><br><span class="line"><span class="attr">address:</span> &#123;&#123; <span class="string">inventory_hostname</span> &#125;&#125;</span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line">  <span class="attr">anonymous:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">webhook:</span></span><br><span class="line">    <span class="attr">cacheTTL:</span> <span class="string">2m0s</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">x509:</span></span><br><span class="line">    <span class="attr">clientCAFile:</span> &#123;&#123; <span class="string">ca_dir</span> &#125;&#125;<span class="string">/ca.pem</span></span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line">  <span class="attr">mode:</span> <span class="string">Webhook</span></span><br><span class="line">  <span class="attr">webhook:</span></span><br><span class="line">    <span class="attr">cacheAuthorizedTTL:</span> <span class="string">5m0s</span></span><br><span class="line">    <span class="attr">cacheUnauthorizedTTL:</span> <span class="string">30s</span></span><br><span class="line"><span class="attr">tlsCertFile:</span> &#123;&#123; <span class="string">ca_dir</span> &#125;&#125;<span class="string">/kubelet.pem</span></span><br><span class="line"><span class="attr">tlsPrivateKeyFile:</span> &#123;&#123; <span class="string">ca_dir</span> &#125;&#125;<span class="string">/kubelet-key.pem</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">cgroupfs</span></span><br><span class="line"><span class="attr">cgroupsPerQOS:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">clusterDNS:</span></span><br><span class="line">  <span class="bullet">-</span> &#123;&#123; <span class="string">CLUSTER_DNS_SVC_IP</span> &#125;&#125;</span><br><span class="line"><span class="attr">clusterDomain:</span> &#123;&#123; <span class="string">CLUSTER_DNS_DOMAIN</span> &#125;&#125;</span><br><span class="line"><span class="attr">configMapAndSecretChangeDetectionStrategy:</span> <span class="string">Watch</span></span><br><span class="line"><span class="attr">containerLogMaxFiles:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">containerLogMaxSize:</span> <span class="string">10Mi</span></span><br><span class="line"><span class="attr">contentType:</span> <span class="string">application/vnd.kubernetes.protobuf</span></span><br><span class="line"><span class="attr">cpuCFSQuota:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: &quot;100ms&quot; The value must be between 1 us and 1 second</span></span><br><span class="line"><span class="attr">cpuCFSQuotaPeriod:</span> <span class="string">100ms</span></span><br><span class="line"><span class="attr">cpuManagerPolicy:</span> <span class="string">none</span></span><br><span class="line"><span class="attr">cpuManagerReconcilePeriod:</span> <span class="string">10s</span></span><br><span class="line"><span class="attr">enableControllerAttachDetach:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: true</span></span><br><span class="line"><span class="attr">enableDebuggingHandlers:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: true</span></span><br><span class="line"><span class="attr">enableSystemLogHandler:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: [&quot;pods&quot;]</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md</span></span><br><span class="line"><span class="comment"># pkg/kubelet/cm/node_container_manager_linux.go:67</span></span><br><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br><span class="line"><span class="comment"># Default: 10</span></span><br><span class="line"><span class="attr">eventBurst:</span> <span class="number">100</span></span><br><span class="line"><span class="comment"># Default: 5</span></span><br><span class="line"><span class="attr">eventRecordQPS:</span> <span class="number">50</span></span><br><span class="line"><span class="comment"># Default:</span></span><br><span class="line"><span class="comment">#   memory.available:  &quot;100Mi&quot;</span></span><br><span class="line"><span class="comment">#   nodefs.available:  &quot;10%&quot;</span></span><br><span class="line"><span class="comment">#   nodefs.inodesFree: &quot;5%&quot;</span></span><br><span class="line"><span class="comment">#   imagefs.available: &quot;15%&quot;</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;15%&quot;</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line"><span class="attr">evictionPressureTransitionPeriod:</span> <span class="string">5m0s</span></span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: &quot;20s&quot;</span></span><br><span class="line"><span class="attr">fileCheckFrequency:</span> <span class="string">10s</span></span><br><span class="line"><span class="comment"># Default: &quot;promiscuous-bridge&quot;</span></span><br><span class="line"><span class="attr">hairpinMode:</span> <span class="string">promiscuous-bridge</span></span><br><span class="line"><span class="attr">healthzPort:</span> <span class="number">10248</span></span><br><span class="line"><span class="comment"># Default: &quot;127.0.0.1&quot;</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> &#123;&#123; <span class="string">inventory_hostname</span> &#125;&#125;</span><br><span class="line"><span class="comment"># Default: &quot;20s&quot;, staticPodUrl 才有用</span></span><br><span class="line"><span class="attr">httpCheckFrequency:</span> <span class="string">0s</span></span><br><span class="line"><span class="attr">imageGCHighThresholdPercent:</span> <span class="number">85</span></span><br><span class="line"><span class="attr">imageGCLowThresholdPercent:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">imageMinimumGCAge:</span> <span class="string">2m0s</span></span><br><span class="line"><span class="comment"># Default: 15</span></span><br><span class="line"><span class="attr">iptablesDropBit:</span> <span class="number">15</span></span><br><span class="line"><span class="comment"># Default: 14</span></span><br><span class="line"><span class="attr">iptablesMasqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="comment"># Default: 10</span></span><br><span class="line"><span class="attr">kubeAPIBurst:</span> <span class="number">100</span></span><br><span class="line"><span class="comment"># Default: 5</span></span><br><span class="line"><span class="attr">kubeAPIQPS:</span> <span class="number">50</span></span><br><span class="line"><span class="comment"># https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/templates/kubelet-config.v1beta1.yaml.j2</span></span><br><span class="line"><span class="comment"># Default: nil</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">inventory_hostname</span> <span class="string">in</span> <span class="string">groups</span>[<span class="string">&#x27;kube_master&#x27;</span>] <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">400m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">896Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">256Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">500Mi</span></span><br><span class="line"><span class="comment"># pkg/kubelet/cm/cgroup_manager_linux.go:257 func (m *cgroupManagerImpl) Exists(name CgroupName) bool &#123;</span></span><br><span class="line"><span class="attr">kubeReservedCgroup:</span> <span class="string">/kube.slice</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">2Gi</span></span><br><span class="line"><span class="attr">systemReservedCgroup:</span> <span class="string">/system.slice</span></span><br><span class="line"><span class="attr">makeIPTablesUtilChains:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: 1000000</span></span><br><span class="line"><span class="attr">maxOpenFiles:</span> <span class="number">1000000</span></span><br><span class="line"><span class="comment"># Default: 110</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">set</span> <span class="string">nodeLen</span> <span class="string">=</span> <span class="string">groups</span>[<span class="string">&#x27;kube_node&#x27;</span>] <span class="string">|</span> <span class="string">length</span> <span class="string">%</span>&#125;</span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">nodeLen</span> <span class="string">==</span> <span class="number">1</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">253</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">elif</span> <span class="string">nodeLen</span> <span class="string">&lt;</span> <span class="number">3</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">200</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">elif</span> <span class="string">nodeLen</span> <span class="string">&gt;=</span> <span class="number">3</span> <span class="string">and</span> <span class="string">nodeLen</span> <span class="string">&lt;=6</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">150</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">110</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="comment"># Default: 40</span></span><br><span class="line"><span class="attr">nodeLeaseDurationSeconds:</span> <span class="number">40</span> <span class="comment"># 看源码乘以了0.25 作为更新间隔了</span></span><br><span class="line"><span class="comment"># Default: &quot;5m&quot; # 节点状态没有更改时候的上报频率，如果有更改就立即更新。NodeLease 启用下它才有用。如果设置了 nodeStatusUpdateFrequency 则它的默认值等于它来向后兼容</span></span><br><span class="line"><span class="attr">nodeStatusReportFrequency:</span> <span class="string">1m0s</span></span><br><span class="line"><span class="attr">nodeStatusUpdateFrequency:</span> <span class="string">10s</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="number">-999</span></span><br><span class="line"><span class="attr">podPidsLimit:</span> <span class="number">-1</span></span><br><span class="line"><span class="attr">port:</span> <span class="number">10250</span></span><br><span class="line"><span class="attr">readOnlyPort:</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># Default: 10</span></span><br><span class="line"><span class="attr">registryBurst:</span> <span class="number">20</span></span><br><span class="line"><span class="comment"># Default: 5</span></span><br><span class="line"><span class="attr">registryPullQPS:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">resolvConf:</span> &#123;<span class="string">%</span> <span class="string">if</span> <span class="string">ansible_distribution</span> <span class="string">==</span> <span class="string">&quot;Ubuntu&quot;</span> <span class="string">and</span> <span class="string">ansible_distribution_major_version|int</span> <span class="string">&gt;</span> <span class="number">16</span> <span class="string">%</span>&#125;<span class="string">/run/systemd/resolve/resolv.conf</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;<span class="string">/etc/resolv.conf</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">rotateCertificates:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: &quot;2m&quot;</span></span><br><span class="line"><span class="attr">runtimeRequestTimeout:</span> <span class="string">2m0s</span></span><br><span class="line"><span class="attr">serializeImagePulls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">staticPodPath:</span> <span class="string">/etc/kubernetes/manifests</span></span><br><span class="line"><span class="comment"># Default: &quot;4h&quot;</span></span><br><span class="line"><span class="attr">streamingConnectionIdleTimeout:</span> <span class="string">20m0s</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># shutdownGracePeriod: 30s</span></span><br><span class="line"><span class="comment"># shutdownGracePeriodCriticalPods: 10s    # 1.21后的特性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Default: &quot;1m&quot; # sync for ConfigMaps and Secrets.</span></span><br><span class="line"><span class="attr">syncFrequency:</span> <span class="string">1m0s</span></span><br><span class="line"><span class="attr">volumeStatsAggPeriod:</span> <span class="string">1m0s</span></span><br><span class="line"><span class="attr">volumePluginDir:</span> <span class="string">/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</span></span><br><span class="line"><span class="attr">tlsCipherSuites:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_128_GCM_SHA256</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_256_GCM_SHA384</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_128_CBC_SHA</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_256_CBC_SHA</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_and_create</span></span>()&#123;</span><br><span class="line"><span class="comment"># pkg/kubelet/cm/cgroup_manager_linux.go:257  func (m *cgroupManagerImpl) Exists(name CgroupName) bool &#123;</span></span><br><span class="line">    <span class="built_in">local</span> cg_controller=<span class="variable">$1</span></span><br><span class="line">    <span class="keyword">if</span> mountpoint -q /sys/fs/cgroup/<span class="variable">$&#123;cg_controller&#125;</span>;<span class="keyword">then</span></span><br><span class="line">        mkdir -p /sys/fs/cgroup/<span class="variable">$&#123;cg_controller&#125;</span>/system.slice</span><br><span class="line">        mkdir -p /sys/fs/cgroup/<span class="variable">$&#123;cg_controller&#125;</span>/kube.slice</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">check_and_create cpu</span><br><span class="line">check_and_create cpuacct</span><br><span class="line">check_and_create cpuset</span><br><span class="line">check_and_create memory</span><br><span class="line">check_and_create systemd</span><br><span class="line">check_and_create pids</span><br><span class="line">check_and_create hugetlb</span><br></pre></td></tr></table></figure><p>关于 pod 数量这块和大佬讨论了下，<code>maxPods</code> 大了的话实际上例如 docker 撑不住，所以没必要太大，我的判断逻辑是节点数量少的时候也就是我们内部的测试环境下，pod 数量调大，客户现场还是推荐的 110。</p><h2 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h2><p>2021/08/23 内部很多机器配置不一致，然后上面的配置会导致起不来，而且我理解错了 <code>enforceNodeAllocatable</code> 的意思了，我以为它是开关，实际上是给这几个创建 cgroup。reserved 配置了就会减去分配的配额，它开了就会强制 cgroup 限制 kube 和 systemd 来预留，也是不推荐配置的。取消它的配置为下面相关：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#ExecStartPre=/bin/bash &#123;&#123; data_dir &#125;&#125;/kube/kubelet/kubelet-cg.sh</span><br><span class="line"></span><br><span class="line">enforceNodeAllocatable:</span><br><span class="line">  - pods</span><br><span class="line">#  - kube-reserved</span><br><span class="line">#  - system-reserved</span><br></pre></td></tr></table></figure><h2 id="oom-killer"><a href="#oom-killer" class="headerlink" title="oom killer"></a>oom killer</h2><p>当系统内存不足时候，内核会调用 <a href="https://lwn.net/Articles/317814/">oom-killer</a> 来选择讲一些进程杀掉，以便能回收一些内存，尽量继续保持系统继续运行。具体选择哪个进程杀掉，这有一套算分的策略，参考因子是进程占用的内存数，进程页表占用的内存数等，<code>oom_score_adj</code> 的值越小，进程得分越少，也就越难被杀掉。它的计算公式大概类似下面，<code>oom_score</code>的取值为[0,1000]，而 <code>oom_score_adj</code> 的取值为[-1000,1000] ，<code>oom_score_adj</code> 是给我们调整的，例如我们不希望某些进程被 oom-killer 杀掉，可以调整它的 <code>oom_score_adj</code> 为 <code>-1000</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oom_score = 内存消耗/总内存 *1000 # 这个不完全对，实际还有 cpu 实际和存活时间</span><br></pre></td></tr></table></figure><p>其中<br>内存消耗包括了：常驻内存RSS + 进程页面 +交换内存<br>总内存就简单了：总的物理内存 +交换分区</p><h3 id="k8s-的-qosClass"><a href="#k8s-的-qosClass" class="headerlink" title="k8s 的 qosClass"></a>k8s 的 qosClass</h3><p>Kubernetes 创建 Pod 时就给它指定了下列三种 <a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/">QoS 类</a>：</p><ul><li>Guaranteed - limit 的 cpu 和 memory 必须设置，并且 request cpu 和 limit 下 cpu 要一样数值，memory 也一样。只设置 limit 的 cpu 和 memory，k8s 会设置与之一样的 requests</li><li>Burstable - 不满足 Guaranteed ，并且 Pod 中至少一个容器具有 memory 或 CPU 请求，limit 和 request 里的 cpu 或者 内存请求数值相等和不相等都没关系</li><li>BestEffort - 所有容器都没有设置 memory 和 CPU 限制或请求</li></ul><p>查看了下，目前我们所有业务 pod 都没配置限制，也就是 <code>BestEffort</code>。下面命令查看 ns 下 pod 的 qosClass</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -o yaml | grep qosClass</span><br></pre></td></tr></table></figure><h3 id="节点-OOM-行为和-qosClass-的-oom-score-adj"><a href="#节点-OOM-行为和-qosClass-的-oom-score-adj" class="headerlink" title="节点 OOM 行为和 qosClass 的 oom_score_adj"></a>节点 OOM 行为和 qosClass 的 oom_score_adj</h3><p>根据<a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/out-of-resource/#%E8%8A%82%E7%82%B9-oom-%E8%A1%8C%E4%B8%BA">官方文档，节点 oom 的行为</a> 为：</p><p>如果节点在 <code>kubelet</code> 回收内存之前经历了系统 OOM（内存不足）事件，它将基于 <a href="https://lwn.net/Articles/391222/">oom-killer</a> 做出响应。</p><p><code>kubelet</code> 基于 pod 的 service 质量为每个容器设置一个 <code>oom_score_adj</code> 值，这个值在容器创建的时候设置的。</p><table><thead><tr><th>Service 质量</th><th>oom_score_adj</th></tr></thead><tbody><tr><td><code>Guaranteed</code></td><td>-997</td></tr><tr><td><code>Burstable</code></td><td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td></tr><tr><td><code>BestEffort</code></td><td>1000</td></tr></tbody></table><p>如果 <code>kubelet</code> 在节点经历系统 OOM 之前无法回收内存，<code>oom_killer</code> 将基于它在节点上<br>使用的内存百分比算出一个 <code>oom_score</code>，并加上 <code>oom_score_adj</code> 得到容器的有效<br><code>oom_score</code>，然后结束得分最高的容器。</p><p>预期的行为应该是拥有最低服务质量并消耗和调度请求相关内存量最多的容器第一个被结束，以回收内存。</p><p>和 pod 驱逐不同，如果一个 Pod 的容器是被 OOM 结束的，基于其 <code>RestartPolicy</code>，<br>它可能会被 <code>kubelet</code> 重新启动。</p><p>在文件 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/kuberuntime_container_linux.go#L53">pkg/kubelet/kuberuntime/kuberuntime_container_linux.go</a> 里的 <code>generateLinuxContainerConfig</code> 和 <a href="https://github.com/kubernetes/kubernetes/blob/d385d0602a1075837bf8713b9f56964c154aede7/pkg/kubelet/qos/policy.go#L40">GetContainerOOMScoreAdjust</a> 可以去了解更多细节。</p><p>主要是 <code>oomScoreAdjust := 1000 - (1000 * container.Resources.Requests.Memory().Value())/memoryCapacity</code>。<br><code>memoryCapacity</code> 是机器的物理内存大小，而不是减去预留后的。最小值就是避免 <code>memoryRequest</code> / <code>机器内存</code> 趋近于 0 ，最大值避免 <code>oomScoreAdjust</code> 等于了最大值 1000 了。 kubelet 和 docker 通常会把他们自身的 <code>oom_score_adj</code> 设置为 <code>-999</code>。</p><p>可以得出一个结论：在非 <code>Guaranteed</code> 和 request 和 limit 为空的 <code>BestEffort</code> 以外，request 内存越大则 <code>oom_score_adj</code> 越小。<code>oom_score_adj</code> 越小， oom 的时候最不会被 oom-kill 杀掉。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>在 32G 的机器上，空闲占用 1G ，我们部署几个 pod 都分为三个 qos 组，每个 都是 12G 的内存请求，看看哪个最先被杀掉 :</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-12-guaranteed</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:0.13.03</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-12-Guaranteed</span></span><br><span class="line"><span class="string">        exec /stress-12-Guaranteed --vm 4  --vm-bytes 12G</span></span><br><span class="line"><span class="string"></span>    <span class="attr">resources:</span></span><br><span class="line">     <span class="attr">limits:</span></span><br><span class="line">       <span class="attr">memory:</span> <span class="string">&quot;13Gi&quot;</span></span><br><span class="line">       <span class="attr">cpu:</span> <span class="string">&quot;300m&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-12-burstable</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:0.13.03</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-12-Burstable</span></span><br><span class="line"><span class="string">        exec /stress-12-Burstable --vm 4  --vm-bytes 12G</span></span><br><span class="line"><span class="string"></span>    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;10Mi&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-12-besteffort</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:0.13.03</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-12-BestEffort</span></span><br><span class="line"><span class="string">        exec /stress-12-BestEffort --vm 4  --vm-bytes 12G</span></span><br><span class="line"><span class="string"></span><span class="meta">---</span></span><br><span class="line"><span class="meta"></span></span><br></pre></td></tr></table></figure><p>创建完后，通过系统日志查看是对的，oom-killer 杀掉的确实是 <code>stress-12-besteffort</code> </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[80389.171797] Memory cgroup out of memory: Kill process 27672 (stress-12-BestE) score 1105 or sacrifice child</span><br><span class="line">[80389.202470] Killed process 27672 (stress-12-BestE), UID 0, total-vm:3189272kB, anon-rss:3145848kB, file-rss:4kB, shmem-rss:8kB</span><br><span class="line">[80391.538015] stress-12-BestE invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=1000</span><br><span class="line">[80391.538021] stress-12-BestE cpuset=597912cbecd66eccbd66c62dfd354bf5497db8e27a5bb04672ee5a84f217fbff mems_allowed=0-3</span><br><span class="line">[80391.538026] CPU: 6 PID: 27991 Comm: stress-12-BestE Kdump: loaded Tainted: G               ------------ T 3.10.0-1127.el7.x86_64 #1</span><br><span class="line">[80391.538028] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 09/21/2015</span><br><span class="line">[80391.538030] Call Trace:</span><br><span class="line">[80391.538041]  [&lt;ffffffff8497ff85&gt;] dump_stack+0x19/0x1b</span><br><span class="line">[80391.538045]  [&lt;ffffffff8497a8a3&gt;] dump_header+0x90/0x229</span><br><span class="line">[80391.538051]  [&lt;ffffffff8449c4a8&gt;] ? ep_poll_callback+0xf8/0x220</span><br><span class="line">[80391.538057]  [&lt;ffffffff843c246e&gt;] oom_kill_process+0x25e/0x3f0</span><br><span class="line">[80391.538062]  [&lt;ffffffff84333a41&gt;] ? cpuset_mems_allowed_intersects+0x21/0x30</span><br><span class="line">[80391.538067]  [&lt;ffffffff84440ba6&gt;] mem_cgroup_oom_synchronize+0x546/0x570</span><br><span class="line">[80391.538071]  [&lt;ffffffff84440020&gt;] ? mem_cgroup_charge_common+0xc0/0xc0</span><br><span class="line">[80391.538075]  [&lt;ffffffff843c2d14&gt;] pagefault_out_of_memory+0x14/0x90</span><br><span class="line">[80391.538078]  [&lt;ffffffff84978db3&gt;] mm_fault_error+0x6a/0x157</span><br><span class="line">[80391.538082]  [&lt;ffffffff8498d8d1&gt;] __do_page_fault+0x491/0x500</span><br><span class="line">[80391.538086]  [&lt;ffffffff8498d975&gt;] do_page_fault+0x35/0x90</span><br><span class="line">[80391.538091]  [&lt;ffffffff84989778&gt;] page_fault+0x28/0x30</span><br></pre></td></tr></table></figure><p>再测试下下面这种内存大小不一致的</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-20-guaranteed</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:0.13.03</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-20-Guaranteed</span></span><br><span class="line"><span class="string">        exec /stress-20-Guaranteed --vm 4  --vm-bytes 20G</span></span><br><span class="line"><span class="string"></span>    <span class="attr">resources:</span></span><br><span class="line">     <span class="attr">limits:</span></span><br><span class="line">       <span class="attr">memory:</span> <span class="string">&quot;24Gi&quot;</span></span><br><span class="line">       <span class="attr">cpu:</span> <span class="string">&quot;4000m&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-8-burstable</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:0.13.03</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-8-Burstable</span></span><br><span class="line"><span class="string">        exec /stress-8-Burstable --vm 4  --vm-bytes 8G</span></span><br><span class="line"><span class="string"></span>    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;300Mi&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-4-besteffort</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:0.13.03</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-4-BestEffort</span></span><br><span class="line"><span class="string">        exec /stress-4-BestEffort --vm 2  --vm-bytes 4G</span></span><br><span class="line"><span class="string"></span><span class="meta">---</span></span><br><span class="line"><span class="meta"></span></span><br></pre></td></tr></table></figure><p>查看日志，<code>stress-20-Guara</code> 被杀掉了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941467] stress-20-Guara invoked oom-killer: gfp_mask=0x50, order=0, oom_score_adj=1000</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941472] stress-20-Guara cpuset=4be3661456aa74304875c3a00646851baea21be3e0667e84dad7ae812d3d0169 mems_allowed=0-3</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941476] CPU: 8 PID: 10195 Comm: stress-20-Guara Kdump: loaded Tainted: G               ------------ T 3.10.0-1127.el7.x86_64 #1</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941478] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 09/21/2015</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941479] Call Trace:</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941492]  [&lt;ffffffff8497ff85&gt;] dump_stack+0x19/0x1b</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941495]  [&lt;ffffffff8497a8a3&gt;] dump_header+0x90/0x229</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941502]  [&lt;ffffffff8449c4a8&gt;] ? ep_poll_callback+0xf8/0x220</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941508]  [&lt;ffffffff843c246e&gt;] oom_kill_process+0x25e/0x3f0</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941512]  [&lt;ffffffff84333a41&gt;] ? cpuset_mems_allowed_intersects+0x21/0x30</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941518]  [&lt;ffffffff84440ba6&gt;] mem_cgroup_oom_synchronize+0x546/0x570</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941520]  [&lt;ffffffff84440020&gt;] ? mem_cgroup_charge_common+0xc0/0xc0</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941523]  [&lt;ffffffff843c2d14&gt;] pagefault_out_of_memory+0x14/0x90</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941525]  [&lt;ffffffff84978db3&gt;] mm_fault_error+0x6a/0x157</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941529]  [&lt;ffffffff8498d8d1&gt;] __do_page_fault+0x491/0x500</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941531]  [&lt;ffffffff8498d975&gt;] do_page_fault+0x35/0x90</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941534]  [&lt;ffffffff84989778&gt;] page_fault+0x28/0x30</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941538] Task in /kubepods/podff2a4320-67f9-4afc-b1a8-0aa39caa8904/4be3661456aa74304875c3a00646851baea21be3e0667e84dad7ae812d3d0169 killed as a result of limit of /kubepods</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941540] memory: usage 29763384kB, limit 29763384kB, failcnt 734923</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941542] memory+swap: usage 29763384kB, limit 9007199254740988kB, failcnt 0</span><br><span class="line">Sep 28 09:54:39 82-174-zhang kernel: [140244.941543] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>按理说不应该被杀掉。查看下进程的 <code>oom_score_adj</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep stress-20-Guar[a]</span><br><span class="line">root     10146  0.0  0.0  43540  2468 ?        Ss   09:54   0:00 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     10181  0.0  0.0  43544   308 ?        S    09:54   0:00 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     10184  0.0  0.0  43544   272 ?        S    09:54   0:00 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     10186  0.0  0.0  43544   276 ?        S    09:54   0:00 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     10188  0.1  0.0  43544   348 ?        S    09:54   0:00 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     11024 99.7 15.9 5286424 5243196 ?     R    09:56   1:23 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     11491 98.6 15.9 5286424 5243164 ?     R    09:57   0:23 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     11539  103 15.9 5286424 5243168 ?     R    09:57   0:16 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">root     11610  101 15.9 5286424 5243192 ?     R    09:57   0:09 /stress-20-Guaranteed --vm 4 --vm-bytes 20G</span><br><span class="line">$ pstree -sp 10146</span><br><span class="line">systemd(1)───dockerd(751)───containerd(838)───containerd-shim(10099)───stress-20-Guara(10146)─┬─stress-20-Guara(10181)───stress-20-Guara(11610)</span><br><span class="line">                                                                                              ├─stress-20-Guara(10184)───stress-20-Guara(11491)</span><br><span class="line">                                                                                              ├─stress-20-Guara(10186)───stress-20-Guara(11539)</span><br><span class="line">                                                                                              └─stress-20-Guara(10188)───stress-20-Guara(11024)</span><br><span class="line">$ cat /proc/11024/oom_score</span><br><span class="line">1160</span><br><span class="line">$ cat /proc/11024/oom_score_adj </span><br><span class="line">1000</span><br><span class="line">$ cat /proc/10146/oom_score</span><br><span class="line">0</span><br><span class="line">$ cat /proc/10146/oom_score_adj </span><br><span class="line">-997</span><br></pre></td></tr></table></figure><p>docker run 个看看 <code>oom_score_adj</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name test  --oom-score-adj -998 nginx:alpine</span><br><span class="line">$ docker exec test cat /proc/*/oom_score_adj</span><br><span class="line">-998</span><br><span class="line">-998</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>最后稍微看了下 <a href="https://github.com/ColinIanKing/stress-ng/blob/802f8afb4f508fb166234bfb7c519f0d3670c860/core-out-of-memory.c">stress-ng 源码</a> 发现了 stress-ng 会设置子进程的 <code>oom_score_adj</code> 成 1000。容器里进程只能增加 <code>oom_score_adj</code> ，不能减少，stress-ng 这块应该是没考虑到容器的情况，已经反馈 <a href="https://github.com/ColinIanKing/stress-ng/issues/150">issue</a> 了。</p><h3 id="no-oom-adjust"><a href="#no-oom-adjust" class="headerlink" title="no-oom-adjust"></a>no-oom-adjust</h3><p>stress-ng 的作者经过 <a href="https://github.com/ColinIanKing/stress-ng/issues/150">issue反馈后</a> 添加了 <code>--no-oom-adjust</code> 选项了，可以继续上面的测试了:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-20-guaranteed</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:temp</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-20-Guaranteed</span></span><br><span class="line"><span class="string">        exec /stress-20-Guaranteed --vm 4  --vm-bytes 20G --no-oom-adjust</span></span><br><span class="line"><span class="string"></span>    <span class="attr">resources:</span></span><br><span class="line">     <span class="attr">limits:</span></span><br><span class="line">       <span class="attr">memory:</span> <span class="string">&quot;22Gi&quot;</span></span><br><span class="line">       <span class="attr">cpu:</span> <span class="string">&quot;4000m&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-8-burstable</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:temp</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-8-Burstable</span></span><br><span class="line"><span class="string">        exec /stress-8-Burstable --vm 4  --vm-bytes 8G --no-oom-adjust</span></span><br><span class="line"><span class="string"></span>    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;300Mi&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-4-besteffort</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">xx.xx.82.174</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ctr</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry.aliyuncs.com/zhangguanzhang/stress-ng:temp</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">        cp stress-ng /stress-4-BestEffort</span></span><br><span class="line"><span class="string">        exec /stress-4-BestEffort --vm 2  --vm-bytes 4G --no-oom-adjust</span></span><br><span class="line"><span class="string"></span><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>apply 后机器上日志最先 oom 的是 <code>4G</code> 这个:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Oct 11 10:38:03 82-174-zhang kernel: [1266038.261910] Memory cgroup out of memory: Kill process 7017 (stress-4-BestEf) score 1070 or sacrifice child</span><br><span class="line">Oct 11 10:38:03 82-174-zhang kernel: [1266038.267444] Killed process 7017 (stress-4-BestEf), UID 0, total-vm:2140696kB, anon-rss:2097272kB, file-rss:8kB, shmem-rss:4kB</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/">官方文档</a> 和 </li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">最初的设计文档</a></li><li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml">defaults/main.yml</a> 和 </li><li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/templates/kubelet-config.v1beta1.yaml.j2">templates/kubelet-config.v1beta1.yaml.j2</a></li><li><a href="https://blog.csdn.net/u010278923/article/details/105688107">linux内核的oom score是咋算出来的</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;我们环境有部分 pod 特殊，单独节点部署，oom 的时候会搞挂一些系统进程，这几天折腾了下配置了下 kubelet 相关的</summary>
      
    
    
    
    
    <category term="kubelet" scheme="http://zhangguanzhang.github.io/tags/kubelet/"/>
    
  </entry>
  
  <entry>
    <title>鲲鹏920的麒麟v10物理服务器断电后无法启动处理</title>
    <link href="http://zhangguanzhang.github.io/2021/07/26/kylin-v10-boot-hang/"/>
    <id>http://zhangguanzhang.github.io/2021/07/26/kylin-v10-boot-hang/</id>
    <published>2021-07-26T12:08:06.000Z</published>
    <updated>2021-07-26T12:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>珠海园区升压前置检查，上周六整个园区关电检查，然后今天来后连不上我们在 鲲鹏920的麒麟v10机器上开的虚拟机了，进 bmc 的 web 看了下是开机进入后卡住。</p><h2 id="信息同步"><a href="#信息同步" class="headerlink" title="信息同步"></a>信息同步</h2><p>是当初安装系统的同事去处理这个事情的，他 bmc 的 web 上去重启在菜单那里按 e 编辑准备改 boot cmdline 进单用户，结果按 e 后要输入用户名和密码，询问了麒麟他们。很久也没给答复。然后就在那干等，上面的虚机有我的环境，我就过去看了下。</p><h3 id="尝试的处理"><a href="#尝试的处理" class="headerlink" title="尝试的处理"></a>尝试的处理</h3><p>麒麟那边的人员没有回复，我打算这边同步尝试下其他手段，而且不只一台无法开机，哪怕麒麟的回复了密码也能同步尝试不同手段。Linux 无法开机的就搞个 yum 系列的新系统镜像挂载到光驱，然后设置成光驱启动，然后开机后进入 iso 的安装界面，<code>Troubleshooting –&gt; Rescue a CentOS Linux system</code>。先进 bmc web 上用java的远程窗口（之前华三的 h5 挂载镜像失败了），挂载本地的 麒麟v10 镜像，然后 bmc web 上设置下次从光驱启动。然后重启服务器，进 <code>Rescue</code> 模式里后，界面是显示乱的。</p><p>这个时候问了下珠海那边有同事能去机房看下吗，同事说正在赶去。然后我赶紧去找了下 centos 7的arm64镜像，后面下载完了进 <code>Rescue</code> 模式里一直卡住。估计没适配国产的服务器。后面尝试了其他国产的系统镜像，没 <code>Rescue</code> 模式。机房现场也有人去了，麒麟v10 镜像<code>Rescue</code> 模式进去后，他那边现场看了下也是显示乱的。</p><h3 id="单用户"><a href="#单用户" class="headerlink" title="单用户"></a>单用户</h3><p>后面麒麟回复了修改 boot 的时候用户名是 <code>root</code> 密码是 <code>Kylin123123</code>，确实能进入编辑 boot 的界面了，但是加了相关选项后还是无法进入单用户，左上角只有一个光标然后一直卡住。询问安装系统的同事，分区是咋搞的，他说 lvm 和有个大盘。然后问他有没有其他正常能开机的机器，他说有一个，我 ssh 上去后看到下面类似的分区：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0 893.1G  0 disk </span><br><span class="line">├─sda1            8:1    0   200M  0 part /boot/efi</span><br><span class="line">├─sda2            8:2    0     1G  0 part /boot</span><br><span class="line">└─sda3            8:3    0   892G  0 part </span><br><span class="line">  ├─klas-root   252:0    0   838G  0 lvm  /</span><br><span class="line">  ├─klas-swap   252:1    0     4G  0 lvm  [SWAP]</span><br><span class="line">  └─klas-backup 252:2    0    50G  0 lvm  </span><br><span class="line">sdb               8:16   0  14.6T  0 disk </span><br><span class="line">└─sdb1            8:17   0     2T  0 part /oskvm</span><br></pre></td></tr></table></figure><p>正常机器的 lvm 信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vgs</span><br><span class="line">  VG   #PV #LV #SN Attr   VSize    VFree</span><br><span class="line">  klas   1   3   0 wz--n- &lt;891.94g    0 </span><br><span class="line">$ lvs</span><br><span class="line">  LV     VG   Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  backup klas -wi-a-----   50.00g                                                    </span><br><span class="line">  root   klas -wi-ao---- &lt;837.94g                                                    </span><br><span class="line">  swap   klas -wi-ao----    4.00g</span><br></pre></td></tr></table></figure><p>推测了下，无法开机的机器是因为把 sdb 搞成 lvm 的原因，所以单用户肯定别想了。</p><h3 id="Rescue-模式"><a href="#Rescue-模式" class="headerlink" title="Rescue 模式"></a>Rescue 模式</h3><p>目前的解决办法只有重装，这个时候快到中午了，其他同事催得急，重装代价太高了，安装慢，而且上面的虚拟机都没了，Rescue 模式后的乱码能解决就有办法了。看看正常机器上有没有显示的相关内核启动参数。</p><p>查找下 <code>grub</code> 的配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd /boot</span><br><span class="line">$ find -type f -name &#x27;grub*&#x27;</span><br><span class="line">./efi/EFI/kylin/grubaa64.efi</span><br><span class="line">./efi/EFI/kylin/grubenv</span><br><span class="line">./efi/EFI/kylin/grub.cfg</span><br></pre></td></tr></table></figure><p>看了下果然有</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ./efi/EFI/kylin/grub.cfg</span><br><span class="line">linux/vmlinuz-4.19.90-17.ky10.aarch64 root=/dev/mapper/klas-root ro crashkernel=auto rd.lvm.lv=klas/root rd.lvm.lv=klas/swap smmu.bypassdev=0x1000:0x17 smmu.bypassdev=0x1000:0x15 crashkernel=1024M,high video=efifb:off video=VGA-1:640x480-32@60me</span><br></pre></td></tr></table></figure><p>然后在 麒麟v10 的 iso 进入了 <code>Troubleshooting –&gt; Rescue</code> 选中 <code>Rescue</code> 先别回车，直接按 e 编辑，<code>linux</code>那行后面加了上面的<code> video=efifb:off video=VGA-1:640x480-32@60me</code>，然后 <code>ctrl + x</code> 启动，显示正常了。和 Centos 的 <code>Rescue</code> 有点不一样，进去后类似进入 emergency 模式里，然后回车继续，进入到了一个 bash。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vgchange -a y</span><br><span class="line">$ lvs</span><br><span class="line">  LV     VG   Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  18     18   -wi-ao----   14.55t                                                    </span><br><span class="line">  backup klas -wi-a-----   50.00g                                                    </span><br><span class="line">  root   klas -wi-ao---- &lt;837.94g                                                    </span><br><span class="line">  swap   klas -wi-ao----    4.00g     </span><br><span class="line">$ mkdir /mnt/sysimage</span><br></pre></td></tr></table></figure><p>然后挂载 root 分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mount /dev/mapper/klas-root /mnt/sysimage</span><br><span class="line">$ chroot /mnt/sysimage</span><br></pre></td></tr></table></figure><p>其实也没必要 chroot 进去，无非就是 vi 多了个前面的路径。取消最后一行的挂载，然后 reboot 后重启就好了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 没chroot进去就是编辑 vi /mnt/sysimage/etc/fstab</span><br><span class="line">$ vi /etc/fstab</span><br><span class="line">...</span><br><span class="line">#/dev/mapper/18-18     /oskvm                    xfs     default   0 0</span><br></pre></td></tr></table></figure><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>可能是分区太大了，lvm 在国产系统开机上有问题。另外也不推荐物理机使用 lvm，能不查资料笔记熟悉 lvm 的命令的人实际上目前不多，会修复的人更少了。lvm 扩容一时爽，恢复火葬场。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ pvs</span><br><span class="line">  PV         VG   Fmt  Attr PSize    PFree </span><br><span class="line">  /dev/sda3  klas lvm2 a--  &lt;891.94g     0 </span><br><span class="line">  /dev/sdb   18   lvm2 a--    14.55t &lt;1.16g</span><br><span class="line">l$ vgs</span><br><span class="line">  VG   #PV #LV #SN Attr   VSize    VFree </span><br><span class="line">  18     1   1   0 wz--n-   14.55t &lt;1.16g</span><br><span class="line">  klas   1   3   0 wz--n- &lt;891.94g     0 </span><br><span class="line">$ lvs</span><br><span class="line">  LV     VG   Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  18     18   -wi-ao----   14.55t                                                    </span><br><span class="line">  backup klas -wi-a-----   50.00g                                                    </span><br><span class="line">  root   klas -wi-ao---- &lt;837.94g                                                    </span><br><span class="line">  swap   klas -wi-ao----    4.00g </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;珠海园区升压前置检查，上周六整个园区关电检查，然后今天来后连不上我们在 鲲鹏920的麒麟v10机器上开的虚拟机了，进 bmc</summary>
      
    
    
    
    
    <category term="kylin" scheme="http://zhangguanzhang.github.io/tags/kylin/"/>
    
    <category term="arm64" scheme="http://zhangguanzhang.github.io/tags/arm64/"/>
    
  </entry>
  
  <entry>
    <title>dlv命令行的远程调试 golang 进程步骤(包含容器进程)</title>
    <link href="http://zhangguanzhang.github.io/2021/07/20/dlv-remote/"/>
    <id>http://zhangguanzhang.github.io/2021/07/20/dlv-remote/</id>
    <published>2021-07-20T20:08:06.000Z</published>
    <updated>2021-07-20T20:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>记录下 dlv 的远程调试，建议不要在代码里加 fmt 去调试。不谈 goland 啥的远程调试，本文章目前只写 dlv 的命令行配合远端调试。</p><h2 id="一些前提须知"><a href="#一些前提须知" class="headerlink" title="一些前提须知"></a>一些前提须知</h2><h3 id="符号链接路径"><a href="#符号链接路径" class="headerlink" title="符号链接路径"></a>符号链接路径</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line"><span class="string">&quot;os&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">f, _ := os.Open(<span class="string">&quot;asdasdasd&quot;</span>)</span><br><span class="line">fmt.Println(f.Name())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码你编译了后，在其他机器上运行，panic 的堆栈信息会是你机器上的路径信息，路径信息是保留的，例如下面的是我在 windows 上交叉编译仍到 Linux 上执行的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">panic: runtime error: invalid memory address or nil pointer dereference</span><br><span class="line">[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x497d50]</span><br><span class="line"></span><br><span class="line">goroutine 1 [running]:</span><br><span class="line">os.(*File).Name(...)</span><br><span class="line">D:/Install/Go/src/os/file.go:55</span><br><span class="line">main.main()</span><br><span class="line">D:/github_dir/go/dlv-test/main.go:10 +0x50</span><br></pre></td></tr></table></figure><p>可以通过下面的编译选项去掉（项目路径也就是<code>$PWD</code>显示的不要带空格，否则会编译报错）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go build -gcflags=&quot;all=-trimpath=$PWD&quot; -asmflags &quot;all=-trimpath=$PWD&quot;  main.go</span><br></pre></td></tr></table></figure><p>使用上面的参数编译完后的，这里注意下下面的 <code>D:/Install/Go</code>，后面文章会用到。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">panic: runtime error: invalid memory address or nil pointer dereference</span><br><span class="line">[signal 0xc0000005 code=0x0 addr=0x0 pc=0x649857]</span><br><span class="line"></span><br><span class="line">goroutine 1 [running]:</span><br><span class="line">os.(*File).Name(...)</span><br><span class="line">        D:/Install/Go/src/os/file.go:55</span><br><span class="line">main.main()</span><br><span class="line">        main2.go:10 +0x57</span><br></pre></td></tr></table></figure><h2 id="dlv-命令行远端调试"><a href="#dlv-命令行远端调试" class="headerlink" title="dlv 命令行远端调试"></a>dlv 命令行远端调试</h2><p>很多时候线上机器都是 Linux ，源码在本地，而且机器上不一定会有 golang，也就是说 <code>dlv debug main.go</code>满足的条件实际上并不多。这里主要讲下 <code>dlv exec</code> 和 <code>dlv attach</code>。<br>exec 是用 dlv 运行编译完的二进制文件，golang 关闭 cgo 编译的就是静态编译了，运行机器上有无 golang 都能运行。attach 是调试一个已经运行的进程。<br>这里我是在linux上运行一个编译好的 gin-demo，然后在目标机器上准备一个 dlv 的二进制文件，用 <code>dlv attach</code> 开一个 server，然后我们在本地有源码的 windows 上 <code>dlv connect</code> 连上去调试。</p><p>这里以我本地的 windows 和 远端的 Linux 做测试。windows 上和 linux 上都已经安装了 dlv 了，并把路径加到 PATH 里了，windows 我下了 git bash。</p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>根据实际开发流程来，windows 上项目编辑文件 <code>main.go</code>，代码随便写的，不要吐槽。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> count <span class="keyword">int</span></span><br><span class="line">r := gin.Default()</span><br><span class="line">r.GET(<span class="string">&quot;/ping&quot;</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">count++</span><br><span class="line">c.JSON(<span class="number">200</span>, gin.H&#123;</span><br><span class="line"><span class="string">&quot;message&quot;</span>: <span class="string">&quot;pong&quot;</span>,</span><br><span class="line"><span class="string">&quot;count&quot;</span>:   count,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;)</span><br><span class="line">r.Run()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go mod init test</span><br></pre></td></tr></table></figure><p>然后推送到代码仓库上，手动或者 robot 触发 CI 构建</p><h3 id="ci-的编译"><a href="#ci-的编译" class="headerlink" title="ci 的编译"></a>ci 的编译</h3><p>Dockerfile 如下，为了避免 dlv 调试出现 <code>Warning: debugging optimized function</code>，我们需要在 <code>-gcflags=</code> 里加 <code>-N -l</code> ，为了防止变量被 <code>Dockerfile</code> 解析，<code>$PWD</code> 全部换成了<code>pwd</code>。<code>$&#123;LDFLAGS&#125;</code>是注入一些 version 信息之类的，可以看我文章<a href="https://zhangguanzhang.github.io/2020/04/27/go-version-ifno/">git工作流下golang项目的version信息该如何处理</a></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> golang:<span class="number">1.16</span>.<span class="number">10</span> as mod</span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> stage=mod</span></span><br><span class="line"><span class="keyword">ARG</span> GOPROXY=https://goproxy.cn,https://mirrors.aliyun.com/goproxy/,https://goproxy.io,direct</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /root/myapp/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> go.mod ./</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> go.sum ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> go mod download</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> mod as builder</span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> stage=intermediate0</span></span><br><span class="line"><span class="keyword">ARG</span> LDFLAGS</span><br><span class="line"><span class="keyword">ARG</span> GOARCH=amd64</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./ ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> CGO_ENABLED=0 GOOS=linux GOARCH=<span class="variable">$&#123;GOARCH&#125;</span> \</span></span><br><span class="line"><span class="bash">   go build -o gin-demo \</span></span><br><span class="line"><span class="bash">   -gcflags=<span class="string">&quot;all=-trimpath=`pwd` -N -l&quot;</span> \</span></span><br><span class="line"><span class="bash">   -asmflags <span class="string">&quot;all=-trimpath=`pwd`&quot;</span> \</span></span><br><span class="line"><span class="bash">   -ldflags <span class="string">&quot;<span class="variable">$&#123;LDFLAGS&#125;</span>&quot;</span> main.go</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> alpine:<span class="number">3.13</span>.<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> MAINTAINER=<span class="string">&quot;zhangguanzhang zhangguanzhang@qq.com&quot;</span> \</span></span><br><span class="line"><span class="bash">    URL=<span class="string">&quot;https://github.com/zhangguanzhang/xxxx&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /root/myapp/gin-demo /gin-demo</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> TZ Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> sed -i <span class="string">&#x27;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&#x27;</span> /etc/apk/repositories &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apk update &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apk add --no-cache \</span></span><br><span class="line"><span class="bash">      curl \</span></span><br><span class="line"><span class="bash">      ca-certificates \</span></span><br><span class="line"><span class="bash">      bash \</span></span><br><span class="line"><span class="bash">      iproute2 \</span></span><br><span class="line"><span class="bash">      tzdata &amp;&amp; \</span></span><br><span class="line"><span class="bash">    ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="built_in">echo</span> Asia/Shanghai &gt; /etc/timezone &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="keyword">if</span> [ ! -e /etc/nsswitch.conf ];<span class="keyword">then</span> <span class="built_in">echo</span> <span class="string">&#x27;hosts: files dns myhostname&#x27;</span> &gt; /etc/nsswitch.conf; <span class="keyword">fi</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">   rm -rf /var/cache/apk/* /tmp/*</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;/gin-demo&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>然后编译完了，模拟下 CD 部署在 Linux 机器上运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name t1 --rm -p 8080:8080 test</span><br></pre></td></tr></table></figure><p>这里我用容器演示，虽然容器的 pid namespaces 隔离了，但是实际上容器里的所有进程还是会在宿主机上有 pid 对应的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep gin-dem[o]</span><br><span class="line">root     15598  0.0  0.1 708540  5128 ?        Ssl  21:10   0:00 /gin-demo</span><br></pre></td></tr></table></figure><h3 id="Linux-上的准备"><a href="#Linux-上的准备" class="headerlink" title="Linux 上的准备"></a>Linux 上的准备</h3><p>拷贝 Linux 的 dlv 可执行文件放到目标机器的 PATH 下，推荐按照 LFS 的规范放 /usr/local/bin/ 下，可以先通过 go install 安装，1.16 后者高于 1.16 版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export GOPROXY=https://goproxy.cn,https://mirrors.aliyun.com/goproxy/,https://goproxy.io,direct</span><br><span class="line">export CGO_ENABLED=0</span><br><span class="line">go install github.com/go-delve/delve/cmd/dlv@latest</span><br><span class="line"></span><br><span class="line">echo `go env GOPATH`/bin/</span><br><span class="line"></span><br><span class="line"># 拷贝 dlv</span><br><span class="line">cp `go env GOPATH`/bin/dlv /usr/local/bin/</span><br></pre></td></tr></table></figure><h4 id="http-接口之类的-server-类调试"><a href="#http-接口之类的-server-类调试" class="headerlink" title="http 接口之类的 server 类调试"></a>http 接口之类的 server 类调试</h4><p>比如我这个 demo 就是要调试接口内部的，也就是运行后触发的主要运行段是在接口内，所以我们不需要 <code>dlv run</code>， 而是下面这样 attach 执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dlv attach $(pgrep gin-demo) --listen=:2345 --headless=true --log=true  \</span><br><span class="line">  --log-output=debugger,debuglineerr,gdbwire,lldbout,rpc --accept-multiclient --api-version=2</span><br></pre></td></tr></table></figure><p><code>$(pgrep gin-demo)</code> 也可以换成具体的 pid 。</p><h4 id="非接口类服务容器内调试"><a href="#非接口类服务容器内调试" class="headerlink" title="非接口类服务容器内调试"></a>非接口类服务容器内调试</h4><p>可能我们的服务一开始就要打断点，也就是 main 开始之类的地方就打断点，用 attach 就不现实了。我们的思路是先把容器起来，然后把 dlv 拷贝进去，<code>dlv exec</code> 执行二进制文件<br>如果是 <code>docker-compose</code> 可以把容器的命令替换了，类似下面</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">entrypoint:</span> [<span class="string">&#x27;sh&#x27;</span>]</span><br><span class="line">    <span class="attr">tty:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>k8s 的话也和上面差不多，然后把探针啥的给取消或者 initDelay 调久点。下面我用 <code>docker run</code> 模拟容器下操作，需要添加 <code>ptrace</code> 权限，否则 dlv 执行会报错 <code>could not launch process: fork/exec /gin-demo: operation not permitted</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --name t1 --rm -tid  --cap-add=SYS_PTRACE --entrypoint sh test</span><br><span class="line">$ docker cp $(which dlv) t1:/bin/</span><br><span class="line">$ docker exec t1 ip a s  eth0</span><br><span class="line">96: eth0@if97: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>容器里执行 dlv ，注意添加 <code>--allow-non-terminal-interactive</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker exec -ti t1  \</span><br><span class="line">  dlv exec /gin-demo --listen=:2345 --allow-non-terminal-interactive --headless=true --log=true  \</span><br><span class="line">  --log-output=debugger,debuglineerr,gdbwire,lldbout,rpc --accept-multiclient --api-version=2</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>，然后在另一个 tty 窗口我们利用 socat 转发成宿主机的端口，没有就安装下。开启 socat 转发：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socat  -d -d TCP-LISTEN:2345,reuseaddr,fork,bind=0.0.0.0 TCP:172.17.0.2:2345</span><br></pre></td></tr></table></figure><h3 id="本机上开始调试"><a href="#本机上开始调试" class="headerlink" title="本机上开始调试"></a>本机上开始调试</h3><p>和 os 无关，我这里是 windows 而已，dlv 也要放在 PATH 里，或者绝对路径运行 dlv。要在源码目录运行，因为 <code>docker build</code> 里容器编译的时候去掉源码的前缀路径，所以 dlv 调试按照链接找文件都会按照相对路径找，所以我们需要在windows 上的源码路径里执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dlv connect 192.168.2.111:2345</span><br></pre></td></tr></table></figure><p>连上之后，没有 Linux 的<code>(dlv) </code>这样的 format，这是 dlv 引用的库在 windows 上的 bug，只要出现了<code>Type &#39;help&#39; for list of commands.</code>说明成功连上了。</p><p>执行下 <code>sources</code> 看下链接路径：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/go/src/...</span><br><span class="line">/usr/local/go/src/...</span><br><span class="line">/usr/local/go/src/..</span><br><span class="line">...</span><br><span class="line">main.go</span><br></pre></td></tr></table></figure><p>前面的是 golang 的内部包，最后面的是我们项目路径的，因为我们 build 的时候 trim 掉前缀路径了，所以看着都是相对路径。<code>/usr/local/go</code> 是 golang 的 docker 镜像里的 <code>GOROOT</code>，我们本地 windows 的 go root 可以通过 cmd 或者 git bash 里使用下面的命令查看:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ go env GOROOT</span><br><span class="line">D:\Install\Go</span><br></pre></td></tr></table></figure><p>源码包已经相对路径存在了，但是 golang 的自带包还没有，我们需要配置映射路径，接着在 dlv 的交互里执行下面的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config substitute-path /usr/local/go D:\Install\Go</span><br></pre></td></tr></table></figure><p>然后我们在 <code>main.go:10</code> 打个断点（也可以包名.方法内的相对第几行打断点），然后 <code>c</code> 执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b main.go:10</span><br><span class="line">Breakpoint 1 (enabled) set at 0x891be0 for main.main.func1() main.go:10</span><br><span class="line">c</span><br></pre></td></tr></table></figure><p>然后我们触发下请求，例如在 Linux 上直接用 curl 触发请求：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:8080/ping</span><br></pre></td></tr></table></figure><p>命令会阻塞住，因为 server 端没回复，我们回到我们的 windows 上，界面有打印下面的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; main.main.func1() main.go:10 (hits goroutine(33):1 total:2) (PC: 0x891be0)</span><br><span class="line">     5: func main() &#123;</span><br><span class="line">     6:         var count int</span><br><span class="line">     7:         r := gin.Default()</span><br><span class="line">     8:         r.GET(&quot;/ping&quot;, func(c *gin.Context) &#123;</span><br><span class="line">     9:                 count++</span><br><span class="line">=&gt;  10:                 c.JSON(200, gin.H&#123;</span><br><span class="line">    11:                         &quot;message&quot;: &quot;pong&quot;,</span><br><span class="line">    12:                         &quot;count&quot;:   count,</span><br><span class="line">    13:                 &#125;)</span><br><span class="line">    14:         &#125;)</span><br><span class="line">    15:         r.Run()</span><br></pre></td></tr></table></figure><p>我们打印下 count 的值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p count</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>然后 c，让代码 continue 。然后能看到我们的 Linux 上的 curl 命令收到 server 端的请求了。</p><p>dlv 里常用的命令如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config max-string-len 1000  # 配置打印变量的输出长度，防止被折叠显示</span><br><span class="line">b file.go:数字              # 文件行数打断点</span><br><span class="line">c                           # 执行到下一个断点</span><br><span class="line">so                          # 直接执行完所在的当前函数</span><br><span class="line">s                           # 单步执行</span><br><span class="line">n  数字                     # 执行到后面的n行那里</span><br></pre></td></tr></table></figure><p>dlv 运行的时候会读取配置文件路径，像上面的<code>config max-string-len 1000</code> 和 <code>config substitute-path /usr/local/go D:\Install\Go</code> 都可以配置在文件里。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/go-delve/delve/issues/2583">https://github.com/go-delve/delve/issues/2583</a></li><li><a href="https://jvns.ca/blog/2020/04/29/why-strace-doesnt-work-in-docker/">https://jvns.ca/blog/2020/04/29/why-strace-doesnt-work-in-docker/</a></li><li><a href="https://github.com/go-delve/delve/issues/515#issuecomment-214911481">https://github.com/go-delve/delve/issues/515#issuecomment-214911481</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;记录下 dlv 的远程调试，建议不要在代码里加 fmt 去调试。不谈 goland 啥的远程调试，本文章目前只写 dlv 的</summary>
      
    
    
    
    
    <category term="golang" scheme="http://zhangguanzhang.github.io/tags/golang/"/>
    
    <category term="dlv" scheme="http://zhangguanzhang.github.io/tags/dlv/"/>
    
  </entry>
  
  <entry>
    <title>编译mips64le架构的consul</title>
    <link href="http://zhangguanzhang.github.io/2021/07/16/consul-mips64le/"/>
    <id>http://zhangguanzhang.github.io/2021/07/16/consul-mips64le/</id>
    <published>2021-07-16T13:08:06.000Z</published>
    <updated>2021-07-16T13:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>建议使用容器编译，否则建议 clone 进 GOPATH 里</p><h3 id="clone"><a href="#clone" class="headerlink" title="clone"></a>clone</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/hashicorp/consul.git</span><br><span class="line"><span class="built_in">cd</span> consul</span><br></pre></td></tr></table></figure><p>线上使用的是 <code>v1.8</code> 版本，这里我以 <code>v1.8.14</code> (2021/07/19 发布的)搞的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout v1.8.14</span><br></pre></td></tr></table></figure><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>拉取需要的镜像。貌似 golang 1.16 更好的支持 mips64 了，所以条件允许的话，这里可以下改下 golang 的版本试试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ head -n2 build-support/docker/Build-Go.dockerfile</span><br><span class="line">ARG GOLANG_VERSION=1.14.11</span><br><span class="line">FROM golang:$&#123;GOLANG_VERSION&#125;</span><br><span class="line">$ docker pull golang:1.14.11</span><br></pre></td></tr></table></figure><h3 id="开始编译"><a href="#开始编译" class="headerlink" title="开始编译"></a>开始编译</h3><p>相关变量可以查看 <code>GNUmakefile</code> ，实际会在容器里运行 <code>build-support/functions/20-build.sh</code> 里的 <code>function build_consul</code> ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">local container_id=$(docker create -it \</span><br><span class="line">   $&#123;volume_mount&#125; \</span><br><span class="line">   -e CGO_ENABLED=0 \</span><br><span class="line">   -e GOLDFLAGS=&quot;$&#123;GOLDFLAGS&#125;&quot; \</span><br><span class="line">   -e GOTAGS=&quot;$&#123;GOTAGS&#125;&quot; \</span><br><span class="line">   $&#123;image_name&#125; \</span><br><span class="line">   ./build-support/scripts/build-local.sh -o &quot;$&#123;XC_OS&#125;&quot; -a &quot;$&#123;XC_ARCH&#125;&quot;)</span><br></pre></td></tr></table></figure><p>而在容器里执行<code>build-support/scripts/build-local.sh  -o &quot;$&#123;XC_OS&#125;&quot; -a &quot;$&#123;XC_ARCH&#125;&quot;</code> 会执行 <code>build-support/functions/20-build.sh</code> 里的 <code>function build_consul_local</code>，主要编译是下面的这行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">debug_run env CGO_ENABLED=0 GOOS=$&#123;os&#125; GOARCH=$&#123;arch&#125; go install -ldflags &quot;$&#123;GOLDFLAGS&#125;&quot; -tags &quot;$&#123;GOTAGS&#125;&quot; &amp;&amp; cp &quot;$&#123;MAIN_GOPATH&#125;/bin/$&#123;GOBIN_EXTRA&#125;$&#123;binname&#125;&quot; &quot;$&#123;outdir&#125;/$&#123;binname&#125;&quot;</span><br></pre></td></tr></table></figure><p>开始编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">export DOCKER_BUILD_QUIET=0</span><br><span class="line">export XC_OS=linux XC_ARCH=mips64le</span><br><span class="line">make consul-docker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">==&gt; Building Consul - OSes: linux, Architectures: mips64le</span><br><span class="line">Building sequentially with go install</span><br><span class="line">---&gt;   linux/mips64le</span><br><span class="line"># github.com/boltdb/bolt</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/bolt_unix.go:62:15: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/bucket.go:135:15: undefined: brokenUnaligned</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:101:13: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:317:12: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:335:10: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:336:8: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:169:19: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:176:14: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:204:17: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:207:7: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:207:7: too many errors</span><br><span class="line">ERROR: Failed to build Consul for linux/mips64le</span><br><span class="line">make: *** [consul-docker] Error 1</span><br></pre></td></tr></table></figure><p>编译报错，是因为 boltdb 的库现在 archive 了。而且相关文件用 golang 的 build tag 区分架构了，没有预置 mips64le 的，搜了几个 issue 后搞了下编译还是失败了，仔细看上面的 path，没有读取 vendor 目录，得改编译容器里的 mod 目录下的 boltdb 文件。所以得 hack 下。</p><h3 id="hack"><a href="#hack" class="headerlink" title="hack"></a>hack</h3><p>先产生临时文件<code>/tmp/hack_consul.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /tmp/hack_consul.sh &lt;&lt; <span class="string">&#x27;consul_eof&#x27;</span></span><br><span class="line">         <span class="keyword">if</span> [ <span class="variable">$arch</span> == <span class="string">&quot;mips64le&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">cat &gt; $(go env GOPATH)/pkg/mod/github.com/boltdb/bolt@v1.3.1/bolt_mips64x.go &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">// +build mips64 mips64le</span><br><span class="line"></span><br><span class="line">package bolt</span><br><span class="line"></span><br><span class="line">// maxMapSize represents the largest mmap size supported by Bolt.</span><br><span class="line">const maxMapSize = 0x8000000000 // 512GB</span><br><span class="line"></span><br><span class="line">// maxAllocSize is the size used when creating array pointers.</span><br><span class="line">const maxAllocSize = 0x7FFFFFFF</span><br><span class="line"></span><br><span class="line">// Are unaligned load/stores broken on this arch?</span><br><span class="line">var brokenUnaligned = <span class="literal">false</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; $(go env GOPATH)/pkg/mod/github.com/boltdb/bolt@v1.3.1/bolt_mips.go &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">// +build mips mipsle</span><br><span class="line"></span><br><span class="line">package bolt</span><br><span class="line"></span><br><span class="line">// maxMapSize represents the largest mmap size supported by Bolt.</span><br><span class="line">const maxMapSize = 0x40000000 // 1GB</span><br><span class="line"></span><br><span class="line">// maxAllocSize is the size used when creating array pointers.</span><br><span class="line">const maxAllocSize = 0xFFFFFFF</span><br><span class="line"></span><br><span class="line">// Are unaligned load/stores broken on this arch?</span><br><span class="line">var brokenUnaligned = <span class="literal">false</span></span><br><span class="line">EOF</span><br><span class="line">         <span class="keyword">fi</span></span><br><span class="line">consul_eof</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>取 go install 命令的行数，在前面一行插入 hack 的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CONSUL_HACK_NUM=$(grep -Pn <span class="string">&#x27;debug_run.+?go install&#x27;</span> build-support/<span class="built_in">functions</span>/20-build.sh | cut -d: -f 1)</span><br><span class="line"></span><br><span class="line">sed -i <span class="string">&quot;$[CONSUL_HACK_NUM-1]r /tmp/hack_consul.sh&quot;</span> build-support/<span class="built_in">functions</span>/20-build.sh</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="继续编译"><a href="#继续编译" class="headerlink" title="继续编译"></a>继续编译</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ make consul-docker</span><br><span class="line">Building Golang build container</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/5 : ARG GOLANG_VERSION=1.14.11</span><br><span class="line">Step 2/5 : FROM golang:$&#123;GOLANG_VERSION&#125;</span><br><span class="line"> ---&gt; f93db70cba35</span><br><span class="line">Step 3/5 : ARG GOTOOLS=&quot;github.com/elazarl/go-bindata-assetfs/...    github.com/hashicorp/go-bindata/...    golang.org/x/tools/cmd/cover    golang.org/x/tools/cmd/stringer    github.com/axw/gocov/gocov    gopkg.in/matm/v1/gocov-html&quot;</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; e0979c09e72f</span><br><span class="line">Step 4/5 : RUN GO111MODULE=on go get -v $&#123;GOTOOLS&#125; &amp;&amp; mkdir -p /consul</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 9cb7abf49b86</span><br><span class="line">Step 5/5 : WORKDIR /consul</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; fb1467e4623c</span><br><span class="line">Successfully built fb1467e4623c</span><br><span class="line">Successfully tagged consul-build-go:latest</span><br><span class="line">==&gt; Building Consul</span><br><span class="line">Ensuring Go modules are up to date</span><br><span class="line">Creating the Go Build Container with image: consul-build-go</span><br><span class="line">Copying the source from &#x27;/root/go/src/github.com/hashicorp/consul&#x27; to /consul</span><br><span class="line">Running build in container</span><br><span class="line">==&gt; Building Consul - OSes: linux, Architectures: mips64le</span><br><span class="line">Building sequentially with go install</span><br><span class="line">---&gt;   linux/mips64le</span><br><span class="line">Copying back artifacts</span><br><span class="line"></span><br><span class="line">$ ls -l pkg/bin/linux_mips64le/</span><br><span class="line">total 112192</span><br><span class="line">-rwxr-xr-x 1 root root 114881061 Jul 16 12:34 consul</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>拷贝到龙芯机器上测试下可否运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ lscpu</span><br><span class="line">Architecture:          mips64el</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                8</span><br><span class="line">On-line CPU(s) list:   0-7</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    4</span><br><span class="line">座：                 2</span><br><span class="line">NUMA 节点：         2</span><br><span class="line">型号名称：        Loongson-3A R4 (Loongson-3B4000)</span><br><span class="line">CPU max MHz:           1800.0000</span><br><span class="line">CPU min MHz:           900.0000</span><br><span class="line">BogoMIPS：            3594.02</span><br><span class="line">L1d 缓存：          64K</span><br><span class="line">L1i 缓存：          64K</span><br><span class="line">L2 缓存：           256K</span><br><span class="line">L3 缓存：           2048K</span><br><span class="line">NUMA 节点0 CPU：    0-3</span><br><span class="line">NUMA 节点1 CPU：    4-7</span><br><span class="line">$ /tmp/consul --version</span><br><span class="line">Consul v1.8.14</span><br><span class="line">Revision 94c1bdd3b+CHANGES</span><br><span class="line">Protocol 2 spoken by default, understands 2 to 3 (agent will automatically use protocol &gt;2 when speaking to compatible agents)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/boltdb/bolt/issues/656">https://github.com/boltdb/bolt/issues/656</a></li><li><a href="https://github.com/boltdb/bolt/pull/663/files">https://github.com/boltdb/bolt/pull/663/files</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;编译&quot;&gt;&lt;a href=&quot;#编译&quot; class=&quot;headerlink&quot; title=&quot;编译&quot;&gt;&lt;/a&gt;编译&lt;/h2&gt;&lt;p&gt;建议使用容器编译，否则建议 clone 进 GOPATH 里&lt;/p&gt;
&lt;h3 id=&quot;clone&quot;&gt;&lt;a href=&quot;#clone&quot; cla</summary>
      
    
    
    
    
    <category term="consul" scheme="http://zhangguanzhang.github.io/tags/consul/"/>
    
    <category term="mips64le" scheme="http://zhangguanzhang.github.io/tags/mips64le/"/>
    
  </entry>
  
  <entry>
    <title>机器重启后 kube-apiserver 无法启动，etcd刷(error &quot;EOF&quot;, ServerName &quot;&quot;)</title>
    <link href="http://zhangguanzhang.github.io/2021/07/06/kube-apiserver-cannot-start-after-reboot/"/>
    <id>http://zhangguanzhang.github.io/2021/07/06/kube-apiserver-cannot-start-after-reboot/</id>
    <published>2021-07-06T14:46:06.000Z</published>
    <updated>2021-07-06T14:46:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>三个 master （etcd 也在 master 上，master上也有 kubelet）和 n 个 node。master 上组件(kube-controller-manager,kube-scheduler,kubelet)的 apiserver 的ip 都是 127.0.0.1:6443。kube-apiserver的 etcd 地址写了三个 etcd 的。k8s 版本为 <code>v1.15.5</code></p><h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>93 这台 master 机器重启后，发现 93 节点 <code>NotReady</code>，上去看了下 kubelet 无法连上本机的 kube-apiserver。kube-apiserver 运行个十几秒后才退出，<code>etcd</code> 一直刷如下日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">etcd: rejected connection from &quot;10.129.173.93:47566&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47614&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47714&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47874&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47948&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br></pre></td></tr></table></figure><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><h3 id="etcd的错误"><a href="#etcd的错误" class="headerlink" title="etcd的错误"></a>etcd的错误</h3><p>这个 EOF 是 etcd 的客户端没正常关闭造成的，etcd 之间也会互相连，先查看下 etcd 状态，因为 k8s 版本是 <code>v1.15.5</code>。默认使用的 etcd v3 api。etcd 的<code>--listen-client-urls</code>里我们包含了一个<code>http://127.0.0.1:2379</code>。所以下面命令不需要带<code>--endpoints=xxxx</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> ETCDCTL_API=3</span><br><span class="line">$ etcdctl member list</span><br><span class="line">172cf33e1b47c1c8, started, etcd2, https://10.129.173.93:2380, https://10.129.173.93:2379</span><br><span class="line">35f3e39e5dd3195e, started, etcd3, https://10.129.173.94:2380, https://10.129.173.94:2379</span><br><span class="line">47a96a577fe753d1, started, etcd1, https://10.129.173.92:2380, https://10.129.173.92:2379</span><br></pre></td></tr></table></figure><p>看下 dbSize，推荐使用 <code>--write-out=table</code> 看，会自动换算人性化的 size。如果需要压缩的 revision 推荐使用<code>-w=json</code> 来获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl endpoint status -w&#x3D;table</span><br></pre></td></tr></table></figure><p>看了下没达到默认的 2G，如果满了需要压缩执行下面的命令。每台的 revision 不同，每台都要执行下面的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 压缩旧版本</span><br><span class="line">etcdctl compact $revision</span><br><span class="line"># 清理碎片</span><br><span class="line">etcdctl defrag</span><br><span class="line"># 忽略告警</span><br><span class="line">etcdctl alarm disarm</span><br></pre></td></tr></table></figure><p>确认 etcd 没有问题，然后其他两个机器的 kube-apiserver 都正常，说明触发 EOF 的是 93 这台上面的 kube-apiserver。停止它后手动前台下。</p><h3 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop kube-apiserver</span><br><span class="line">$ systemctl cat kube-apiserver</span><br></pre></td></tr></table></figure><p>然后终端上用<code>ExecStart</code>的部分，把<code>--v=2</code>改成<code>--v=5</code>启动，下面是输出。全部放出来，方便其他遇到的人搜到这篇文章。太长了，我就先放到最后吧。</p><p>启动后 34 秒后刷了一堆<code>Get https://localhost:6443/apis/xxxxxxx: dial tcp 127.0.0.1:6443: i/o timeout</code> 就退出了。参照之前的<a href="https://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/">文章</a>查了下进程，确认没有安全软件安全狗。</p><p>除去 panic 和明确的 error 和 flag 相关的报错，k8s 相关的二进制无法启动基本是和 ipv6 有关系。查看下 ipv6 情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -a |&amp; grep -E <span class="string">&#x27;(all|default)\.disable_ipv6&#x27;</span></span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br></pre></td></tr></table></figure><p>无论是显示的关还是开，反转下这俩参数，然后再试试发现启动后不会退出了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.disable_ipv6=0</span><br><span class="line">sysctl -w net.ipv6.conf.default.disable_ipv6=0</span><br></pre></td></tr></table></figure><p>然后参数固化下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;&gt; &#x2F;etc&#x2F;sysctl.conf &lt;&lt;EOF</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 &#x3D; 0</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 &#x3D; 0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="启动异常时候的日志"><a href="#启动异常时候的日志" class="headerlink" title="启动异常时候的日志"></a>启动异常时候的日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br><span class="line">1306</span><br><span class="line">1307</span><br><span class="line">1308</span><br><span class="line">1309</span><br><span class="line">1310</span><br><span class="line">1311</span><br><span class="line">1312</span><br><span class="line">1313</span><br><span class="line">1314</span><br><span class="line">1315</span><br><span class="line">1316</span><br><span class="line">1317</span><br><span class="line">1318</span><br><span class="line">1319</span><br><span class="line">1320</span><br><span class="line">1321</span><br><span class="line">1322</span><br><span class="line">1323</span><br><span class="line">1324</span><br><span class="line">1325</span><br><span class="line">1326</span><br><span class="line">1327</span><br><span class="line">1328</span><br><span class="line">1329</span><br><span class="line">1330</span><br><span class="line">1331</span><br><span class="line">1332</span><br><span class="line">1333</span><br><span class="line">1334</span><br><span class="line">1335</span><br><span class="line">1336</span><br><span class="line">1337</span><br><span class="line">1338</span><br><span class="line">1339</span><br><span class="line">1340</span><br><span class="line">1341</span><br><span class="line">1342</span><br><span class="line">1343</span><br><span class="line">1344</span><br><span class="line">1345</span><br><span class="line">1346</span><br><span class="line">1347</span><br><span class="line">1348</span><br><span class="line">1349</span><br><span class="line">1350</span><br><span class="line">1351</span><br><span class="line">1352</span><br><span class="line">1353</span><br><span class="line">1354</span><br><span class="line">1355</span><br><span class="line">1356</span><br><span class="line">1357</span><br><span class="line">1358</span><br><span class="line">1359</span><br><span class="line">1360</span><br><span class="line">1361</span><br><span class="line">1362</span><br><span class="line">1363</span><br><span class="line">1364</span><br><span class="line">1365</span><br><span class="line">1366</span><br><span class="line">1367</span><br><span class="line">1368</span><br><span class="line">1369</span><br><span class="line">1370</span><br><span class="line">1371</span><br><span class="line">1372</span><br><span class="line">1373</span><br><span class="line">1374</span><br><span class="line">1375</span><br><span class="line">1376</span><br><span class="line">1377</span><br><span class="line">1378</span><br><span class="line">1379</span><br><span class="line">1380</span><br><span class="line">1381</span><br><span class="line">1382</span><br><span class="line">1383</span><br><span class="line">1384</span><br><span class="line">1385</span><br><span class="line">1386</span><br><span class="line">1387</span><br><span class="line">1388</span><br><span class="line">1389</span><br><span class="line">1390</span><br><span class="line">1391</span><br><span class="line">1392</span><br><span class="line">1393</span><br><span class="line">1394</span><br><span class="line">1395</span><br><span class="line">1396</span><br><span class="line">1397</span><br><span class="line">1398</span><br><span class="line">1399</span><br><span class="line">1400</span><br><span class="line">1401</span><br><span class="line">1402</span><br><span class="line">1403</span><br><span class="line">1404</span><br><span class="line">1405</span><br><span class="line">1406</span><br><span class="line">1407</span><br><span class="line">1408</span><br><span class="line">1409</span><br><span class="line">1410</span><br><span class="line">1411</span><br><span class="line">1412</span><br><span class="line">1413</span><br><span class="line">1414</span><br><span class="line">1415</span><br><span class="line">1416</span><br><span class="line">1417</span><br><span class="line">1418</span><br><span class="line">1419</span><br><span class="line">1420</span><br><span class="line">1421</span><br><span class="line">1422</span><br><span class="line">1423</span><br><span class="line">1424</span><br><span class="line">1425</span><br><span class="line">1426</span><br><span class="line">1427</span><br><span class="line">1428</span><br><span class="line">1429</span><br><span class="line">1430</span><br><span class="line">1431</span><br><span class="line">1432</span><br><span class="line">1433</span><br><span class="line">1434</span><br><span class="line">1435</span><br><span class="line">1436</span><br><span class="line">1437</span><br><span class="line">1438</span><br><span class="line">1439</span><br><span class="line">1440</span><br><span class="line">1441</span><br><span class="line">1442</span><br><span class="line">1443</span><br><span class="line">1444</span><br><span class="line">1445</span><br><span class="line">1446</span><br><span class="line">1447</span><br><span class="line">1448</span><br><span class="line">1449</span><br><span class="line">1450</span><br><span class="line">1451</span><br><span class="line">1452</span><br><span class="line">1453</span><br><span class="line">1454</span><br><span class="line">1455</span><br><span class="line">1456</span><br><span class="line">1457</span><br><span class="line">1458</span><br><span class="line">1459</span><br><span class="line">1460</span><br><span class="line">1461</span><br><span class="line">1462</span><br><span class="line">1463</span><br><span class="line">1464</span><br><span class="line">1465</span><br><span class="line">1466</span><br><span class="line">1467</span><br><span class="line">1468</span><br><span class="line">1469</span><br><span class="line">1470</span><br><span class="line">1471</span><br><span class="line">1472</span><br><span class="line">1473</span><br><span class="line">1474</span><br><span class="line">1475</span><br><span class="line">1476</span><br><span class="line">1477</span><br><span class="line">1478</span><br><span class="line">1479</span><br><span class="line">1480</span><br><span class="line">1481</span><br><span class="line">1482</span><br><span class="line">1483</span><br><span class="line">1484</span><br><span class="line">1485</span><br><span class="line">1486</span><br><span class="line">1487</span><br><span class="line">1488</span><br><span class="line">1489</span><br><span class="line">1490</span><br><span class="line">1491</span><br><span class="line">1492</span><br><span class="line">1493</span><br><span class="line">1494</span><br><span class="line">1495</span><br><span class="line">1496</span><br><span class="line">1497</span><br><span class="line">1498</span><br><span class="line">1499</span><br><span class="line">1500</span><br><span class="line">1501</span><br><span class="line">1502</span><br><span class="line">1503</span><br><span class="line">1504</span><br><span class="line">1505</span><br><span class="line">1506</span><br><span class="line">1507</span><br><span class="line">1508</span><br><span class="line">1509</span><br><span class="line">1510</span><br><span class="line">1511</span><br><span class="line">1512</span><br><span class="line">1513</span><br><span class="line">1514</span><br><span class="line">1515</span><br><span class="line">1516</span><br><span class="line">1517</span><br><span class="line">1518</span><br><span class="line">1519</span><br><span class="line">1520</span><br><span class="line">1521</span><br><span class="line">1522</span><br><span class="line">1523</span><br><span class="line">1524</span><br><span class="line">1525</span><br><span class="line">1526</span><br><span class="line">1527</span><br><span class="line">1528</span><br><span class="line">1529</span><br><span class="line">1530</span><br><span class="line">1531</span><br><span class="line">1532</span><br><span class="line">1533</span><br><span class="line">1534</span><br><span class="line">1535</span><br><span class="line">1536</span><br><span class="line">1537</span><br><span class="line">1538</span><br><span class="line">1539</span><br><span class="line">1540</span><br><span class="line">1541</span><br><span class="line">1542</span><br><span class="line">1543</span><br><span class="line">1544</span><br><span class="line">1545</span><br><span class="line">1546</span><br><span class="line">1547</span><br><span class="line">1548</span><br><span class="line">1549</span><br><span class="line">1550</span><br><span class="line">1551</span><br><span class="line">1552</span><br><span class="line">1553</span><br><span class="line">1554</span><br><span class="line">1555</span><br><span class="line">1556</span><br><span class="line">1557</span><br><span class="line">1558</span><br><span class="line">1559</span><br><span class="line">1560</span><br><span class="line">1561</span><br><span class="line">1562</span><br><span class="line">1563</span><br><span class="line">1564</span><br><span class="line">1565</span><br><span class="line">1566</span><br><span class="line">1567</span><br><span class="line">1568</span><br><span class="line">1569</span><br><span class="line">1570</span><br><span class="line">1571</span><br><span class="line">1572</span><br><span class="line">1573</span><br><span class="line">1574</span><br><span class="line">1575</span><br><span class="line">1576</span><br><span class="line">1577</span><br><span class="line">1578</span><br><span class="line">1579</span><br><span class="line">1580</span><br><span class="line">1581</span><br><span class="line">1582</span><br><span class="line">1583</span><br><span class="line">1584</span><br><span class="line">1585</span><br><span class="line">1586</span><br><span class="line">1587</span><br><span class="line">1588</span><br><span class="line">1589</span><br><span class="line">1590</span><br><span class="line">1591</span><br><span class="line">1592</span><br><span class="line">1593</span><br><span class="line">1594</span><br><span class="line">1595</span><br><span class="line">1596</span><br><span class="line">1597</span><br><span class="line">1598</span><br><span class="line">1599</span><br><span class="line">1600</span><br><span class="line">1601</span><br><span class="line">1602</span><br><span class="line">1603</span><br><span class="line">1604</span><br><span class="line">1605</span><br><span class="line">1606</span><br><span class="line">1607</span><br><span class="line">1608</span><br><span class="line">1609</span><br><span class="line">1610</span><br><span class="line">1611</span><br><span class="line">1612</span><br><span class="line">1613</span><br><span class="line">1614</span><br><span class="line">1615</span><br><span class="line">1616</span><br><span class="line">1617</span><br><span class="line">1618</span><br><span class="line">1619</span><br><span class="line">1620</span><br><span class="line">1621</span><br><span class="line">1622</span><br><span class="line">1623</span><br><span class="line">1624</span><br><span class="line">1625</span><br><span class="line">1626</span><br><span class="line">1627</span><br><span class="line">1628</span><br><span class="line">1629</span><br><span class="line">1630</span><br><span class="line">1631</span><br><span class="line">1632</span><br><span class="line">1633</span><br><span class="line">1634</span><br><span class="line">1635</span><br><span class="line">1636</span><br><span class="line">1637</span><br><span class="line">1638</span><br><span class="line">1639</span><br><span class="line">1640</span><br><span class="line">1641</span><br><span class="line">1642</span><br><span class="line">1643</span><br><span class="line">1644</span><br><span class="line">1645</span><br><span class="line">1646</span><br><span class="line">1647</span><br><span class="line">1648</span><br><span class="line">1649</span><br><span class="line">1650</span><br><span class="line">1651</span><br><span class="line">1652</span><br><span class="line">1653</span><br><span class="line">1654</span><br><span class="line">1655</span><br><span class="line">1656</span><br><span class="line">1657</span><br><span class="line">1658</span><br><span class="line">1659</span><br><span class="line">1660</span><br><span class="line">1661</span><br><span class="line">1662</span><br><span class="line">1663</span><br><span class="line">1664</span><br><span class="line">1665</span><br><span class="line">1666</span><br><span class="line">1667</span><br><span class="line">1668</span><br><span class="line">1669</span><br><span class="line">1670</span><br><span class="line">1671</span><br><span class="line">1672</span><br><span class="line">1673</span><br><span class="line">1674</span><br><span class="line">1675</span><br><span class="line">1676</span><br><span class="line">1677</span><br><span class="line">1678</span><br><span class="line">1679</span><br><span class="line">1680</span><br><span class="line">1681</span><br><span class="line">1682</span><br><span class="line">1683</span><br><span class="line">1684</span><br><span class="line">1685</span><br><span class="line">1686</span><br><span class="line">1687</span><br><span class="line">1688</span><br><span class="line">1689</span><br><span class="line">1690</span><br><span class="line">1691</span><br><span class="line">1692</span><br><span class="line">1693</span><br><span class="line">1694</span><br><span class="line">1695</span><br><span class="line">1696</span><br><span class="line">1697</span><br><span class="line">1698</span><br><span class="line">1699</span><br><span class="line">1700</span><br><span class="line">1701</span><br><span class="line">1702</span><br><span class="line">1703</span><br><span class="line">1704</span><br><span class="line">1705</span><br><span class="line">1706</span><br><span class="line">1707</span><br><span class="line">1708</span><br><span class="line">1709</span><br><span class="line">1710</span><br><span class="line">1711</span><br><span class="line">1712</span><br><span class="line">1713</span><br><span class="line">1714</span><br><span class="line">1715</span><br><span class="line">1716</span><br><span class="line">1717</span><br><span class="line">1718</span><br><span class="line">1719</span><br><span class="line">1720</span><br><span class="line">1721</span><br><span class="line">1722</span><br><span class="line">1723</span><br><span class="line">1724</span><br><span class="line">1725</span><br><span class="line">1726</span><br><span class="line">1727</span><br><span class="line">1728</span><br><span class="line">1729</span><br><span class="line">1730</span><br><span class="line">1731</span><br><span class="line">1732</span><br><span class="line">1733</span><br><span class="line">1734</span><br><span class="line">1735</span><br><span class="line">1736</span><br><span class="line">1737</span><br><span class="line">1738</span><br><span class="line">1739</span><br><span class="line">1740</span><br><span class="line">1741</span><br><span class="line">1742</span><br><span class="line">1743</span><br><span class="line">1744</span><br><span class="line">1745</span><br><span class="line">1746</span><br><span class="line">1747</span><br><span class="line">1748</span><br><span class="line">1749</span><br><span class="line">1750</span><br><span class="line">1751</span><br><span class="line">1752</span><br><span class="line">1753</span><br><span class="line">1754</span><br><span class="line">1755</span><br><span class="line">1756</span><br><span class="line">1757</span><br><span class="line">1758</span><br><span class="line">1759</span><br><span class="line">1760</span><br><span class="line">1761</span><br><span class="line">1762</span><br><span class="line">1763</span><br><span class="line">1764</span><br><span class="line">1765</span><br><span class="line">1766</span><br><span class="line">1767</span><br><span class="line">1768</span><br><span class="line">1769</span><br><span class="line">1770</span><br><span class="line">1771</span><br></pre></td><td class="code"><pre><span class="line">Flag --admission-control has been deprecated, Use --enable-admission-plugins or --disable-admission-plugins instead. Will be removed in a future version.</span><br><span class="line">Flag --insecure-port has been deprecated, This flag will be removed in a future version.</span><br><span class="line">Flag --insecure-bind-address has been deprecated, This flag will be removed in a future version.</span><br><span class="line">Flag --enable-swagger-ui has been deprecated, swagger 1.2 support has been removed</span><br><span class="line">I0705 22:38:00.836860   18145 flags.go:33] FLAG: --address&#x3D;&quot;127.0.0.1&quot;</span><br><span class="line">I0705 22:38:00.836914   18145 flags.go:33] FLAG: --admission-control&#x3D;&quot;[NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction]&quot;</span><br><span class="line">I0705 22:38:00.836926   18145 flags.go:33] FLAG: --admission-control-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.836932   18145 flags.go:33] FLAG: --advertise-address&#x3D;&quot;&lt;nil&gt;&quot;</span><br><span class="line">I0705 22:38:00.836936   18145 flags.go:33] FLAG: --allow-privileged&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.836942   18145 flags.go:33] FLAG: --alsologtostderr&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836948   18145 flags.go:33] FLAG: --anonymous-auth&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836952   18145 flags.go:33] FLAG: --api-audiences&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.836959   18145 flags.go:33] FLAG: --apiserver-count&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.836965   18145 flags.go:33] FLAG: --audit-dynamic-configuration&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836969   18145 flags.go:33] FLAG: --audit-log-batch-buffer-size&#x3D;&quot;10000&quot;</span><br><span class="line">I0705 22:38:00.836973   18145 flags.go:33] FLAG: --audit-log-batch-max-size&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.836977   18145 flags.go:33] FLAG: --audit-log-batch-max-wait&#x3D;&quot;0s&quot;</span><br><span class="line">I0705 22:38:00.836982   18145 flags.go:33] FLAG: --audit-log-batch-throttle-burst&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.836986   18145 flags.go:33] FLAG: --audit-log-batch-throttle-enable&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836990   18145 flags.go:33] FLAG: --audit-log-batch-throttle-qps&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.836995   18145 flags.go:33] FLAG: --audit-log-format&#x3D;&quot;json&quot;</span><br><span class="line">I0705 22:38:00.837000   18145 flags.go:33] FLAG: --audit-log-maxage&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837004   18145 flags.go:33] FLAG: --audit-log-maxbackup&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837008   18145 flags.go:33] FLAG: --audit-log-maxsize&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837011   18145 flags.go:33] FLAG: --audit-log-mode&#x3D;&quot;blocking&quot;</span><br><span class="line">I0705 22:38:00.837015   18145 flags.go:33] FLAG: --audit-log-path&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837019   18145 flags.go:33] FLAG: --audit-log-truncate-enabled&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837023   18145 flags.go:33] FLAG: --audit-log-truncate-max-batch-size&#x3D;&quot;10485760&quot;</span><br><span class="line">I0705 22:38:00.837029   18145 flags.go:33] FLAG: --audit-log-truncate-max-event-size&#x3D;&quot;102400&quot;</span><br><span class="line">I0705 22:38:00.837034   18145 flags.go:33] FLAG: --audit-log-version&#x3D;&quot;audit.k8s.io&#x2F;v1&quot;</span><br><span class="line">I0705 22:38:00.837038   18145 flags.go:33] FLAG: --audit-policy-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837042   18145 flags.go:33] FLAG: --audit-webhook-batch-buffer-size&#x3D;&quot;10000&quot;</span><br><span class="line">I0705 22:38:00.837046   18145 flags.go:33] FLAG: --audit-webhook-batch-initial-backoff&#x3D;&quot;10s&quot;</span><br><span class="line">I0705 22:38:00.837051   18145 flags.go:33] FLAG: --audit-webhook-batch-max-size&#x3D;&quot;400&quot;</span><br><span class="line">I0705 22:38:00.837055   18145 flags.go:33] FLAG: --audit-webhook-batch-max-wait&#x3D;&quot;30s&quot;</span><br><span class="line">I0705 22:38:00.837060   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-burst&#x3D;&quot;15&quot;</span><br><span class="line">I0705 22:38:00.837064   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-enable&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837068   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-qps&#x3D;&quot;10&quot;</span><br><span class="line">I0705 22:38:00.837073   18145 flags.go:33] FLAG: --audit-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837076   18145 flags.go:33] FLAG: --audit-webhook-initial-backoff&#x3D;&quot;10s&quot;</span><br><span class="line">I0705 22:38:00.837080   18145 flags.go:33] FLAG: --audit-webhook-mode&#x3D;&quot;batch&quot;</span><br><span class="line">I0705 22:38:00.837086   18145 flags.go:33] FLAG: --audit-webhook-truncate-enabled&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837090   18145 flags.go:33] FLAG: --audit-webhook-truncate-max-batch-size&#x3D;&quot;10485760&quot;</span><br><span class="line">I0705 22:38:00.837094   18145 flags.go:33] FLAG: --audit-webhook-truncate-max-event-size&#x3D;&quot;102400&quot;</span><br><span class="line">I0705 22:38:00.837098   18145 flags.go:33] FLAG: --audit-webhook-version&#x3D;&quot;audit.k8s.io&#x2F;v1&quot;</span><br><span class="line">I0705 22:38:00.837103   18145 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl&#x3D;&quot;2m0s&quot;</span><br><span class="line">I0705 22:38:00.837107   18145 flags.go:33] FLAG: --authentication-token-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837119   18145 flags.go:33] FLAG: --authorization-mode&#x3D;&quot;[Node,RBAC]&quot;</span><br><span class="line">I0705 22:38:00.837125   18145 flags.go:33] FLAG: --authorization-policy-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837128   18145 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl&#x3D;&quot;5m0s&quot;</span><br><span class="line">I0705 22:38:00.837132   18145 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl&#x3D;&quot;30s&quot;</span><br><span class="line">I0705 22:38:00.837136   18145 flags.go:33] FLAG: --authorization-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837140   18145 flags.go:33] FLAG: --basic-auth-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;basic-auth.csv&quot;</span><br><span class="line">I0705 22:38:00.837145   18145 flags.go:33] FLAG: --bind-address&#x3D;&quot;0.0.0.0&quot;</span><br><span class="line">I0705 22:38:00.837149   18145 flags.go:33] FLAG: --cert-dir&#x3D;&quot;&#x2F;var&#x2F;run&#x2F;kubernetes&quot;</span><br><span class="line">I0705 22:38:00.837153   18145 flags.go:33] FLAG: --client-ca-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837158   18145 flags.go:33] FLAG: --cloud-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837162   18145 flags.go:33] FLAG: --cloud-provider&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837165   18145 flags.go:33] FLAG: --cloud-provider-gce-lb-src-cidrs&#x3D;&quot;130.211.0.0&#x2F;22,209.85.152.0&#x2F;22,209.85.204.0&#x2F;22,35.191.0.0&#x2F;16&quot;</span><br><span class="line">I0705 22:38:00.837172   18145 flags.go:33] FLAG: --contention-profiling&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837176   18145 flags.go:33] FLAG: --cors-allowed-origins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837182   18145 flags.go:33] FLAG: --default-not-ready-toleration-seconds&#x3D;&quot;300&quot;</span><br><span class="line">I0705 22:38:00.837186   18145 flags.go:33] FLAG: --default-unreachable-toleration-seconds&#x3D;&quot;300&quot;</span><br><span class="line">I0705 22:38:00.837190   18145 flags.go:33] FLAG: --default-watch-cache-size&#x3D;&quot;100&quot;</span><br><span class="line">I0705 22:38:00.837194   18145 flags.go:33] FLAG: --delete-collection-workers&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.837198   18145 flags.go:33] FLAG: --deserialization-cache-size&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837202   18145 flags.go:33] FLAG: --disable-admission-plugins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837206   18145 flags.go:33] FLAG: --enable-admission-plugins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837213   18145 flags.go:33] FLAG: --enable-aggregator-routing&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837217   18145 flags.go:33] FLAG: --enable-bootstrap-token-auth&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837221   18145 flags.go:33] FLAG: --enable-garbage-collector&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837225   18145 flags.go:33] FLAG: --enable-inflight-quota-handler&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837228   18145 flags.go:33] FLAG: --enable-logs-handler&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837232   18145 flags.go:33] FLAG: --enable-swagger-ui&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837236   18145 flags.go:33] FLAG: --encryption-provider-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837240   18145 flags.go:33] FLAG: --endpoint-reconciler-type&#x3D;&quot;lease&quot;</span><br><span class="line">I0705 22:38:00.837244   18145 flags.go:33] FLAG: --etcd-cafile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837248   18145 flags.go:33] FLAG: --etcd-certfile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837253   18145 flags.go:33] FLAG: --etcd-compaction-interval&#x3D;&quot;5m0s&quot;</span><br><span class="line">I0705 22:38:00.837257   18145 flags.go:33] FLAG: --etcd-count-metric-poll-period&#x3D;&quot;1m0s&quot;</span><br><span class="line">I0705 22:38:00.837261   18145 flags.go:33] FLAG: --etcd-keyfile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837265   18145 flags.go:33] FLAG: --etcd-prefix&#x3D;&quot;&#x2F;registry&quot;</span><br><span class="line">I0705 22:38:00.837269   18145 flags.go:33] FLAG: --etcd-servers&#x3D;&quot;[https:&#x2F;&#x2F;10.129.173.92:2379,https:&#x2F;&#x2F;10.129.173.93:2379,https:&#x2F;&#x2F;10.129.173.94:2379]&quot;</span><br><span class="line">I0705 22:38:00.837277   18145 flags.go:33] FLAG: --etcd-servers-overrides&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837283   18145 flags.go:33] FLAG: --event-ttl&#x3D;&quot;1h0m0s&quot;</span><br><span class="line">I0705 22:38:00.837287   18145 flags.go:33] FLAG: --experimental-encryption-provider-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837291   18145 flags.go:33] FLAG: --external-hostname&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837296   18145 flags.go:33] FLAG: --feature-gates&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837303   18145 flags.go:33] FLAG: --help&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837307   18145 flags.go:33] FLAG: --http2-max-streams-per-connection&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837311   18145 flags.go:33] FLAG: --insecure-bind-address&#x3D;&quot;127.0.0.1&quot;</span><br><span class="line">I0705 22:38:00.837315   18145 flags.go:33] FLAG: --insecure-port&#x3D;&quot;8073&quot;</span><br><span class="line">I0705 22:38:00.837319   18145 flags.go:33] FLAG: --kubelet-certificate-authority&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837323   18145 flags.go:33] FLAG: --kubelet-client-certificate&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837331   18145 flags.go:33] FLAG: --kubelet-client-key&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837336   18145 flags.go:33] FLAG: --kubelet-https&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837340   18145 flags.go:33] FLAG: --kubelet-port&#x3D;&quot;10250&quot;</span><br><span class="line">I0705 22:38:00.837346   18145 flags.go:33] FLAG: --kubelet-preferred-address-types&#x3D;&quot;[Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]&quot;</span><br><span class="line">I0705 22:38:00.837352   18145 flags.go:33] FLAG: --kubelet-read-only-port&#x3D;&quot;10255&quot;</span><br><span class="line">I0705 22:38:00.837356   18145 flags.go:33] FLAG: --kubelet-timeout&#x3D;&quot;5s&quot;</span><br><span class="line">I0705 22:38:00.837360   18145 flags.go:33] FLAG: --kubernetes-service-node-port&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837364   18145 flags.go:33] FLAG: --log-backtrace-at&#x3D;&quot;:0&quot;</span><br><span class="line">I0705 22:38:00.837370   18145 flags.go:33] FLAG: --log-dir&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837375   18145 flags.go:33] FLAG: --log-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837379   18145 flags.go:33] FLAG: --log-file-max-size&#x3D;&quot;1800&quot;</span><br><span class="line">I0705 22:38:00.837383   18145 flags.go:33] FLAG: --log-flush-frequency&#x3D;&quot;5s&quot;</span><br><span class="line">I0705 22:38:00.837387   18145 flags.go:33] FLAG: --logtostderr&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837391   18145 flags.go:33] FLAG: --master-service-namespace&#x3D;&quot;default&quot;</span><br><span class="line">I0705 22:38:00.837395   18145 flags.go:33] FLAG: --max-connection-bytes-per-sec&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837399   18145 flags.go:33] FLAG: --max-mutating-requests-inflight&#x3D;&quot;200&quot;</span><br><span class="line">I0705 22:38:00.837403   18145 flags.go:33] FLAG: --max-requests-inflight&#x3D;&quot;400&quot;</span><br><span class="line">I0705 22:38:00.837407   18145 flags.go:33] FLAG: --min-request-timeout&#x3D;&quot;1800&quot;</span><br><span class="line">I0705 22:38:00.837411   18145 flags.go:33] FLAG: --oidc-ca-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837415   18145 flags.go:33] FLAG: --oidc-client-id&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837419   18145 flags.go:33] FLAG: --oidc-groups-claim&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837422   18145 flags.go:33] FLAG: --oidc-groups-prefix&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837426   18145 flags.go:33] FLAG: --oidc-issuer-url&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837430   18145 flags.go:33] FLAG: --oidc-required-claim&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837436   18145 flags.go:33] FLAG: --oidc-signing-algs&#x3D;&quot;[RS256]&quot;</span><br><span class="line">I0705 22:38:00.837443   18145 flags.go:33] FLAG: --oidc-username-claim&#x3D;&quot;sub&quot;</span><br><span class="line">I0705 22:38:00.837446   18145 flags.go:33] FLAG: --oidc-username-prefix&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837450   18145 flags.go:33] FLAG: --port&#x3D;&quot;8073&quot;</span><br><span class="line">I0705 22:38:00.837454   18145 flags.go:33] FLAG: --profiling&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837458   18145 flags.go:33] FLAG: --proxy-client-cert-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;aggregator-proxy.pem&quot;</span><br><span class="line">I0705 22:38:00.837463   18145 flags.go:33] FLAG: --proxy-client-key-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;aggregator-proxy-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837468   18145 flags.go:33] FLAG: --request-timeout&#x3D;&quot;1m0s&quot;</span><br><span class="line">I0705 22:38:00.837472   18145 flags.go:33] FLAG: --requestheader-allowed-names&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837476   18145 flags.go:33] FLAG: --requestheader-client-ca-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837480   18145 flags.go:33] FLAG: --requestheader-extra-headers-prefix&#x3D;&quot;[X-Remote-Extra-]&quot;</span><br><span class="line">I0705 22:38:00.837487   18145 flags.go:33] FLAG: --requestheader-group-headers&#x3D;&quot;[X-Remote-Group]&quot;</span><br><span class="line">I0705 22:38:00.837492   18145 flags.go:33] FLAG: --requestheader-username-headers&#x3D;&quot;[X-Remote-User]&quot;</span><br><span class="line">I0705 22:38:00.837498   18145 flags.go:33] FLAG: --runtime-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837505   18145 flags.go:33] FLAG: --secure-port&#x3D;&quot;6443&quot;</span><br><span class="line">I0705 22:38:00.837510   18145 flags.go:33] FLAG: --service-account-api-audiences&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837515   18145 flags.go:33] FLAG: --service-account-issuer&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837518   18145 flags.go:33] FLAG: --service-account-key-file&#x3D;&quot;[&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca-key.pem]&quot;</span><br><span class="line">I0705 22:38:00.837526   18145 flags.go:33] FLAG: --service-account-lookup&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837530   18145 flags.go:33] FLAG: --service-account-max-token-expiration&#x3D;&quot;0s&quot;</span><br><span class="line">I0705 22:38:00.837534   18145 flags.go:33] FLAG: --service-account-signing-key-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837538   18145 flags.go:33] FLAG: --service-cluster-ip-range&#x3D;&quot;172.26.0.0&#x2F;16&quot;</span><br><span class="line">I0705 22:38:00.837544   18145 flags.go:33] FLAG: --service-node-port-range&#x3D;&quot;20000-40000&quot;</span><br><span class="line">I0705 22:38:00.837558   18145 flags.go:33] FLAG: --skip-headers&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837566   18145 flags.go:33] FLAG: --skip-log-headers&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837570   18145 flags.go:33] FLAG: --ssh-keyfile&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837574   18145 flags.go:33] FLAG: --ssh-user&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837578   18145 flags.go:33] FLAG: --stderrthreshold&#x3D;&quot;2&quot;</span><br><span class="line">I0705 22:38:00.837582   18145 flags.go:33] FLAG: --storage-backend&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837585   18145 flags.go:33] FLAG: --storage-media-type&#x3D;&quot;application&#x2F;vnd.kubernetes.protobuf&quot;</span><br><span class="line">I0705 22:38:00.837589   18145 flags.go:33] FLAG: --target-ram-mb&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837607   18145 flags.go:33] FLAG: --tls-cert-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837613   18145 flags.go:33] FLAG: --tls-cipher-suites&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837617   18145 flags.go:33] FLAG: --tls-min-version&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837621   18145 flags.go:33] FLAG: --tls-private-key-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837626   18145 flags.go:33] FLAG: --tls-sni-cert-key&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837631   18145 flags.go:33] FLAG: --token-auth-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837635   18145 flags.go:33] FLAG: --v&#x3D;&quot;5&quot;</span><br><span class="line">I0705 22:38:00.837639   18145 flags.go:33] FLAG: --version&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837645   18145 flags.go:33] FLAG: --vmodule&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837652   18145 flags.go:33] FLAG: --watch-cache&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837656   18145 flags.go:33] FLAG: --watch-cache-sizes&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837838   18145 interface.go:384] Looking for default routes with IPv4 addresses</span><br><span class="line">I0705 22:38:00.837845   18145 interface.go:389] Default route transits interface &quot;eth0&quot;</span><br><span class="line">I0705 22:38:00.838072   18145 interface.go:196] Interface eth0 is up</span><br><span class="line">I0705 22:38:00.838128   18145 interface.go:244] Interface &quot;eth0&quot; has 1 addresses :[10.129.173.93&#x2F;22].</span><br><span class="line">I0705 22:38:00.838147   18145 interface.go:211] Checking addr  10.129.173.93&#x2F;22.</span><br><span class="line">I0705 22:38:00.838154   18145 interface.go:218] IP found 10.129.173.93</span><br><span class="line">I0705 22:38:00.838161   18145 interface.go:250] Found valid IPv4 address 10.129.173.93 for interface &quot;eth0&quot;.</span><br><span class="line">I0705 22:38:00.838167   18145 interface.go:395] Found active IP 10.129.173.93 </span><br><span class="line">I0705 22:38:00.838183   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">I0705 22:38:00.838191   18145 server.go:560] external host was not specified, using 10.129.173.93</span><br><span class="line">I0705 22:38:00.838200   18145 server.go:603] Initializing cache sizes based on 0MB limit</span><br><span class="line">I0705 22:38:00.838462   18145 server.go:147] Version: v1.15.5</span><br><span class="line">I0705 22:38:01.255999   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:01.256040   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:01.256057   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">E0705 22:38:01.256668   18145 prometheus.go:55] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256708   18145 prometheus.go:68] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256750   18145 prometheus.go:82] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256782   18145 prometheus.go:96] failed to register workDuration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256822   18145 prometheus.go:112] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256858   18145 prometheus.go:126] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256876   18145 prometheus.go:152] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256902   18145 prometheus.go:164] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256976   18145 prometheus.go:176] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257022   18145 prometheus.go:188] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257057   18145 prometheus.go:203] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257086   18145 prometheus.go:216] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">I0705 22:38:01.257111   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:01.257119   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:01.260013   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.260032   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.260104   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.260268   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.266868   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.266952   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.267066   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.267074   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: &quot;10.129.173.94:2379&quot; is up but not pinned (already pinned &quot;10.129.173.92:2379&quot;)</span><br><span class="line">W0705 22:38:01.267176   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.267294   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.267308   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.267337   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.267425   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.273677   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.273740   18145 storage_factory.go:50] Storage caching is enabled for *apiextensions.CustomResourceDefinition with capacity 100</span><br><span class="line">I0705 22:38:01.273759   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.273837   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.273893   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.274465   18145 store.go:1343] Monitoring customresourcedefinitions.apiextensions.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions</span><br><span class="line">I0705 22:38:01.274588   18145 reflector.go:160] Listing and watching *apiextensions.CustomResourceDefinition from storage&#x2F;cacher.go:&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions</span><br><span class="line">I0705 22:38:01.277287   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.297217   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">I0705 22:38:01.297246   18145 master.go:233] Using reconciler: lease</span><br><span class="line">I0705 22:38:01.297293   18145 storage_factory.go:285] storing apiServerIPInfo in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.297799   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.297812   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.297844   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.297883   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.304364   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.304410   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.304499   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.304538   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.305969   18145 storage_factory.go:285] storing podtemplates in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.306443   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.306455   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.306484   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.306515   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.312747   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.312802   18145 storage_factory.go:50] Storage caching is enabled for *core.PodTemplate with capacity 100</span><br><span class="line">I0705 22:38:01.312802   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.312893   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.312929   18145 store.go:1343] Monitoring podtemplates count at &lt;storage-prefix&gt;&#x2F;&#x2F;podtemplates</span><br><span class="line">W0705 22:38:01.312949   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.312969   18145 reflector.go:160] Listing and watching *core.PodTemplate from storage&#x2F;cacher.go:&#x2F;podtemplates</span><br><span class="line">I0705 22:38:01.312962   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.313529   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.313541   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.313570   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.313640   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.316212   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.319641   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.319682   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.319684   18145 storage_factory.go:46] Storage caching is disabled for *core.Event</span><br><span class="line">W0705 22:38:01.319767   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.319773   18145 store.go:1343] Monitoring events count at &lt;storage-prefix&gt;&#x2F;&#x2F;events</span><br><span class="line">W0705 22:38:01.319806   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.319815   18145 storage_factory.go:285] storing limitranges in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.320331   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.320344   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.320372   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.320409   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.326178   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.326213   18145 storage_factory.go:50] Storage caching is enabled for *core.LimitRange with capacity 100</span><br><span class="line">I0705 22:38:01.326227   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.326298   18145 store.go:1343] Monitoring limitranges count at &lt;storage-prefix&gt;&#x2F;&#x2F;limitranges</span><br><span class="line">I0705 22:38:01.326328   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.326344   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.326352   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.326365   18145 reflector.go:160] Listing and watching *core.LimitRange from storage&#x2F;cacher.go:&#x2F;limitranges</span><br><span class="line">I0705 22:38:01.326789   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.326801   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.326827   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.327266   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.329416   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.333949   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.334002   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.334049   18145 storage_factory.go:50] Storage caching is enabled for *core.ResourceQuota with capacity 100</span><br><span class="line">W0705 22:38:01.334117   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.334183   18145 store.go:1343] Monitoring resourcequotas count at &lt;storage-prefix&gt;&#x2F;&#x2F;resourcequotas</span><br><span class="line">I0705 22:38:01.334225   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.334240   18145 reflector.go:160] Listing and watching *core.ResourceQuota from storage&#x2F;cacher.go:&#x2F;resourcequotas</span><br><span class="line">W0705 22:38:01.334251   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.334409   18145 storage_factory.go:285] storing secrets in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.334997   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.335011   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.335060   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.335107   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.336791   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.341685   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.341737   18145 storage_factory.go:50] Storage caching is enabled for *core.Secret with capacity 100</span><br><span class="line">I0705 22:38:01.341753   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.341819   18145 store.go:1343] Monitoring secrets count at &lt;storage-prefix&gt;&#x2F;&#x2F;secrets</span><br><span class="line">W0705 22:38:01.341843   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.341854   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.341855   18145 reflector.go:160] Listing and watching *core.Secret from storage&#x2F;cacher.go:&#x2F;secrets</span><br><span class="line">I0705 22:38:01.341953   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.342468   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.342480   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.342535   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.342579   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.345126   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.348069   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.348110   18145 storage_factory.go:50] Storage caching is enabled for *core.PersistentVolume with capacity 100</span><br><span class="line">I0705 22:38:01.348133   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.348190   18145 store.go:1343] Monitoring persistentvolumes count at &lt;storage-prefix&gt;&#x2F;&#x2F;persistentvolumes</span><br><span class="line">W0705 22:38:01.348228   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.348239   18145 reflector.go:160] Listing and watching *core.PersistentVolume from storage&#x2F;cacher.go:&#x2F;persistentvolumes</span><br><span class="line">W0705 22:38:01.348248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.348373   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.348916   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.348930   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.348986   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.349049   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.350015   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.355158   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.355202   18145 storage_factory.go:50] Storage caching is enabled for *core.PersistentVolumeClaim with capacity 100</span><br><span class="line">I0705 22:38:01.355203   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.355297   18145 store.go:1343] Monitoring persistentvolumeclaims count at &lt;storage-prefix&gt;&#x2F;&#x2F;persistentvolumeclaims</span><br><span class="line">I0705 22:38:01.355343   18145 reflector.go:160] Listing and watching *core.PersistentVolumeClaim from storage&#x2F;cacher.go:&#x2F;persistentvolumeclaims</span><br><span class="line">W0705 22:38:01.355357   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.355377   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.355467   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.355442   18145 storage_factory.go:285] storing configmaps in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.355993   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.356010   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.356046   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.356101   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.360455   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.361771   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.361808   18145 storage_factory.go:50] Storage caching is enabled for *core.ConfigMap with capacity 100</span><br><span class="line">I0705 22:38:01.361847   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.361872   18145 store.go:1343] Monitoring configmaps count at &lt;storage-prefix&gt;&#x2F;&#x2F;configmaps</span><br><span class="line">W0705 22:38:01.361918   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.361923   18145 reflector.go:160] Listing and watching *core.ConfigMap from storage&#x2F;cacher.go:&#x2F;configmaps</span><br><span class="line">W0705 22:38:01.361936   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.361979   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.362436   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.362448   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.362475   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.362509   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.368369   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.368402   18145 storage_factory.go:50] Storage caching is enabled for *core.Namespace with capacity 100</span><br><span class="line">I0705 22:38:01.368406   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.368475   18145 store.go:1343] Monitoring namespaces count at &lt;storage-prefix&gt;&#x2F;&#x2F;namespaces</span><br><span class="line">W0705 22:38:01.368480   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.368527   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.368523   18145 reflector.go:160] Listing and watching *core.Namespace from storage&#x2F;cacher.go:&#x2F;namespaces</span><br><span class="line">I0705 22:38:01.368640   18145 storage_factory.go:285] storing endpoints in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.369074   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.369088   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.369118   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.369152   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.376059   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.376127   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.376137   18145 storage_factory.go:50] Storage caching is enabled for *core.Endpoints with capacity 1000</span><br><span class="line">W0705 22:38:01.376216   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.376219   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.376246   18145 store.go:1343] Monitoring endpoints count at &lt;storage-prefix&gt;&#x2F;&#x2F;services&#x2F;endpoints</span><br><span class="line">I0705 22:38:01.376288   18145 reflector.go:160] Listing and watching *core.Endpoints from storage&#x2F;cacher.go:&#x2F;services&#x2F;endpoints</span><br><span class="line">I0705 22:38:01.376450   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.376450   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.377222   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.377246   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.377295   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.377372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.379460   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.382925   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.382961   18145 storage_factory.go:50] Storage caching is enabled for *core.Node with capacity 1000</span><br><span class="line">I0705 22:38:01.382977   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.383040   18145 store.go:1343] Monitoring nodes count at &lt;storage-prefix&gt;&#x2F;&#x2F;minions</span><br><span class="line">W0705 22:38:01.383054   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.383063   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.383102   18145 reflector.go:160] Listing and watching *core.Node from storage&#x2F;cacher.go:&#x2F;minions</span><br><span class="line">I0705 22:38:01.383436   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.383884   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.383896   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.383921   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.383955   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.386054   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.389904   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.389951   18145 storage_factory.go:50] Storage caching is enabled for *core.Pod with capacity 1000</span><br><span class="line">I0705 22:38:01.389990   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.390047   18145 store.go:1343] Monitoring pods count at &lt;storage-prefix&gt;&#x2F;&#x2F;pods</span><br><span class="line">W0705 22:38:01.390100   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.390107   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.390140   18145 reflector.go:160] Listing and watching *core.Pod from storage&#x2F;cacher.go:&#x2F;pods</span><br><span class="line">I0705 22:38:01.390180   18145 storage_factory.go:285] storing serviceaccounts in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.391245   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.391263   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.391310   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.391381   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.398242   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.398285   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.398297   18145 storage_factory.go:50] Storage caching is enabled for *core.ServiceAccount with capacity 100</span><br><span class="line">W0705 22:38:01.398378   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.398395   18145 store.go:1343] Monitoring serviceaccounts count at &lt;storage-prefix&gt;&#x2F;&#x2F;serviceaccounts</span><br><span class="line">W0705 22:38:01.398413   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.398524   18145 reflector.go:160] Listing and watching *core.ServiceAccount from storage&#x2F;cacher.go:&#x2F;serviceaccounts</span><br><span class="line">I0705 22:38:01.398559   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.399760   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.399789   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.399832   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.399894   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.402304   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.406325   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.406360   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.406413   18145 storage_factory.go:50] Storage caching is enabled for *core.Service with capacity 1000</span><br><span class="line">I0705 22:38:01.406421   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.406516   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.406546   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.406580   18145 store.go:1343] Monitoring services count at &lt;storage-prefix&gt;&#x2F;&#x2F;services&#x2F;specs</span><br><span class="line">I0705 22:38:01.406645   18145 reflector.go:160] Listing and watching *core.Service from storage&#x2F;cacher.go:&#x2F;services&#x2F;specs</span><br><span class="line">I0705 22:38:01.406642   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.407151   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.407163   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.407208   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.407260   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.412982   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.413048   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.413128   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.413155   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.413252   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.413526   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.413539   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.413567   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.413619   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.414368   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.419479   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.419514   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.419579   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.419647   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.419647   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.420108   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.420119   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.420144   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.420203   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.426157   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.426208   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.426217   18145 storage_factory.go:50] Storage caching is enabled for *core.ReplicationController with capacity 100</span><br><span class="line">W0705 22:38:01.426293   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.426306   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.426312   18145 store.go:1343] Monitoring replicationcontrollers count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllers</span><br><span class="line">I0705 22:38:01.426333   18145 reflector.go:160] Listing and watching *core.ReplicationController from storage&#x2F;cacher.go:&#x2F;controllers</span><br><span class="line">I0705 22:38:01.428279   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.480260   18145 storage_factory.go:285] storing bindings in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.480424   18145 storage_factory.go:285] storing componentstatuses in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.480904   18145 storage_factory.go:285] storing configmaps in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.481312   18145 storage_factory.go:285] storing endpoints in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.481698   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482104   18145 storage_factory.go:285] storing limitranges in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482346   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482426   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482543   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482825   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483147   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483255   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483733   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483904   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484203   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484330   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484703   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484828   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484905   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484984   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485119   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485212   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485326   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485764   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485937   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486385   18145 storage_factory.go:285] storing podtemplates in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486832   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486982   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487134   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487533   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487697   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488101   18145 storage_factory.go:285] storing secrets in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488494   18145 storage_factory.go:285] storing serviceaccounts in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488886   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489313   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489466   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489537   18145 master.go:417] Skipping disabled API group &quot;auditregistration.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489550   18145 master.go:425] Enabling API group &quot;authentication.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489562   18145 master.go:425] Enabling API group &quot;authorization.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489679   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.490153   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.490164   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.490200   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.490237   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.496374   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.496423   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.496450   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.496501   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">W0705 22:38:01.496531   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.496536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.496589   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.496664   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.497098   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.497109   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.497135   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.497176   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.500515   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.502989   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.503023   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.503047   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.503101   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">W0705 22:38:01.503114   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.503161   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.503155   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.503245   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.503665   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.503676   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.503702   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.503743   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.506520   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.509091   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.509124   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.509161   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.509195   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.509211   18145 master.go:425] Enabling API group &quot;autoscaling&quot;.</span><br><span class="line">W0705 22:38:01.509223   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.509248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.509265   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.509331   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.509807   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.509821   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.509848   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.509885   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.515303   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.515327   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.515362   18145 storage_factory.go:50] Storage caching is enabled for *batch.Job with capacity 100</span><br><span class="line">W0705 22:38:01.515407   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.515414   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.515436   18145 store.go:1343] Monitoring jobs.batch count at &lt;storage-prefix&gt;&#x2F;&#x2F;jobs</span><br><span class="line">I0705 22:38:01.515472   18145 reflector.go:160] Listing and watching *batch.Job from storage&#x2F;cacher.go:&#x2F;jobs</span><br><span class="line">I0705 22:38:01.515529   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.516011   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.516023   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.516047   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.516080   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.516374   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.517439   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.522214   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.522244   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.522277   18145 storage_factory.go:50] Storage caching is enabled for *batch.CronJob with capacity 100</span><br><span class="line">W0705 22:38:01.522297   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.522325   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.522347   18145 store.go:1343] Monitoring cronjobs.batch count at &lt;storage-prefix&gt;&#x2F;&#x2F;cronjobs</span><br><span class="line">I0705 22:38:01.522361   18145 master.go:425] Enabling API group &quot;batch&quot;.</span><br><span class="line">I0705 22:38:01.522409   18145 reflector.go:160] Listing and watching *batch.CronJob from storage&#x2F;cacher.go:&#x2F;cronjobs</span><br><span class="line">I0705 22:38:01.522456   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.522889   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.522901   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.522926   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.522959   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.525358   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.529067   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.529096   18145 storage_factory.go:50] Storage caching is enabled for *certificates.CertificateSigningRequest with capacity 100</span><br><span class="line">I0705 22:38:01.529108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.529164   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.529169   18145 store.go:1343] Monitoring certificatesigningrequests.certificates.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;certificatesigningrequests</span><br><span class="line">W0705 22:38:01.529172   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.529187   18145 master.go:425] Enabling API group &quot;certificates.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.529202   18145 reflector.go:160] Listing and watching *certificates.CertificateSigningRequest from storage&#x2F;cacher.go:&#x2F;certificatesigningrequests</span><br><span class="line">I0705 22:38:01.529330   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.529757   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.529769   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.529799   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.529831   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.535795   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.535826   18145 storage_factory.go:50] Storage caching is enabled for *coordination.Lease with capacity 100</span><br><span class="line">I0705 22:38:01.535851   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.535884   18145 store.go:1343] Monitoring leases.coordination.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;leases</span><br><span class="line">W0705 22:38:01.535922   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.535951   18145 reflector.go:160] Listing and watching *coordination.Lease from storage&#x2F;cacher.go:&#x2F;leases</span><br><span class="line">W0705 22:38:01.535986   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.536022   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.536035   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.536453   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.536464   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.536487   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.536530   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.538229   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.543527   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.543566   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: &quot;10.129.173.92:2379&quot; is up but not pinned (already pinned &quot;10.129.173.94:2379&quot;)</span><br><span class="line">I0705 22:38:01.543625   18145 storage_factory.go:50] Storage caching is enabled for *coordination.Lease with capacity 100</span><br><span class="line">I0705 22:38:01.543664   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.543732   18145 store.go:1343] Monitoring leases.coordination.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;leases</span><br><span class="line">I0705 22:38:01.544459   18145 reflector.go:160] Listing and watching *coordination.Lease from storage&#x2F;cacher.go:&#x2F;leases</span><br><span class="line">I0705 22:38:01.544471   18145 master.go:425] Enabling API group &quot;coordination.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.544507   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.544785   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.544955   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">W0705 22:38:01.544970   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.546012   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.546032   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.546074   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.546142   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.546857   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.552338   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.552372   18145 storage_factory.go:50] Storage caching is enabled for *core.ReplicationController with capacity 100</span><br><span class="line">I0705 22:38:01.552402   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.552431   18145 store.go:1343] Monitoring replicationcontrollers count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllers</span><br><span class="line">W0705 22:38:01.552478   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.552513   18145 reflector.go:160] Listing and watching *core.ReplicationController from storage&#x2F;cacher.go:&#x2F;controllers</span><br><span class="line">W0705 22:38:01.552563   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.552569   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.553038   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.553048   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.553075   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.553108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.554206   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.559089   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.559133   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.559160   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.559227   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">W0705 22:38:01.559248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.559277   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.559295   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.559365   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.559903   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.559919   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.559950   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.560049   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.562229   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.565407   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.565440   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.565451   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">W0705 22:38:01.565511   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.565538   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.565544   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.565609   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.565689   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.566128   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.566140   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.566173   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.566208   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.572128   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.572160   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.572184   18145 storage_factory.go:50] Storage caching is enabled for *networking.Ingress with capacity 100</span><br><span class="line">W0705 22:38:01.572230   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.572257   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.572275   18145 store.go:1343] Monitoring ingresses.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;ingress</span><br><span class="line">I0705 22:38:01.572328   18145 reflector.go:160] Listing and watching *networking.Ingress from storage&#x2F;cacher.go:&#x2F;ingress</span><br><span class="line">I0705 22:38:01.572433   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.573010   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.573029   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.573060   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.573099   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.573357   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.578443   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.578472   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodSecurityPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.578477   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.578536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.578548   18145 store.go:1343] Monitoring podsecuritypolicies.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;podsecuritypolicy</span><br><span class="line">I0705 22:38:01.578574   18145 reflector.go:160] Listing and watching *policy.PodSecurityPolicy from storage&#x2F;cacher.go:&#x2F;podsecuritypolicy</span><br><span class="line">W0705 22:38:01.578581   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.578667   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.579104   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.579114   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.579140   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.579171   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.579353   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.580264   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.584838   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.584871   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.584886   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.584942   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.584975   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.584943   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.584987   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.585104   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.585519   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.585529   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.585553   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.585612   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.590996   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.591666   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.591699   18145 storage_factory.go:50] Storage caching is enabled for *networking.NetworkPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.591744   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.591767   18145 store.go:1343] Monitoring networkpolicies.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;networkpolicies</span><br><span class="line">I0705 22:38:01.591780   18145 master.go:425] Enabling API group &quot;extensions&quot;.</span><br><span class="line">W0705 22:38:01.591812   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.591816   18145 reflector.go:160] Listing and watching *networking.NetworkPolicy from storage&#x2F;cacher.go:&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.591814   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.591919   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.592337   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.592347   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.592382   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.592420   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.593678   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.598473   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.598508   18145 storage_factory.go:50] Storage caching is enabled for *networking.NetworkPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.598534   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.598556   18145 store.go:1343] Monitoring networkpolicies.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.598620   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.598621   18145 reflector.go:160] Listing and watching *networking.NetworkPolicy from storage&#x2F;cacher.go:&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.598670   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.598678   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.599263   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.599279   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.599307   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.599340   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.605672   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.605867   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.605898   18145 storage_factory.go:50] Storage caching is enabled for *networking.Ingress with capacity 100</span><br><span class="line">I0705 22:38:01.605918   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.605966   18145 store.go:1343] Monitoring ingresses.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;ingress</span><br><span class="line">I0705 22:38:01.605981   18145 master.go:425] Enabling API group &quot;networking.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.605983   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.606016   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.606017   18145 storage_factory.go:285] storing runtimeclasses.node.k8s.io in node.k8s.io&#x2F;v1beta1, reading as node.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.606036   18145 reflector.go:160] Listing and watching *networking.Ingress from storage&#x2F;cacher.go:&#x2F;ingress</span><br><span class="line">I0705 22:38:01.606558   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.606571   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.606613   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.606667   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.612169   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.612196   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.612210   18145 storage_factory.go:50] Storage caching is enabled for *node.RuntimeClass with capacity 100</span><br><span class="line">W0705 22:38:01.612254   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.612274   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.612300   18145 store.go:1343] Monitoring runtimeclasses.node.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;runtimeclasses</span><br><span class="line">I0705 22:38:01.612325   18145 master.go:425] Enabling API group &quot;node.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.612343   18145 reflector.go:160] Listing and watching *node.RuntimeClass from storage&#x2F;cacher.go:&#x2F;runtimeclasses</span><br><span class="line">I0705 22:38:01.612348   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.612449   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.612946   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.612958   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.612989   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.613026   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.618576   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.618621   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.618644   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodDisruptionBudget with capacity 100</span><br><span class="line">W0705 22:38:01.618680   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.618713   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.618732   18145 store.go:1343] Monitoring poddisruptionbudgets.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;poddisruptionbudgets</span><br><span class="line">I0705 22:38:01.618756   18145 reflector.go:160] Listing and watching *policy.PodDisruptionBudget from storage&#x2F;cacher.go:&#x2F;poddisruptionbudgets</span><br><span class="line">I0705 22:38:01.618878   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.619304   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.619315   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.619340   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.619372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.620634   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.621640   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.625143   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.625177   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodSecurityPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.625238   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.625266   18145 reflector.go:160] Listing and watching *policy.PodSecurityPolicy from storage&#x2F;cacher.go:&#x2F;podsecuritypolicy</span><br><span class="line">I0705 22:38:01.625246   18145 store.go:1343] Monitoring podsecuritypolicies.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;podsecuritypolicy</span><br><span class="line">W0705 22:38:01.625317   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.625324   18145 master.go:425] Enabling API group &quot;policy&quot;.</span><br><span class="line">W0705 22:38:01.625351   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.625367   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.625915   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.625934   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.625979   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.626040   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.627123   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.632111   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.632147   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.632160   18145 storage_factory.go:50] Storage caching is enabled for *rbac.Role with capacity 100</span><br><span class="line">W0705 22:38:01.632224   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.632295   18145 store.go:1343] Monitoring roles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;roles</span><br><span class="line">W0705 22:38:01.632296   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.632328   18145 reflector.go:160] Listing and watching *rbac.Role from storage&#x2F;cacher.go:&#x2F;roles</span><br><span class="line">I0705 22:38:01.632406   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.632889   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.632904   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.632930   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.632988   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.634959   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.639061   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.639098   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.639170   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.639101   18145 storage_factory.go:50] Storage caching is enabled for *rbac.RoleBinding with capacity 100</span><br><span class="line">W0705 22:38:01.639217   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.639258   18145 store.go:1343] Monitoring rolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.639292   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.639326   18145 reflector.go:160] Listing and watching *rbac.RoleBinding from storage&#x2F;cacher.go:&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.639724   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.639736   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.639761   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.639792   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.641705   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.645513   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.645546   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRole with capacity 100</span><br><span class="line">I0705 22:38:01.645589   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.645616   18145 store.go:1343] Monitoring clusterroles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterroles</span><br><span class="line">I0705 22:38:01.645686   18145 reflector.go:160] Listing and watching *rbac.ClusterRole from storage&#x2F;cacher.go:&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.645696   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.645689   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.645776   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.646238   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.646250   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.646277   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.646312   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.652231   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.652258   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.652280   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRoleBinding with capacity 100</span><br><span class="line">W0705 22:38:01.652325   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.652374   18145 store.go:1343] Monitoring clusterrolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterrolebindings</span><br><span class="line">W0705 22:38:01.652385   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.652417   18145 reflector.go:160] Listing and watching *rbac.ClusterRoleBinding from storage&#x2F;cacher.go:&#x2F;clusterrolebindings</span><br><span class="line">I0705 22:38:01.652484   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.652951   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.652963   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.652990   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.653027   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.653114   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.655550   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.659098   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.659135   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.659162   18145 storage_factory.go:50] Storage caching is enabled for *rbac.Role with capacity 100</span><br><span class="line">W0705 22:38:01.659210   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.659216   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.659231   18145 store.go:1343] Monitoring roles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;roles</span><br><span class="line">I0705 22:38:01.659290   18145 reflector.go:160] Listing and watching *rbac.Role from storage&#x2F;cacher.go:&#x2F;roles</span><br><span class="line">I0705 22:38:01.659382   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.659882   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.659894   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.659921   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.659976   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.661045   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.666391   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.666434   18145 storage_factory.go:50] Storage caching is enabled for *rbac.RoleBinding with capacity 100</span><br><span class="line">I0705 22:38:01.666455   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.666501   18145 store.go:1343] Monitoring rolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.666563   18145 reflector.go:160] Listing and watching *rbac.RoleBinding from storage&#x2F;cacher.go:&#x2F;rolebindings</span><br><span class="line">W0705 22:38:01.666586   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.666570   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.667163   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.667615   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.667628   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.667659   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.667696   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.669460   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.673937   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.673974   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRole with capacity 100</span><br><span class="line">I0705 22:38:01.673998   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.674038   18145 store.go:1343] Monitoring clusterroles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.674107   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.674139   18145 reflector.go:160] Listing and watching *rbac.ClusterRole from storage&#x2F;cacher.go:&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.674241   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.674246   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.674250   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.674768   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.674781   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.674809   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.674843   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.679062   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.680225   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.680261   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRoleBinding with capacity 100</span><br><span class="line">I0705 22:38:01.680286   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.680333   18145 store.go:1343] Monitoring clusterrolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterrolebindings</span><br><span class="line">I0705 22:38:01.680363   18145 master.go:425] Enabling API group &quot;rbac.authorization.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.680382   18145 reflector.go:160] Listing and watching *rbac.ClusterRoleBinding from storage&#x2F;cacher.go:&#x2F;clusterrolebindings</span><br><span class="line">W0705 22:38:01.680417   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.680389   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.682487   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.682847   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.682943   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.682958   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.682986   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.683031   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.688965   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.689018   18145 storage_factory.go:50] Storage caching is enabled for *scheduling.PriorityClass with capacity 100</span><br><span class="line">I0705 22:38:01.689064   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.689105   18145 store.go:1343] Monitoring priorityclasses.scheduling.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;priorityclasses</span><br><span class="line">W0705 22:38:01.689143   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.689144   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.689160   18145 reflector.go:160] Listing and watching *scheduling.PriorityClass from storage&#x2F;cacher.go:&#x2F;priorityclasses</span><br><span class="line">I0705 22:38:01.689271   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.689806   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.689819   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.689858   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.689924   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.691729   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.696232   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.696268   18145 storage_factory.go:50] Storage caching is enabled for *scheduling.PriorityClass with capacity 100</span><br><span class="line">I0705 22:38:01.696271   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.696334   18145 store.go:1343] Monitoring priorityclasses.scheduling.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;priorityclasses</span><br><span class="line">I0705 22:38:01.696348   18145 master.go:425] Enabling API group &quot;scheduling.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.696351   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.696382   18145 reflector.go:160] Listing and watching *scheduling.PriorityClass from storage&#x2F;cacher.go:&#x2F;priorityclasses</span><br><span class="line">W0705 22:38:01.696400   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.696458   18145 master.go:417] Skipping disabled API group &quot;settings.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.696568   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.697044   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.697056   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.697082   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.697116   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.697826   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.703204   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.703245   18145 storage_factory.go:50] Storage caching is enabled for *storage.StorageClass with capacity 100</span><br><span class="line">I0705 22:38:01.703295   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.703320   18145 store.go:1343] Monitoring storageclasses.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;storageclasses</span><br><span class="line">W0705 22:38:01.703359   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.703394   18145 reflector.go:160] Listing and watching *storage.StorageClass from storage&#x2F;cacher.go:&#x2F;storageclasses</span><br><span class="line">I0705 22:38:01.703456   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.703398   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.703972   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.703988   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.704034   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.704108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.705744   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.710344   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.710372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.710376   18145 storage_factory.go:50] Storage caching is enabled for *storage.VolumeAttachment with capacity 100</span><br><span class="line">W0705 22:38:01.710430   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.710445   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.710450   18145 store.go:1343] Monitoring volumeattachments.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.710464   18145 reflector.go:160] Listing and watching *storage.VolumeAttachment from storage&#x2F;cacher.go:&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.710483   18145 storage_factory.go:285] storing csinodes.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.710942   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.710954   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.710990   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.711054   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.713079   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.716838   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.716867   18145 storage_factory.go:50] Storage caching is enabled for *storage.CSINode with capacity 100</span><br><span class="line">I0705 22:38:01.716906   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.716965   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.717340   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.717348   18145 store.go:1343] Monitoring csinodes.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;csinodes</span><br><span class="line">I0705 22:38:01.717384   18145 storage_factory.go:285] storing csidrivers.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.717555   18145 reflector.go:160] Listing and watching *storage.CSINode from storage&#x2F;cacher.go:&#x2F;csinodes</span><br><span class="line">I0705 22:38:01.719158   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.719180   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.719235   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.719314   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.725650   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.725704   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.725750   18145 storage_factory.go:50] Storage caching is enabled for *storage.CSIDriver with capacity 100</span><br><span class="line">W0705 22:38:01.725830   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.725833   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.725881   18145 store.go:1343] Monitoring csidrivers.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;csidrivers</span><br><span class="line">I0705 22:38:01.725922   18145 reflector.go:160] Listing and watching *storage.CSIDriver from storage&#x2F;cacher.go:&#x2F;csidrivers</span><br><span class="line">I0705 22:38:01.726070   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.726147   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.726622   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.726641   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.726682   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.726742   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.727907   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.733148   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.733192   18145 storage_factory.go:50] Storage caching is enabled for *storage.StorageClass with capacity 100</span><br><span class="line">I0705 22:38:01.733195   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.733264   18145 store.go:1343] Monitoring storageclasses.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;storageclasses</span><br><span class="line">W0705 22:38:01.733277   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.733282   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.733318   18145 reflector.go:160] Listing and watching *storage.StorageClass from storage&#x2F;cacher.go:&#x2F;storageclasses</span><br><span class="line">I0705 22:38:01.733393   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.733942   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.733956   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.733983   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.734017   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.739573   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.739653   18145 storage_factory.go:50] Storage caching is enabled for *storage.VolumeAttachment with capacity 100</span><br><span class="line">I0705 22:38:01.739670   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.739743   18145 store.go:1343] Monitoring volumeattachments.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.739770   18145 master.go:425] Enabling API group &quot;storage.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.739785   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.739794   18145 reflector.go:160] Listing and watching *storage.VolumeAttachment from storage&#x2F;cacher.go:&#x2F;volumeattachments</span><br><span class="line">W0705 22:38:01.739820   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.739830   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.739938   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.740394   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.740405   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.740433   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.740471   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.745885   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.746023   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.746067   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">I0705 22:38:01.746068   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.746150   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.746192   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">W0705 22:38:01.746199   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.746195   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.746278   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.746862   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.746876   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.746903   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.746941   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.752900   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.752930   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.752989   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.752995   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.753021   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.753114   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">I0705 22:38:01.753145   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">I0705 22:38:01.753245   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.753664   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.753785   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.753799   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.753835   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.753881   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.759938   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.759972   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">I0705 22:38:01.759996   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.760029   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">W0705 22:38:01.760072   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.760088   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.760094   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.760179   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.760648   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.760660   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.760693   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.760752   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.761629   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.762737   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.766537   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.766570   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">I0705 22:38:01.766574   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.766639   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">W0705 22:38:01.766666   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.766667   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.766671   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.766798   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.767271   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.767283   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.767309   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.767408   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.773951   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.773995   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.773998   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.774074   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.774125   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.774172   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.774188   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.774230   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.774946   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.774963   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.774996   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.775036   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.776461   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.779672   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.780664   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.780705   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.780734   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.780809   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.780836   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">W0705 22:38:01.780811   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.780867   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.780956   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.781418   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.781428   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.781460   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.781499   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.782842   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.787419   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.787444   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.787502   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.787530   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.787553   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.787656   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.787742   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.787796   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.788235   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.788246   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.788273   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.788315   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.794135   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.794631   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.794681   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.794701   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">W0705 22:38:01.794771   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.794819   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">W0705 22:38:01.794829   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.794861   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.794960   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.795409   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.795421   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.795450   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.795502   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.797129   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.801393   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.801432   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.801471   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">W0705 22:38:01.801508   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.801536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.801545   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.801562   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.801702   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.802137   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.802147   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.802184   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.802251   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.807741   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.807776   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.807792   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.807849   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.807897   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.807922   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.807964   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.807960   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.808424   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.808435   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.809098   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.809163   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.809901   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.814015   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.814893   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.814938   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.814943   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.815016   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.815024   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.815026   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.815048   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.815185   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.815699   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.815712   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.815762   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.815819   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.821338   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.821378   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.821343   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.821406   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.821445   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.821497   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.821514   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.821554   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.821562   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.822050   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.822063   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.822109   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.822173   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.828748   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.828791   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.828823   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">W0705 22:38:01.828858   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.828865   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.828923   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.828944   18145 master.go:425] Enabling API group &quot;apps&quot;.</span><br><span class="line">I0705 22:38:01.828977   18145 storage_factory.go:285] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.828998   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.829405   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.829417   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.829444   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.829477   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.832275   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.833018   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.835374   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.835425   18145 storage_factory.go:50] Storage caching is enabled for *admissionregistration.ValidatingWebhookConfiguration with capacity 100</span><br><span class="line">I0705 22:38:01.835447   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.835516   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.835523   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.835521   18145 store.go:1343] Monitoring validatingwebhookconfigurations.admissionregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;validatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.835546   18145 reflector.go:160] Listing and watching *admissionregistration.ValidatingWebhookConfiguration from storage&#x2F;cacher.go:&#x2F;validatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.835570   18145 storage_factory.go:285] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.836092   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.836103   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.836130   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.836163   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.837500   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.843429   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.843486   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.843504   18145 storage_factory.go:50] Storage caching is enabled for *admissionregistration.MutatingWebhookConfiguration with capacity 100</span><br><span class="line">W0705 22:38:01.843562   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.843696   18145 store.go:1343] Monitoring mutatingwebhookconfigurations.admissionregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;mutatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.843729   18145 master.go:425] Enabling API group &quot;admissionregistration.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.843815   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.843787   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.844020   18145 reflector.go:160] Listing and watching *admissionregistration.MutatingWebhookConfiguration from storage&#x2F;cacher.go:&#x2F;mutatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.844236   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.844544   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.844556   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.844583   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.844651   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.850614   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.850627   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.850643   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.850668   18145 storage_factory.go:46] Storage caching is disabled for *core.Event</span><br><span class="line">I0705 22:38:01.850771   18145 store.go:1343] Monitoring events count at &lt;storage-prefix&gt;&#x2F;&#x2F;events</span><br><span class="line">I0705 22:38:01.850791   18145 master.go:425] Enabling API group &quot;events.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.850817   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.850841   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">W0705 22:38:01.850860   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.954244   18145 storage_factory.go:285] storing tokenreviews.authentication.k8s.io in authentication.k8s.io&#x2F;v1, reading as authentication.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954407   18145 storage_factory.go:285] storing tokenreviews.authentication.k8s.io in authentication.k8s.io&#x2F;v1, reading as authentication.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954618   18145 storage_factory.go:285] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954700   18145 storage_factory.go:285] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954785   18145 storage_factory.go:285] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954852   18145 storage_factory.go:285] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955039   18145 storage_factory.go:285] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955186   18145 storage_factory.go:285] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955253   18145 storage_factory.go:285] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955326   18145 storage_factory.go:285] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955939   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956125   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956633   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956819   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957276   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957422   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957939   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.958119   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.958615   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.959458   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.959508   18145 genericapiserver.go:351] Skipping API batch&#x2F;v2alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.959965   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960065   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960202   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960724   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961138   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961675   18145 storage_factory.go:285] storing daemonsets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961861   18145 storage_factory.go:285] storing daemonsets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962283   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962374   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962535   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962709   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963138   18145 storage_factory.go:285] storing ingresses.extensions in networking.k8s.io&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963278   18145 storage_factory.go:285] storing ingresses.extensions in networking.k8s.io&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963727   18145 storage_factory.go:285] storing networkpolicies.extensions in networking.k8s.io&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964049   18145 storage_factory.go:285] storing podsecuritypolicies.extensions in policy&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964505   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964702   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965393   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965440   18145 storage_factory.go:285] storing replicationcontrollers.extensions in v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965587   18145 storage_factory.go:285] storing replicationcontrollers.extensions in v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966078   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966475   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966634   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.967053   18145 storage_factory.go:285] storing runtimeclasses.node.k8s.io in node.k8s.io&#x2F;v1beta1, reading as node.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.967095   18145 genericapiserver.go:351] Skipping API node.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.967569   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.967746   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968069   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968450   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968782   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969174   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969538   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969891   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.970204   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.971161   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.971562   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.971614   18145 genericapiserver.go:351] Skipping API rbac.authorization.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.971959   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.972304   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.972342   18145 genericapiserver.go:351] Skipping API scheduling.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.972684   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973037   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973178   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973545   18145 storage_factory.go:285] storing csidrivers.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973841   18145 storage_factory.go:285] storing csinodes.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.974138   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.974435   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.974482   18145 genericapiserver.go:351] Skipping API storage.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.974974   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.975387   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.975552   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976543   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976761   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976906   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977277   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977404   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977530   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977953   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978104   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978247   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978707   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979080   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979237   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979612   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979751   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979874   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980238   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980373   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980500   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980918   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.981586   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.981758   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982238   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982585   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982685   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982814   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982947   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983330   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983478   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983624   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984096   18145 storage_factory.go:285] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984466   18145 storage_factory.go:285] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984926   18145 storage_factory.go:285] storing events.events.k8s.io in v1, reading as events.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:02.254177   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.254203   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.254244   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.254372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.261077   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:02.261128   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:02.261209   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.261287   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.603860   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;,&quot;poststarthook&#x2F;bootstrap-controller&quot;,&quot;poststarthook&#x2F;rbac&#x2F;bootstrap-roles&quot;,&quot;poststarthook&#x2F;scheduling&#x2F;bootstrap-system-priority-classes&quot;,&quot;poststarthook&#x2F;ca-registration&quot;,&quot;poststarthook&#x2F;start-kube-apiserver-admission-initializer&quot;</span><br><span class="line">I0705 22:38:02.646973   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;</span><br><span class="line">E0705 22:38:02.648065   18145 prometheus.go:55] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648116   18145 prometheus.go:68] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648155   18145 prometheus.go:82] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648181   18145 prometheus.go:96] failed to register workDuration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648208   18145 prometheus.go:112] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648232   18145 prometheus.go:126] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648256   18145 prometheus.go:152] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648279   18145 prometheus.go:164] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648323   18145 prometheus.go:176] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648401   18145 prometheus.go:188] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648426   18145 prometheus.go:203] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648449   18145 prometheus.go:216] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">I0705 22:38:02.648476   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:02.648483   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:02.650334   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.650349   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.650387   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.650508   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.656909   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:02.656960   18145 storage_factory.go:50] Storage caching is enabled for *apiregistration.APIService with capacity 1000</span><br><span class="line">I0705 22:38:02.657001   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:02.657086   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.657094   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.657250   18145 store.go:1343] Monitoring apiservices.apiregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.657324   18145 reflector.go:160] Listing and watching *apiregistration.APIService from storage&#x2F;cacher.go:&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.657709   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.657729   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.657761   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.657796   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.663447   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:02.663513   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.663518   18145 storage_factory.go:50] Storage caching is enabled for *apiregistration.APIService with capacity 1000</span><br><span class="line">W0705 22:38:02.663656   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.663610   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.663728   18145 store.go:1343] Monitoring apiservices.apiregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.663773   18145 reflector.go:160] Listing and watching *apiregistration.APIService from storage&#x2F;cacher.go:&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.665260   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:02.666477   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:02.674592   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:02.674627   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:03.616355   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:03.616397   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:04.268801   18145 deprecated_insecure_serving.go:53] Serving insecurely on 127.0.0.1:8073</span><br><span class="line">I0705 22:38:04.268882   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;,&quot;poststarthook&#x2F;bootstrap-controller&quot;,&quot;poststarthook&#x2F;rbac&#x2F;bootstrap-roles&quot;,&quot;poststarthook&#x2F;scheduling&#x2F;bootstrap-system-priority-classes&quot;,&quot;poststarthook&#x2F;ca-registration&quot;,&quot;poststarthook&#x2F;start-kube-apiserver-admission-initializer&quot;,&quot;poststarthook&#x2F;start-kube-aggregator-informers&quot;,&quot;poststarthook&#x2F;apiservice-registration-controller&quot;,&quot;poststarthook&#x2F;apiservice-status-available-controller&quot;,&quot;poststarthook&#x2F;apiservice-openapi-controller&quot;,&quot;poststarthook&#x2F;kube-apiserver-autoregistration&quot;,&quot;autoregister-completion&quot;</span><br><span class="line">I0705 22:38:04.269477   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269524   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269546   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269551   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269572   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269582   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269487   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269624   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269643   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269667   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269626   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269685   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;policy&#x2F;v1beta1</span><br><span class="line">I0705 22:38:04.269694   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269711   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269754   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269755   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.269648   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269805   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269807   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269843   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269863   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.269740   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269905   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269926   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1</span><br><span class="line">I0705 22:38:04.269501   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270002   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270029   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269615   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270116   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270137   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270197   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;persistentvolumes?limit&#x3D;500&amp;resourceVersion&#x3D;0: (896.994µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47384]</span><br><span class="line">I0705 22:38:04.270317   18145 wrap.go:47] GET &#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (810.468µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47382]</span><br><span class="line">I0705 22:38:04.269636   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270387   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270408   18145 secure_serving.go:116] Serving securely on [::]:6443</span><br><span class="line">I0705 22:38:04.269501   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270438   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270446   18145 apiservice_controller.go:94] Starting APIServiceRegistrationController</span><br><span class="line">I0705 22:38:04.270455   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270468   18145 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller</span><br><span class="line">I0705 22:38:04.270497   18145 wrap.go:47] GET &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses?limit&#x3D;500&amp;resourceVersion&#x3D;0: (944.078µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47386]</span><br><span class="line">I0705 22:38:04.270522   18145 crdregistration_controller.go:112] Starting crd-autoregister controller</span><br><span class="line">I0705 22:38:04.270535   18145 controller_utils.go:1029] Waiting for caches to sync for crd-autoregister controller</span><br><span class="line">I0705 22:38:04.270538   18145 available_controller.go:376] Starting AvailableConditionController</span><br><span class="line">I0705 22:38:04.270559   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;replicationcontrollers?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.21442ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47388]</span><br><span class="line">I0705 22:38:04.270572   18145 naming_controller.go:288] Starting NamingConditionController</span><br><span class="line">I0705 22:38:04.270575   18145 establishing_controller.go:73] Starting EstablishingController</span><br><span class="line">I0705 22:38:04.270652   18145 nonstructuralschema_controller.go:191] Starting NonStructuralSchemaConditionController</span><br><span class="line">I0705 22:38:04.270550   18145 crd_finalizer.go:255] Starting CRDFinalizer</span><br><span class="line">I0705 22:38:04.270509   18145 autoregister_controller.go:140] Starting autoregister controller</span><br><span class="line">I0705 22:38:04.270683   18145 cache.go:32] Waiting for caches to sync for autoregister controller</span><br><span class="line">I0705 22:38:04.270523   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:04.270694   18145 wrap.go:47] GET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.172166ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47392]</span><br><span class="line">I0705 22:38:04.270730   18145 controller.go:81] Starting OpenAPI AggregationController</span><br><span class="line">I0705 22:38:04.270745   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:04.270567   18145 customresource_discovery_controller.go:208] Starting DiscoveryController</span><br><span class="line">I0705 22:38:04.270566   18145 cache.go:32] Waiting for caches to sync for AvailableConditionController controller</span><br><span class="line">I0705 22:38:04.270560   18145 controller.go:83] Starting OpenAPI controller</span><br><span class="line">I0705 22:38:04.270815   18145 reflector.go:122] Starting reflector *v1.Node (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.269778   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270831   18145 reflector.go:160] Listing and watching *v1.Node from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271226   18145 reflector.go:122] Starting reflector *v1.VolumeAttachment (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271245   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;nodes?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.966293ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47378]</span><br><span class="line">I0705 22:38:04.271268   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.271289   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.271294   18145 reflector.go:122] Starting reflector *v1.ClusterRoleBinding (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271313   18145 reflector.go:160] Listing and watching *v1.ClusterRoleBinding from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271226   18145 reflector.go:122] Starting reflector *v1.Role (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271330   18145 reflector.go:160] Listing and watching *v1.Role from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271335   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;services?limit&#x3D;500&amp;resourceVersion&#x3D;0: (2.065372ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47380]</span><br><span class="line">I0705 22:38:04.271277   18145 reflector.go:122] Starting reflector *v1.Service (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271421   18145 reflector.go:160] Listing and watching *v1.Service from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271299   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1</span><br><span class="line">I0705 22:38:04.271246   18145 reflector.go:160] Listing and watching *v1.VolumeAttachment from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.270926   18145 reflector.go:122] Starting reflector *v1.ClusterRole (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271521   18145 reflector.go:160] Listing and watching *v1.ClusterRole from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271547   18145 reflector.go:122] Starting reflector *v1.Namespace (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271560   18145 reflector.go:160] Listing and watching *v1.Namespace from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271568   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m1s</span><br><span class="line">I0705 22:38:04.271011   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.271591   18145 reflector.go:122] Starting reflector *v1.StorageClass (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271628   18145 reflector.go:160] Listing and watching *v1.StorageClass from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.270877   18145 reflector.go:122] Starting reflector *apiextensions.CustomResourceDefinition (5m0s) from k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271679   18145 reflector.go:160] Listing and watching *apiextensions.CustomResourceDefinition from k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271690   18145 reflector.go:122] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271728   18145 reflector.go:160] Listing and watching *v1.ServiceAccount from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271618   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.271772   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.271831   18145 reflector.go:122] Starting reflector *v1.LimitRange (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271845   18145 reflector.go:122] Starting reflector *v1.RoleBinding (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271870   18145 reflector.go:160] Listing and watching *v1.RoleBinding from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271856   18145 reflector.go:160] Listing and watching *v1.LimitRange from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271105   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.59494ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47390]</span><br><span class="line">I0705 22:38:04.272020   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;persistentvolumes, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m27s</span><br><span class="line">I0705 22:38:04.271112   18145 reflector.go:122] Starting reflector *v1.PersistentVolume (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271077   18145 reflector.go:122] Starting reflector *v1.Secret (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272079   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272104   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272086   18145 reflector.go:160] Listing and watching *v1.Secret from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272125   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.271432   18145 reflector.go:122] Starting reflector *v1.Endpoints (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272173   18145 reflector.go:160] Listing and watching *v1.Endpoints from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271027   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272219   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272236   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;policy&#x2F;v1beta1</span><br><span class="line">E0705 22:38:04.272283   18145 controller.go:148] Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: &#x2F;registry&#x2F;masterleases&#x2F;10.129.173.93, ResourceVersion: 0, AdditionalErrorMsg: </span><br><span class="line">I0705 22:38:04.271034   18145 reflector.go:122] Starting reflector *v1.ResourceQuota (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272332   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m58s</span><br><span class="line">I0705 22:38:04.272351   18145 reflector.go:160] Listing and watching *v1.ResourceQuota from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272413   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.904677ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:04.272069   18145 reflector.go:160] Listing and watching *v1.PersistentVolume from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272410   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m16s</span><br><span class="line">I0705 22:38:04.271906   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.535252ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:04.270860   18145 reflector.go:122] Starting reflector *apiregistration.APIService (30s) from k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271255   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272581   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272618   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272540   18145 reflector.go:160] Listing and watching *apiregistration.APIService from k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.272651   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272689   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272550   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272736   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272749   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272763   18145 reflector.go:122] Starting reflector *v1.Pod (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272781   18145 reflector.go:160] Listing and watching *v1.Pod from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272707   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272807   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272709   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272823   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272852   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;replicationcontrollers, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;9m32s</span><br><span class="line">I0705 22:38:04.272924   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;nodes, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m3s</span><br><span class="line">I0705 22:38:04.273029   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;9m31s</span><br><span class="line">I0705 22:38:04.273029   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;services, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m33s</span><br><span class="line">I0705 22:38:04.274175   18145 wrap.go:47] GET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (4.688192ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47394]</span><br><span class="line">I0705 22:38:04.276744   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.276763   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.276773   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.276915   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m28s</span><br><span class="line">I0705 22:38:04.278832   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;pods?fieldSelector&#x3D;status.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&amp;limit&#x3D;500&amp;resourceVersion&#x3D;0: (9.407581ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47396]</span><br><span class="line">I0705 22:38:04.289292   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.289312   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.289321   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.289495   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;pods, rv&#x3D;75099443 labels&#x3D; fields&#x3D;status.phase!&#x3D;Failed,status.phase!&#x3D;Succeeded timeout&#x3D;7m21s</span><br><span class="line">I0705 22:38:05.268153   18145 controller.go:107] OpenAPI AggregationController: Processing item </span><br><span class="line">I0705 22:38:05.268190   18145 controller.go:130] OpenAPI AggregationController: action for item : Nothing (removed from the queue).</span><br><span class="line">I0705 22:38:05.268201   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000000</span><br><span class="line">I0705 22:38:05.268208   18145 controller.go:130] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).</span><br><span class="line">I0705 22:38:05.268215   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:05.268278   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:05.268310   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:05.268358   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:05.268389   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:05.268397   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:06.268539   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:06.268663   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:06.268687   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:06.268752   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:06.268776   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:06.268785   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.268914   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:07.269013   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.269030   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.269080   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:07.269105   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.269113   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.554461   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.554491   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:07.554501   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:07.556815   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.426905ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:08.269219   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:08.269327   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.269343   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:08.269394   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:08.269425   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.269434   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:08.419039   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.419077   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:08.419089   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:08.421916   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.948807ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:09.269557   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:09.269664   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:09.269680   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:09.269747   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:09.269778   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:09.269786   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:10.269881   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:10.269989   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:10.270004   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:10.270056   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:10.270083   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:10.270092   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.270200   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:11.270306   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.270321   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.270373   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:11.270398   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.270408   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.483133   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.483166   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:11.483178   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:11.485580   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.529963ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:11.598249   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.598284   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:11.598294   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:11.601107   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.928152ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:12.270550   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:12.270681   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:12.270698   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:12.270752   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:12.270801   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:12.270809   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.270887   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:13.270984   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.271000   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.271050   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:13.271074   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.271083   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.693554   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.693619   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:13.693637   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:13.696146   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.666525ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:14.271182   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:14.271290   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.271305   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:14.271356   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:14.271380   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.271398   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:14.380002   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.380035   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:14.380045   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:14.382250   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.312715ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:15.271510   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:15.271590   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:15.271616   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:15.271664   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:15.271688   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:15.271696   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:16.271827   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:16.271937   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:16.271980   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:16.272042   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:16.272065   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:16.272073   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.272204   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:17.272313   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.272329   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.272380   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:17.272405   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.272414   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.283099   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.283123   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:17.283133   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:17.285196   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.159257ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:17.870495   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.870526   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:17.870543   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:17.873004   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.577282ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:18.272560   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:18.272685   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:18.272701   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:18.272760   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:18.272786   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:18.272794   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.272924   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:19.273020   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.273035   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.273084   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:19.273106   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.273116   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.659566   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.659613   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:19.659624   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:19.662463   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.967004ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:20.273261   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:20.273367   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:20.273382   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:20.273429   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:20.273451   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:20.273459   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:21.273614   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:21.273740   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:21.273759   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:21.273840   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:21.273866   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:21.273878   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.126431   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.126472   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:22.126482   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:22.129022   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.675659ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:22.273990   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:22.274084   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.274097   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.274146   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:22.274168   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.274176   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.957210   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.957246   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:22.957255   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:22.959473   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.335823ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:23.274299   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:23.274407   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:23.274422   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:23.274471   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:23.274492   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:23.274500   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.274621   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:24.274734   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.274749   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.274801   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:24.274824   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.274832   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.618447   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.618488   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:24.618498   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:24.621739   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (3.351212ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:25.274963   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:25.275074   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:25.275092   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:25.275145   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:25.275181   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:25.275190   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.275293   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:26.275395   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.275410   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.275465   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:26.275488   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.275497   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.925578   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.925641   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:26.925652   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:26.927894   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.404426ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:27.275645   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:27.275777   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:27.275793   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:27.275846   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:27.275868   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:27.275877   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.276002   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:28.276107   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.276122   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.276172   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:28.276194   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.276203   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.664065   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.664097   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:28.664107   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:28.666395   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.399236ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:29.276334   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:29.276422   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:29.276436   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:29.276489   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:29.276522   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:29.276531   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.276644   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:30.276774   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.276790   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.276843   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:30.276868   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.276877   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.616203   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.616236   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:30.616246   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:30.618555   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.432457ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:31.277014   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:31.277123   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:31.277138   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:31.277186   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:31.277207   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:31.277216   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.277331   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:32.277437   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.277452   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.277515   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:32.277542   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.277551   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.511091   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.511128   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:32.511138   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:32.513369   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.359242ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:33.277687   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:33.277802   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:33.277817   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:33.277883   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:33.277906   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:33.277915   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:34.270818   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:34.271625   18145 trace.go:81] Trace[916838159]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271124387 +0800 CST m&#x3D;+3.485745147) (total time: 30.000454926s):</span><br><span class="line">Trace[916838159]: [30.000454926s] [30.000454926s] END</span><br><span class="line">E0705 22:38:34.271664   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Node: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;nodes?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271749   18145 trace.go:81] Trace[1068818673]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271318857 +0800 CST m&#x3D;+3.485939616) (total time: 30.000409266s):</span><br><span class="line">Trace[1068818673]: [30.000409266s] [30.000409266s] END</span><br><span class="line">I0705 22:38:34.271762   18145 trace.go:81] Trace[1453832293]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271338371 +0800 CST m&#x3D;+3.485959213) (total time: 30.00039864s):</span><br><span class="line">Trace[1453832293]: [30.00039864s] [30.00039864s] END</span><br><span class="line">E0705 22:38:34.271784   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Role: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;roles?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271764   18145 trace.go:81] Trace[1415476892]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271426707 +0800 CST m&#x3D;+3.486047513) (total time: 30.000302054s):</span><br><span class="line">Trace[1415476892]: [30.000302054s] [30.000302054s] END</span><br><span class="line">E0705 22:38:34.271817   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Service: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;services?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">E0705 22:38:34.271764   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ClusterRoleBinding: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterrolebindings?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271863   18145 trace.go:81] Trace[1943398002]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27156591 +0800 CST m&#x3D;+3.486186674) (total time: 30.000282292s):</span><br><span class="line">Trace[1943398002]: [30.000282292s] [30.000282292s] END</span><br><span class="line">E0705 22:38:34.271873   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Namespace: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;namespaces?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271862   18145 trace.go:81] Trace[2040039466]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271529276 +0800 CST m&#x3D;+3.486150039) (total time: 30.000321445s):</span><br><span class="line">Trace[2040039466]: [30.000321445s] [30.000321445s] END</span><br><span class="line">E0705 22:38:34.271884   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ClusterRole: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterroles?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271906   18145 trace.go:81] Trace[1482887971]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271448982 +0800 CST m&#x3D;+3.486069767) (total time: 30.000436887s):</span><br><span class="line">Trace[1482887971]: [30.000436887s] [30.000436887s] END</span><br><span class="line">E0705 22:38:34.271923   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.VolumeAttachment: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;volumeattachments?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271952   18145 trace.go:81] Trace[1817353754]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271633058 +0800 CST m&#x3D;+3.486253814) (total time: 30.000305162s):</span><br><span class="line">Trace[1817353754]: [30.000305162s] [30.000305162s] END</span><br><span class="line">E0705 22:38:34.271961   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.StorageClass: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272034   18145 trace.go:81] Trace[1163666912]: &quot;Reflector k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27168685 +0800 CST m&#x3D;+3.486307613) (total time: 30.000322616s):</span><br><span class="line">Trace[1163666912]: [30.000322616s] [30.000322616s] END</span><br><span class="line">E0705 22:38:34.272081   18145 reflector.go:125] k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117: Failed to list *apiextensions.CustomResourceDefinition: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;apiextensions.k8s.io&#x2F;v1beta1&#x2F;customresourcedefinitions?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272069   18145 trace.go:81] Trace[821616895]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271737097 +0800 CST m&#x3D;+3.486357868) (total time: 30.0003199s):</span><br><span class="line">Trace[821616895]: [30.0003199s] [30.0003199s] END</span><br><span class="line">E0705 22:38:34.272096   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ServiceAccount: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;serviceaccounts?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272148   18145 trace.go:81] Trace[1495176524]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271923158 +0800 CST m&#x3D;+3.486543922) (total time: 30.000215143s):</span><br><span class="line">Trace[1495176524]: [30.000215143s] [30.000215143s] END</span><br><span class="line">E0705 22:38:34.272162   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.LimitRange: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;limitranges?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272297   18145 trace.go:81] Trace[1304373005]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271876523 +0800 CST m&#x3D;+3.486497285) (total time: 30.000397052s):</span><br><span class="line">Trace[1304373005]: [30.000397052s] [30.000397052s] END</span><br><span class="line">E0705 22:38:34.272308   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.RoleBinding: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;rolebindings?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272401   18145 trace.go:81] Trace[2092646535]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27211704 +0800 CST m&#x3D;+3.486737797) (total time: 30.000268938s):</span><br><span class="line">Trace[2092646535]: [30.000268938s] [30.000268938s] END</span><br><span class="line">E0705 22:38:34.272410   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Secret: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;secrets?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272483   18145 trace.go:81] Trace[475362393]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272182677 +0800 CST m&#x3D;+3.486803437) (total time: 30.000287299s):</span><br><span class="line">Trace[475362393]: [30.000287299s] [30.000287299s] END</span><br><span class="line">E0705 22:38:34.272490   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Endpoints: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;endpoints?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272688   18145 trace.go:81] Trace[990205759]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272462183 +0800 CST m&#x3D;+3.487082944) (total time: 30.000213936s):</span><br><span class="line">Trace[990205759]: [30.000213936s] [30.000213936s] END</span><br><span class="line">E0705 22:38:34.272697   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.PersistentVolume: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;persistentvolumes?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272692   18145 trace.go:81] Trace[24141869]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27236239 +0800 CST m&#x3D;+3.486983158) (total time: 30.000315126s):</span><br><span class="line">Trace[24141869]: [30.000315126s] [30.000315126s] END</span><br><span class="line">E0705 22:38:34.272707   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ResourceQuota: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;resourcequotas?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272982   18145 trace.go:81] Trace[868760541]: &quot;Reflector k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272649159 +0800 CST m&#x3D;+3.487269923) (total time: 30.000318724s):</span><br><span class="line">Trace[868760541]: [30.000318724s] [30.000318724s] END</span><br><span class="line">E0705 22:38:34.272991   18145 reflector.go:125] k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117: Failed to list *apiregistration.APIService: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;apiregistration.k8s.io&#x2F;v1&#x2F;apiservices?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.273086   18145 trace.go:81] Trace[1379481655]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272785699 +0800 CST m&#x3D;+3.487406462) (total time: 30.000282688s):</span><br><span class="line">Trace[1379481655]: [30.000282688s] [30.000282688s] END</span><br><span class="line">E0705 22:38:34.273100   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Pod: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;pods?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">F0705 22:38:34.275033   18145 controller.go:157] Unable to perform initial IP allocation check: unable to refresh the service IP block: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;services: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.303248   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:34.341834   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:34.341849   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:34.341904   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:34.341926   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:34.341934   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/kubernetes/kubernetes/issues/82067">https://github.com/kubernetes/kubernetes/issues/82067</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;p&gt;三个 master （etcd 也在 master 上，master上也有 kubelet）和 n 个 node。maste</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="systemd" scheme="http://zhangguanzhang.github.io/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>Job for docker.service canceled</title>
    <link href="http://zhangguanzhang.github.io/2021/07/05/systemctl-start-docker-canceled/"/>
    <id>http://zhangguanzhang.github.io/2021/07/05/systemctl-start-docker-canceled/</id>
    <published>2021-07-05T17:08:06.000Z</published>
    <updated>2021-07-05T17:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>内部安装 docker 的脚本报错 docker 安装失败。然后启动发现下面奇怪的问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line">$ systemctl start docker</span><br><span class="line">Job for docker.service canceled.</span><br></pre></td></tr></table></figure><p>但是用 <code>service docker start</code> 能启动，这就很迷，尝试前台启动也无啥错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ service docker start</span><br><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Mon 2021-07-05 14:41:30 CST; 15s ago</span><br><span class="line">     Docs: http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line">$ systemctl cat docker</span><br><span class="line"># &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Docker Application Container Engine</span><br><span class="line">Documentation&#x3D;http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment&#x3D;&quot;PATH&#x3D;&#x2F;data&#x2F;kube&#x2F;bin:&#x2F;bin:&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;sbin&quot;</span><br><span class="line">ExecStart&#x3D;&#x2F;data&#x2F;kube&#x2F;bin&#x2F;dockerd </span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;sh -c &#39;&#x2F;sbin&#x2F;iptables --wait -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;sh -c &#39;&#x2F;sbin&#x2F;iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecReload&#x3D;&#x2F;bin&#x2F;kill -s HUP $MAINPID</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">RestartSec&#x3D;5</span><br><span class="line">LimitNOFILE&#x3D;infinity</span><br><span class="line">LimitNPROC&#x3D;infinity</span><br><span class="line">LimitCORE&#x3D;infinity</span><br><span class="line">Delegate&#x3D;yes</span><br><span class="line">KillMode&#x3D;process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>从上面<code>systemctl cat docker</code>看是没有依赖服务的，如果官方<code>rpm</code> 包安装的会依赖<code>containerd</code>。不过先看下失败的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl --failed</span><br><span class="line">  UNIT       LOAD   ACTIVE SUB    DESCRIPTION                                                                                                                                                                                              </span><br><span class="line">● data.mount loaded failed failed &#x2F;data                                                                                                                                                                                                    </span><br><span class="line"></span><br><span class="line">LOAD   &#x3D; Reflects whether the unit definition was properly loaded.</span><br><span class="line">ACTIVE &#x3D; The high-level unit activation state, i.e. generalization of SUB.</span><br><span class="line">SUB    &#x3D; The low-level unit activation state, values depend on unit type.</span><br><span class="line"></span><br><span class="line">1 loaded units listed. Pass --all to see loaded but inactive units, too.</span><br><span class="line">To show all installed unit files use &#39;systemctl list-unit-files&#39;.</span><br></pre></td></tr></table></figure><p>信息被冲没了，后面拿其他机器信息复制下，<code>systemctl start</code>会连 dbus 之类的，而 service 不会，结合前面的 data 挂载失败，系统应该是 <code>emergency</code> 半启动导致的，看了下果然</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status emergency </span><br><span class="line">● emergency.service - Emergency Shell</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;emergency.service; static; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Mon 2021-07-05 17:18:28 CST; 5min ago</span><br><span class="line">     Docs: man:sulogin(8)</span><br><span class="line">  Process: 674 ExecStartPre&#x3D;&#x2F;bin&#x2F;plymouth --wait quit (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 675 (systemd-sulogin)</span><br><span class="line">    Tasks: 5 (limit: 4915)</span><br><span class="line">   Memory: 32.9M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;emergency.service</span><br><span class="line">           ├─647 &#x2F;sbin&#x2F;sulogin</span><br><span class="line">           ├─675 &#x2F;lib&#x2F;systemd&#x2F;systemd-sulogin-shell emergency</span><br><span class="line">           ├─676 &#x2F;sbin&#x2F;sulogin</span><br><span class="line">           ├─677 bash</span><br><span class="line">           └─678 &#x2F;sbin&#x2F;sulogin</span><br><span class="line"></span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: Started Emergency Shell.</span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: emergency.service: Found left-over process 647 (sulogin) in control group while starting unit. Ignoring.</span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.</span><br></pre></td></tr></table></figure><p>然后看了下<code>/etc/fstab</code>，是把<code>defaults</code>写成了<code>default</code>导致的无法挂载。然后解决重启后好了。</p><p>询问了测试人员，她说她改了<code>/etc/fstab</code>后看启动<code>emergency mode</code>的输入root密码提示，然后输入root密码进去。然后启动 sshd 失败，然后用 <code>service sshd start</code>。然后 ssh 上去部署docker，然后我们这边 ssh 上来看，之前接触的centos 版本在 emergency 模式貌似不会有网。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;故障现象&quot;&gt;&lt;a href=&quot;#故障现象&quot; class=&quot;headerlink&quot; title=&quot;故障现象&quot;&gt;&lt;/a&gt;故障现象&lt;/h2&gt;&lt;p&gt;内部安装 docker 的脚本报错 docker 安装失败。然后启动发现下面奇怪的问题:&lt;/p&gt;
&lt;figure class</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="systemd" scheme="http://zhangguanzhang.github.io/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>openshift 4.5.9 etcd损坏+脑裂修复过程</title>
    <link href="http://zhangguanzhang.github.io/2021/06/08/ocp4.5.9-restore-etcd/"/>
    <id>http://zhangguanzhang.github.io/2021/06/08/ocp4.5.9-restore-etcd/</id>
    <published>2021-06-08T18:36:06.000Z</published>
    <updated>2021-06-08T18:36:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h2><p>内部机器和环境都是在 vcenter 里，之前的 ocp 集群是 3 master + 1 worker，也就是之前的<a href="./ocp-4.5-install.md">openshift 4.5.9 离线安装</a>后的环境，后面有几台宿主机负载太高，同事看我机器负载最高，关了几台，这几天需要用下 <code>openshift</code> 环境。登录到 <code>bastion</code> 上 get 超时，看了下 haproxy 的 stat web，全部红了。。然后把所有机器开机后发现还是起不来。</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>openshift 的 master 节点和 kubeadm 很像，几个组件都是 <code>staticPod</code> 形式起的。客户端也不是 <code>docker</code>，使用 <code>crictl</code> 就行了</p><h3 id="查看-kube-apiserver"><a href="#查看-kube-apiserver" class="headerlink" title="查看 kube-apiserver"></a>查看 kube-apiserver</h3><p>ssh 到 master1 上，查看日志发现是 etcd 无法起来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh -i ~/.ssh/new_rsa core@10.x.45.251</span><br><span class="line">crictl ps -a | grep kube-apiserver</span><br><span class="line">crictl logs xxx</span><br></pre></td></tr></table></figure><p>etcd 只有一台正常，一台日志报错 snap 文件损坏，一台报错 revision 太低，先进入正常的那台上面 etcd 容器（下文所有 etcdctl 都是在 etcd 容器里执行的，进容器就是下面命令）:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | grep etcd</span><br><span class="line">crictl <span class="built_in">exec</span> -ti xxx bash</span><br><span class="line"></span><br><span class="line">$ env | grep ETCDCTL</span><br><span class="line">ETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master2.openshift4.example.com.crt</span><br><span class="line">ETCDCTL_ENDPOINTS=https://10.x.45.251:2379,https://10.x.45.252:2379,https://10.x.45.222:2379</span><br><span class="line">ETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span><br><span class="line">ETCDCTL_API=3</span><br><span class="line">ETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master2.openshift4.example.com.key</span><br><span class="line"></span><br><span class="line">$ etcd --version</span><br><span class="line">etcd Version: 3.4.9</span><br><span class="line">Git SHA: 4657b9e</span><br><span class="line">Go Version: go1.13.4</span><br><span class="line">Go OS/Arch: linux/amd64</span><br></pre></td></tr></table></figure><h3 id="故障的开始"><a href="#故障的开始" class="headerlink" title="故障的开始"></a>故障的开始</h3><p>这里有个知识点就是 etcd 和 etcdctl 的一些命令行选项都可以被环境变量替代，例如上面的这些。忘了从哪个版本开始了。 <code>etcdctl snapshot save </code> 时候 <code>endpoints</code> 只能指定一个节点，尝试备份，结果卡住：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl snapshot save 0608-etcd.db</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623124958.5931418,<span class="string">&quot;caller&quot;</span>,<span class="string">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;create temporary db file&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;0608-etcd.db.part&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="一点进展"><a href="#一点进展" class="headerlink" title="一点进展"></a>一点进展</h3><p>这套集群当初以为就用一下，没考虑备份。然后在机器上乱逛，发现了高版本集群是自带了备份的（至少我这个版本是自带了）。在目录 <code>/etc/kubernetes/rollbackcopy</code> 下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/kubernetes/</span><br><span class="line">$ ll rollbackcopy/*</span><br><span class="line">rollbackcopy/currentVersion.latest:</span><br><span class="line">total 707152</span><br><span class="line">-rw-r--r--. 1 root root        58 Jun  8 09:17 backupenv.json</span><br><span class="line">-rw-------. 1 root root 724054048 Jun  8 09:17 snapshot_2021-06-08_091655.db</span><br><span class="line">-rw-------. 1 root root     59426 Jun  8 09:17 static_kuberesources_2021-06-08_091655.tar.gz</span><br><span class="line"></span><br><span class="line">rollbackcopy/currentVersion.prev:</span><br><span class="line">total 707152</span><br><span class="line">-rw-r--r--. 1 root root        58 Jun  8 08:11 backupenv.json</span><br><span class="line">-rw-------. 1 root root 724054048 Jun  8 08:11 snapshot_2021-06-08_081148.db</span><br><span class="line">-rw-------. 1 root root     59426 Jun  8 08:11 static_kuberesources_2021-06-08_081148.tar.gz</span><br></pre></td></tr></table></figure><p>然后像用 etcdctl 恢复备份，从容器里拷贝出来，结果 crictl 没 cp 命令，查看了下 etcdctl 的挂载，想进 etcd 容器里把 etcdctl 复制到挂载的路径上，这样宿主机上就有了。结果搞出来之后，习惯性的把二进制文件移到<code>/usr/local/bin/</code>下，结果 tab 按键补全看到有下面几个脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">cluster-backup.sh  cluster-restore.sh  recover-kubeconfig.sh</span><br></pre></td></tr></table></figure><h4 id="自带的备份恢复"><a href="#自带的备份恢复" class="headerlink" title="自带的备份恢复"></a>自带的备份恢复</h4><p>查看了下 <code>cluster-restore.sh</code> 脚本，脚本第一个参数是指定备份目录，也就是上面发现的目录，<strong>尝试在不正常的两个节点上</strong> 运行下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/kubernetes/rollbackcopy/currentVersion.latest</span><br><span class="line">$ cluster-restore.sh .</span><br><span class="line">...stopping kube-apiserver-pod.yml</span><br><span class="line">...stopping kube-controller-manager-pod.yml</span><br><span class="line">...stopping kube-scheduler-pod.yml</span><br><span class="line">...stopping etcd-pod.yml</span><br><span class="line">Waiting <span class="keyword">for</span> container etcd to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container etcdctl to stop</span><br><span class="line">...................................complete</span><br><span class="line">Waiting <span class="keyword">for</span> container etcd-metrics to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-controller-manager to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-apiserver to stop</span><br><span class="line">.........................complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-scheduler to stop</span><br><span class="line">complete</span><br><span class="line">starting restore-etcd static pod</span><br><span class="line">starting kube-apiserver-pod.yml</span><br><span class="line">static-pod-resource/kube-apiserver-pod-50/kube-apiserver-pod.yaml</span><br><span class="line">starting kube-controller-manager-pod.yml</span><br><span class="line">static-pod-resource/kube-controller-manager-pod-7/kube-controller-manager-pod.yml</span><br><span class="line">starting kube-scheduler-pod.yml</span><br><span class="line">static-pod-resource/kube-scheduler-pod-7/kube-scheduler-pod.yml</span><br></pre></td></tr></table></figure><p>这个脚本运行期间的等待容器停止要根据实际情况可能需要自己去手动stop，可以使用下面的去 stop 相关容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">STATIC_POD_CONTAINERS=(<span class="string">&quot;etcd&quot;</span> <span class="string">&quot;etcdctl&quot;</span> <span class="string">&quot;etcd-metrics&quot;</span> <span class="string">&quot;kube-controller-manager&quot;</span> <span class="string">&quot;kube-apiserver&quot;</span> <span class="string">&quot;kube-scheduler&quot;</span>)</span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">wait_for_containers_to_stop</span></span>()&#123;</span><br><span class="line">  <span class="built_in">local</span> CONTAINERS=(<span class="string">&quot;<span class="variable">$@</span>&quot;</span>) ctrID</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> NAME <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;CONTAINERS[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Waiting for container <span class="variable">$&#123;NAME&#125;</span> to stop&quot;</span></span><br><span class="line">    ctrID=<span class="string">&quot;<span class="subst">$(crictl ps --label io.kubernetes.container.name=$&#123;NAME&#125; -q)</span>&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$ctrID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">      crictl stop <span class="variable">$ctrID</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">wait_for_containers_to_stop <span class="variable">$&#123;STATIC_POD_CONTAINERS[*]&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行完后的状态:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ crictl ps -a</span><br><span class="line">CONTAINER           IMAGE                                                              CREATED             STATE               NAME                                          ATTEMPT             POD ID</span><br><span class="line">44f09a621803e       5e0c1e21da05b4b6455632cb70d9e29c76a5710e0bfa129ebef00d1cc1d5ee85   39 seconds ago      Running             etcd                                          0                   ad5b9a73adb3a</span><br><span class="line">b728530b11ea7       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-scheduler                                1                   af1e63ab536fd</span><br><span class="line">ec9b583321b64       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-apiserver                                26                  f68ce6fa73ad6</span><br><span class="line">05431a2d159b9       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-controller-manager                       1                   cc1a03361154d</span><br><span class="line">4a1d734b14faf       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   36 minutes ago      Exited              kube-apiserver                                25                  f68ce6fa73ad6</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><code>kube-apiserver</code>起来了，然后能使用 oc 了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ oc get node</span><br><span class="line">NAME                             STATUS   ROLES           AGE    VERSION</span><br><span class="line">master1.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">master2.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">master3.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">worker1.openshift4.example.com   Ready    worker          259d   v1.18.3+6c42de8</span><br></pre></td></tr></table></figure><h3 id="etcd-的脑裂"><a href="#etcd-的脑裂" class="headerlink" title="etcd 的脑裂"></a>etcd 的脑裂</h3><p>然后我的开发 namespaces 下有个 pod pending，<code>kubectl</code> 删了下报错，大致是 etcd 删不掉啥的。然后看了下 etcd 的状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl endpoint status --write-out=table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 7ad933dac58f4549 |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   30260441 |           30260441 |        |</span><br><span class="line">| https://10.x.45.252:2379 | f4351098cae1d407 |   3.4.9 |  726 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   40195520 |           40195520 |        |</span><br><span class="line">| https://10.x.45.222:2379 | 2399ef0cea33ebf3 |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   50408739 |           50408739 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>没错，脑裂了。三个都找不到其他的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">7ad933dac58f4549, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl member list</span><br><span class="line">f7f6c198cb519536, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl member list</span><br><span class="line">ac62bec820f40228, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="处理脑裂"><a href="#处理脑裂" class="headerlink" title="处理脑裂"></a>处理脑裂</h4><h5 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h5><p>尝试下 <code>move-leader</code> 命令看看能否操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl move-leader 7ad933dac58f4549</span><br><span class="line">2021-06-08 02:59:35.782766 C | pkg/flags: conflicting environment variable <span class="string">&quot;ETCDCTL_ENDPOINTS&quot;</span> is shadowed by corresponding command-line flag (either <span class="built_in">unset</span> environment variable or <span class="built_in">disable</span> flag)</span><br></pre></td></tr></table></figure><p>说环境变量和命令行同时设置了 endpoints ，unset 下它后尝试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ unset ETCDCTL_ENDPOINTS</span><br><span class="line">$ export ETCDCTL_ENDPOINTS&#x3D;https:&#x2F;&#x2F;10.x.45.222:2379</span><br><span class="line">$ etcdctl move-leader 7ad933dac58f4549</span><br><span class="line">2021-06-08 03:00:11.019150 C | pkg&#x2F;flags: conflicting environment variable &quot;ETCDCTL_CERT&quot; is shadowed by corresponding command-line flag (either unset environment variable or disable flag)</span><br></pre></td></tr></table></figure><p>然后另一个变量报错，搜了下这个是 <code>etcdctl move-leader</code> 的 bug，见 <a href="https://github.com/etcd-io/etcd/pull/12757">pr</a> ，手动 unset 相关 <code>ETCDCTL_xxx</code> 变量后执行下报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl --endpoints https://10.x.45.222:2379 \</span><br><span class="line">  --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \</span><br><span class="line">  --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master3.openshift4.example.com.crt \</span><br><span class="line">  --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master3.openshift4.example.com.key \</span><br><span class="line">  move-leader 7ad933dac58f4549</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T03:07:08.767Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;endpoint://client-27e0f779-7d90-4be3-9491-f0e915374f3c/10.xxx.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = FailedPrecondition desc = etcdserver: bad leader transferee&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>好吧，<code>move-leader</code>对这种场景用不了。不过现在三个都起来了，应该是能备份了。准备在其他节点上用备份恢复下，先看了下 etcd staticPod 的 yaml <code>/etc/kubernetes/manifests/etcd-pod.yaml</code> 的内容启动参数是否需要调整：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line">        <span class="comment">#!/bin/sh</span></span><br><span class="line">        <span class="string">set</span> <span class="string">-euo</span> <span class="string">pipefail</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="string">if</span> [ <span class="string">!</span> <span class="string">-z</span> <span class="string">$(ls</span> <span class="string">-A</span> <span class="string">&quot;/var/lib/etcd&quot;</span><span class="string">)</span> ]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;please delete the contents of data directory before restoring, running the restore script will do this for you&quot;</span></span><br><span class="line">          <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if we have backup file to be restored</span></span><br><span class="line">        <span class="comment"># if the file exist, check if it has not changed size in last 5 seconds</span></span><br><span class="line">        <span class="string">if</span> [ <span class="string">!</span> <span class="string">-f</span> <span class="string">/var/lib/etcd-backup/snapshot.db</span> ]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;please make a copy of the snapshot db file, then move that copy to /var/lib/etcd-backup/snapshot.db&quot;</span></span><br><span class="line">          <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">else</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>从逻辑看是启动的时候如果数据目录必须不为空，然后<code>/var/lib/etcd-backup/</code>目录得存在备份的 db 文件，看了下挂载目录，宿主机上也是这个目录。打算先在第一个 master 节点的 etcd 容器里备份。然后用备份文件在其他节点恢复备份。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl  snapshot save /var/lib/etcd-backup/snapshot.db</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143517.6130972,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;created temporary db file&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;/var/lib/etcd-backup/snapshot.db.part&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:11:57.621Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/maintenance.go:200&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;opened snapshot stream; downloading&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143517.6213868,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:127&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;fetching snapshot&quot;</span>,<span class="string">&quot;endpoint&quot;</span>:<span class="string">&quot;https://10.x.45.251:2379&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:12:03.315Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/maintenance.go:208&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;completed snapshot read; closing&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143524.4544568,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:142&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;fetched snapshot&quot;</span>,<span class="string">&quot;endpoint&quot;</span>:<span class="string">&quot;https://10.x.45.251:2379&quot;</span>,<span class="string">&quot;size&quot;</span>:<span class="string">&quot;724 MB&quot;</span>,<span class="string">&quot;took&quot;</span>:6.841299825&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143524.4545853,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:152&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;saved&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;/var/lib/etcd-backup/snapshot.db&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>然后停掉后面的两台 master 节点的 etcd。改名 <code>/var/lib/etcd</code> 目录（不要一上来就删除目录，改名是永远最稳妥的手段）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/</span><br><span class="line">cp manifests/etcd-pod.yaml .</span><br><span class="line">mv manifests/etcd-pod.yaml /tmp/</span><br><span class="line"></span><br><span class="line">etcdID=<span class="string">&quot;<span class="subst">$(crictl ps --label io.kubernetes.container.name=etcd -q)</span>&quot;</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$etcdID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    crictl stop <span class="variable">$etcdID</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">mv /var/lib/etcd /var/lib/etcd-bak</span><br></pre></td></tr></table></figure><p>然后用密钥 ssh 到其他机器上把 ssh 的 root 和密码登录开了来让我们可以使用 scp 过去，然后备份文件<code>/var/lib/etcd-backup/snapshot.db</code> scp 过去到其余 master 上的同样路径。</p><h5 id="处理脑裂-1"><a href="#处理脑裂-1" class="headerlink" title="处理脑裂"></a>处理脑裂</h5><p>细心观察看前面的每个 endpoint 下 member list 是看到的自己的。所以是在 master1 上逐渐添加其他 member。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member add master2.openshift4.example.com --peer-urls=https://10.x.45.252:2380</span><br><span class="line">Member 3e27197aa4521ea0 added to cluster 1c2134e7d41c45b1</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;master2.openshift4.example.com&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;master2.openshift4.example.com=https://10.x.45.252:2380,master1.openshift4.example.com=https://10.x.45.251:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">&quot;https://10.x.45.252:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure><p>然后在第二个 master 上改下 etcd 的 staticPod yaml <code>/tmp/etcd-pod.yaml</code>。根据自身的实际更改，删掉备份和恢复相关的逻辑。主要是更改 <code>ETCD_INITIAL_CLUSTER</code> 为所有集群，格式为 <code>$&#123;name1&#125;=https://$&#123;ip1&#125;:2380,$&#123;name2&#125;=https://$&#123;ip2&#125;:2380...</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="comment">#export ETCD_INITIAL_CLUSTER=xxx</span></span><br><span class="line">        <span class="string">NAME_ETCD_ARRAY=()</span></span><br><span class="line">        <span class="string">for</span> <span class="string">i</span> <span class="string">in</span> <span class="string">$(env</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-Po</span> <span class="string">&#x27;(?&lt;=NODE_).+(?=_ETCD_NAME)&#x27;</span> <span class="string">|</span> <span class="string">sort</span> <span class="string">);do</span></span><br><span class="line">            <span class="string">etcd_name=NODE_$&#123;i&#125;_ETCD_NAME</span></span><br><span class="line">            <span class="string">url_host_var=NODE_$&#123;i&#125;_ETCD_URL_HOST</span></span><br><span class="line">            <span class="string">NAME_ETCD_ARRAY+=(&quot;$&#123;!etcd_name&#125;=https://$&#123;!url_host_var&#125;:2380&quot;)</span></span><br><span class="line">        <span class="string">done</span></span><br><span class="line">        <span class="string">export</span> <span class="string">ETCD_INITIAL_CLUSTER=$(</span> <span class="string">echo</span> <span class="string">$&#123;NAME_ETCD_ARRAY[*]&#125;</span> <span class="string">|</span> <span class="string">tr</span> <span class="string">&#x27; &#x27;</span> <span class="string">&#x27;,&#x27;</span> <span class="string">)</span></span><br></pre></td></tr></table></figure><p>改好后在 master2 上启动 etcd ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /tmp/etcd-pod.yaml /etc/kubernetes/manifests/</span><br></pre></td></tr></table></figure><p>在 master1 上查看:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, unstarted, , https://10.x.45.252:2380, , <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, unstarted, , https://10.x.45.252:2380, , <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:26:16.669Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;passthrough:///https://10.x.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \&quot;transport: Error while dialing dial tcp 10.x.45.222:2379: connect: connection refused\&quot;&quot;</span>&#125;</span><br><span class="line">Failed to get the status of endpoint https://10.x.45.222:2379 (context deadline exceeded)</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       1041 |               1041 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       1041 |               1041 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>好消息，然后恢复第三个，添加第三个member：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member add master3.openshift4.example.com --peer-urls=https://10.x.45.222:2380</span><br><span class="line">Member d319fa1cbb0e28fe added to cluster 1c2134e7d41c45b1</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;master3.openshift4.example.com&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;master2.openshift4.example.com=https://10.x.45.252:2380,master1.openshift4.example.com=https://10.x.45.251:2380,master3.openshift4.example.com=https://10.x.45.222:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">&quot;https://10.x.45.222:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure><p>第三个 etcd yaml 也像之前一样更改。启动后在持续观察：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:32:55.197Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;passthrough:///https://10.x.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;</span>&#125;</span><br><span class="line">Failed to get the status of endpoint https://10.x.45.222:2379 (context deadline exceeded)</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2388 |               2388 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2388 |               2388 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2593 |               2593 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2593 |               2593 |        |</span><br><span class="line">| https://10.x.45.222:2379 | d319fa1cbb0e28fe |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2596 |               2596 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">| https://10.x.45.222:2379 | d319fa1cbb0e28fe |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>单独看每个 endpoint 的 member list 正常否：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>然后看了下 node not ready了，approve 了所有 csr后就好了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc get csr</span><br><span class="line">oc adm certificate approve xxx</span><br></pre></td></tr></table></figure><p>然后测试了下，kubectl 能删掉 pod 了。后续等稳定后再手动备份下</p><h5 id="无法调度和-logs-报错-remote-error-tls-internal-error"><a href="#无法调度和-logs-报错-remote-error-tls-internal-error" class="headerlink" title="无法调度和 logs 报错 remote error: tls: internal error"></a>无法调度和 logs 报错 remote error: tls: internal error</h5><p>同时也无法调度，master 上去看 <code>kube-apiserver</code>日志刷：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authentication.go:53] Unable to authenticate the request due to an error: x509: certificate signed by unkown authority</span><br></pre></td></tr></table></figure><p>搜了下都没解决办法，最后自己在 master 上屏直觉找到解决办法了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> :;<span class="keyword">do</span></span><br><span class="line">  sleep 2</span><br><span class="line">  oc get csr -o name | xargs -r oc adm certificate approve</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>另一个窗口 ssh 到 master上停掉 <code>cert-syncer</code> 相关容器:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | awk <span class="string">&#x27;/Running/&amp;&amp;/-cert-syncer/&#123;print $1&#125;&#x27;</span> | xargs -r crictl stop</span><br></pre></td></tr></table></figure><h4 id="一些疑惑"><a href="#一些疑惑" class="headerlink" title="一些疑惑"></a>一些疑惑</h4><p>后面尝试自带的 yaml 文件+那个备份脚本恢复的就是脑裂集群，询问了个 4.7.13 集群的，看了下 etcd 的 yaml 文件是下面的。没有启动前恢复备份啥的了。可能我这个版本才存在这种问题吧。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">                    <span class="comment">#!/bin/sh</span></span><br><span class="line">                    <span class="string">set</span> <span class="string">-euo</span> <span class="string">pipefail</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">etcdctl</span> <span class="string">member</span> <span class="string">list</span> <span class="string">||</span> <span class="literal">true</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># this has a non-zero return code if the command is non-zero.  If you use an export first, it doesn&#x27;t and you</span></span><br><span class="line">                    <span class="comment"># will succeed when you should fail.</span></span><br><span class="line">                    <span class="string">ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--endpoints=$&#123;ALL_ETCD_ENDPOINTS&#125;</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--data-dir=/var/lib/etcd</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--target-peer-url-host=$&#123;NODE_master11_cluster_lonlife_dev_ETCD_URL_HOST&#125;</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--target-name=master11.cluster.lonlife.dev)</span></span><br><span class="line">                     <span class="string">export</span> <span class="string">ETCD_INITIAL_CLUSTER</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># we cannot use the \&quot;normal\&quot; port conflict initcontainer because when we upgrade, the existing static pod will never yield,</span></span><br><span class="line">                    <span class="comment"># so we do the detection in etcd container itsefl.</span></span><br><span class="line">                    <span class="string">echo</span> <span class="string">-n</span> <span class="string">\&quot;Waiting</span> <span class="string">for</span> <span class="string">ports</span> <span class="number">2379</span><span class="string">,</span> <span class="number">2380 </span><span class="string">and</span> <span class="number">9978 </span><span class="string">to</span> <span class="string">be</span> <span class="string">released.\&quot;</span></span><br><span class="line">                    <span class="string">while</span> [ <span class="string">-n</span> <span class="string">\&quot;$(ss</span> <span class="string">-Htan</span> <span class="string">&#x27;( sport = 2379 or sport = 2380 or sport = 9978 )&#x27;</span><span class="string">)\&quot;</span> ]<span class="string">;</span> <span class="string">do</span></span><br><span class="line">                      <span class="string">echo</span> <span class="string">-n</span> <span class="string">\&quot;.\&quot;</span></span><br><span class="line">                      <span class="string">sleep</span> <span class="number">1</span></span><br><span class="line">                    <span class="string">done</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">export</span> <span class="string">ETCD_NAME=$&#123;NODE_master11_cluster_lonlife_dev_ETCD_NAME&#125;</span></span><br><span class="line">                    <span class="string">env</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">ETCD</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-v</span> <span class="string">NODE</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">set</span> <span class="string">-x</span></span><br><span class="line">                    <span class="comment"># See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice</span></span><br><span class="line">                    <span class="string">exec</span> <span class="string">ionice</span> <span class="string">-c2</span> <span class="string">-n0</span> <span class="string">etcd</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--log-level=info</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--initial-advertise-peer-urls=https://$&#123;NODE_master11_cluster_lonlife_dev_IP&#125;:2380</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--client-cert-auth=true</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-client-cert-auth=true</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--advertise-client-urls=https://$&#123;NODE_master11_cluster_lonlife_dev_IP&#125;:2379</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-client-urls=https://0.0.0.0:2379</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-peer-urls=https://0.0.0.0:2380</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-metrics-urls=https://0.0.0.0:9978</span> <span class="string">||</span>  <span class="string">mv</span> <span class="string">/etc/kubernetes/etcd-backup-dir/etcd-member.yaml</span> <span class="string">/etc/kubernetes/manifests</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://etcd.io/docs/v3.4/op-guide/runtime-configuration/">etcd op guide</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/backup_and_restore/backing-up-etcd.html">backing-up-etcd</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言介绍&quot;&gt;&lt;a href=&quot;#前言介绍&quot; class=&quot;headerlink&quot; title=&quot;前言介绍&quot;&gt;&lt;/a&gt;前言介绍&lt;/h2&gt;&lt;p&gt;内部机器和环境都是在 vcenter 里，之前的 ocp 集群是 3 master + 1 worker，也就是之前的&lt;a </summary>
      
    
    
    
    
    <category term="openshift" scheme="http://zhangguanzhang.github.io/tags/openshift/"/>
    
    <category term="ocp" scheme="http://zhangguanzhang.github.io/tags/ocp/"/>
    
  </entry>
  
  <entry>
    <title>docker-ce 18.09.3 启动panic: invalid freelist page: 56, page type is leaf的解决处理</title>
    <link href="http://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/"/>
    <id>http://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/</id>
    <published>2021-05-26T19:52:37.000Z</published>
    <updated>2021-05-26T19:52:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>这个问题和之前的<a href="https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/">docker-18.06.3-ce启动panic: invalid page type: 0: 0的解决处理</a>差不多，不过 db 文件不同。客户停止 docker 后起不来了，查看日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe -u docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17+08:00&quot; level&#x3D;warning msg&#x3D;&quot;The \&quot;graph\&quot; config file option is deprecated. Please use \&quot;data-root\&quot; instead.&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.841082978+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not change group &#x2F;var&#x2F;run&#x2F;docker.sock to docker: group docker not found&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.858839019+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.btrfs&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.859857612+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.aufs&quot; error&#x3D;&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860062002+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.zfs&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860084186+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter zfs in metadata plugin&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860090453+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter btrfs in metadata plugin&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860095653+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter aufs in metadata plugin&quot; error&#x3D;&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.866980   19061 flags.go:33] FLAG: --container-runtime&#x3D;&quot;docker&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.866984   19061 flags.go:33] FLAG: --container-runtime-endpoint&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;dockershim.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867014   19061 flags.go:33] FLAG: --docker&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867018   19061 flags.go:33] FLAG: --docker-endpoint&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867022   19061 flags.go:33] FLAG: --docker-env-metadata-whitelist&#x3D;&quot;&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867025   19061 flags.go:33] FLAG: --docker-only&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867028   19061 flags.go:33] FLAG: --docker-root&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;docker&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867032   19061 flags.go:33] FLAG: --docker-tls&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867036   19061 flags.go:33] FLAG: --docker-tls-ca&#x3D;&quot;ca.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867039   19061 flags.go:33] FLAG: --docker-tls-cert&#x3D;&quot;cert.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867043   19061 flags.go:33] FLAG: --docker-tls-key&#x3D;&quot;key.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867138   19061 flags.go:33] FLAG: --experimental-dockershim&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867143   19061 flags.go:33] FLAG: --experimental-dockershim-root-directory&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;dockershim&quot;</span><br><span class="line">5月 26 18:42:18 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:18.208058217+08:00&quot; level&#x3D;error msg&#x3D;&quot;Failed to load container 90e5da1293b04c3eab30e9f7a2d5714aba563b2ab20d15c67aee1d1d79d3154c: json: cannot unmarshal number into Go value of type container.Container&quot;</span><br><span class="line">5月 26 18:42:18 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:18.214213879+08:00&quot; level&#x3D;error msg&#x3D;&quot;Failed to load container 99abceba7154b7dba08cc03bc1651fd18f38b0d485dcd2c195f479fc375a8216: invalid character &#39;e&#39; looking for beginning of value&quot;</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: panic: invalid freelist page: 56, page type is leaf</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: goroutine 1 [running]:</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*freelist).read(0xc424b121b0, 0x7fc46c7b1000)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;freelist.go:237 +0x2ff</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).loadFreelist.func1()</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:292 +0x12d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: sync.(*Once).Do(0xc420b70328, 0xc420fb1f58)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;sync&#x2F;once.go:44 +0xc0</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).loadFreelist(0xc420b701e0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:285 +0x50</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.Open(0xc422bddad0, 0x2b, 0x1a4, 0xc420fb2070, 0x20c754c, 0x23816c0, 0x25ceee8)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:262 +0x316</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb.(*BoltDB).getDBhandle(0xc424b10960, 0x32ef098, 0xc424b109a4, 0x7fc47c368e38)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb&#x2F;boltdb.go:113 +0x93</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb.(*BoltDB).List(0xc424b10960, 0xc422b99fe0, 0x1b, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb&#x2F;boltdb.go:274 +0xbb</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*cache).kmap(0xc424a60fe0, 0x26b6dc0, 0xc422a0e4d0, 0x4467ea, 0xc422b99f9a, 0x1a668ce)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;cache.go:43 +0x18a</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*cache).list(0xc424a60fe0, 0x26b6dc0, 0xc422a0e4d0, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;cache.go:164 +0x7b</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*datastore).List(0xc42352b180, 0xc422b99f80, 0x1b, 0x26b6dc0, 0xc422a0e4d0, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;datastore.go:517 +0x179</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).populateNetworks(0xc42203d5c0, 0x5, 0x1a6a443)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge_store.go:50 +0xe0</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).initStore(0xc42203d5c0, 0xc424b316b0, 0x0, 0xc423529bc0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge_store.go:35 +0x207</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).configure(0xc42203d5c0, 0xc424b316b0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge.go:378 +0x1b4</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.Init(0x268abc0, 0xc424119740, 0xc424b316b0, 0x0, 0xffffffffffffffff)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge.go:161 +0xa2</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drvregistry.(*DrvRegistry).AddDriver(0xc424119740, 0x1a6a539, 0x6, 0x266fe78, 0xc424b316b0, 0xc42264df80, 0x8)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drvregistry&#x2F;drvregistry.go:72 +0x48</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork.New(0xc42264df80, 0x9, 0x10, 0xc4209e03f0, 0xc420c7f620, 0xc42264df80, 0x9)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;controller.go:220 +0x4a5</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).initNetworkController(0xc420a281e0, 0xc42087d900, 0xc420c7f620, 0xc420a281e0, 0xc42239c740, 0xc420c7f620, 0xc420c7f5f0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon_unix.go:807 +0xa9</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).restore(0xc420a281e0, 0xc4209b8300, 0xc4208e4790)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:419 +0xd6f</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.NewDaemon(0x26a6240, 0xc4209b8300, 0xc42087d900, 0xc4209e03f0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:987 +0x2c4b</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.(*DaemonCli).start(0xc42024b890, 0xc42018f320, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;daemon.go:180 +0x74f</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.runDaemon(0xc42018f320, 0xc420876200, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker_unix.go:7 +0x47</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.newDaemonCommand.func1(0xc42087ac80, 0x32ef098, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:29 +0x5d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).execute(0xc42087ac80, 0xc42003a1e0, 0x0, 0x0, 0xc42087ac80, 0xc42003a1e0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:762 +0x46a</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).ExecuteC(0xc42087ac80, 0x267bb40, 0x2254580, 0x267bb50)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:852 +0x30c</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).Execute(0xc42087ac80, 0xc42000e020, 0x4d067f)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:800 +0x2d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.main()</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:70 +0xa2</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: docker.service: main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENT</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: Unit docker.service entered failed state.</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: docker.service failed.</span><br></pre></td></tr></table></figure><p>首先得根据这个 panic 的堆栈，调用关系是 <code>main.main</code> -&gt; <code>cobra</code> -&gt; <code>docker daemon</code> -&gt; <code>daemon.(*Daemon).restore</code> -&gt; <code>initNetworkController</code> -&gt; <code>libnetwork/datastore/cache</code> -&gt; <code>boltdb</code><br>docker 使用了 boltdb 存储了网络信息成 <code>db</code> 文件，但是这个 db 文件损坏了，导致读取字节序列化错误类型，去 docker 的目录 find 下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ find &#x2F;data&#x2F;kube&#x2F;docker -type f -size -5M -name &#39;*.db&#39; | grep -v overlay2</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;volumes&#x2F;metadata.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;network&#x2F;files&#x2F;local-kv.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;builder&#x2F;fscache.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;snapshots.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;metadata.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;cache.db</span><br></pre></td></tr></table></figure><p>改名 db 文件重启 docker 解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /data/kube/docker/network/files/local-kv.db&#123;,.bak&#125;</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这个问题和之前的&lt;a href=&quot;https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/&quot;&gt;docker-18.06.3-ce启动panic: invalid page type</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/categories/kubernetes/docker/"/>
    
    <category term="panic" scheme="http://zhangguanzhang.github.io/categories/kubernetes/docker/panic/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>一次单节点单个pod网络问题排查过程</title>
    <link href="http://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/"/>
    <id>http://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/</id>
    <published>2021-04-30T11:28:30.000Z</published>
    <updated>2021-04-30T11:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="about"><a href="#about" class="headerlink" title="about"></a>about</h2><p>现场反馈客户环境上业务不正常，根据调用链去看某个业务A日志，发现无法请求另一个业务B，把业务 A 的探针取消了，加上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tty: true</span><br><span class="line">command: [&quot;bash&quot;]</span><br></pre></td></tr></table></figure><p>起来后进去 curl 了下 B 对应的 svcIP 接口是能通的。然后手动起业务进程，再开个窗口 exec 进去 curl 发现就不通了，k8s node数量是只有一个，并且只有这一个 pod 有问题。后面排查到是用户的安全软件导致的。软件名是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep agent</span><br><span class="line">root       6349  0.3  0.1 21046316 116820 ?     Sl   11:08   0:02 /CloudResetPwdUpdateAgent/depend/jre1.8.0_232/bin/java -Dorg.tanukisoftware.wrapper.WrapperSimpleApp.maxStartMainWait=40 -Djava.library.path=../lib -classpath ../lib/resetpwdupdateagent.jar:../lib/wrapper.jar:../lib/json-20160810.jar:../lib/log4j-api-2.8.2.jar:../lib/log4j-core-2.8.2.jar -Dwrapper.key=osxWGEBk6yYtP6sr -Dwrapper.backend=pipe -Dwrapper.disable_console_input=TRUE -Dwrapper.pid=6019 -Dwrapper.version=3.5.26 -Dwrapper.native_library=wrapper -Dwrapper.arch=x86 -Dwrapper.service=TRUE -Dwrapper.cpu.timeout=10 -Dwrapper.jvmid=1 org.tanukisoftware.wrapper.WrapperSimpleApp CloudResetPwdUpdateAgent</span><br><span class="line">root      13860 76.1  0.3 796288 253072 ?       Sl   11:08   8:27 /usr/local/dbappsecurity/edr/agent_service runservice</span><br><span class="line">root      14188  0.0  0.0  46004  6000 ?        S    11:08   0:00 /usr/local/dbappsecurity/edr/agent_daemon</span><br><span class="line">root      17399  0.0  0.0 112712   976 pts/0    S+   11:19   0:00 grep --color=auto agent</span><br><span class="line">root      22206  0.0  0.0  22496  1448 ?        S    11:08   0:00 vm-agent</span><br><span class="line">root      22215  0.1  0.0 628744  4104 ?        Sl   11:08   0:01 vm-agent</span><br></pre></td></tr></table></figure><p>杀掉 <code>dbappsecurity</code> 两个进程后重建业务 A 的 pod 后就正常了。</p><p>之前也遇到过安全软件导致 pod 网络通信异常 eof 的，列举一些国产遇到过的软件软件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ds_agent  # 查下 agent 关键字</span><br><span class="line">qaxsafed  # 奇安信，查下 qax 看看有没有其他的</span><br><span class="line">secdog    # 也查下 dog 和 sec</span><br><span class="line">sangfor_watchdog # 这个不影响，但是有它基本是深信服的虚拟化环境，会和flannel的8472端口冲突</span><br><span class="line">YDservice</span><br><span class="line">Symantec</span><br><span class="line">start360su_safed   # 推荐 ps aux | grep safe 先查下</span><br><span class="line">gov_defence_service</span><br><span class="line">gov_defence_guard      #  ps aux | grep defence </span><br><span class="line">wsssr_defence_daemon   # 奇安信服务器安全加固系统，和下面是一起的。目前遇到过影响 socat 运行和容器进程访问另一个机器上的mysql端口</span><br><span class="line">wsssr_defence_service</span><br><span class="line">wsssr_defence_agent   # 影响pod网络</span><br><span class="line">ics_agent</span><br><span class="line"></span><br><span class="line">/opt/nubosh/vmsec-host/intedrity/bin/icsintedrity # docker -p 的都无法访问</span><br><span class="line">/opt/nubosh/vmsec-host/file/bin/icsfilesec</span><br><span class="line"></span><br><span class="line">edr_sec_plan   # ps aux | grep edr ,深信服的 edr，这个会下发 iptables 规则，配置错了会影响 node 之间，以及 pod 和 pod 之间通信</span><br></pre></td></tr></table></figure><h2 id="一些卸载笔记"><a href="#一些卸载笔记" class="headerlink" title="一些卸载笔记"></a>一些卸载笔记</h2><p>wsssr_defence_daemon</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/wsssr_defence_agent</span><br><span class="line">./uninstall</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;about&quot;&gt;&lt;a href=&quot;#about&quot; class=&quot;headerlink&quot; title=&quot;about&quot;&gt;&lt;/a&gt;about&lt;/h2&gt;&lt;p&gt;现场反馈客户环境上业务不正常，根据调用链去看某个业务A日志，发现无法请求另一个业务B，把业务 A 的探针取消了，加上</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubelet 和 runc 编译关闭 kmem</title>
    <link href="http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/"/>
    <id>http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/</id>
    <published>2021-04-08T17:28:30.000Z</published>
    <updated>2021-04-08T17:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提详情"><a href="#前提详情" class="headerlink" title="前提详情"></a>前提详情</h2><p>在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修复。</p><p>网上有言论说升级 Linux 内核至 <code>kernel-3.10.0-1075.el7</code> 及以上就可以修复这个问题，详细可见 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1507149#c101">slab leak causing a crash when using kmem control group</a>。但是我测试了下面的都不行：</p><ul><li>CentOS7.4</li><li>CentOS7.6</li><li>CentOS7.7的 3.10.0-1062.el7.x86_64 </li><li>CentOS Linux release 7.8.2003 (Core) - 3.10.0-1127.el7.x86_64</li></ul><p>Linux其余发行版内核如果大于等于 4.4 应该没问题。<br>这里我们编译 kubelet 关闭 kmem。</p><h2 id="准备条件"><a href="#准备条件" class="headerlink" title="准备条件"></a>准备条件</h2><p>这里我们使用的编译参数会使用容器编译的，不需要宿主机上安装 golang，安装个 docker 就行了。</p><ol><li><p><code>1c 4g</code> 的机器，这里我是使用 <code> CentOS 7.8.2003 (Core)</code><br>机器配置 2g 内存的时候编译提示 oom，升级到 4g 内存才编译成功的。</p></li><li><p>最好安装最新版本的docker</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable --now firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -ri &#x27;/^[^#]*SELINUX=/s#=.+$#=disabled#&#x27; /etc/selinux/config</span><br><span class="line"></span><br><span class="line">cat&gt;/etc/security/limits.d/custom.conf&lt;&lt;EOF</span><br><span class="line">*       soft    nproc   131072</span><br><span class="line">*       hard    nproc   131072</span><br><span class="line">*       soft    nofile  131072</span><br><span class="line">*       hard    nofile  131072</span><br><span class="line">root    soft    nproc   131072</span><br><span class="line">root    hard    nproc   131072</span><br><span class="line">root    soft    nofile  131072</span><br><span class="line">root    hard    nofile  131072</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat&lt;&lt;EOF &gt; /etc/sysctl.d/docker.conf</span><br><span class="line"># 要求iptables对bridge的数据进行处理</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-arptables = 1</span><br><span class="line"># 开启转发</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line">curl -fsSL &quot;https://get.docker.com/&quot; | \</span><br><span class="line">  sed -r &#x27;/add-repo \$yum_repo/a sed -i &quot;s#https://download.docker.com#http://mirrors.aliyun.com/docker-ce#&quot; /etc/yum.repos.d/docker-*.repo &#x27; | \</span><br><span class="line">    bash -s -- --mirror Aliyun</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/docker/</span><br><span class="line">cat&gt;/etc/docker/daemon.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;bip&quot;: &quot;172.17.0.1/16&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https://fz5yth0r.mirror.aliyuncs.com&quot;,</span><br><span class="line">    &quot;https://dockerhub.mirrors.nwafu.edu.cn&quot;,</span><br><span class="line">    &quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">    &quot;https://reg-mirror.qiniu.com&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/systemd/system/docker.service.d/</span><br><span class="line">cat&gt;/etc/systemd/system/docker.service.d/10-docker.conf&lt;&lt;EOF</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPost=/sbin/iptables --wait -I FORWARD -s 0.0.0.0/0 -j ACCEPT</span><br><span class="line">ExecStopPost=/bin/bash -c &#x27;/sbin/iptables --wait -D FORWARD -s 0.0.0.0/0 -j ACCEPT &amp;&gt; /dev/null || :&#x27;</span><br><span class="line">ExecStartPost=/sbin/iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost=/bin/bash -c &#x27;/sbin/iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; /dev/null || :&#x27;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y epel-release bash-completion </span><br><span class="line">cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/</span><br><span class="line"></span><br><span class="line">systemctl enable --now docker</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><h3 id="确定版本"><a href="#确定版本" class="headerlink" title="确定版本"></a>确定版本</h3><p>查看下我们目前使用的 kubelet 版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"> kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>安装编译的基础依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y rsync make</span><br></pre></td></tr></table></figure><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><p>这里我们使用容器编译，所以下载到啥地方都行，也不需要安装 go。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">cd kubernetes</span><br><span class="line">git checkout v1.15.5</span><br></pre></td></tr></table></figure><h3 id="前提操作"><a href="#前提操作" class="headerlink" title="前提操作"></a>前提操作</h3><p>查看 cross 镜像的版本号</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat build/build-image/cross/VERSION</span><br><span class="line">v1.12.10-1</span><br></pre></td></tr></table></figure><p>拉国内的镜像，然后改名，高版本，好像 1.16 以上最终镜像名是 <code>k8s.gcr.io/build-image/kube-cross</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull registry.aliyuncs.com/k8sxio/kube-cross:v1.12.10-1</span><br><span class="line">$ docker tag registry.aliyuncs.com/k8sxio/kube-cross:v1.12.10-1 k8s.gcr.io/kube-cross:v1.12.10-1</span><br><span class="line"></span><br><span class="line"># 高版本</span><br><span class="line">docker tag  registry.aliyuncs.com/k8sxio/kube-cross:xxx k8s.gcr.io/build-image/kube-cross:xxxx</span><br></pre></td></tr></table></figure><p>编译，这个参数测试在 v1.15.5 里可用，网上的 <code>make BUILDTAGS=&quot;nokmem&quot; WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=&quot;-N -l&quot;</code> 会无法编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在v1.15.5似乎无用</span><br><span class="line">./build/run.sh make kubelet KUBE_BUILD_PLATFORMS=linux/amd64 BUILDTAGS=&quot;nokmem&quot;</span><br><span class="line"># 用下面的</span><br><span class="line">./build/run.sh make kubelet GOFLAGS=&quot;-v -tags=nokmem&quot; KUBE_BUILD_PLATFORMS=linux/amd64</span><br></pre></td></tr></table></figure><p>查看编译完成的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7 kubernetes]# ls -l _output/dockerized/bin/linux/amd64/</span><br><span class="line">total 202880</span><br><span class="line">-rwxr-xr-x 1 root root   9203530 Apr  7 22:02 conversion-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9207908 Apr  7 22:02 deepcopy-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9156147 Apr  7 22:02 defaulter-gen</span><br><span class="line">-rwxr-xr-x 1 root root   4709220 Apr  7 22:02 go2make</span><br><span class="line">-rwxr-xr-x 1 root root   2894872 Apr  7 22:03 go-bindata</span><br><span class="line">-rwxr-xr-x 1 root root 157545104 Apr  7 22:13 kubelet</span><br><span class="line">-rwxr-xr-x 1 root root  15018430 Apr  7 22:03 openapi-gen</span><br></pre></td></tr></table></figure><h2 id="runc-关闭-kmem"><a href="#runc-关闭-kmem" class="headerlink" title="runc 关闭 kmem"></a>runc 关闭 kmem</h2><p>v1.0.0-rc94起 kmem 设置就被忽略了，意味着直接下载最新版本的 runc 就行了，不需要自己去编译 runc 了。</p><p> <code>arm64</code> 的 runc 可以直接下面这样编译，不过 arm64 目前国内的系统的内核都很高不存在这个问题，没必要编译 runc 。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make shell</span><br><span class="line">make localcross</span><br></pre></td></tr></table></figure><h3 id="1-0-0-rc10-版本"><a href="#1-0-0-rc10-版本" class="headerlink" title="1.0.0-rc10 版本"></a>1.0.0-rc10 版本</h3><p><a href="https://cloud.tencent.com/developer/article/1743789">https://cloud.tencent.com/developer/article/1743789</a> 这个文章里说了 runc 也需要关闭</p><p>如果下面命令能成功执行则说明 runc 没关闭 kmem ，当然此方法不是绝对的，新 runc 下命令执行没有问题的，要看文章最后面的 <code>memory.kmem.slabinfo</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --name test --kernel-memory 100M nginx:alpine</span><br></pre></td></tr></table></figure><p><code>19.03.14</code> 测试发现可以运行，说明没有关闭，查看它的 <code>runc</code> 版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><p>根据 commit 跳转</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/opencontainers/runc/commit/dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br></pre></td></tr></table></figure><p>根据这个 commit，找到了是 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10">https://github.com/opencontainers/runc/tree/v1.0.0-rc10</a> 这个 tag，</p><p>编译支持的 tag 见 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags">https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags</a><br>编译参数从 <a href="https://github.com/opencontainers/runc/blob/v1.0.0-rc10/script/release.sh#L30">release 脚本</a> 找到是下面的参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make BUILDTAGS=&quot;seccomp selinux apparmor&quot; static</span><br><span class="line"><span class="meta">#</span><span class="bash"> 后面的新版本貌似默认的 tags 是 seccomp 了</span></span><br></pre></td></tr></table></figure><p>下载源码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/opencontainers/runc.git</span><br><span class="line">cd runc</span><br><span class="line">git checkout v1.0.0-rc10</span><br></pre></td></tr></table></figure><p>直接上面的编译参数是无法编译成功的，因为很多依赖都是 <code>ubuntu</code>下面的。看了下 <code>Makefile</code> 里面提供了一个起 ubuntu 的容器，我们可以进去编译。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make shell</span><br></pre></td></tr></table></figure><p>直接 <code>make shell</code> 的话，它第一步是 <code>make runcimage</code> 会先构建镜像，然后用这个镜像起一个容器，构建的最后一步会失败，因为下面的 <code>busybox</code> 的 <code>rootfs</code> 下载地址变动了，我们得 <code>hack</code> 下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">get_busybox()&#123;</span><br><span class="line">case $(go env GOARCH) in</span><br><span class="line">arm64)</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-arm64v8/glibc/busybox.tar.xz&#x27;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-amd64/glibc/busybox.tar.xz&#x27;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>hack 之前我们先测试下，获取下输出的镜像名是<code>runc_dev:HEAD</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make shell</span></span><br><span class="line">docker build  -t runc_dev:HEAD .</span><br><span class="line">^Cmake: *** [runcimage] Interruptaemon  557.1kB</span><br></pre></td></tr></table></figure><p>因为过程会取 git 的一些信息，为了不影响，我们先拷贝文件 <code>tests/integration/multi-arch.bash</code> 和 <code>Dockerfile</code>。我们先手动构建出镜像，再删除掉这俩文件保持 git status。</p><p>先修改 <code>Dockerfile</code> 让它使用 <code>tests/integration/multi-arch.bash.new</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp Dockerfile Dockerfile.new</span><br><span class="line">vi Dockerfile.new </span><br><span class="line">tail -n2 Dockerfile.new</span><br><span class="line">RUN . tests/integration/multi-arch.bash.new \</span><br><span class="line">    &amp;&amp; curl -o- -sSL `get_busybox` | tar xfJC - $&#123;ROOTFS&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>新文件下载地址可以在 <a href="https://github.com/opencontainers/runc/blob/bb28c44f12bf24ea64590edfb4f23a4b4d2eaae8/tests/integration/get-images.sh#L59">master 分支最新的脚本里</a> 找到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get &quot;$BUSYBOX_IMAGE&quot; \</span><br><span class="line">&quot;https://github.com/docker-library/busybox/raw/dist-$&#123;arch&#125;/stable/glibc/busybox.tar.xz&quot;</span><br></pre></td></tr></table></figure><p>按照下面修改好</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp  tests/integration/multi-arch.bash tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> grep busybox tests/integration/multi-arch.bash.new</span></span><br><span class="line">get_busybox()&#123;</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-arm64v8/glibc/busybox.tar.xz&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">&#x27;https://github.com/docker-library/busybox/raw/dist-amd64/glibc/busybox.tar.xz&#x27;</span></span></span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-amd64/stable/glibc/busybox.tar.xz&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>手动编译镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build  -t runc_dev:HEAD -f Dockerfile.new .</span><br></pre></td></tr></table></figure><p>移走文件，保持 <code>git status</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv Dockerfile.new tests/integration/multi-arch.bash.new /tmp</span><br></pre></td></tr></table></figure><p>进入容器里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -ti --privileged --rm -v $PWD:/go/src/github.com/opencontainers/runc runc_dev:HEAD bash</span><br></pre></td></tr></table></figure><p>编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make BUILDTAGS=<span class="string">&quot;seccomp selinux apparmor nokmem&quot;</span> static</span></span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o runc .</span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o contrib/cmd/recvtty/recvtty ./contrib/cmd/recvtty</span><br></pre></td></tr></table></figure><p>查看信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod u+x runc</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ldd runc</span></span><br><span class="line">not a dynamic executable</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><h2 id="查看-kmem-开启"><a href="#查看-kmem-开启" class="headerlink" title="查看 kmem 开启"></a>查看 kmem 开启</h2><p>环境信息:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux 82.174-zh 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">$ cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core)</span><br></pre></td></tr></table></figure><p>判断 cgroup kernel memory 是否激活的方式。查看对应 POD container 下的 <code>memory.kmem.slabinfo</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /sys/fs/cgroup/memory/kubepods</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有内容，说明kubelet开了 kmem</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat memory.kmem.slabinfo</span></span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br></pre></td></tr></table></figure><p>pod 目录下面的容器目录或者<code>/sys/fs/cgroup/memory/docker/&lt;uuid&gt;</code>如果有 <code>memory.kmem.slabinfo</code> 则说明 <code>runc</code> 没关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -l | grep pod</span></span><br><span class="line">drwxr-xr-x  5 root root 0 4月   8 11:27 pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28</span><br><span class="line">drwxr-xr-x  4 root root 0 4月   8 11:26 pod64def35b-b44e-410d-9782-745bd47834ca</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/ | grep -E <span class="string">&#x27;^d&#x27;</span></span></span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:28 1b37f9f78f93546e3e4407f03aa84c92e95c99655467e62814ae17e0a0e68686</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 a578004f702d7b20d4b08d49c08cbb6c3ef2b3d08a62f087f5c7be0d022d9d9d</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326/memory.kmem.slabinfo</span> </span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br><span class="line">taskstats              0      0    328   24    2 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">shmem_inode_cache    216    216    680   24    4 : tunables    0    0    0 : slabdata      9      9      0</span><br><span class="line">inode_cache          162    162    592   27    4 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">Acpi-ParseExt        112    112     72   56    1 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">selinux_inode_security      0      0     40  102    1 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">RAWv6                  0      0   1216   26    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">UDP                    0      0   1088   30    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-8192           0      0   8192    4    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">net_namespace          0      0   5184    6    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">pid_namespace          0      0   2200   14    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">mqueue_inode_cache      0      0    896   36    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-2048         112    112   2048   16    8 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-32           768    768     32  128    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">kmalloc-512          128    128    512   32    4 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">kmalloc-128          256    256    128   32    1 : tunables    0    0    0 : slabdata      8      8      0</span><br><span class="line">kmalloc-8           6144   6144      8  512    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">anon_vma             306    306     80   51    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">idr_layer_cache      165    165   2112   15    8 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">vm_area_struct       259    259    216   37    2 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">mnt_cache            231    231    384   21    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">mm_struct             20     20   1600   20    8 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">signal_cache           0      0   1152   28    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">sighand_cache          0      0   2112   15    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">files_cache           50     50    640   25    4 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">kernfs_node_cache    108    108    112   36    1 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-192          273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">task_xstate          156    156    832   39    8 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">task_struct           24     24   4048    8    8 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-1024         160    160   1024   32    8 : tunables    0    0    0 : slabdata      5      5      0</span><br><span class="line">kmalloc-64           768    768     64   64    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">sock_inode_cache      75     75    640   25    4 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">proc_inode_cache     264    264    656   24    4 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">dentry               273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">kmalloc-16          2816   2816     16  256    1 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">kmalloc-96           294    294     96   42    1 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-256          352    352    256   32    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">shared_policy_node     85     85     48   85    1 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">kmalloc-4096         104    104   4096    8    8 : tunables    0    0    0 : slabdata     13     13      0</span><br></pre></td></tr></table></figure><p><code>memory.kmem.slabinfo</code>里有内容说明是开启的</p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>只有在 pod 配置了 memory limit 的时候才打开 memory accounting，即 kmem。我们下面利用 flannel pod测试下，先手动创建 cgroup</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ grep memory /proc/cgroups </span><br><span class="line">memory8871</span><br><span class="line"></span><br><span class="line">$ mkdir /sys/fs/cgroup/memory/test</span><br><span class="line">$ for i in `seq 1 65535`;do mkdir /sys/fs/cgroup/memory/test/test-$&#123;i&#125;; done</span><br><span class="line">$ grep memory /proc/cgroups </span><br><span class="line">memory8655131</span><br></pre></td></tr></table></figure><p>释放出三个，删除当前节点的 flannel，可以创建出来，然后再删除新的，无法创建出来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rmdir /sys/fs/cgroup/memory/test/test-&#123;1..3&#125;</span><br><span class="line">$ kubectl -n kube-system delete pod kube-flannel-ds-z2cgq</span><br><span class="line">....</span><br><span class="line">  Warning  FailedCreatePodContainer  2s (x4 over 35s)  kubelet, 10.14.82.174  unable to ensure pod container exists: failed to create container for [kubepods burstable pod5a41f53f-5ce8-4123-8199-1a865219f297] : mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod5a41f53f-5ce8-4123-8199-1a865219f297: no space left on device</span><br></pre></td></tr></table></figure><p>替换编译好的后，先关闭 <code>kubelet</code> 和 <code>docker</code>， 关闭自启动 <code>systemctl disable docker kubelet</code>。reboot 后，查看目录 <code>/sys/fs/cgroup/memory/</code> 下的 <code>kubepods</code> 是不是不存在。然后启动 docker 和 kubelet。等带内存 limit 的 flannel 调度上来后下面命令查看。输出是 <code>Input/output error</code> 说明已经关闭了。下面 docker 目录不存在的话就 docker run 创建下 mem limit 的容器</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find /sys/fs/cgroup/memory/docker/ -name memory.kmem.slabinfo -exec cat &#123;&#125;  \;</span><br><span class="line">find /sys/fs/cgroup/memory/kubepods/ -name memory.kmem.slabinfo -exec cat &#123;&#125;  \;</span><br></pre></td></tr></table></figure><p>还有种方法是看 slab 的个数，删除 limit 的 pod后等重建看看数量增长否：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /sys/kernel/slab  | wc -l</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://eddycjy.com/posts/why-container-memory-exceed2/">https://eddycjy.com/posts/why-container-memory-exceed2/</a></li><li><a href="https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub:+unable+to+allocate+memory+on+node+-1">https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub%3A+unable+to+allocate+memory+on+node+-1</a></li><li><a href="https://github.com/kubernetes/kubernetes/blob/v1.20.6/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem_disabled.go">https://github.com/kubernetes/kubernetes/blob/v1.20.6/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem_disabled.go</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前提详情&quot;&gt;&lt;a href=&quot;#前提详情&quot; class=&quot;headerlink&quot; title=&quot;前提详情&quot;&gt;&lt;/a&gt;前提详情&lt;/h2&gt;&lt;p&gt;在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="kmem" scheme="http://zhangguanzhang.github.io/categories/k8s/kmem/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
</feed>
