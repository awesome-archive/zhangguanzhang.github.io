<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhangguanzhang</title>
  
  <subtitle>站在巨人的肩膀上</subtitle>
  <link href="http://zhangguanzhang.github.io/atom.xml" rel="self"/>
  
  <link href="http://zhangguanzhang.github.io/"/>
  <updated>2021-07-06T14:46:06.000Z</updated>
  <id>http://zhangguanzhang.github.io/</id>
  
  <author>
    <name>Zhangguanzhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器重启后 kube-apiserver 无法启动，etcd刷(error &quot;EOF&quot;, ServerName &quot;&quot;)</title>
    <link href="http://zhangguanzhang.github.io/2021/07/06/kube-apiserver-cannot-start-after-reboot/"/>
    <id>http://zhangguanzhang.github.io/2021/07/06/kube-apiserver-cannot-start-after-reboot/</id>
    <published>2021-07-06T14:46:06.000Z</published>
    <updated>2021-07-06T14:46:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>三个 master （etcd 也在 master 上，master上也有 kubelet）和 n 个 node。master 上组件(kube-controller-manager,kube-scheduler,kubelet)的 apiserver 的ip 都是 127.0.0.1:6443。kube-apiserver的 etcd 地址写了三个 etcd 的。k8s 版本为 <code>v1.15.5</code></p><h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>93 这台 master 机器重启后，发现 93 节点 <code>NotReady</code>，上去看了下 kubelet 无法连上本机的 kube-apiserver。kube-apiserver 运行个十几秒后才退出，<code>etcd</code> 一直刷如下日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">etcd: rejected connection from &quot;10.129.173.93:47566&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47614&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47714&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47874&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47948&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br></pre></td></tr></table></figure><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><h3 id="etcd的错误"><a href="#etcd的错误" class="headerlink" title="etcd的错误"></a>etcd的错误</h3><p>这个 EOF 是 etcd 的客户端没正常关闭造成的，etcd 之间也会互相连，先查看下 etcd 状态，因为 k8s 版本是 <code>v1.15.5</code>。默认使用的 etcd v3 api。etcd 的<code>--listen-client-urls</code>里我们包含了一个<code>http://127.0.0.1:2379</code>。所以下面命令不需要带<code>--endpoints=xxxx</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> ETCDCTL_API=3</span><br><span class="line">$ etcdctl member list</span><br><span class="line">172cf33e1b47c1c8, started, etcd2, https://10.129.173.93:2380, https://10.129.173.93:2379</span><br><span class="line">35f3e39e5dd3195e, started, etcd3, https://10.129.173.94:2380, https://10.129.173.94:2379</span><br><span class="line">47a96a577fe753d1, started, etcd1, https://10.129.173.92:2380, https://10.129.173.92:2379</span><br></pre></td></tr></table></figure><p>看下 dbSize，推荐使用 <code>--write-out=table</code> 看，会自动换算人性化的 size。如果需要压缩的 revision 推荐使用<code>-w=json</code> 来获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl endpoint status -w&#x3D;table</span><br></pre></td></tr></table></figure><p>看了下没达到默认的 2G，如果满了需要压缩执行下面的命令。每台的 revision 不同，每台都要执行下面的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 压缩旧版本</span><br><span class="line">etcdctl compact $revision</span><br><span class="line"># 清理碎片</span><br><span class="line">etcdctl defrag</span><br><span class="line"># 忽略告警</span><br><span class="line">etcdctl alarm disarm</span><br></pre></td></tr></table></figure><p>确认 etcd 没有问题，然后其他两个机器的 kube-apiserver 都正常，说明触发 EOF 的是 93 这台上面的 kube-apiserver。停止它后手动重启下：</p><h3 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop kube-apiserver</span><br><span class="line">$ systemctl cat kube-apiserver</span><br></pre></td></tr></table></figure><p>然后终端上用<code>ExecStart</code>的部分，把<code>--v=2</code>改成<code>--v=5</code>启动，下面是输出。全部放出来，方便其他遇到的人搜到这篇文章。太长了，我就先放到最后吧。</p><p>启动后 34 秒后刷了一堆<code>Get https://localhost:6443/apis/rbac.authorization.k8s.io/v1/clusterroles?limit=500&amp;resourceVersion=0: dial tcp 127.0.0.1:6443: i/o timeout</code> 就退出了。参照之前的<a href="https://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/">文章</a>查了下进程，确认没有安全软件安全狗。</p><p>除去 panic 和明确的 error 和 flag 相关的报错，k8s 相关的二进制无法启动基本是和 ipv6 有关系。查看下 ipv6 情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -a |&amp; grep -E <span class="string">&#x27;(all|default)\.disable_ipv6&#x27;</span></span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br></pre></td></tr></table></figure><p>无论是显示的关还是开，反转下这俩参数，然后再试试发现启动后不会退出了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.disable_ipv6=0</span><br><span class="line">sysctl -w net.ipv6.conf.default.disable_ipv6=0</span><br></pre></td></tr></table></figure><p>然后参数固化下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;&gt; &#x2F;etc&#x2F;sysctl.conf &lt;&lt;EOF</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 &#x3D; 0</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 &#x3D; 0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br><span class="line">1306</span><br><span class="line">1307</span><br><span class="line">1308</span><br><span class="line">1309</span><br><span class="line">1310</span><br><span class="line">1311</span><br><span class="line">1312</span><br><span class="line">1313</span><br><span class="line">1314</span><br><span class="line">1315</span><br><span class="line">1316</span><br><span class="line">1317</span><br><span class="line">1318</span><br><span class="line">1319</span><br><span class="line">1320</span><br><span class="line">1321</span><br><span class="line">1322</span><br><span class="line">1323</span><br><span class="line">1324</span><br><span class="line">1325</span><br><span class="line">1326</span><br><span class="line">1327</span><br><span class="line">1328</span><br><span class="line">1329</span><br><span class="line">1330</span><br><span class="line">1331</span><br><span class="line">1332</span><br><span class="line">1333</span><br><span class="line">1334</span><br><span class="line">1335</span><br><span class="line">1336</span><br><span class="line">1337</span><br><span class="line">1338</span><br><span class="line">1339</span><br><span class="line">1340</span><br><span class="line">1341</span><br><span class="line">1342</span><br><span class="line">1343</span><br><span class="line">1344</span><br><span class="line">1345</span><br><span class="line">1346</span><br><span class="line">1347</span><br><span class="line">1348</span><br><span class="line">1349</span><br><span class="line">1350</span><br><span class="line">1351</span><br><span class="line">1352</span><br><span class="line">1353</span><br><span class="line">1354</span><br><span class="line">1355</span><br><span class="line">1356</span><br><span class="line">1357</span><br><span class="line">1358</span><br><span class="line">1359</span><br><span class="line">1360</span><br><span class="line">1361</span><br><span class="line">1362</span><br><span class="line">1363</span><br><span class="line">1364</span><br><span class="line">1365</span><br><span class="line">1366</span><br><span class="line">1367</span><br><span class="line">1368</span><br><span class="line">1369</span><br><span class="line">1370</span><br><span class="line">1371</span><br><span class="line">1372</span><br><span class="line">1373</span><br><span class="line">1374</span><br><span class="line">1375</span><br><span class="line">1376</span><br><span class="line">1377</span><br><span class="line">1378</span><br><span class="line">1379</span><br><span class="line">1380</span><br><span class="line">1381</span><br><span class="line">1382</span><br><span class="line">1383</span><br><span class="line">1384</span><br><span class="line">1385</span><br><span class="line">1386</span><br><span class="line">1387</span><br><span class="line">1388</span><br><span class="line">1389</span><br><span class="line">1390</span><br><span class="line">1391</span><br><span class="line">1392</span><br><span class="line">1393</span><br><span class="line">1394</span><br><span class="line">1395</span><br><span class="line">1396</span><br><span class="line">1397</span><br><span class="line">1398</span><br><span class="line">1399</span><br><span class="line">1400</span><br><span class="line">1401</span><br><span class="line">1402</span><br><span class="line">1403</span><br><span class="line">1404</span><br><span class="line">1405</span><br><span class="line">1406</span><br><span class="line">1407</span><br><span class="line">1408</span><br><span class="line">1409</span><br><span class="line">1410</span><br><span class="line">1411</span><br><span class="line">1412</span><br><span class="line">1413</span><br><span class="line">1414</span><br><span class="line">1415</span><br><span class="line">1416</span><br><span class="line">1417</span><br><span class="line">1418</span><br><span class="line">1419</span><br><span class="line">1420</span><br><span class="line">1421</span><br><span class="line">1422</span><br><span class="line">1423</span><br><span class="line">1424</span><br><span class="line">1425</span><br><span class="line">1426</span><br><span class="line">1427</span><br><span class="line">1428</span><br><span class="line">1429</span><br><span class="line">1430</span><br><span class="line">1431</span><br><span class="line">1432</span><br><span class="line">1433</span><br><span class="line">1434</span><br><span class="line">1435</span><br><span class="line">1436</span><br><span class="line">1437</span><br><span class="line">1438</span><br><span class="line">1439</span><br><span class="line">1440</span><br><span class="line">1441</span><br><span class="line">1442</span><br><span class="line">1443</span><br><span class="line">1444</span><br><span class="line">1445</span><br><span class="line">1446</span><br><span class="line">1447</span><br><span class="line">1448</span><br><span class="line">1449</span><br><span class="line">1450</span><br><span class="line">1451</span><br><span class="line">1452</span><br><span class="line">1453</span><br><span class="line">1454</span><br><span class="line">1455</span><br><span class="line">1456</span><br><span class="line">1457</span><br><span class="line">1458</span><br><span class="line">1459</span><br><span class="line">1460</span><br><span class="line">1461</span><br><span class="line">1462</span><br><span class="line">1463</span><br><span class="line">1464</span><br><span class="line">1465</span><br><span class="line">1466</span><br><span class="line">1467</span><br><span class="line">1468</span><br><span class="line">1469</span><br><span class="line">1470</span><br><span class="line">1471</span><br><span class="line">1472</span><br><span class="line">1473</span><br><span class="line">1474</span><br><span class="line">1475</span><br><span class="line">1476</span><br><span class="line">1477</span><br><span class="line">1478</span><br><span class="line">1479</span><br><span class="line">1480</span><br><span class="line">1481</span><br><span class="line">1482</span><br><span class="line">1483</span><br><span class="line">1484</span><br><span class="line">1485</span><br><span class="line">1486</span><br><span class="line">1487</span><br><span class="line">1488</span><br><span class="line">1489</span><br><span class="line">1490</span><br><span class="line">1491</span><br><span class="line">1492</span><br><span class="line">1493</span><br><span class="line">1494</span><br><span class="line">1495</span><br><span class="line">1496</span><br><span class="line">1497</span><br><span class="line">1498</span><br><span class="line">1499</span><br><span class="line">1500</span><br><span class="line">1501</span><br><span class="line">1502</span><br><span class="line">1503</span><br><span class="line">1504</span><br><span class="line">1505</span><br><span class="line">1506</span><br><span class="line">1507</span><br><span class="line">1508</span><br><span class="line">1509</span><br><span class="line">1510</span><br><span class="line">1511</span><br><span class="line">1512</span><br><span class="line">1513</span><br><span class="line">1514</span><br><span class="line">1515</span><br><span class="line">1516</span><br><span class="line">1517</span><br><span class="line">1518</span><br><span class="line">1519</span><br><span class="line">1520</span><br><span class="line">1521</span><br><span class="line">1522</span><br><span class="line">1523</span><br><span class="line">1524</span><br><span class="line">1525</span><br><span class="line">1526</span><br><span class="line">1527</span><br><span class="line">1528</span><br><span class="line">1529</span><br><span class="line">1530</span><br><span class="line">1531</span><br><span class="line">1532</span><br><span class="line">1533</span><br><span class="line">1534</span><br><span class="line">1535</span><br><span class="line">1536</span><br><span class="line">1537</span><br><span class="line">1538</span><br><span class="line">1539</span><br><span class="line">1540</span><br><span class="line">1541</span><br><span class="line">1542</span><br><span class="line">1543</span><br><span class="line">1544</span><br><span class="line">1545</span><br><span class="line">1546</span><br><span class="line">1547</span><br><span class="line">1548</span><br><span class="line">1549</span><br><span class="line">1550</span><br><span class="line">1551</span><br><span class="line">1552</span><br><span class="line">1553</span><br><span class="line">1554</span><br><span class="line">1555</span><br><span class="line">1556</span><br><span class="line">1557</span><br><span class="line">1558</span><br><span class="line">1559</span><br><span class="line">1560</span><br><span class="line">1561</span><br><span class="line">1562</span><br><span class="line">1563</span><br><span class="line">1564</span><br><span class="line">1565</span><br><span class="line">1566</span><br><span class="line">1567</span><br><span class="line">1568</span><br><span class="line">1569</span><br><span class="line">1570</span><br><span class="line">1571</span><br><span class="line">1572</span><br><span class="line">1573</span><br><span class="line">1574</span><br><span class="line">1575</span><br><span class="line">1576</span><br><span class="line">1577</span><br><span class="line">1578</span><br><span class="line">1579</span><br><span class="line">1580</span><br><span class="line">1581</span><br><span class="line">1582</span><br><span class="line">1583</span><br><span class="line">1584</span><br><span class="line">1585</span><br><span class="line">1586</span><br><span class="line">1587</span><br><span class="line">1588</span><br><span class="line">1589</span><br><span class="line">1590</span><br><span class="line">1591</span><br><span class="line">1592</span><br><span class="line">1593</span><br><span class="line">1594</span><br><span class="line">1595</span><br><span class="line">1596</span><br><span class="line">1597</span><br><span class="line">1598</span><br><span class="line">1599</span><br><span class="line">1600</span><br><span class="line">1601</span><br><span class="line">1602</span><br><span class="line">1603</span><br><span class="line">1604</span><br><span class="line">1605</span><br><span class="line">1606</span><br><span class="line">1607</span><br><span class="line">1608</span><br><span class="line">1609</span><br><span class="line">1610</span><br><span class="line">1611</span><br><span class="line">1612</span><br><span class="line">1613</span><br><span class="line">1614</span><br><span class="line">1615</span><br><span class="line">1616</span><br><span class="line">1617</span><br><span class="line">1618</span><br><span class="line">1619</span><br><span class="line">1620</span><br><span class="line">1621</span><br><span class="line">1622</span><br><span class="line">1623</span><br><span class="line">1624</span><br><span class="line">1625</span><br><span class="line">1626</span><br><span class="line">1627</span><br><span class="line">1628</span><br><span class="line">1629</span><br><span class="line">1630</span><br><span class="line">1631</span><br><span class="line">1632</span><br><span class="line">1633</span><br><span class="line">1634</span><br><span class="line">1635</span><br><span class="line">1636</span><br><span class="line">1637</span><br><span class="line">1638</span><br><span class="line">1639</span><br><span class="line">1640</span><br><span class="line">1641</span><br><span class="line">1642</span><br><span class="line">1643</span><br><span class="line">1644</span><br><span class="line">1645</span><br><span class="line">1646</span><br><span class="line">1647</span><br><span class="line">1648</span><br><span class="line">1649</span><br><span class="line">1650</span><br><span class="line">1651</span><br><span class="line">1652</span><br><span class="line">1653</span><br><span class="line">1654</span><br><span class="line">1655</span><br><span class="line">1656</span><br><span class="line">1657</span><br><span class="line">1658</span><br><span class="line">1659</span><br><span class="line">1660</span><br><span class="line">1661</span><br><span class="line">1662</span><br><span class="line">1663</span><br><span class="line">1664</span><br><span class="line">1665</span><br><span class="line">1666</span><br><span class="line">1667</span><br><span class="line">1668</span><br><span class="line">1669</span><br><span class="line">1670</span><br><span class="line">1671</span><br><span class="line">1672</span><br><span class="line">1673</span><br><span class="line">1674</span><br><span class="line">1675</span><br><span class="line">1676</span><br><span class="line">1677</span><br><span class="line">1678</span><br><span class="line">1679</span><br><span class="line">1680</span><br><span class="line">1681</span><br><span class="line">1682</span><br><span class="line">1683</span><br><span class="line">1684</span><br><span class="line">1685</span><br><span class="line">1686</span><br><span class="line">1687</span><br><span class="line">1688</span><br><span class="line">1689</span><br><span class="line">1690</span><br><span class="line">1691</span><br><span class="line">1692</span><br><span class="line">1693</span><br><span class="line">1694</span><br><span class="line">1695</span><br><span class="line">1696</span><br><span class="line">1697</span><br><span class="line">1698</span><br><span class="line">1699</span><br><span class="line">1700</span><br><span class="line">1701</span><br><span class="line">1702</span><br><span class="line">1703</span><br><span class="line">1704</span><br><span class="line">1705</span><br><span class="line">1706</span><br><span class="line">1707</span><br><span class="line">1708</span><br><span class="line">1709</span><br><span class="line">1710</span><br><span class="line">1711</span><br><span class="line">1712</span><br><span class="line">1713</span><br><span class="line">1714</span><br><span class="line">1715</span><br><span class="line">1716</span><br><span class="line">1717</span><br><span class="line">1718</span><br><span class="line">1719</span><br><span class="line">1720</span><br><span class="line">1721</span><br><span class="line">1722</span><br><span class="line">1723</span><br><span class="line">1724</span><br><span class="line">1725</span><br><span class="line">1726</span><br><span class="line">1727</span><br><span class="line">1728</span><br><span class="line">1729</span><br><span class="line">1730</span><br><span class="line">1731</span><br><span class="line">1732</span><br><span class="line">1733</span><br><span class="line">1734</span><br><span class="line">1735</span><br><span class="line">1736</span><br><span class="line">1737</span><br><span class="line">1738</span><br><span class="line">1739</span><br><span class="line">1740</span><br><span class="line">1741</span><br><span class="line">1742</span><br><span class="line">1743</span><br><span class="line">1744</span><br><span class="line">1745</span><br><span class="line">1746</span><br><span class="line">1747</span><br><span class="line">1748</span><br><span class="line">1749</span><br><span class="line">1750</span><br><span class="line">1751</span><br><span class="line">1752</span><br><span class="line">1753</span><br><span class="line">1754</span><br><span class="line">1755</span><br><span class="line">1756</span><br><span class="line">1757</span><br><span class="line">1758</span><br><span class="line">1759</span><br><span class="line">1760</span><br><span class="line">1761</span><br><span class="line">1762</span><br><span class="line">1763</span><br><span class="line">1764</span><br><span class="line">1765</span><br><span class="line">1766</span><br><span class="line">1767</span><br><span class="line">1768</span><br><span class="line">1769</span><br><span class="line">1770</span><br><span class="line">1771</span><br></pre></td><td class="code"><pre><span class="line">Flag --admission-control has been deprecated, Use --enable-admission-plugins or --disable-admission-plugins instead. Will be removed in a future version.</span><br><span class="line">Flag --insecure-port has been deprecated, This flag will be removed in a future version.</span><br><span class="line">Flag --insecure-bind-address has been deprecated, This flag will be removed in a future version.</span><br><span class="line">Flag --enable-swagger-ui has been deprecated, swagger 1.2 support has been removed</span><br><span class="line">I0705 22:38:00.836860   18145 flags.go:33] FLAG: --address&#x3D;&quot;127.0.0.1&quot;</span><br><span class="line">I0705 22:38:00.836914   18145 flags.go:33] FLAG: --admission-control&#x3D;&quot;[NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction]&quot;</span><br><span class="line">I0705 22:38:00.836926   18145 flags.go:33] FLAG: --admission-control-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.836932   18145 flags.go:33] FLAG: --advertise-address&#x3D;&quot;&lt;nil&gt;&quot;</span><br><span class="line">I0705 22:38:00.836936   18145 flags.go:33] FLAG: --allow-privileged&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.836942   18145 flags.go:33] FLAG: --alsologtostderr&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836948   18145 flags.go:33] FLAG: --anonymous-auth&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836952   18145 flags.go:33] FLAG: --api-audiences&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.836959   18145 flags.go:33] FLAG: --apiserver-count&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.836965   18145 flags.go:33] FLAG: --audit-dynamic-configuration&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836969   18145 flags.go:33] FLAG: --audit-log-batch-buffer-size&#x3D;&quot;10000&quot;</span><br><span class="line">I0705 22:38:00.836973   18145 flags.go:33] FLAG: --audit-log-batch-max-size&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.836977   18145 flags.go:33] FLAG: --audit-log-batch-max-wait&#x3D;&quot;0s&quot;</span><br><span class="line">I0705 22:38:00.836982   18145 flags.go:33] FLAG: --audit-log-batch-throttle-burst&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.836986   18145 flags.go:33] FLAG: --audit-log-batch-throttle-enable&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836990   18145 flags.go:33] FLAG: --audit-log-batch-throttle-qps&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.836995   18145 flags.go:33] FLAG: --audit-log-format&#x3D;&quot;json&quot;</span><br><span class="line">I0705 22:38:00.837000   18145 flags.go:33] FLAG: --audit-log-maxage&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837004   18145 flags.go:33] FLAG: --audit-log-maxbackup&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837008   18145 flags.go:33] FLAG: --audit-log-maxsize&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837011   18145 flags.go:33] FLAG: --audit-log-mode&#x3D;&quot;blocking&quot;</span><br><span class="line">I0705 22:38:00.837015   18145 flags.go:33] FLAG: --audit-log-path&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837019   18145 flags.go:33] FLAG: --audit-log-truncate-enabled&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837023   18145 flags.go:33] FLAG: --audit-log-truncate-max-batch-size&#x3D;&quot;10485760&quot;</span><br><span class="line">I0705 22:38:00.837029   18145 flags.go:33] FLAG: --audit-log-truncate-max-event-size&#x3D;&quot;102400&quot;</span><br><span class="line">I0705 22:38:00.837034   18145 flags.go:33] FLAG: --audit-log-version&#x3D;&quot;audit.k8s.io&#x2F;v1&quot;</span><br><span class="line">I0705 22:38:00.837038   18145 flags.go:33] FLAG: --audit-policy-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837042   18145 flags.go:33] FLAG: --audit-webhook-batch-buffer-size&#x3D;&quot;10000&quot;</span><br><span class="line">I0705 22:38:00.837046   18145 flags.go:33] FLAG: --audit-webhook-batch-initial-backoff&#x3D;&quot;10s&quot;</span><br><span class="line">I0705 22:38:00.837051   18145 flags.go:33] FLAG: --audit-webhook-batch-max-size&#x3D;&quot;400&quot;</span><br><span class="line">I0705 22:38:00.837055   18145 flags.go:33] FLAG: --audit-webhook-batch-max-wait&#x3D;&quot;30s&quot;</span><br><span class="line">I0705 22:38:00.837060   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-burst&#x3D;&quot;15&quot;</span><br><span class="line">I0705 22:38:00.837064   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-enable&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837068   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-qps&#x3D;&quot;10&quot;</span><br><span class="line">I0705 22:38:00.837073   18145 flags.go:33] FLAG: --audit-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837076   18145 flags.go:33] FLAG: --audit-webhook-initial-backoff&#x3D;&quot;10s&quot;</span><br><span class="line">I0705 22:38:00.837080   18145 flags.go:33] FLAG: --audit-webhook-mode&#x3D;&quot;batch&quot;</span><br><span class="line">I0705 22:38:00.837086   18145 flags.go:33] FLAG: --audit-webhook-truncate-enabled&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837090   18145 flags.go:33] FLAG: --audit-webhook-truncate-max-batch-size&#x3D;&quot;10485760&quot;</span><br><span class="line">I0705 22:38:00.837094   18145 flags.go:33] FLAG: --audit-webhook-truncate-max-event-size&#x3D;&quot;102400&quot;</span><br><span class="line">I0705 22:38:00.837098   18145 flags.go:33] FLAG: --audit-webhook-version&#x3D;&quot;audit.k8s.io&#x2F;v1&quot;</span><br><span class="line">I0705 22:38:00.837103   18145 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl&#x3D;&quot;2m0s&quot;</span><br><span class="line">I0705 22:38:00.837107   18145 flags.go:33] FLAG: --authentication-token-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837119   18145 flags.go:33] FLAG: --authorization-mode&#x3D;&quot;[Node,RBAC]&quot;</span><br><span class="line">I0705 22:38:00.837125   18145 flags.go:33] FLAG: --authorization-policy-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837128   18145 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl&#x3D;&quot;5m0s&quot;</span><br><span class="line">I0705 22:38:00.837132   18145 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl&#x3D;&quot;30s&quot;</span><br><span class="line">I0705 22:38:00.837136   18145 flags.go:33] FLAG: --authorization-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837140   18145 flags.go:33] FLAG: --basic-auth-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;basic-auth.csv&quot;</span><br><span class="line">I0705 22:38:00.837145   18145 flags.go:33] FLAG: --bind-address&#x3D;&quot;0.0.0.0&quot;</span><br><span class="line">I0705 22:38:00.837149   18145 flags.go:33] FLAG: --cert-dir&#x3D;&quot;&#x2F;var&#x2F;run&#x2F;kubernetes&quot;</span><br><span class="line">I0705 22:38:00.837153   18145 flags.go:33] FLAG: --client-ca-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837158   18145 flags.go:33] FLAG: --cloud-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837162   18145 flags.go:33] FLAG: --cloud-provider&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837165   18145 flags.go:33] FLAG: --cloud-provider-gce-lb-src-cidrs&#x3D;&quot;130.211.0.0&#x2F;22,209.85.152.0&#x2F;22,209.85.204.0&#x2F;22,35.191.0.0&#x2F;16&quot;</span><br><span class="line">I0705 22:38:00.837172   18145 flags.go:33] FLAG: --contention-profiling&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837176   18145 flags.go:33] FLAG: --cors-allowed-origins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837182   18145 flags.go:33] FLAG: --default-not-ready-toleration-seconds&#x3D;&quot;300&quot;</span><br><span class="line">I0705 22:38:00.837186   18145 flags.go:33] FLAG: --default-unreachable-toleration-seconds&#x3D;&quot;300&quot;</span><br><span class="line">I0705 22:38:00.837190   18145 flags.go:33] FLAG: --default-watch-cache-size&#x3D;&quot;100&quot;</span><br><span class="line">I0705 22:38:00.837194   18145 flags.go:33] FLAG: --delete-collection-workers&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.837198   18145 flags.go:33] FLAG: --deserialization-cache-size&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837202   18145 flags.go:33] FLAG: --disable-admission-plugins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837206   18145 flags.go:33] FLAG: --enable-admission-plugins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837213   18145 flags.go:33] FLAG: --enable-aggregator-routing&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837217   18145 flags.go:33] FLAG: --enable-bootstrap-token-auth&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837221   18145 flags.go:33] FLAG: --enable-garbage-collector&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837225   18145 flags.go:33] FLAG: --enable-inflight-quota-handler&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837228   18145 flags.go:33] FLAG: --enable-logs-handler&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837232   18145 flags.go:33] FLAG: --enable-swagger-ui&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837236   18145 flags.go:33] FLAG: --encryption-provider-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837240   18145 flags.go:33] FLAG: --endpoint-reconciler-type&#x3D;&quot;lease&quot;</span><br><span class="line">I0705 22:38:00.837244   18145 flags.go:33] FLAG: --etcd-cafile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837248   18145 flags.go:33] FLAG: --etcd-certfile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837253   18145 flags.go:33] FLAG: --etcd-compaction-interval&#x3D;&quot;5m0s&quot;</span><br><span class="line">I0705 22:38:00.837257   18145 flags.go:33] FLAG: --etcd-count-metric-poll-period&#x3D;&quot;1m0s&quot;</span><br><span class="line">I0705 22:38:00.837261   18145 flags.go:33] FLAG: --etcd-keyfile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837265   18145 flags.go:33] FLAG: --etcd-prefix&#x3D;&quot;&#x2F;registry&quot;</span><br><span class="line">I0705 22:38:00.837269   18145 flags.go:33] FLAG: --etcd-servers&#x3D;&quot;[https:&#x2F;&#x2F;10.129.173.92:2379,https:&#x2F;&#x2F;10.129.173.93:2379,https:&#x2F;&#x2F;10.129.173.94:2379]&quot;</span><br><span class="line">I0705 22:38:00.837277   18145 flags.go:33] FLAG: --etcd-servers-overrides&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837283   18145 flags.go:33] FLAG: --event-ttl&#x3D;&quot;1h0m0s&quot;</span><br><span class="line">I0705 22:38:00.837287   18145 flags.go:33] FLAG: --experimental-encryption-provider-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837291   18145 flags.go:33] FLAG: --external-hostname&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837296   18145 flags.go:33] FLAG: --feature-gates&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837303   18145 flags.go:33] FLAG: --help&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837307   18145 flags.go:33] FLAG: --http2-max-streams-per-connection&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837311   18145 flags.go:33] FLAG: --insecure-bind-address&#x3D;&quot;127.0.0.1&quot;</span><br><span class="line">I0705 22:38:00.837315   18145 flags.go:33] FLAG: --insecure-port&#x3D;&quot;8073&quot;</span><br><span class="line">I0705 22:38:00.837319   18145 flags.go:33] FLAG: --kubelet-certificate-authority&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837323   18145 flags.go:33] FLAG: --kubelet-client-certificate&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837331   18145 flags.go:33] FLAG: --kubelet-client-key&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837336   18145 flags.go:33] FLAG: --kubelet-https&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837340   18145 flags.go:33] FLAG: --kubelet-port&#x3D;&quot;10250&quot;</span><br><span class="line">I0705 22:38:00.837346   18145 flags.go:33] FLAG: --kubelet-preferred-address-types&#x3D;&quot;[Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]&quot;</span><br><span class="line">I0705 22:38:00.837352   18145 flags.go:33] FLAG: --kubelet-read-only-port&#x3D;&quot;10255&quot;</span><br><span class="line">I0705 22:38:00.837356   18145 flags.go:33] FLAG: --kubelet-timeout&#x3D;&quot;5s&quot;</span><br><span class="line">I0705 22:38:00.837360   18145 flags.go:33] FLAG: --kubernetes-service-node-port&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837364   18145 flags.go:33] FLAG: --log-backtrace-at&#x3D;&quot;:0&quot;</span><br><span class="line">I0705 22:38:00.837370   18145 flags.go:33] FLAG: --log-dir&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837375   18145 flags.go:33] FLAG: --log-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837379   18145 flags.go:33] FLAG: --log-file-max-size&#x3D;&quot;1800&quot;</span><br><span class="line">I0705 22:38:00.837383   18145 flags.go:33] FLAG: --log-flush-frequency&#x3D;&quot;5s&quot;</span><br><span class="line">I0705 22:38:00.837387   18145 flags.go:33] FLAG: --logtostderr&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837391   18145 flags.go:33] FLAG: --master-service-namespace&#x3D;&quot;default&quot;</span><br><span class="line">I0705 22:38:00.837395   18145 flags.go:33] FLAG: --max-connection-bytes-per-sec&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837399   18145 flags.go:33] FLAG: --max-mutating-requests-inflight&#x3D;&quot;200&quot;</span><br><span class="line">I0705 22:38:00.837403   18145 flags.go:33] FLAG: --max-requests-inflight&#x3D;&quot;400&quot;</span><br><span class="line">I0705 22:38:00.837407   18145 flags.go:33] FLAG: --min-request-timeout&#x3D;&quot;1800&quot;</span><br><span class="line">I0705 22:38:00.837411   18145 flags.go:33] FLAG: --oidc-ca-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837415   18145 flags.go:33] FLAG: --oidc-client-id&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837419   18145 flags.go:33] FLAG: --oidc-groups-claim&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837422   18145 flags.go:33] FLAG: --oidc-groups-prefix&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837426   18145 flags.go:33] FLAG: --oidc-issuer-url&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837430   18145 flags.go:33] FLAG: --oidc-required-claim&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837436   18145 flags.go:33] FLAG: --oidc-signing-algs&#x3D;&quot;[RS256]&quot;</span><br><span class="line">I0705 22:38:00.837443   18145 flags.go:33] FLAG: --oidc-username-claim&#x3D;&quot;sub&quot;</span><br><span class="line">I0705 22:38:00.837446   18145 flags.go:33] FLAG: --oidc-username-prefix&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837450   18145 flags.go:33] FLAG: --port&#x3D;&quot;8073&quot;</span><br><span class="line">I0705 22:38:00.837454   18145 flags.go:33] FLAG: --profiling&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837458   18145 flags.go:33] FLAG: --proxy-client-cert-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;aggregator-proxy.pem&quot;</span><br><span class="line">I0705 22:38:00.837463   18145 flags.go:33] FLAG: --proxy-client-key-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;aggregator-proxy-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837468   18145 flags.go:33] FLAG: --request-timeout&#x3D;&quot;1m0s&quot;</span><br><span class="line">I0705 22:38:00.837472   18145 flags.go:33] FLAG: --requestheader-allowed-names&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837476   18145 flags.go:33] FLAG: --requestheader-client-ca-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837480   18145 flags.go:33] FLAG: --requestheader-extra-headers-prefix&#x3D;&quot;[X-Remote-Extra-]&quot;</span><br><span class="line">I0705 22:38:00.837487   18145 flags.go:33] FLAG: --requestheader-group-headers&#x3D;&quot;[X-Remote-Group]&quot;</span><br><span class="line">I0705 22:38:00.837492   18145 flags.go:33] FLAG: --requestheader-username-headers&#x3D;&quot;[X-Remote-User]&quot;</span><br><span class="line">I0705 22:38:00.837498   18145 flags.go:33] FLAG: --runtime-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837505   18145 flags.go:33] FLAG: --secure-port&#x3D;&quot;6443&quot;</span><br><span class="line">I0705 22:38:00.837510   18145 flags.go:33] FLAG: --service-account-api-audiences&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837515   18145 flags.go:33] FLAG: --service-account-issuer&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837518   18145 flags.go:33] FLAG: --service-account-key-file&#x3D;&quot;[&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca-key.pem]&quot;</span><br><span class="line">I0705 22:38:00.837526   18145 flags.go:33] FLAG: --service-account-lookup&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837530   18145 flags.go:33] FLAG: --service-account-max-token-expiration&#x3D;&quot;0s&quot;</span><br><span class="line">I0705 22:38:00.837534   18145 flags.go:33] FLAG: --service-account-signing-key-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837538   18145 flags.go:33] FLAG: --service-cluster-ip-range&#x3D;&quot;172.26.0.0&#x2F;16&quot;</span><br><span class="line">I0705 22:38:00.837544   18145 flags.go:33] FLAG: --service-node-port-range&#x3D;&quot;20000-40000&quot;</span><br><span class="line">I0705 22:38:00.837558   18145 flags.go:33] FLAG: --skip-headers&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837566   18145 flags.go:33] FLAG: --skip-log-headers&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837570   18145 flags.go:33] FLAG: --ssh-keyfile&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837574   18145 flags.go:33] FLAG: --ssh-user&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837578   18145 flags.go:33] FLAG: --stderrthreshold&#x3D;&quot;2&quot;</span><br><span class="line">I0705 22:38:00.837582   18145 flags.go:33] FLAG: --storage-backend&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837585   18145 flags.go:33] FLAG: --storage-media-type&#x3D;&quot;application&#x2F;vnd.kubernetes.protobuf&quot;</span><br><span class="line">I0705 22:38:00.837589   18145 flags.go:33] FLAG: --target-ram-mb&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837607   18145 flags.go:33] FLAG: --tls-cert-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837613   18145 flags.go:33] FLAG: --tls-cipher-suites&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837617   18145 flags.go:33] FLAG: --tls-min-version&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837621   18145 flags.go:33] FLAG: --tls-private-key-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837626   18145 flags.go:33] FLAG: --tls-sni-cert-key&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837631   18145 flags.go:33] FLAG: --token-auth-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837635   18145 flags.go:33] FLAG: --v&#x3D;&quot;5&quot;</span><br><span class="line">I0705 22:38:00.837639   18145 flags.go:33] FLAG: --version&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837645   18145 flags.go:33] FLAG: --vmodule&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837652   18145 flags.go:33] FLAG: --watch-cache&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837656   18145 flags.go:33] FLAG: --watch-cache-sizes&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837838   18145 interface.go:384] Looking for default routes with IPv4 addresses</span><br><span class="line">I0705 22:38:00.837845   18145 interface.go:389] Default route transits interface &quot;eth0&quot;</span><br><span class="line">I0705 22:38:00.838072   18145 interface.go:196] Interface eth0 is up</span><br><span class="line">I0705 22:38:00.838128   18145 interface.go:244] Interface &quot;eth0&quot; has 1 addresses :[10.129.173.93&#x2F;22].</span><br><span class="line">I0705 22:38:00.838147   18145 interface.go:211] Checking addr  10.129.173.93&#x2F;22.</span><br><span class="line">I0705 22:38:00.838154   18145 interface.go:218] IP found 10.129.173.93</span><br><span class="line">I0705 22:38:00.838161   18145 interface.go:250] Found valid IPv4 address 10.129.173.93 for interface &quot;eth0&quot;.</span><br><span class="line">I0705 22:38:00.838167   18145 interface.go:395] Found active IP 10.129.173.93 </span><br><span class="line">I0705 22:38:00.838183   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">I0705 22:38:00.838191   18145 server.go:560] external host was not specified, using 10.129.173.93</span><br><span class="line">I0705 22:38:00.838200   18145 server.go:603] Initializing cache sizes based on 0MB limit</span><br><span class="line">I0705 22:38:00.838462   18145 server.go:147] Version: v1.15.5</span><br><span class="line">I0705 22:38:01.255999   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:01.256040   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:01.256057   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">E0705 22:38:01.256668   18145 prometheus.go:55] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256708   18145 prometheus.go:68] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256750   18145 prometheus.go:82] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256782   18145 prometheus.go:96] failed to register workDuration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256822   18145 prometheus.go:112] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256858   18145 prometheus.go:126] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256876   18145 prometheus.go:152] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256902   18145 prometheus.go:164] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256976   18145 prometheus.go:176] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257022   18145 prometheus.go:188] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257057   18145 prometheus.go:203] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257086   18145 prometheus.go:216] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">I0705 22:38:01.257111   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:01.257119   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:01.260013   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.260032   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.260104   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.260268   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.266868   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.266952   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.267066   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.267074   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: &quot;10.129.173.94:2379&quot; is up but not pinned (already pinned &quot;10.129.173.92:2379&quot;)</span><br><span class="line">W0705 22:38:01.267176   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.267294   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.267308   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.267337   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.267425   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.273677   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.273740   18145 storage_factory.go:50] Storage caching is enabled for *apiextensions.CustomResourceDefinition with capacity 100</span><br><span class="line">I0705 22:38:01.273759   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.273837   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.273893   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.274465   18145 store.go:1343] Monitoring customresourcedefinitions.apiextensions.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions</span><br><span class="line">I0705 22:38:01.274588   18145 reflector.go:160] Listing and watching *apiextensions.CustomResourceDefinition from storage&#x2F;cacher.go:&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions</span><br><span class="line">I0705 22:38:01.277287   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.297217   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">I0705 22:38:01.297246   18145 master.go:233] Using reconciler: lease</span><br><span class="line">I0705 22:38:01.297293   18145 storage_factory.go:285] storing apiServerIPInfo in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.297799   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.297812   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.297844   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.297883   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.304364   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.304410   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.304499   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.304538   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.305969   18145 storage_factory.go:285] storing podtemplates in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.306443   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.306455   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.306484   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.306515   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.312747   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.312802   18145 storage_factory.go:50] Storage caching is enabled for *core.PodTemplate with capacity 100</span><br><span class="line">I0705 22:38:01.312802   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.312893   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.312929   18145 store.go:1343] Monitoring podtemplates count at &lt;storage-prefix&gt;&#x2F;&#x2F;podtemplates</span><br><span class="line">W0705 22:38:01.312949   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.312969   18145 reflector.go:160] Listing and watching *core.PodTemplate from storage&#x2F;cacher.go:&#x2F;podtemplates</span><br><span class="line">I0705 22:38:01.312962   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.313529   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.313541   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.313570   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.313640   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.316212   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.319641   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.319682   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.319684   18145 storage_factory.go:46] Storage caching is disabled for *core.Event</span><br><span class="line">W0705 22:38:01.319767   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.319773   18145 store.go:1343] Monitoring events count at &lt;storage-prefix&gt;&#x2F;&#x2F;events</span><br><span class="line">W0705 22:38:01.319806   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.319815   18145 storage_factory.go:285] storing limitranges in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.320331   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.320344   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.320372   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.320409   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.326178   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.326213   18145 storage_factory.go:50] Storage caching is enabled for *core.LimitRange with capacity 100</span><br><span class="line">I0705 22:38:01.326227   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.326298   18145 store.go:1343] Monitoring limitranges count at &lt;storage-prefix&gt;&#x2F;&#x2F;limitranges</span><br><span class="line">I0705 22:38:01.326328   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.326344   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.326352   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.326365   18145 reflector.go:160] Listing and watching *core.LimitRange from storage&#x2F;cacher.go:&#x2F;limitranges</span><br><span class="line">I0705 22:38:01.326789   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.326801   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.326827   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.327266   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.329416   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.333949   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.334002   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.334049   18145 storage_factory.go:50] Storage caching is enabled for *core.ResourceQuota with capacity 100</span><br><span class="line">W0705 22:38:01.334117   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.334183   18145 store.go:1343] Monitoring resourcequotas count at &lt;storage-prefix&gt;&#x2F;&#x2F;resourcequotas</span><br><span class="line">I0705 22:38:01.334225   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.334240   18145 reflector.go:160] Listing and watching *core.ResourceQuota from storage&#x2F;cacher.go:&#x2F;resourcequotas</span><br><span class="line">W0705 22:38:01.334251   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.334409   18145 storage_factory.go:285] storing secrets in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.334997   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.335011   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.335060   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.335107   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.336791   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.341685   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.341737   18145 storage_factory.go:50] Storage caching is enabled for *core.Secret with capacity 100</span><br><span class="line">I0705 22:38:01.341753   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.341819   18145 store.go:1343] Monitoring secrets count at &lt;storage-prefix&gt;&#x2F;&#x2F;secrets</span><br><span class="line">W0705 22:38:01.341843   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.341854   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.341855   18145 reflector.go:160] Listing and watching *core.Secret from storage&#x2F;cacher.go:&#x2F;secrets</span><br><span class="line">I0705 22:38:01.341953   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.342468   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.342480   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.342535   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.342579   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.345126   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.348069   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.348110   18145 storage_factory.go:50] Storage caching is enabled for *core.PersistentVolume with capacity 100</span><br><span class="line">I0705 22:38:01.348133   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.348190   18145 store.go:1343] Monitoring persistentvolumes count at &lt;storage-prefix&gt;&#x2F;&#x2F;persistentvolumes</span><br><span class="line">W0705 22:38:01.348228   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.348239   18145 reflector.go:160] Listing and watching *core.PersistentVolume from storage&#x2F;cacher.go:&#x2F;persistentvolumes</span><br><span class="line">W0705 22:38:01.348248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.348373   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.348916   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.348930   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.348986   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.349049   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.350015   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.355158   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.355202   18145 storage_factory.go:50] Storage caching is enabled for *core.PersistentVolumeClaim with capacity 100</span><br><span class="line">I0705 22:38:01.355203   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.355297   18145 store.go:1343] Monitoring persistentvolumeclaims count at &lt;storage-prefix&gt;&#x2F;&#x2F;persistentvolumeclaims</span><br><span class="line">I0705 22:38:01.355343   18145 reflector.go:160] Listing and watching *core.PersistentVolumeClaim from storage&#x2F;cacher.go:&#x2F;persistentvolumeclaims</span><br><span class="line">W0705 22:38:01.355357   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.355377   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.355467   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.355442   18145 storage_factory.go:285] storing configmaps in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.355993   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.356010   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.356046   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.356101   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.360455   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.361771   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.361808   18145 storage_factory.go:50] Storage caching is enabled for *core.ConfigMap with capacity 100</span><br><span class="line">I0705 22:38:01.361847   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.361872   18145 store.go:1343] Monitoring configmaps count at &lt;storage-prefix&gt;&#x2F;&#x2F;configmaps</span><br><span class="line">W0705 22:38:01.361918   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.361923   18145 reflector.go:160] Listing and watching *core.ConfigMap from storage&#x2F;cacher.go:&#x2F;configmaps</span><br><span class="line">W0705 22:38:01.361936   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.361979   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.362436   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.362448   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.362475   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.362509   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.368369   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.368402   18145 storage_factory.go:50] Storage caching is enabled for *core.Namespace with capacity 100</span><br><span class="line">I0705 22:38:01.368406   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.368475   18145 store.go:1343] Monitoring namespaces count at &lt;storage-prefix&gt;&#x2F;&#x2F;namespaces</span><br><span class="line">W0705 22:38:01.368480   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.368527   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.368523   18145 reflector.go:160] Listing and watching *core.Namespace from storage&#x2F;cacher.go:&#x2F;namespaces</span><br><span class="line">I0705 22:38:01.368640   18145 storage_factory.go:285] storing endpoints in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.369074   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.369088   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.369118   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.369152   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.376059   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.376127   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.376137   18145 storage_factory.go:50] Storage caching is enabled for *core.Endpoints with capacity 1000</span><br><span class="line">W0705 22:38:01.376216   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.376219   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.376246   18145 store.go:1343] Monitoring endpoints count at &lt;storage-prefix&gt;&#x2F;&#x2F;services&#x2F;endpoints</span><br><span class="line">I0705 22:38:01.376288   18145 reflector.go:160] Listing and watching *core.Endpoints from storage&#x2F;cacher.go:&#x2F;services&#x2F;endpoints</span><br><span class="line">I0705 22:38:01.376450   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.376450   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.377222   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.377246   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.377295   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.377372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.379460   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.382925   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.382961   18145 storage_factory.go:50] Storage caching is enabled for *core.Node with capacity 1000</span><br><span class="line">I0705 22:38:01.382977   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.383040   18145 store.go:1343] Monitoring nodes count at &lt;storage-prefix&gt;&#x2F;&#x2F;minions</span><br><span class="line">W0705 22:38:01.383054   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.383063   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.383102   18145 reflector.go:160] Listing and watching *core.Node from storage&#x2F;cacher.go:&#x2F;minions</span><br><span class="line">I0705 22:38:01.383436   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.383884   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.383896   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.383921   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.383955   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.386054   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.389904   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.389951   18145 storage_factory.go:50] Storage caching is enabled for *core.Pod with capacity 1000</span><br><span class="line">I0705 22:38:01.389990   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.390047   18145 store.go:1343] Monitoring pods count at &lt;storage-prefix&gt;&#x2F;&#x2F;pods</span><br><span class="line">W0705 22:38:01.390100   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.390107   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.390140   18145 reflector.go:160] Listing and watching *core.Pod from storage&#x2F;cacher.go:&#x2F;pods</span><br><span class="line">I0705 22:38:01.390180   18145 storage_factory.go:285] storing serviceaccounts in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.391245   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.391263   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.391310   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.391381   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.398242   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.398285   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.398297   18145 storage_factory.go:50] Storage caching is enabled for *core.ServiceAccount with capacity 100</span><br><span class="line">W0705 22:38:01.398378   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.398395   18145 store.go:1343] Monitoring serviceaccounts count at &lt;storage-prefix&gt;&#x2F;&#x2F;serviceaccounts</span><br><span class="line">W0705 22:38:01.398413   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.398524   18145 reflector.go:160] Listing and watching *core.ServiceAccount from storage&#x2F;cacher.go:&#x2F;serviceaccounts</span><br><span class="line">I0705 22:38:01.398559   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.399760   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.399789   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.399832   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.399894   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.402304   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.406325   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.406360   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.406413   18145 storage_factory.go:50] Storage caching is enabled for *core.Service with capacity 1000</span><br><span class="line">I0705 22:38:01.406421   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.406516   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.406546   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.406580   18145 store.go:1343] Monitoring services count at &lt;storage-prefix&gt;&#x2F;&#x2F;services&#x2F;specs</span><br><span class="line">I0705 22:38:01.406645   18145 reflector.go:160] Listing and watching *core.Service from storage&#x2F;cacher.go:&#x2F;services&#x2F;specs</span><br><span class="line">I0705 22:38:01.406642   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.407151   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.407163   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.407208   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.407260   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.412982   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.413048   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.413128   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.413155   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.413252   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.413526   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.413539   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.413567   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.413619   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.414368   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.419479   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.419514   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.419579   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.419647   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.419647   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.420108   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.420119   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.420144   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.420203   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.426157   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.426208   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.426217   18145 storage_factory.go:50] Storage caching is enabled for *core.ReplicationController with capacity 100</span><br><span class="line">W0705 22:38:01.426293   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.426306   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.426312   18145 store.go:1343] Monitoring replicationcontrollers count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllers</span><br><span class="line">I0705 22:38:01.426333   18145 reflector.go:160] Listing and watching *core.ReplicationController from storage&#x2F;cacher.go:&#x2F;controllers</span><br><span class="line">I0705 22:38:01.428279   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.480260   18145 storage_factory.go:285] storing bindings in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.480424   18145 storage_factory.go:285] storing componentstatuses in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.480904   18145 storage_factory.go:285] storing configmaps in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.481312   18145 storage_factory.go:285] storing endpoints in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.481698   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482104   18145 storage_factory.go:285] storing limitranges in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482346   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482426   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482543   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482825   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483147   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483255   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483733   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483904   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484203   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484330   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484703   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484828   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484905   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484984   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485119   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485212   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485326   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485764   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485937   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486385   18145 storage_factory.go:285] storing podtemplates in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486832   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486982   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487134   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487533   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487697   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488101   18145 storage_factory.go:285] storing secrets in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488494   18145 storage_factory.go:285] storing serviceaccounts in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488886   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489313   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489466   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489537   18145 master.go:417] Skipping disabled API group &quot;auditregistration.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489550   18145 master.go:425] Enabling API group &quot;authentication.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489562   18145 master.go:425] Enabling API group &quot;authorization.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489679   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.490153   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.490164   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.490200   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.490237   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.496374   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.496423   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.496450   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.496501   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">W0705 22:38:01.496531   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.496536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.496589   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.496664   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.497098   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.497109   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.497135   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.497176   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.500515   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.502989   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.503023   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.503047   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.503101   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">W0705 22:38:01.503114   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.503161   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.503155   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.503245   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.503665   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.503676   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.503702   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.503743   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.506520   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.509091   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.509124   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.509161   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.509195   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.509211   18145 master.go:425] Enabling API group &quot;autoscaling&quot;.</span><br><span class="line">W0705 22:38:01.509223   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.509248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.509265   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.509331   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.509807   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.509821   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.509848   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.509885   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.515303   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.515327   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.515362   18145 storage_factory.go:50] Storage caching is enabled for *batch.Job with capacity 100</span><br><span class="line">W0705 22:38:01.515407   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.515414   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.515436   18145 store.go:1343] Monitoring jobs.batch count at &lt;storage-prefix&gt;&#x2F;&#x2F;jobs</span><br><span class="line">I0705 22:38:01.515472   18145 reflector.go:160] Listing and watching *batch.Job from storage&#x2F;cacher.go:&#x2F;jobs</span><br><span class="line">I0705 22:38:01.515529   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.516011   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.516023   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.516047   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.516080   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.516374   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.517439   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.522214   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.522244   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.522277   18145 storage_factory.go:50] Storage caching is enabled for *batch.CronJob with capacity 100</span><br><span class="line">W0705 22:38:01.522297   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.522325   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.522347   18145 store.go:1343] Monitoring cronjobs.batch count at &lt;storage-prefix&gt;&#x2F;&#x2F;cronjobs</span><br><span class="line">I0705 22:38:01.522361   18145 master.go:425] Enabling API group &quot;batch&quot;.</span><br><span class="line">I0705 22:38:01.522409   18145 reflector.go:160] Listing and watching *batch.CronJob from storage&#x2F;cacher.go:&#x2F;cronjobs</span><br><span class="line">I0705 22:38:01.522456   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.522889   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.522901   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.522926   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.522959   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.525358   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.529067   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.529096   18145 storage_factory.go:50] Storage caching is enabled for *certificates.CertificateSigningRequest with capacity 100</span><br><span class="line">I0705 22:38:01.529108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.529164   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.529169   18145 store.go:1343] Monitoring certificatesigningrequests.certificates.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;certificatesigningrequests</span><br><span class="line">W0705 22:38:01.529172   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.529187   18145 master.go:425] Enabling API group &quot;certificates.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.529202   18145 reflector.go:160] Listing and watching *certificates.CertificateSigningRequest from storage&#x2F;cacher.go:&#x2F;certificatesigningrequests</span><br><span class="line">I0705 22:38:01.529330   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.529757   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.529769   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.529799   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.529831   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.535795   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.535826   18145 storage_factory.go:50] Storage caching is enabled for *coordination.Lease with capacity 100</span><br><span class="line">I0705 22:38:01.535851   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.535884   18145 store.go:1343] Monitoring leases.coordination.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;leases</span><br><span class="line">W0705 22:38:01.535922   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.535951   18145 reflector.go:160] Listing and watching *coordination.Lease from storage&#x2F;cacher.go:&#x2F;leases</span><br><span class="line">W0705 22:38:01.535986   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.536022   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.536035   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.536453   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.536464   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.536487   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.536530   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.538229   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.543527   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.543566   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: &quot;10.129.173.92:2379&quot; is up but not pinned (already pinned &quot;10.129.173.94:2379&quot;)</span><br><span class="line">I0705 22:38:01.543625   18145 storage_factory.go:50] Storage caching is enabled for *coordination.Lease with capacity 100</span><br><span class="line">I0705 22:38:01.543664   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.543732   18145 store.go:1343] Monitoring leases.coordination.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;leases</span><br><span class="line">I0705 22:38:01.544459   18145 reflector.go:160] Listing and watching *coordination.Lease from storage&#x2F;cacher.go:&#x2F;leases</span><br><span class="line">I0705 22:38:01.544471   18145 master.go:425] Enabling API group &quot;coordination.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.544507   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.544785   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.544955   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">W0705 22:38:01.544970   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.546012   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.546032   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.546074   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.546142   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.546857   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.552338   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.552372   18145 storage_factory.go:50] Storage caching is enabled for *core.ReplicationController with capacity 100</span><br><span class="line">I0705 22:38:01.552402   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.552431   18145 store.go:1343] Monitoring replicationcontrollers count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllers</span><br><span class="line">W0705 22:38:01.552478   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.552513   18145 reflector.go:160] Listing and watching *core.ReplicationController from storage&#x2F;cacher.go:&#x2F;controllers</span><br><span class="line">W0705 22:38:01.552563   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.552569   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.553038   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.553048   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.553075   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.553108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.554206   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.559089   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.559133   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.559160   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.559227   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">W0705 22:38:01.559248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.559277   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.559295   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.559365   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.559903   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.559919   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.559950   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.560049   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.562229   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.565407   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.565440   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.565451   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">W0705 22:38:01.565511   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.565538   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.565544   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.565609   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.565689   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.566128   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.566140   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.566173   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.566208   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.572128   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.572160   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.572184   18145 storage_factory.go:50] Storage caching is enabled for *networking.Ingress with capacity 100</span><br><span class="line">W0705 22:38:01.572230   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.572257   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.572275   18145 store.go:1343] Monitoring ingresses.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;ingress</span><br><span class="line">I0705 22:38:01.572328   18145 reflector.go:160] Listing and watching *networking.Ingress from storage&#x2F;cacher.go:&#x2F;ingress</span><br><span class="line">I0705 22:38:01.572433   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.573010   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.573029   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.573060   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.573099   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.573357   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.578443   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.578472   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodSecurityPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.578477   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.578536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.578548   18145 store.go:1343] Monitoring podsecuritypolicies.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;podsecuritypolicy</span><br><span class="line">I0705 22:38:01.578574   18145 reflector.go:160] Listing and watching *policy.PodSecurityPolicy from storage&#x2F;cacher.go:&#x2F;podsecuritypolicy</span><br><span class="line">W0705 22:38:01.578581   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.578667   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.579104   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.579114   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.579140   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.579171   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.579353   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.580264   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.584838   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.584871   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.584886   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.584942   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.584975   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.584943   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.584987   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.585104   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.585519   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.585529   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.585553   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.585612   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.590996   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.591666   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.591699   18145 storage_factory.go:50] Storage caching is enabled for *networking.NetworkPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.591744   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.591767   18145 store.go:1343] Monitoring networkpolicies.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;networkpolicies</span><br><span class="line">I0705 22:38:01.591780   18145 master.go:425] Enabling API group &quot;extensions&quot;.</span><br><span class="line">W0705 22:38:01.591812   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.591816   18145 reflector.go:160] Listing and watching *networking.NetworkPolicy from storage&#x2F;cacher.go:&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.591814   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.591919   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.592337   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.592347   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.592382   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.592420   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.593678   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.598473   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.598508   18145 storage_factory.go:50] Storage caching is enabled for *networking.NetworkPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.598534   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.598556   18145 store.go:1343] Monitoring networkpolicies.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.598620   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.598621   18145 reflector.go:160] Listing and watching *networking.NetworkPolicy from storage&#x2F;cacher.go:&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.598670   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.598678   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.599263   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.599279   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.599307   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.599340   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.605672   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.605867   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.605898   18145 storage_factory.go:50] Storage caching is enabled for *networking.Ingress with capacity 100</span><br><span class="line">I0705 22:38:01.605918   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.605966   18145 store.go:1343] Monitoring ingresses.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;ingress</span><br><span class="line">I0705 22:38:01.605981   18145 master.go:425] Enabling API group &quot;networking.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.605983   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.606016   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.606017   18145 storage_factory.go:285] storing runtimeclasses.node.k8s.io in node.k8s.io&#x2F;v1beta1, reading as node.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.606036   18145 reflector.go:160] Listing and watching *networking.Ingress from storage&#x2F;cacher.go:&#x2F;ingress</span><br><span class="line">I0705 22:38:01.606558   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.606571   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.606613   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.606667   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.612169   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.612196   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.612210   18145 storage_factory.go:50] Storage caching is enabled for *node.RuntimeClass with capacity 100</span><br><span class="line">W0705 22:38:01.612254   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.612274   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.612300   18145 store.go:1343] Monitoring runtimeclasses.node.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;runtimeclasses</span><br><span class="line">I0705 22:38:01.612325   18145 master.go:425] Enabling API group &quot;node.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.612343   18145 reflector.go:160] Listing and watching *node.RuntimeClass from storage&#x2F;cacher.go:&#x2F;runtimeclasses</span><br><span class="line">I0705 22:38:01.612348   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.612449   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.612946   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.612958   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.612989   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.613026   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.618576   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.618621   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.618644   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodDisruptionBudget with capacity 100</span><br><span class="line">W0705 22:38:01.618680   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.618713   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.618732   18145 store.go:1343] Monitoring poddisruptionbudgets.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;poddisruptionbudgets</span><br><span class="line">I0705 22:38:01.618756   18145 reflector.go:160] Listing and watching *policy.PodDisruptionBudget from storage&#x2F;cacher.go:&#x2F;poddisruptionbudgets</span><br><span class="line">I0705 22:38:01.618878   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.619304   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.619315   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.619340   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.619372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.620634   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.621640   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.625143   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.625177   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodSecurityPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.625238   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.625266   18145 reflector.go:160] Listing and watching *policy.PodSecurityPolicy from storage&#x2F;cacher.go:&#x2F;podsecuritypolicy</span><br><span class="line">I0705 22:38:01.625246   18145 store.go:1343] Monitoring podsecuritypolicies.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;podsecuritypolicy</span><br><span class="line">W0705 22:38:01.625317   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.625324   18145 master.go:425] Enabling API group &quot;policy&quot;.</span><br><span class="line">W0705 22:38:01.625351   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.625367   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.625915   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.625934   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.625979   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.626040   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.627123   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.632111   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.632147   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.632160   18145 storage_factory.go:50] Storage caching is enabled for *rbac.Role with capacity 100</span><br><span class="line">W0705 22:38:01.632224   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.632295   18145 store.go:1343] Monitoring roles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;roles</span><br><span class="line">W0705 22:38:01.632296   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.632328   18145 reflector.go:160] Listing and watching *rbac.Role from storage&#x2F;cacher.go:&#x2F;roles</span><br><span class="line">I0705 22:38:01.632406   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.632889   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.632904   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.632930   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.632988   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.634959   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.639061   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.639098   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.639170   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.639101   18145 storage_factory.go:50] Storage caching is enabled for *rbac.RoleBinding with capacity 100</span><br><span class="line">W0705 22:38:01.639217   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.639258   18145 store.go:1343] Monitoring rolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.639292   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.639326   18145 reflector.go:160] Listing and watching *rbac.RoleBinding from storage&#x2F;cacher.go:&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.639724   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.639736   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.639761   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.639792   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.641705   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.645513   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.645546   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRole with capacity 100</span><br><span class="line">I0705 22:38:01.645589   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.645616   18145 store.go:1343] Monitoring clusterroles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterroles</span><br><span class="line">I0705 22:38:01.645686   18145 reflector.go:160] Listing and watching *rbac.ClusterRole from storage&#x2F;cacher.go:&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.645696   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.645689   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.645776   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.646238   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.646250   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.646277   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.646312   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.652231   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.652258   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.652280   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRoleBinding with capacity 100</span><br><span class="line">W0705 22:38:01.652325   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.652374   18145 store.go:1343] Monitoring clusterrolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterrolebindings</span><br><span class="line">W0705 22:38:01.652385   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.652417   18145 reflector.go:160] Listing and watching *rbac.ClusterRoleBinding from storage&#x2F;cacher.go:&#x2F;clusterrolebindings</span><br><span class="line">I0705 22:38:01.652484   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.652951   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.652963   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.652990   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.653027   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.653114   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.655550   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.659098   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.659135   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.659162   18145 storage_factory.go:50] Storage caching is enabled for *rbac.Role with capacity 100</span><br><span class="line">W0705 22:38:01.659210   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.659216   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.659231   18145 store.go:1343] Monitoring roles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;roles</span><br><span class="line">I0705 22:38:01.659290   18145 reflector.go:160] Listing and watching *rbac.Role from storage&#x2F;cacher.go:&#x2F;roles</span><br><span class="line">I0705 22:38:01.659382   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.659882   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.659894   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.659921   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.659976   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.661045   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.666391   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.666434   18145 storage_factory.go:50] Storage caching is enabled for *rbac.RoleBinding with capacity 100</span><br><span class="line">I0705 22:38:01.666455   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.666501   18145 store.go:1343] Monitoring rolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.666563   18145 reflector.go:160] Listing and watching *rbac.RoleBinding from storage&#x2F;cacher.go:&#x2F;rolebindings</span><br><span class="line">W0705 22:38:01.666586   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.666570   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.667163   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.667615   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.667628   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.667659   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.667696   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.669460   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.673937   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.673974   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRole with capacity 100</span><br><span class="line">I0705 22:38:01.673998   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.674038   18145 store.go:1343] Monitoring clusterroles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.674107   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.674139   18145 reflector.go:160] Listing and watching *rbac.ClusterRole from storage&#x2F;cacher.go:&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.674241   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.674246   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.674250   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.674768   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.674781   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.674809   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.674843   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.679062   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.680225   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.680261   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRoleBinding with capacity 100</span><br><span class="line">I0705 22:38:01.680286   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.680333   18145 store.go:1343] Monitoring clusterrolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterrolebindings</span><br><span class="line">I0705 22:38:01.680363   18145 master.go:425] Enabling API group &quot;rbac.authorization.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.680382   18145 reflector.go:160] Listing and watching *rbac.ClusterRoleBinding from storage&#x2F;cacher.go:&#x2F;clusterrolebindings</span><br><span class="line">W0705 22:38:01.680417   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.680389   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.682487   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.682847   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.682943   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.682958   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.682986   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.683031   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.688965   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.689018   18145 storage_factory.go:50] Storage caching is enabled for *scheduling.PriorityClass with capacity 100</span><br><span class="line">I0705 22:38:01.689064   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.689105   18145 store.go:1343] Monitoring priorityclasses.scheduling.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;priorityclasses</span><br><span class="line">W0705 22:38:01.689143   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.689144   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.689160   18145 reflector.go:160] Listing and watching *scheduling.PriorityClass from storage&#x2F;cacher.go:&#x2F;priorityclasses</span><br><span class="line">I0705 22:38:01.689271   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.689806   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.689819   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.689858   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.689924   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.691729   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.696232   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.696268   18145 storage_factory.go:50] Storage caching is enabled for *scheduling.PriorityClass with capacity 100</span><br><span class="line">I0705 22:38:01.696271   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.696334   18145 store.go:1343] Monitoring priorityclasses.scheduling.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;priorityclasses</span><br><span class="line">I0705 22:38:01.696348   18145 master.go:425] Enabling API group &quot;scheduling.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.696351   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.696382   18145 reflector.go:160] Listing and watching *scheduling.PriorityClass from storage&#x2F;cacher.go:&#x2F;priorityclasses</span><br><span class="line">W0705 22:38:01.696400   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.696458   18145 master.go:417] Skipping disabled API group &quot;settings.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.696568   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.697044   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.697056   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.697082   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.697116   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.697826   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.703204   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.703245   18145 storage_factory.go:50] Storage caching is enabled for *storage.StorageClass with capacity 100</span><br><span class="line">I0705 22:38:01.703295   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.703320   18145 store.go:1343] Monitoring storageclasses.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;storageclasses</span><br><span class="line">W0705 22:38:01.703359   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.703394   18145 reflector.go:160] Listing and watching *storage.StorageClass from storage&#x2F;cacher.go:&#x2F;storageclasses</span><br><span class="line">I0705 22:38:01.703456   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.703398   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.703972   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.703988   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.704034   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.704108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.705744   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.710344   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.710372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.710376   18145 storage_factory.go:50] Storage caching is enabled for *storage.VolumeAttachment with capacity 100</span><br><span class="line">W0705 22:38:01.710430   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.710445   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.710450   18145 store.go:1343] Monitoring volumeattachments.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.710464   18145 reflector.go:160] Listing and watching *storage.VolumeAttachment from storage&#x2F;cacher.go:&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.710483   18145 storage_factory.go:285] storing csinodes.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.710942   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.710954   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.710990   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.711054   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.713079   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.716838   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.716867   18145 storage_factory.go:50] Storage caching is enabled for *storage.CSINode with capacity 100</span><br><span class="line">I0705 22:38:01.716906   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.716965   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.717340   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.717348   18145 store.go:1343] Monitoring csinodes.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;csinodes</span><br><span class="line">I0705 22:38:01.717384   18145 storage_factory.go:285] storing csidrivers.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.717555   18145 reflector.go:160] Listing and watching *storage.CSINode from storage&#x2F;cacher.go:&#x2F;csinodes</span><br><span class="line">I0705 22:38:01.719158   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.719180   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.719235   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.719314   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.725650   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.725704   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.725750   18145 storage_factory.go:50] Storage caching is enabled for *storage.CSIDriver with capacity 100</span><br><span class="line">W0705 22:38:01.725830   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.725833   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.725881   18145 store.go:1343] Monitoring csidrivers.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;csidrivers</span><br><span class="line">I0705 22:38:01.725922   18145 reflector.go:160] Listing and watching *storage.CSIDriver from storage&#x2F;cacher.go:&#x2F;csidrivers</span><br><span class="line">I0705 22:38:01.726070   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.726147   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.726622   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.726641   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.726682   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.726742   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.727907   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.733148   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.733192   18145 storage_factory.go:50] Storage caching is enabled for *storage.StorageClass with capacity 100</span><br><span class="line">I0705 22:38:01.733195   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.733264   18145 store.go:1343] Monitoring storageclasses.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;storageclasses</span><br><span class="line">W0705 22:38:01.733277   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.733282   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.733318   18145 reflector.go:160] Listing and watching *storage.StorageClass from storage&#x2F;cacher.go:&#x2F;storageclasses</span><br><span class="line">I0705 22:38:01.733393   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.733942   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.733956   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.733983   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.734017   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.739573   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.739653   18145 storage_factory.go:50] Storage caching is enabled for *storage.VolumeAttachment with capacity 100</span><br><span class="line">I0705 22:38:01.739670   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.739743   18145 store.go:1343] Monitoring volumeattachments.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.739770   18145 master.go:425] Enabling API group &quot;storage.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.739785   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.739794   18145 reflector.go:160] Listing and watching *storage.VolumeAttachment from storage&#x2F;cacher.go:&#x2F;volumeattachments</span><br><span class="line">W0705 22:38:01.739820   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.739830   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.739938   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.740394   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.740405   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.740433   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.740471   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.745885   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.746023   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.746067   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">I0705 22:38:01.746068   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.746150   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.746192   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">W0705 22:38:01.746199   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.746195   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.746278   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.746862   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.746876   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.746903   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.746941   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.752900   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.752930   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.752989   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.752995   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.753021   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.753114   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">I0705 22:38:01.753145   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">I0705 22:38:01.753245   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.753664   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.753785   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.753799   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.753835   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.753881   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.759938   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.759972   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">I0705 22:38:01.759996   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.760029   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">W0705 22:38:01.760072   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.760088   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.760094   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.760179   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.760648   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.760660   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.760693   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.760752   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.761629   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.762737   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.766537   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.766570   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">I0705 22:38:01.766574   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.766639   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">W0705 22:38:01.766666   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.766667   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.766671   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.766798   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.767271   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.767283   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.767309   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.767408   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.773951   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.773995   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.773998   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.774074   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.774125   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.774172   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.774188   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.774230   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.774946   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.774963   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.774996   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.775036   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.776461   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.779672   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.780664   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.780705   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.780734   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.780809   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.780836   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">W0705 22:38:01.780811   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.780867   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.780956   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.781418   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.781428   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.781460   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.781499   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.782842   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.787419   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.787444   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.787502   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.787530   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.787553   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.787656   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.787742   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.787796   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.788235   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.788246   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.788273   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.788315   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.794135   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.794631   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.794681   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.794701   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">W0705 22:38:01.794771   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.794819   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">W0705 22:38:01.794829   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.794861   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.794960   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.795409   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.795421   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.795450   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.795502   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.797129   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.801393   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.801432   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.801471   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">W0705 22:38:01.801508   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.801536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.801545   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.801562   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.801702   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.802137   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.802147   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.802184   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.802251   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.807741   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.807776   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.807792   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.807849   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.807897   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.807922   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.807964   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.807960   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.808424   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.808435   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.809098   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.809163   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.809901   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.814015   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.814893   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.814938   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.814943   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.815016   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.815024   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.815026   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.815048   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.815185   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.815699   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.815712   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.815762   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.815819   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.821338   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.821378   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.821343   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.821406   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.821445   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.821497   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.821514   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.821554   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.821562   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.822050   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.822063   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.822109   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.822173   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.828748   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.828791   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.828823   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">W0705 22:38:01.828858   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.828865   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.828923   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.828944   18145 master.go:425] Enabling API group &quot;apps&quot;.</span><br><span class="line">I0705 22:38:01.828977   18145 storage_factory.go:285] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.828998   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.829405   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.829417   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.829444   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.829477   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.832275   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.833018   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.835374   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.835425   18145 storage_factory.go:50] Storage caching is enabled for *admissionregistration.ValidatingWebhookConfiguration with capacity 100</span><br><span class="line">I0705 22:38:01.835447   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.835516   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.835523   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.835521   18145 store.go:1343] Monitoring validatingwebhookconfigurations.admissionregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;validatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.835546   18145 reflector.go:160] Listing and watching *admissionregistration.ValidatingWebhookConfiguration from storage&#x2F;cacher.go:&#x2F;validatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.835570   18145 storage_factory.go:285] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.836092   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.836103   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.836130   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.836163   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.837500   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.843429   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.843486   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.843504   18145 storage_factory.go:50] Storage caching is enabled for *admissionregistration.MutatingWebhookConfiguration with capacity 100</span><br><span class="line">W0705 22:38:01.843562   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.843696   18145 store.go:1343] Monitoring mutatingwebhookconfigurations.admissionregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;mutatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.843729   18145 master.go:425] Enabling API group &quot;admissionregistration.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.843815   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.843787   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.844020   18145 reflector.go:160] Listing and watching *admissionregistration.MutatingWebhookConfiguration from storage&#x2F;cacher.go:&#x2F;mutatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.844236   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.844544   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.844556   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.844583   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.844651   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.850614   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.850627   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.850643   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.850668   18145 storage_factory.go:46] Storage caching is disabled for *core.Event</span><br><span class="line">I0705 22:38:01.850771   18145 store.go:1343] Monitoring events count at &lt;storage-prefix&gt;&#x2F;&#x2F;events</span><br><span class="line">I0705 22:38:01.850791   18145 master.go:425] Enabling API group &quot;events.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.850817   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.850841   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">W0705 22:38:01.850860   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.954244   18145 storage_factory.go:285] storing tokenreviews.authentication.k8s.io in authentication.k8s.io&#x2F;v1, reading as authentication.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954407   18145 storage_factory.go:285] storing tokenreviews.authentication.k8s.io in authentication.k8s.io&#x2F;v1, reading as authentication.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954618   18145 storage_factory.go:285] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954700   18145 storage_factory.go:285] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954785   18145 storage_factory.go:285] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954852   18145 storage_factory.go:285] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955039   18145 storage_factory.go:285] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955186   18145 storage_factory.go:285] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955253   18145 storage_factory.go:285] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955326   18145 storage_factory.go:285] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955939   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956125   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956633   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956819   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957276   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957422   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957939   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.958119   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.958615   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.959458   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.959508   18145 genericapiserver.go:351] Skipping API batch&#x2F;v2alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.959965   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960065   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960202   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960724   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961138   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961675   18145 storage_factory.go:285] storing daemonsets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961861   18145 storage_factory.go:285] storing daemonsets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962283   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962374   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962535   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962709   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963138   18145 storage_factory.go:285] storing ingresses.extensions in networking.k8s.io&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963278   18145 storage_factory.go:285] storing ingresses.extensions in networking.k8s.io&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963727   18145 storage_factory.go:285] storing networkpolicies.extensions in networking.k8s.io&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964049   18145 storage_factory.go:285] storing podsecuritypolicies.extensions in policy&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964505   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964702   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965393   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965440   18145 storage_factory.go:285] storing replicationcontrollers.extensions in v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965587   18145 storage_factory.go:285] storing replicationcontrollers.extensions in v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966078   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966475   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966634   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.967053   18145 storage_factory.go:285] storing runtimeclasses.node.k8s.io in node.k8s.io&#x2F;v1beta1, reading as node.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.967095   18145 genericapiserver.go:351] Skipping API node.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.967569   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.967746   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968069   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968450   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968782   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969174   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969538   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969891   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.970204   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.971161   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.971562   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.971614   18145 genericapiserver.go:351] Skipping API rbac.authorization.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.971959   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.972304   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.972342   18145 genericapiserver.go:351] Skipping API scheduling.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.972684   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973037   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973178   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973545   18145 storage_factory.go:285] storing csidrivers.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973841   18145 storage_factory.go:285] storing csinodes.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.974138   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.974435   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.974482   18145 genericapiserver.go:351] Skipping API storage.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.974974   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.975387   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.975552   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976543   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976761   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976906   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977277   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977404   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977530   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977953   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978104   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978247   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978707   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979080   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979237   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979612   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979751   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979874   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980238   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980373   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980500   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980918   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.981586   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.981758   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982238   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982585   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982685   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982814   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982947   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983330   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983478   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983624   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984096   18145 storage_factory.go:285] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984466   18145 storage_factory.go:285] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984926   18145 storage_factory.go:285] storing events.events.k8s.io in v1, reading as events.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:02.254177   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.254203   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.254244   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.254372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.261077   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:02.261128   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:02.261209   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.261287   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.603860   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;,&quot;poststarthook&#x2F;bootstrap-controller&quot;,&quot;poststarthook&#x2F;rbac&#x2F;bootstrap-roles&quot;,&quot;poststarthook&#x2F;scheduling&#x2F;bootstrap-system-priority-classes&quot;,&quot;poststarthook&#x2F;ca-registration&quot;,&quot;poststarthook&#x2F;start-kube-apiserver-admission-initializer&quot;</span><br><span class="line">I0705 22:38:02.646973   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;</span><br><span class="line">E0705 22:38:02.648065   18145 prometheus.go:55] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648116   18145 prometheus.go:68] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648155   18145 prometheus.go:82] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648181   18145 prometheus.go:96] failed to register workDuration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648208   18145 prometheus.go:112] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648232   18145 prometheus.go:126] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648256   18145 prometheus.go:152] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648279   18145 prometheus.go:164] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648323   18145 prometheus.go:176] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648401   18145 prometheus.go:188] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648426   18145 prometheus.go:203] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648449   18145 prometheus.go:216] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">I0705 22:38:02.648476   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:02.648483   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:02.650334   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.650349   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.650387   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.650508   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.656909   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:02.656960   18145 storage_factory.go:50] Storage caching is enabled for *apiregistration.APIService with capacity 1000</span><br><span class="line">I0705 22:38:02.657001   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:02.657086   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.657094   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.657250   18145 store.go:1343] Monitoring apiservices.apiregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.657324   18145 reflector.go:160] Listing and watching *apiregistration.APIService from storage&#x2F;cacher.go:&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.657709   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.657729   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.657761   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.657796   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.663447   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:02.663513   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.663518   18145 storage_factory.go:50] Storage caching is enabled for *apiregistration.APIService with capacity 1000</span><br><span class="line">W0705 22:38:02.663656   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.663610   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.663728   18145 store.go:1343] Monitoring apiservices.apiregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.663773   18145 reflector.go:160] Listing and watching *apiregistration.APIService from storage&#x2F;cacher.go:&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.665260   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:02.666477   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:02.674592   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:02.674627   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:03.616355   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:03.616397   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:04.268801   18145 deprecated_insecure_serving.go:53] Serving insecurely on 127.0.0.1:8073</span><br><span class="line">I0705 22:38:04.268882   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;,&quot;poststarthook&#x2F;bootstrap-controller&quot;,&quot;poststarthook&#x2F;rbac&#x2F;bootstrap-roles&quot;,&quot;poststarthook&#x2F;scheduling&#x2F;bootstrap-system-priority-classes&quot;,&quot;poststarthook&#x2F;ca-registration&quot;,&quot;poststarthook&#x2F;start-kube-apiserver-admission-initializer&quot;,&quot;poststarthook&#x2F;start-kube-aggregator-informers&quot;,&quot;poststarthook&#x2F;apiservice-registration-controller&quot;,&quot;poststarthook&#x2F;apiservice-status-available-controller&quot;,&quot;poststarthook&#x2F;apiservice-openapi-controller&quot;,&quot;poststarthook&#x2F;kube-apiserver-autoregistration&quot;,&quot;autoregister-completion&quot;</span><br><span class="line">I0705 22:38:04.269477   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269524   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269546   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269551   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269572   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269582   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269487   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269624   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269643   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269667   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269626   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269685   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;policy&#x2F;v1beta1</span><br><span class="line">I0705 22:38:04.269694   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269711   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269754   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269755   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.269648   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269805   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269807   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269843   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269863   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.269740   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269905   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269926   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1</span><br><span class="line">I0705 22:38:04.269501   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270002   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270029   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269615   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270116   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270137   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270197   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;persistentvolumes?limit&#x3D;500&amp;resourceVersion&#x3D;0: (896.994µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47384]</span><br><span class="line">I0705 22:38:04.270317   18145 wrap.go:47] GET &#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (810.468µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47382]</span><br><span class="line">I0705 22:38:04.269636   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270387   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270408   18145 secure_serving.go:116] Serving securely on [::]:6443</span><br><span class="line">I0705 22:38:04.269501   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270438   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270446   18145 apiservice_controller.go:94] Starting APIServiceRegistrationController</span><br><span class="line">I0705 22:38:04.270455   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270468   18145 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller</span><br><span class="line">I0705 22:38:04.270497   18145 wrap.go:47] GET &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses?limit&#x3D;500&amp;resourceVersion&#x3D;0: (944.078µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47386]</span><br><span class="line">I0705 22:38:04.270522   18145 crdregistration_controller.go:112] Starting crd-autoregister controller</span><br><span class="line">I0705 22:38:04.270535   18145 controller_utils.go:1029] Waiting for caches to sync for crd-autoregister controller</span><br><span class="line">I0705 22:38:04.270538   18145 available_controller.go:376] Starting AvailableConditionController</span><br><span class="line">I0705 22:38:04.270559   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;replicationcontrollers?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.21442ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47388]</span><br><span class="line">I0705 22:38:04.270572   18145 naming_controller.go:288] Starting NamingConditionController</span><br><span class="line">I0705 22:38:04.270575   18145 establishing_controller.go:73] Starting EstablishingController</span><br><span class="line">I0705 22:38:04.270652   18145 nonstructuralschema_controller.go:191] Starting NonStructuralSchemaConditionController</span><br><span class="line">I0705 22:38:04.270550   18145 crd_finalizer.go:255] Starting CRDFinalizer</span><br><span class="line">I0705 22:38:04.270509   18145 autoregister_controller.go:140] Starting autoregister controller</span><br><span class="line">I0705 22:38:04.270683   18145 cache.go:32] Waiting for caches to sync for autoregister controller</span><br><span class="line">I0705 22:38:04.270523   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:04.270694   18145 wrap.go:47] GET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.172166ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47392]</span><br><span class="line">I0705 22:38:04.270730   18145 controller.go:81] Starting OpenAPI AggregationController</span><br><span class="line">I0705 22:38:04.270745   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:04.270567   18145 customresource_discovery_controller.go:208] Starting DiscoveryController</span><br><span class="line">I0705 22:38:04.270566   18145 cache.go:32] Waiting for caches to sync for AvailableConditionController controller</span><br><span class="line">I0705 22:38:04.270560   18145 controller.go:83] Starting OpenAPI controller</span><br><span class="line">I0705 22:38:04.270815   18145 reflector.go:122] Starting reflector *v1.Node (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.269778   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270831   18145 reflector.go:160] Listing and watching *v1.Node from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271226   18145 reflector.go:122] Starting reflector *v1.VolumeAttachment (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271245   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;nodes?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.966293ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47378]</span><br><span class="line">I0705 22:38:04.271268   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.271289   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.271294   18145 reflector.go:122] Starting reflector *v1.ClusterRoleBinding (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271313   18145 reflector.go:160] Listing and watching *v1.ClusterRoleBinding from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271226   18145 reflector.go:122] Starting reflector *v1.Role (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271330   18145 reflector.go:160] Listing and watching *v1.Role from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271335   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;services?limit&#x3D;500&amp;resourceVersion&#x3D;0: (2.065372ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47380]</span><br><span class="line">I0705 22:38:04.271277   18145 reflector.go:122] Starting reflector *v1.Service (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271421   18145 reflector.go:160] Listing and watching *v1.Service from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271299   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1</span><br><span class="line">I0705 22:38:04.271246   18145 reflector.go:160] Listing and watching *v1.VolumeAttachment from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.270926   18145 reflector.go:122] Starting reflector *v1.ClusterRole (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271521   18145 reflector.go:160] Listing and watching *v1.ClusterRole from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271547   18145 reflector.go:122] Starting reflector *v1.Namespace (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271560   18145 reflector.go:160] Listing and watching *v1.Namespace from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271568   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m1s</span><br><span class="line">I0705 22:38:04.271011   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.271591   18145 reflector.go:122] Starting reflector *v1.StorageClass (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271628   18145 reflector.go:160] Listing and watching *v1.StorageClass from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.270877   18145 reflector.go:122] Starting reflector *apiextensions.CustomResourceDefinition (5m0s) from k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271679   18145 reflector.go:160] Listing and watching *apiextensions.CustomResourceDefinition from k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271690   18145 reflector.go:122] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271728   18145 reflector.go:160] Listing and watching *v1.ServiceAccount from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271618   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.271772   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.271831   18145 reflector.go:122] Starting reflector *v1.LimitRange (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271845   18145 reflector.go:122] Starting reflector *v1.RoleBinding (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271870   18145 reflector.go:160] Listing and watching *v1.RoleBinding from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271856   18145 reflector.go:160] Listing and watching *v1.LimitRange from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271105   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.59494ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47390]</span><br><span class="line">I0705 22:38:04.272020   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;persistentvolumes, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m27s</span><br><span class="line">I0705 22:38:04.271112   18145 reflector.go:122] Starting reflector *v1.PersistentVolume (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271077   18145 reflector.go:122] Starting reflector *v1.Secret (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272079   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272104   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272086   18145 reflector.go:160] Listing and watching *v1.Secret from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272125   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.271432   18145 reflector.go:122] Starting reflector *v1.Endpoints (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272173   18145 reflector.go:160] Listing and watching *v1.Endpoints from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271027   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272219   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272236   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;policy&#x2F;v1beta1</span><br><span class="line">E0705 22:38:04.272283   18145 controller.go:148] Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: &#x2F;registry&#x2F;masterleases&#x2F;10.129.173.93, ResourceVersion: 0, AdditionalErrorMsg: </span><br><span class="line">I0705 22:38:04.271034   18145 reflector.go:122] Starting reflector *v1.ResourceQuota (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272332   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m58s</span><br><span class="line">I0705 22:38:04.272351   18145 reflector.go:160] Listing and watching *v1.ResourceQuota from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272413   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.904677ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:04.272069   18145 reflector.go:160] Listing and watching *v1.PersistentVolume from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272410   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m16s</span><br><span class="line">I0705 22:38:04.271906   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.535252ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:04.270860   18145 reflector.go:122] Starting reflector *apiregistration.APIService (30s) from k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271255   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272581   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272618   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272540   18145 reflector.go:160] Listing and watching *apiregistration.APIService from k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.272651   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272689   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272550   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272736   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272749   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272763   18145 reflector.go:122] Starting reflector *v1.Pod (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272781   18145 reflector.go:160] Listing and watching *v1.Pod from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272707   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272807   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272709   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272823   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272852   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;replicationcontrollers, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;9m32s</span><br><span class="line">I0705 22:38:04.272924   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;nodes, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m3s</span><br><span class="line">I0705 22:38:04.273029   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;9m31s</span><br><span class="line">I0705 22:38:04.273029   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;services, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m33s</span><br><span class="line">I0705 22:38:04.274175   18145 wrap.go:47] GET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (4.688192ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47394]</span><br><span class="line">I0705 22:38:04.276744   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.276763   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.276773   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.276915   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m28s</span><br><span class="line">I0705 22:38:04.278832   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;pods?fieldSelector&#x3D;status.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&amp;limit&#x3D;500&amp;resourceVersion&#x3D;0: (9.407581ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47396]</span><br><span class="line">I0705 22:38:04.289292   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.289312   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.289321   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.289495   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;pods, rv&#x3D;75099443 labels&#x3D; fields&#x3D;status.phase!&#x3D;Failed,status.phase!&#x3D;Succeeded timeout&#x3D;7m21s</span><br><span class="line">I0705 22:38:05.268153   18145 controller.go:107] OpenAPI AggregationController: Processing item </span><br><span class="line">I0705 22:38:05.268190   18145 controller.go:130] OpenAPI AggregationController: action for item : Nothing (removed from the queue).</span><br><span class="line">I0705 22:38:05.268201   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000000</span><br><span class="line">I0705 22:38:05.268208   18145 controller.go:130] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).</span><br><span class="line">I0705 22:38:05.268215   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:05.268278   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:05.268310   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:05.268358   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:05.268389   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:05.268397   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:06.268539   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:06.268663   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:06.268687   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:06.268752   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:06.268776   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:06.268785   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.268914   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:07.269013   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.269030   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.269080   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:07.269105   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.269113   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.554461   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.554491   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:07.554501   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:07.556815   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.426905ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:08.269219   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:08.269327   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.269343   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:08.269394   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:08.269425   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.269434   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:08.419039   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.419077   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:08.419089   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:08.421916   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.948807ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:09.269557   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:09.269664   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:09.269680   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:09.269747   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:09.269778   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:09.269786   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:10.269881   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:10.269989   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:10.270004   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:10.270056   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:10.270083   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:10.270092   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.270200   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:11.270306   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.270321   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.270373   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:11.270398   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.270408   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.483133   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.483166   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:11.483178   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:11.485580   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.529963ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:11.598249   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.598284   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:11.598294   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:11.601107   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.928152ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:12.270550   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:12.270681   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:12.270698   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:12.270752   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:12.270801   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:12.270809   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.270887   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:13.270984   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.271000   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.271050   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:13.271074   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.271083   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.693554   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.693619   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:13.693637   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:13.696146   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.666525ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:14.271182   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:14.271290   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.271305   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:14.271356   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:14.271380   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.271398   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:14.380002   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.380035   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:14.380045   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:14.382250   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.312715ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:15.271510   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:15.271590   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:15.271616   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:15.271664   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:15.271688   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:15.271696   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:16.271827   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:16.271937   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:16.271980   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:16.272042   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:16.272065   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:16.272073   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.272204   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:17.272313   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.272329   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.272380   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:17.272405   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.272414   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.283099   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.283123   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:17.283133   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:17.285196   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.159257ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:17.870495   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.870526   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:17.870543   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:17.873004   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.577282ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:18.272560   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:18.272685   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:18.272701   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:18.272760   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:18.272786   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:18.272794   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.272924   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:19.273020   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.273035   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.273084   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:19.273106   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.273116   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.659566   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.659613   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:19.659624   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:19.662463   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.967004ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:20.273261   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:20.273367   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:20.273382   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:20.273429   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:20.273451   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:20.273459   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:21.273614   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:21.273740   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:21.273759   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:21.273840   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:21.273866   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:21.273878   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.126431   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.126472   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:22.126482   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:22.129022   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.675659ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:22.273990   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:22.274084   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.274097   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.274146   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:22.274168   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.274176   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.957210   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.957246   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:22.957255   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:22.959473   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.335823ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:23.274299   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:23.274407   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:23.274422   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:23.274471   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:23.274492   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:23.274500   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.274621   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:24.274734   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.274749   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.274801   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:24.274824   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.274832   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.618447   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.618488   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:24.618498   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:24.621739   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (3.351212ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:25.274963   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:25.275074   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:25.275092   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:25.275145   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:25.275181   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:25.275190   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.275293   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:26.275395   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.275410   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.275465   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:26.275488   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.275497   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.925578   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.925641   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:26.925652   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:26.927894   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.404426ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:27.275645   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:27.275777   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:27.275793   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:27.275846   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:27.275868   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:27.275877   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.276002   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:28.276107   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.276122   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.276172   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:28.276194   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.276203   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.664065   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.664097   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:28.664107   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:28.666395   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.399236ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:29.276334   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:29.276422   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:29.276436   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:29.276489   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:29.276522   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:29.276531   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.276644   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:30.276774   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.276790   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.276843   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:30.276868   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.276877   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.616203   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.616236   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:30.616246   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:30.618555   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.432457ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:31.277014   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:31.277123   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:31.277138   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:31.277186   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:31.277207   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:31.277216   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.277331   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:32.277437   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.277452   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.277515   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:32.277542   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.277551   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.511091   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.511128   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:32.511138   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:32.513369   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.359242ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:33.277687   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:33.277802   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:33.277817   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:33.277883   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:33.277906   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:33.277915   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:34.270818   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:34.271625   18145 trace.go:81] Trace[916838159]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271124387 +0800 CST m&#x3D;+3.485745147) (total time: 30.000454926s):</span><br><span class="line">Trace[916838159]: [30.000454926s] [30.000454926s] END</span><br><span class="line">E0705 22:38:34.271664   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Node: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;nodes?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271749   18145 trace.go:81] Trace[1068818673]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271318857 +0800 CST m&#x3D;+3.485939616) (total time: 30.000409266s):</span><br><span class="line">Trace[1068818673]: [30.000409266s] [30.000409266s] END</span><br><span class="line">I0705 22:38:34.271762   18145 trace.go:81] Trace[1453832293]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271338371 +0800 CST m&#x3D;+3.485959213) (total time: 30.00039864s):</span><br><span class="line">Trace[1453832293]: [30.00039864s] [30.00039864s] END</span><br><span class="line">E0705 22:38:34.271784   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Role: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;roles?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271764   18145 trace.go:81] Trace[1415476892]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271426707 +0800 CST m&#x3D;+3.486047513) (total time: 30.000302054s):</span><br><span class="line">Trace[1415476892]: [30.000302054s] [30.000302054s] END</span><br><span class="line">E0705 22:38:34.271817   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Service: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;services?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">E0705 22:38:34.271764   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ClusterRoleBinding: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterrolebindings?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271863   18145 trace.go:81] Trace[1943398002]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27156591 +0800 CST m&#x3D;+3.486186674) (total time: 30.000282292s):</span><br><span class="line">Trace[1943398002]: [30.000282292s] [30.000282292s] END</span><br><span class="line">E0705 22:38:34.271873   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Namespace: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;namespaces?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271862   18145 trace.go:81] Trace[2040039466]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271529276 +0800 CST m&#x3D;+3.486150039) (total time: 30.000321445s):</span><br><span class="line">Trace[2040039466]: [30.000321445s] [30.000321445s] END</span><br><span class="line">E0705 22:38:34.271884   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ClusterRole: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterroles?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271906   18145 trace.go:81] Trace[1482887971]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271448982 +0800 CST m&#x3D;+3.486069767) (total time: 30.000436887s):</span><br><span class="line">Trace[1482887971]: [30.000436887s] [30.000436887s] END</span><br><span class="line">E0705 22:38:34.271923   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.VolumeAttachment: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;volumeattachments?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271952   18145 trace.go:81] Trace[1817353754]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271633058 +0800 CST m&#x3D;+3.486253814) (total time: 30.000305162s):</span><br><span class="line">Trace[1817353754]: [30.000305162s] [30.000305162s] END</span><br><span class="line">E0705 22:38:34.271961   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.StorageClass: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272034   18145 trace.go:81] Trace[1163666912]: &quot;Reflector k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27168685 +0800 CST m&#x3D;+3.486307613) (total time: 30.000322616s):</span><br><span class="line">Trace[1163666912]: [30.000322616s] [30.000322616s] END</span><br><span class="line">E0705 22:38:34.272081   18145 reflector.go:125] k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117: Failed to list *apiextensions.CustomResourceDefinition: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;apiextensions.k8s.io&#x2F;v1beta1&#x2F;customresourcedefinitions?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272069   18145 trace.go:81] Trace[821616895]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271737097 +0800 CST m&#x3D;+3.486357868) (total time: 30.0003199s):</span><br><span class="line">Trace[821616895]: [30.0003199s] [30.0003199s] END</span><br><span class="line">E0705 22:38:34.272096   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ServiceAccount: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;serviceaccounts?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272148   18145 trace.go:81] Trace[1495176524]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271923158 +0800 CST m&#x3D;+3.486543922) (total time: 30.000215143s):</span><br><span class="line">Trace[1495176524]: [30.000215143s] [30.000215143s] END</span><br><span class="line">E0705 22:38:34.272162   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.LimitRange: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;limitranges?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272297   18145 trace.go:81] Trace[1304373005]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271876523 +0800 CST m&#x3D;+3.486497285) (total time: 30.000397052s):</span><br><span class="line">Trace[1304373005]: [30.000397052s] [30.000397052s] END</span><br><span class="line">E0705 22:38:34.272308   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.RoleBinding: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;rolebindings?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272401   18145 trace.go:81] Trace[2092646535]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27211704 +0800 CST m&#x3D;+3.486737797) (total time: 30.000268938s):</span><br><span class="line">Trace[2092646535]: [30.000268938s] [30.000268938s] END</span><br><span class="line">E0705 22:38:34.272410   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Secret: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;secrets?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272483   18145 trace.go:81] Trace[475362393]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272182677 +0800 CST m&#x3D;+3.486803437) (total time: 30.000287299s):</span><br><span class="line">Trace[475362393]: [30.000287299s] [30.000287299s] END</span><br><span class="line">E0705 22:38:34.272490   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Endpoints: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;endpoints?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272688   18145 trace.go:81] Trace[990205759]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272462183 +0800 CST m&#x3D;+3.487082944) (total time: 30.000213936s):</span><br><span class="line">Trace[990205759]: [30.000213936s] [30.000213936s] END</span><br><span class="line">E0705 22:38:34.272697   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.PersistentVolume: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;persistentvolumes?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272692   18145 trace.go:81] Trace[24141869]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27236239 +0800 CST m&#x3D;+3.486983158) (total time: 30.000315126s):</span><br><span class="line">Trace[24141869]: [30.000315126s] [30.000315126s] END</span><br><span class="line">E0705 22:38:34.272707   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ResourceQuota: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;resourcequotas?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272982   18145 trace.go:81] Trace[868760541]: &quot;Reflector k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272649159 +0800 CST m&#x3D;+3.487269923) (total time: 30.000318724s):</span><br><span class="line">Trace[868760541]: [30.000318724s] [30.000318724s] END</span><br><span class="line">E0705 22:38:34.272991   18145 reflector.go:125] k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117: Failed to list *apiregistration.APIService: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;apiregistration.k8s.io&#x2F;v1&#x2F;apiservices?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.273086   18145 trace.go:81] Trace[1379481655]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272785699 +0800 CST m&#x3D;+3.487406462) (total time: 30.000282688s):</span><br><span class="line">Trace[1379481655]: [30.000282688s] [30.000282688s] END</span><br><span class="line">E0705 22:38:34.273100   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Pod: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;pods?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">F0705 22:38:34.275033   18145 controller.go:157] Unable to perform initial IP allocation check: unable to refresh the service IP block: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;services: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.303248   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:34.341834   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:34.341849   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:34.341904   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:34.341926   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:34.341934   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/kubernetes/kubernetes/issues/82067">https://github.com/kubernetes/kubernetes/issues/82067</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;p&gt;三个 master （etcd 也在 master 上，master上也有 kubelet）和 n 个 node。maste</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="systemd" scheme="http://zhangguanzhang.github.io/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>Job for docker.service canceled</title>
    <link href="http://zhangguanzhang.github.io/2021/07/05/systemctl-start-docker-canceled/"/>
    <id>http://zhangguanzhang.github.io/2021/07/05/systemctl-start-docker-canceled/</id>
    <published>2021-07-05T17:08:06.000Z</published>
    <updated>2021-07-05T17:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>内部安装 docker 的脚本报错 docker 安装失败。然后启动发现下面奇怪的问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line">$ systemctl start docker</span><br><span class="line">Job for docker.service canceled.</span><br></pre></td></tr></table></figure><p>但是用 <code>service docker start</code> 能启动，这就很迷，尝试前台启动也无啥错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ service docker start</span><br><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Mon 2021-07-05 14:41:30 CST; 15s ago</span><br><span class="line">     Docs: http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line">$ systemctl cat docker</span><br><span class="line"># &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Docker Application Container Engine</span><br><span class="line">Documentation&#x3D;http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment&#x3D;&quot;PATH&#x3D;&#x2F;data&#x2F;kube&#x2F;bin:&#x2F;bin:&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;sbin&quot;</span><br><span class="line">ExecStart&#x3D;&#x2F;data&#x2F;kube&#x2F;bin&#x2F;dockerd </span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;sh -c &#39;&#x2F;sbin&#x2F;iptables --wait -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;sh -c &#39;&#x2F;sbin&#x2F;iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecReload&#x3D;&#x2F;bin&#x2F;kill -s HUP $MAINPID</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">RestartSec&#x3D;5</span><br><span class="line">LimitNOFILE&#x3D;infinity</span><br><span class="line">LimitNPROC&#x3D;infinity</span><br><span class="line">LimitCORE&#x3D;infinity</span><br><span class="line">Delegate&#x3D;yes</span><br><span class="line">KillMode&#x3D;process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>从上面<code>systemctl cat docker</code>看是没有依赖服务的，如果官方<code>rpm</code> 包安装的会依赖<code>containerd</code>。不过先看下失败的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl --failed</span><br><span class="line">  UNIT       LOAD   ACTIVE SUB    DESCRIPTION                                                                                                                                                                                              </span><br><span class="line">● data.mount loaded failed failed &#x2F;data                                                                                                                                                                                                    </span><br><span class="line"></span><br><span class="line">LOAD   &#x3D; Reflects whether the unit definition was properly loaded.</span><br><span class="line">ACTIVE &#x3D; The high-level unit activation state, i.e. generalization of SUB.</span><br><span class="line">SUB    &#x3D; The low-level unit activation state, values depend on unit type.</span><br><span class="line"></span><br><span class="line">1 loaded units listed. Pass --all to see loaded but inactive units, too.</span><br><span class="line">To show all installed unit files use &#39;systemctl list-unit-files&#39;.</span><br></pre></td></tr></table></figure><p>信息被冲没了，后面拿其他机器信息复制下，<code>systemctl start</code>会连 dbus 之类的，而 service 不会，结合前面的 data 挂载失败，系统应该是 <code>emergency</code> 半启动导致的，看了下果然</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status emergency </span><br><span class="line">● emergency.service - Emergency Shell</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;emergency.service; static; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Mon 2021-07-05 17:18:28 CST; 5min ago</span><br><span class="line">     Docs: man:sulogin(8)</span><br><span class="line">  Process: 674 ExecStartPre&#x3D;&#x2F;bin&#x2F;plymouth --wait quit (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 675 (systemd-sulogin)</span><br><span class="line">    Tasks: 5 (limit: 4915)</span><br><span class="line">   Memory: 32.9M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;emergency.service</span><br><span class="line">           ├─647 &#x2F;sbin&#x2F;sulogin</span><br><span class="line">           ├─675 &#x2F;lib&#x2F;systemd&#x2F;systemd-sulogin-shell emergency</span><br><span class="line">           ├─676 &#x2F;sbin&#x2F;sulogin</span><br><span class="line">           ├─677 bash</span><br><span class="line">           └─678 &#x2F;sbin&#x2F;sulogin</span><br><span class="line"></span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: Started Emergency Shell.</span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: emergency.service: Found left-over process 647 (sulogin) in control group while starting unit. Ignoring.</span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.</span><br></pre></td></tr></table></figure><p>然后看了下<code>/etc/fstab</code>，是把<code>defaults</code>写成了<code>default</code>导致的无法挂载。然后解决重启后好了。</p><p>询问了测试人员，她说她改了<code>/etc/fstab</code>后看启动<code>emergency mode</code>的输入root密码提示，然后输入root密码进去。然后启动 sshd 失败，然后用 <code>service sshd start</code>。然后 ssh 上去部署docker，然后我们这边 ssh 上来看，之前接触的centos 版本在 emergency 模式貌似不会有网。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;故障现象&quot;&gt;&lt;a href=&quot;#故障现象&quot; class=&quot;headerlink&quot; title=&quot;故障现象&quot;&gt;&lt;/a&gt;故障现象&lt;/h2&gt;&lt;p&gt;内部安装 docker 的脚本报错 docker 安装失败。然后启动发现下面奇怪的问题:&lt;/p&gt;
&lt;figure class</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="systemd" scheme="http://zhangguanzhang.github.io/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>openshift 4.5.9 etcd损坏+脑裂修复过程</title>
    <link href="http://zhangguanzhang.github.io/2021/06/08/ocp4.5.9-restore-etcd/"/>
    <id>http://zhangguanzhang.github.io/2021/06/08/ocp4.5.9-restore-etcd/</id>
    <published>2021-06-08T18:36:06.000Z</published>
    <updated>2021-06-08T18:36:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h2><p>内部机器和环境都是在 vcenter 里，之前的 ocp 集群是 3 master + 1 worker，也就是之前的<a href="./ocp-4.5-install.md">openshift 4.5.9 离线安装</a>后的环境，后面有几台宿主机负载太高，同事看我机器负载最高，关了几台，这几天需要用下 <code>openshift</code> 环境。登录到 <code>bastion</code> 上 get 超时，看了下 haproxy 的 stat web，全部红了。。然后把所有机器开机后发现还是起不来。</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>openshift 的 master 节点和 kubeadm 很像，几个组件都是 <code>staticPod</code> 形式起的。客户端也不是 <code>docker</code>，使用 <code>crictl</code> 就行了</p><h3 id="查看-kube-apiserver"><a href="#查看-kube-apiserver" class="headerlink" title="查看 kube-apiserver"></a>查看 kube-apiserver</h3><p>ssh 到 master1 上，查看日志发现是 etcd 无法起来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh -i ~/.ssh/new_rsa core@10.x.45.251</span><br><span class="line">crictl ps -a | grep kube-apiserver</span><br><span class="line">crictl logs xxx</span><br></pre></td></tr></table></figure><p>etcd 只有一台正常，一台日志报错 snap 文件损坏，一台报错 revision 太低，先进入正常的那台上面 etcd 容器（下文所有 etcdctl 都是在 etcd 容器里执行的，进容器就是下面命令）:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | grep etcd</span><br><span class="line">crictl <span class="built_in">exec</span> -ti xxx bash</span><br><span class="line"></span><br><span class="line">$ env | grep ETCDCTL</span><br><span class="line">ETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master2.openshift4.example.com.crt</span><br><span class="line">ETCDCTL_ENDPOINTS=https://10.x.45.251:2379,https://10.x.45.252:2379,https://10.x.45.222:2379</span><br><span class="line">ETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span><br><span class="line">ETCDCTL_API=3</span><br><span class="line">ETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master2.openshift4.example.com.key</span><br><span class="line"></span><br><span class="line">$ etcd --version</span><br><span class="line">etcd Version: 3.4.9</span><br><span class="line">Git SHA: 4657b9e</span><br><span class="line">Go Version: go1.13.4</span><br><span class="line">Go OS/Arch: linux/amd64</span><br></pre></td></tr></table></figure><h3 id="故障的开始"><a href="#故障的开始" class="headerlink" title="故障的开始"></a>故障的开始</h3><p>这里有个知识点就是 etcd 和 etcdctl 的一些命令行选项都可以被环境变量替代，例如上面的这些。忘了从哪个版本开始了。 <code>etcdctl snapshot save </code> 时候 <code>endpoints</code> 只能指定一个节点，尝试备份，结果卡住：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl snapshot save 0608-etcd.db</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623124958.5931418,<span class="string">&quot;caller&quot;</span>,<span class="string">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;create temporary db file&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;0608-etcd.db.part&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="一点进展"><a href="#一点进展" class="headerlink" title="一点进展"></a>一点进展</h3><p>这套集群当初以为就用一下，没考虑备份。然后在机器上乱逛，发现了高版本集群是自带了备份的（至少我这个版本是自带了）。在目录 <code>/etc/kubernetes/rollbackcopy</code> 下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/kubernetes/</span><br><span class="line">$ ll rollbackcopy/*</span><br><span class="line">rollbackcopy/currentVersion.latest:</span><br><span class="line">total 707152</span><br><span class="line">-rw-r--r--. 1 root root        58 Jun  8 09:17 backupenv.json</span><br><span class="line">-rw-------. 1 root root 724054048 Jun  8 09:17 snapshot_2021-06-08_091655.db</span><br><span class="line">-rw-------. 1 root root     59426 Jun  8 09:17 static_kuberesources_2021-06-08_091655.tar.gz</span><br><span class="line"></span><br><span class="line">rollbackcopy/currentVersion.prev:</span><br><span class="line">total 707152</span><br><span class="line">-rw-r--r--. 1 root root        58 Jun  8 08:11 backupenv.json</span><br><span class="line">-rw-------. 1 root root 724054048 Jun  8 08:11 snapshot_2021-06-08_081148.db</span><br><span class="line">-rw-------. 1 root root     59426 Jun  8 08:11 static_kuberesources_2021-06-08_081148.tar.gz</span><br></pre></td></tr></table></figure><p>然后像用 etcdctl 恢复备份，从容器里拷贝出来，结果 crictl 没 cp 命令，查看了下 etcdctl 的挂载，想进 etcd 容器里把 etcdctl 复制到挂载的路径上，这样宿主机上就有了。结果搞出来之后，习惯性的把二进制文件移到<code>/usr/local/bin/</code>下，结果 tab 按键补全看到有下面几个脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">cluster-backup.sh  cluster-restore.sh  recover-kubeconfig.sh</span><br></pre></td></tr></table></figure><h4 id="自带的备份恢复"><a href="#自带的备份恢复" class="headerlink" title="自带的备份恢复"></a>自带的备份恢复</h4><p>查看了下 <code>cluster-restore.sh</code> 脚本，脚本第一个参数是指定备份目录，也就是上面发现的目录，<strong>尝试在不正常的两个节点上</strong> 运行下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/kubernetes/rollbackcopy/currentVersion.latest</span><br><span class="line">$ cluster-restore.sh .</span><br><span class="line">...stopping kube-apiserver-pod.yml</span><br><span class="line">...stopping kube-controller-manager-pod.yml</span><br><span class="line">...stopping kube-scheduler-pod.yml</span><br><span class="line">...stopping etcd-pod.yml</span><br><span class="line">Waiting <span class="keyword">for</span> container etcd to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container etcdctl to stop</span><br><span class="line">...................................complete</span><br><span class="line">Waiting <span class="keyword">for</span> container etcd-metrics to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-controller-manager to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-apiserver to stop</span><br><span class="line">.........................complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-scheduler to stop</span><br><span class="line">complete</span><br><span class="line">starting restore-etcd static pod</span><br><span class="line">starting kube-apiserver-pod.yml</span><br><span class="line">static-pod-resource/kube-apiserver-pod-50/kube-apiserver-pod.yaml</span><br><span class="line">starting kube-controller-manager-pod.yml</span><br><span class="line">static-pod-resource/kube-controller-manager-pod-7/kube-controller-manager-pod.yml</span><br><span class="line">starting kube-scheduler-pod.yml</span><br><span class="line">static-pod-resource/kube-scheduler-pod-7/kube-scheduler-pod.yml</span><br></pre></td></tr></table></figure><p>这个脚本运行期间的等待容器停止要根据实际情况可能需要自己去手动stop，可以使用下面的去 stop 相关容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">STATIC_POD_CONTAINERS=(<span class="string">&quot;etcd&quot;</span> <span class="string">&quot;etcdctl&quot;</span> <span class="string">&quot;etcd-metrics&quot;</span> <span class="string">&quot;kube-controller-manager&quot;</span> <span class="string">&quot;kube-apiserver&quot;</span> <span class="string">&quot;kube-scheduler&quot;</span>)</span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">wait_for_containers_to_stop</span></span>()&#123;</span><br><span class="line">  <span class="built_in">local</span> CONTAINERS=(<span class="string">&quot;<span class="variable">$@</span>&quot;</span>) ctrID</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> NAME <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;CONTAINERS[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Waiting for container <span class="variable">$&#123;NAME&#125;</span> to stop&quot;</span></span><br><span class="line">    ctrID=<span class="string">&quot;<span class="subst">$(crictl ps --label io.kubernetes.container.name=$&#123;NAME&#125; -q)</span>&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$ctrID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">      crictl stop <span class="variable">$ctrID</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">wait_for_containers_to_stop <span class="variable">$&#123;STATIC_POD_CONTAINERS[*]&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行完后的状态:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ crictl ps -a</span><br><span class="line">CONTAINER           IMAGE                                                              CREATED             STATE               NAME                                          ATTEMPT             POD ID</span><br><span class="line">44f09a621803e       5e0c1e21da05b4b6455632cb70d9e29c76a5710e0bfa129ebef00d1cc1d5ee85   39 seconds ago      Running             etcd                                          0                   ad5b9a73adb3a</span><br><span class="line">b728530b11ea7       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-scheduler                                1                   af1e63ab536fd</span><br><span class="line">ec9b583321b64       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-apiserver                                26                  f68ce6fa73ad6</span><br><span class="line">05431a2d159b9       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-controller-manager                       1                   cc1a03361154d</span><br><span class="line">4a1d734b14faf       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   36 minutes ago      Exited              kube-apiserver                                25                  f68ce6fa73ad6</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><code>kube-apiserver</code>起来了，然后能使用 oc 了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ oc get node</span><br><span class="line">NAME                             STATUS   ROLES           AGE    VERSION</span><br><span class="line">master1.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">master2.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">master3.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">worker1.openshift4.example.com   Ready    worker          259d   v1.18.3+6c42de8</span><br></pre></td></tr></table></figure><h3 id="etcd-的脑裂"><a href="#etcd-的脑裂" class="headerlink" title="etcd 的脑裂"></a>etcd 的脑裂</h3><p>然后我的开发 namespaces 下有个 pod pending，<code>kubectl</code> 删了下报错，大致是 etcd 删不掉啥的。然后看了下 etcd 的状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl endpoint status --write-out=table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 7ad933dac58f4549 |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   30260441 |           30260441 |        |</span><br><span class="line">| https://10.x.45.252:2379 | f4351098cae1d407 |   3.4.9 |  726 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   40195520 |           40195520 |        |</span><br><span class="line">| https://10.x.45.222:2379 | 2399ef0cea33ebf3 |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   50408739 |           50408739 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>没错，脑裂了。三个都找不到其他的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">7ad933dac58f4549, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl member list</span><br><span class="line">f7f6c198cb519536, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl member list</span><br><span class="line">ac62bec820f40228, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="处理脑裂"><a href="#处理脑裂" class="headerlink" title="处理脑裂"></a>处理脑裂</h4><h5 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h5><p>尝试下 <code>move-leader</code> 命令看看能否操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl move-leader 7ad933dac58f4549</span><br><span class="line">2021-06-08 02:59:35.782766 C | pkg/flags: conflicting environment variable <span class="string">&quot;ETCDCTL_ENDPOINTS&quot;</span> is shadowed by corresponding command-line flag (either <span class="built_in">unset</span> environment variable or <span class="built_in">disable</span> flag)</span><br></pre></td></tr></table></figure><p>说环境变量和命令行同时设置了 endpoints ，unset 下它后尝试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ unset ETCDCTL_ENDPOINTS</span><br><span class="line">$ export ETCDCTL_ENDPOINTS&#x3D;https:&#x2F;&#x2F;10.x.45.222:2379</span><br><span class="line">$ etcdctl move-leader 7ad933dac58f4549</span><br><span class="line">2021-06-08 03:00:11.019150 C | pkg&#x2F;flags: conflicting environment variable &quot;ETCDCTL_CERT&quot; is shadowed by corresponding command-line flag (either unset environment variable or disable flag)</span><br></pre></td></tr></table></figure><p>然后另一个变量报错，搜了下这个是 <code>etcdctl move-leader</code> 的 bug，见 <a href="https://github.com/etcd-io/etcd/pull/12757">pr</a> ，手动 unset 相关 <code>ETCDCTL_xxx</code> 变量后执行下报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl --endpoints https://10.x.45.222:2379 \</span><br><span class="line">  --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \</span><br><span class="line">  --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master3.openshift4.example.com.crt \</span><br><span class="line">  --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master3.openshift4.example.com.key \</span><br><span class="line">  move-leader 7ad933dac58f4549</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T03:07:08.767Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;endpoint://client-27e0f779-7d90-4be3-9491-f0e915374f3c/10.xxx.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = FailedPrecondition desc = etcdserver: bad leader transferee&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>好吧，<code>move-leader</code>对这种场景用不了。不过现在三个都起来了，应该是能备份了。准备在其他节点上用备份恢复下，先看了下 etcd staticPod 的 yaml <code>/etc/kubernetes/manifests/etcd-pod.yaml</code> 的内容启动参数是否需要调整：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line">        <span class="comment">#!/bin/sh</span></span><br><span class="line">        <span class="string">set</span> <span class="string">-euo</span> <span class="string">pipefail</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="string">if</span> [ <span class="string">!</span> <span class="string">-z</span> <span class="string">$(ls</span> <span class="string">-A</span> <span class="string">&quot;/var/lib/etcd&quot;</span><span class="string">)</span> ]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;please delete the contents of data directory before restoring, running the restore script will do this for you&quot;</span></span><br><span class="line">          <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if we have backup file to be restored</span></span><br><span class="line">        <span class="comment"># if the file exist, check if it has not changed size in last 5 seconds</span></span><br><span class="line">        <span class="string">if</span> [ <span class="string">!</span> <span class="string">-f</span> <span class="string">/var/lib/etcd-backup/snapshot.db</span> ]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;please make a copy of the snapshot db file, then move that copy to /var/lib/etcd-backup/snapshot.db&quot;</span></span><br><span class="line">          <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">else</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>从逻辑看是启动的时候如果数据目录必须不为空，然后<code>/var/lib/etcd-backup/</code>目录得存在备份的 db 文件，看了下挂载目录，宿主机上也是这个目录。打算先在第一个 master 节点的 etcd 容器里备份。然后用备份文件在其他节点恢复备份。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl  snapshot save /var/lib/etcd-backup/snapshot.db</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143517.6130972,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;created temporary db file&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;/var/lib/etcd-backup/snapshot.db.part&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:11:57.621Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/maintenance.go:200&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;opened snapshot stream; downloading&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143517.6213868,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:127&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;fetching snapshot&quot;</span>,<span class="string">&quot;endpoint&quot;</span>:<span class="string">&quot;https://10.x.45.251:2379&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:12:03.315Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/maintenance.go:208&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;completed snapshot read; closing&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143524.4544568,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:142&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;fetched snapshot&quot;</span>,<span class="string">&quot;endpoint&quot;</span>:<span class="string">&quot;https://10.x.45.251:2379&quot;</span>,<span class="string">&quot;size&quot;</span>:<span class="string">&quot;724 MB&quot;</span>,<span class="string">&quot;took&quot;</span>:6.841299825&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143524.4545853,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:152&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;saved&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;/var/lib/etcd-backup/snapshot.db&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>然后停掉后面的两台 master 节点的 etcd。改名 <code>/var/lib/etcd</code> 目录（不要一上来就删除目录，改名是永远最稳妥的手段）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/</span><br><span class="line">cp manifests/etcd-pod.yaml .</span><br><span class="line">mv manifests/etcd-pod.yaml /tmp/</span><br><span class="line"></span><br><span class="line">etcdID=<span class="string">&quot;<span class="subst">$(crictl ps --label io.kubernetes.container.name=etcd -q)</span>&quot;</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$etcdID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    crictl stop <span class="variable">$etcdID</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">mv /var/lib/etcd /var/lib/etcd-bak</span><br></pre></td></tr></table></figure><p>然后用密钥 ssh 到其他机器上把 ssh 的 root 和密码登录开了来让我们可以使用 scp 过去，然后备份文件<code>/var/lib/etcd-backup/snapshot.db</code> scp 过去到其余 master 上的同样路径。</p><h5 id="处理脑裂-1"><a href="#处理脑裂-1" class="headerlink" title="处理脑裂"></a>处理脑裂</h5><p>细心观察看前面的每个 endpoint 下 member list 是看到的自己的。所以是在 master1 上逐渐添加其他 member。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member add master2.openshift4.example.com --peer-urls=https://10.x.45.252:2380</span><br><span class="line">Member 3e27197aa4521ea0 added to cluster 1c2134e7d41c45b1</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;master2.openshift4.example.com&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;master2.openshift4.example.com=https://10.x.45.252:2380,master1.openshift4.example.com=https://10.x.45.251:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">&quot;https://10.x.45.252:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure><p>然后在第二个 master 上改下 etcd 的 staticPod yaml <code>/tmp/etcd-pod.yaml</code>。根据自身的实际更改，删掉备份和恢复相关的逻辑。主要是更改 <code>ETCD_INITIAL_CLUSTER</code> 为所有集群，格式为 <code>$&#123;name1&#125;=https://$&#123;ip1&#125;:2380,$&#123;name2&#125;=https://$&#123;ip2&#125;:2380...</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="comment">#export ETCD_INITIAL_CLUSTER=xxx</span></span><br><span class="line">        <span class="string">NAME_ETCD_ARRAY=()</span></span><br><span class="line">        <span class="string">for</span> <span class="string">i</span> <span class="string">in</span> <span class="string">$(env</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-Po</span> <span class="string">&#x27;(?&lt;=NODE_).+(?=_ETCD_NAME)&#x27;</span> <span class="string">|</span> <span class="string">sort</span> <span class="string">);do</span></span><br><span class="line">            <span class="string">etcd_name=NODE_$&#123;i&#125;_ETCD_NAME</span></span><br><span class="line">            <span class="string">url_host_var=NODE_$&#123;i&#125;_ETCD_URL_HOST</span></span><br><span class="line">            <span class="string">NAME_ETCD_ARRAY+=(&quot;$&#123;!etcd_name&#125;=https://$&#123;!url_host_var&#125;:2380&quot;)</span></span><br><span class="line">        <span class="string">done</span></span><br><span class="line">        <span class="string">export</span> <span class="string">ETCD_INITIAL_CLUSTER=$(</span> <span class="string">echo</span> <span class="string">$&#123;NAME_ETCD_ARRAY[*]&#125;</span> <span class="string">|</span> <span class="string">tr</span> <span class="string">&#x27; &#x27;</span> <span class="string">&#x27;,&#x27;</span> <span class="string">)</span></span><br></pre></td></tr></table></figure><p>改好后在 master2 上启动 etcd ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /tmp/etcd-pod.yaml /etc/kubernetes/manifests/</span><br></pre></td></tr></table></figure><p>在 master1 上查看:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, unstarted, , https://10.x.45.252:2380, , <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, unstarted, , https://10.x.45.252:2380, , <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:26:16.669Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;passthrough:///https://10.x.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \&quot;transport: Error while dialing dial tcp 10.x.45.222:2379: connect: connection refused\&quot;&quot;</span>&#125;</span><br><span class="line">Failed to get the status of endpoint https://10.x.45.222:2379 (context deadline exceeded)</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       1041 |               1041 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       1041 |               1041 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>好消息，然后恢复第三个，添加第三个member：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member add master3.openshift4.example.com --peer-urls=https://10.x.45.222:2380</span><br><span class="line">Member d319fa1cbb0e28fe added to cluster 1c2134e7d41c45b1</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;master3.openshift4.example.com&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;master2.openshift4.example.com=https://10.x.45.252:2380,master1.openshift4.example.com=https://10.x.45.251:2380,master3.openshift4.example.com=https://10.x.45.222:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">&quot;https://10.x.45.222:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure><p>第三个 etcd yaml 也像之前一样更改。启动后在持续观察：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:32:55.197Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;passthrough:///https://10.x.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;</span>&#125;</span><br><span class="line">Failed to get the status of endpoint https://10.x.45.222:2379 (context deadline exceeded)</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2388 |               2388 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2388 |               2388 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2593 |               2593 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2593 |               2593 |        |</span><br><span class="line">| https://10.x.45.222:2379 | d319fa1cbb0e28fe |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2596 |               2596 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">| https://10.x.45.222:2379 | d319fa1cbb0e28fe |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>单独看每个 endpoint 的 member list 正常否：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>然后看了下 node not ready了，approve 了所有 csr后就好了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc get csr</span><br><span class="line">oc adm certificate approve xxx</span><br></pre></td></tr></table></figure><p>然后测试了下，kubectl 能删掉 pod 了。后续等稳定后再手动备份下</p><h5 id="无法调度和-logs-报错-remote-error-tls-internal-error"><a href="#无法调度和-logs-报错-remote-error-tls-internal-error" class="headerlink" title="无法调度和 logs 报错 remote error: tls: internal error"></a>无法调度和 logs 报错 remote error: tls: internal error</h5><p>同时也无法调度，master 上去看 <code>kube-apiserver</code>日志刷：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authentication.go:53] Unable to authenticate the request due to an error: x509: certificate signed by unkown authority</span><br></pre></td></tr></table></figure><p>搜了下都没解决办法，最后自己在 master 上屏直觉找到解决办法了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> :;<span class="keyword">do</span></span><br><span class="line">  sleep 2</span><br><span class="line">  oc get csr -o name | xargs -r oc adm certificate approve</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>另一个窗口 ssh 到 master上停掉 <code>cert-syncer</code> 相关容器:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | awk <span class="string">&#x27;/Running/&amp;&amp;/-cert-syncer/&#123;print $1&#125;&#x27;</span> | xargs -r crictl stop</span><br></pre></td></tr></table></figure><h4 id="一些疑惑"><a href="#一些疑惑" class="headerlink" title="一些疑惑"></a>一些疑惑</h4><p>后面尝试自带的 yaml 文件+那个备份脚本恢复的就是脑裂集群，询问了个 4.7.13 集群的，看了下 etcd 的 yaml 文件是下面的。没有启动前恢复备份啥的了。可能我这个版本才存在这种问题吧。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">                    <span class="comment">#!/bin/sh</span></span><br><span class="line">                    <span class="string">set</span> <span class="string">-euo</span> <span class="string">pipefail</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">etcdctl</span> <span class="string">member</span> <span class="string">list</span> <span class="string">||</span> <span class="literal">true</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># this has a non-zero return code if the command is non-zero.  If you use an export first, it doesn&#x27;t and you</span></span><br><span class="line">                    <span class="comment"># will succeed when you should fail.</span></span><br><span class="line">                    <span class="string">ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--endpoints=$&#123;ALL_ETCD_ENDPOINTS&#125;</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--data-dir=/var/lib/etcd</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--target-peer-url-host=$&#123;NODE_master11_cluster_lonlife_dev_ETCD_URL_HOST&#125;</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--target-name=master11.cluster.lonlife.dev)</span></span><br><span class="line">                     <span class="string">export</span> <span class="string">ETCD_INITIAL_CLUSTER</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># we cannot use the \&quot;normal\&quot; port conflict initcontainer because when we upgrade, the existing static pod will never yield,</span></span><br><span class="line">                    <span class="comment"># so we do the detection in etcd container itsefl.</span></span><br><span class="line">                    <span class="string">echo</span> <span class="string">-n</span> <span class="string">\&quot;Waiting</span> <span class="string">for</span> <span class="string">ports</span> <span class="number">2379</span><span class="string">,</span> <span class="number">2380 </span><span class="string">and</span> <span class="number">9978 </span><span class="string">to</span> <span class="string">be</span> <span class="string">released.\&quot;</span></span><br><span class="line">                    <span class="string">while</span> [ <span class="string">-n</span> <span class="string">\&quot;$(ss</span> <span class="string">-Htan</span> <span class="string">&#x27;( sport = 2379 or sport = 2380 or sport = 9978 )&#x27;</span><span class="string">)\&quot;</span> ]<span class="string">;</span> <span class="string">do</span></span><br><span class="line">                      <span class="string">echo</span> <span class="string">-n</span> <span class="string">\&quot;.\&quot;</span></span><br><span class="line">                      <span class="string">sleep</span> <span class="number">1</span></span><br><span class="line">                    <span class="string">done</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">export</span> <span class="string">ETCD_NAME=$&#123;NODE_master11_cluster_lonlife_dev_ETCD_NAME&#125;</span></span><br><span class="line">                    <span class="string">env</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">ETCD</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-v</span> <span class="string">NODE</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">set</span> <span class="string">-x</span></span><br><span class="line">                    <span class="comment"># See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice</span></span><br><span class="line">                    <span class="string">exec</span> <span class="string">ionice</span> <span class="string">-c2</span> <span class="string">-n0</span> <span class="string">etcd</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--log-level=info</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--initial-advertise-peer-urls=https://$&#123;NODE_master11_cluster_lonlife_dev_IP&#125;:2380</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--client-cert-auth=true</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-client-cert-auth=true</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--advertise-client-urls=https://$&#123;NODE_master11_cluster_lonlife_dev_IP&#125;:2379</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-client-urls=https://0.0.0.0:2379</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-peer-urls=https://0.0.0.0:2380</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-metrics-urls=https://0.0.0.0:9978</span> <span class="string">||</span>  <span class="string">mv</span> <span class="string">/etc/kubernetes/etcd-backup-dir/etcd-member.yaml</span> <span class="string">/etc/kubernetes/manifests</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://etcd.io/docs/v3.4/op-guide/runtime-configuration/">etcd op guide</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/backup_and_restore/backing-up-etcd.html">backing-up-etcd</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言介绍&quot;&gt;&lt;a href=&quot;#前言介绍&quot; class=&quot;headerlink&quot; title=&quot;前言介绍&quot;&gt;&lt;/a&gt;前言介绍&lt;/h2&gt;&lt;p&gt;内部机器和环境都是在 vcenter 里，之前的 ocp 集群是 3 master + 1 worker，也就是之前的&lt;a </summary>
      
    
    
    
    
    <category term="openshift" scheme="http://zhangguanzhang.github.io/tags/openshift/"/>
    
    <category term="ocp" scheme="http://zhangguanzhang.github.io/tags/ocp/"/>
    
  </entry>
  
  <entry>
    <title>docker-ce 18.09.3 启动panic: invalid freelist page: 56, page type is leaf的解决处理</title>
    <link href="http://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/"/>
    <id>http://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/</id>
    <published>2021-05-26T19:52:37.000Z</published>
    <updated>2021-05-26T19:52:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>这个问题和之前的<a href="https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/">docker-18.06.3-ce启动panic: invalid page type: 0: 0的解决处理</a>差不多，不过 db 文件不同。客户停止 docker 后起不来了，查看日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe -u docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17+08:00&quot; level&#x3D;warning msg&#x3D;&quot;The \&quot;graph\&quot; config file option is deprecated. Please use \&quot;data-root\&quot; instead.&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.841082978+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not change group &#x2F;var&#x2F;run&#x2F;docker.sock to docker: group docker not found&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.858839019+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.btrfs&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.859857612+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.aufs&quot; error&#x3D;&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860062002+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.zfs&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860084186+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter zfs in metadata plugin&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860090453+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter btrfs in metadata plugin&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860095653+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter aufs in metadata plugin&quot; error&#x3D;&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.866980   19061 flags.go:33] FLAG: --container-runtime&#x3D;&quot;docker&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.866984   19061 flags.go:33] FLAG: --container-runtime-endpoint&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;dockershim.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867014   19061 flags.go:33] FLAG: --docker&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867018   19061 flags.go:33] FLAG: --docker-endpoint&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867022   19061 flags.go:33] FLAG: --docker-env-metadata-whitelist&#x3D;&quot;&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867025   19061 flags.go:33] FLAG: --docker-only&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867028   19061 flags.go:33] FLAG: --docker-root&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;docker&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867032   19061 flags.go:33] FLAG: --docker-tls&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867036   19061 flags.go:33] FLAG: --docker-tls-ca&#x3D;&quot;ca.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867039   19061 flags.go:33] FLAG: --docker-tls-cert&#x3D;&quot;cert.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867043   19061 flags.go:33] FLAG: --docker-tls-key&#x3D;&quot;key.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867138   19061 flags.go:33] FLAG: --experimental-dockershim&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867143   19061 flags.go:33] FLAG: --experimental-dockershim-root-directory&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;dockershim&quot;</span><br><span class="line">5月 26 18:42:18 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:18.208058217+08:00&quot; level&#x3D;error msg&#x3D;&quot;Failed to load container 90e5da1293b04c3eab30e9f7a2d5714aba563b2ab20d15c67aee1d1d79d3154c: json: cannot unmarshal number into Go value of type container.Container&quot;</span><br><span class="line">5月 26 18:42:18 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:18.214213879+08:00&quot; level&#x3D;error msg&#x3D;&quot;Failed to load container 99abceba7154b7dba08cc03bc1651fd18f38b0d485dcd2c195f479fc375a8216: invalid character &#39;e&#39; looking for beginning of value&quot;</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: panic: invalid freelist page: 56, page type is leaf</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: goroutine 1 [running]:</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*freelist).read(0xc424b121b0, 0x7fc46c7b1000)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;freelist.go:237 +0x2ff</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).loadFreelist.func1()</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:292 +0x12d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: sync.(*Once).Do(0xc420b70328, 0xc420fb1f58)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;sync&#x2F;once.go:44 +0xc0</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).loadFreelist(0xc420b701e0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:285 +0x50</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.Open(0xc422bddad0, 0x2b, 0x1a4, 0xc420fb2070, 0x20c754c, 0x23816c0, 0x25ceee8)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:262 +0x316</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb.(*BoltDB).getDBhandle(0xc424b10960, 0x32ef098, 0xc424b109a4, 0x7fc47c368e38)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb&#x2F;boltdb.go:113 +0x93</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb.(*BoltDB).List(0xc424b10960, 0xc422b99fe0, 0x1b, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb&#x2F;boltdb.go:274 +0xbb</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*cache).kmap(0xc424a60fe0, 0x26b6dc0, 0xc422a0e4d0, 0x4467ea, 0xc422b99f9a, 0x1a668ce)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;cache.go:43 +0x18a</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*cache).list(0xc424a60fe0, 0x26b6dc0, 0xc422a0e4d0, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;cache.go:164 +0x7b</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*datastore).List(0xc42352b180, 0xc422b99f80, 0x1b, 0x26b6dc0, 0xc422a0e4d0, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;datastore.go:517 +0x179</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).populateNetworks(0xc42203d5c0, 0x5, 0x1a6a443)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge_store.go:50 +0xe0</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).initStore(0xc42203d5c0, 0xc424b316b0, 0x0, 0xc423529bc0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge_store.go:35 +0x207</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).configure(0xc42203d5c0, 0xc424b316b0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge.go:378 +0x1b4</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.Init(0x268abc0, 0xc424119740, 0xc424b316b0, 0x0, 0xffffffffffffffff)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge.go:161 +0xa2</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drvregistry.(*DrvRegistry).AddDriver(0xc424119740, 0x1a6a539, 0x6, 0x266fe78, 0xc424b316b0, 0xc42264df80, 0x8)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drvregistry&#x2F;drvregistry.go:72 +0x48</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork.New(0xc42264df80, 0x9, 0x10, 0xc4209e03f0, 0xc420c7f620, 0xc42264df80, 0x9)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;controller.go:220 +0x4a5</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).initNetworkController(0xc420a281e0, 0xc42087d900, 0xc420c7f620, 0xc420a281e0, 0xc42239c740, 0xc420c7f620, 0xc420c7f5f0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon_unix.go:807 +0xa9</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).restore(0xc420a281e0, 0xc4209b8300, 0xc4208e4790)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:419 +0xd6f</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.NewDaemon(0x26a6240, 0xc4209b8300, 0xc42087d900, 0xc4209e03f0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:987 +0x2c4b</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.(*DaemonCli).start(0xc42024b890, 0xc42018f320, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;daemon.go:180 +0x74f</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.runDaemon(0xc42018f320, 0xc420876200, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker_unix.go:7 +0x47</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.newDaemonCommand.func1(0xc42087ac80, 0x32ef098, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:29 +0x5d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).execute(0xc42087ac80, 0xc42003a1e0, 0x0, 0x0, 0xc42087ac80, 0xc42003a1e0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:762 +0x46a</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).ExecuteC(0xc42087ac80, 0x267bb40, 0x2254580, 0x267bb50)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:852 +0x30c</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).Execute(0xc42087ac80, 0xc42000e020, 0x4d067f)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:800 +0x2d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.main()</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:70 +0xa2</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: docker.service: main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENT</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: Unit docker.service entered failed state.</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: docker.service failed.</span><br></pre></td></tr></table></figure><p>首先得根据这个 panic 的堆栈，调用关系是 <code>main.main</code> -&gt; <code>cobra</code> -&gt; <code>docker daemon</code> -&gt; <code>daemon.(*Daemon).restore</code> -&gt; <code>initNetworkController</code> -&gt; <code>libnetwork/datastore/cache</code> -&gt; <code>boltdb</code><br>docker 使用了 boltdb 存储了网络信息成 <code>db</code> 文件，但是这个 db 文件损坏了，导致读取字节序列化错误类型，去 docker 的目录 find 下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ find &#x2F;data&#x2F;kube&#x2F;docker -type f -size -5M -name &#39;*.db&#39; | grep -v overlay2</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;volumes&#x2F;metadata.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;network&#x2F;files&#x2F;local-kv.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;builder&#x2F;fscache.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;snapshots.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;metadata.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;cache.db</span><br></pre></td></tr></table></figure><p>改名 db 文件重启 docker 解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /data/kube/docker/network/files/local-kv.db&#123;,.bak&#125;</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这个问题和之前的&lt;a href=&quot;https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/&quot;&gt;docker-18.06.3-ce启动panic: invalid page type</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/categories/kubernetes/docker/"/>
    
    <category term="panic" scheme="http://zhangguanzhang.github.io/categories/kubernetes/docker/panic/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>一次单节点单个pod网络问题排查过程</title>
    <link href="http://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/"/>
    <id>http://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/</id>
    <published>2021-04-30T11:28:30.000Z</published>
    <updated>2021-04-30T11:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="about"><a href="#about" class="headerlink" title="about"></a>about</h2><p>现场反馈客户环境上业务不正常，根据调用链去看某个业务A日志，发现无法请求另一个业务B，把业务 A 的探针取消了，加上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tty: true</span><br><span class="line">command: [&quot;bash&quot;]</span><br></pre></td></tr></table></figure><p>起来后进去 curl 了下 B 对应的 svcIP 接口是能通的。然后手动起业务进程，再开个窗口 exec 进去 curl 发现就不通了，k8s node数量是只有一个，并且只有这一个 pod 有问题。后面排查到是用户的安全软件导致的。软件名是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep agent</span><br><span class="line">root       6349  0.3  0.1 21046316 116820 ?     Sl   11:08   0:02 &#x2F;CloudResetPwdUpdateAgent&#x2F;depend&#x2F;jre1.8.0_232&#x2F;bin&#x2F;java -Dorg.tanukisoftware.wrapper.WrapperSimpleApp.maxStartMainWait&#x3D;40 -Djava.library.path&#x3D;..&#x2F;lib -classpath ..&#x2F;lib&#x2F;resetpwdupdateagent.jar:..&#x2F;lib&#x2F;wrapper.jar:..&#x2F;lib&#x2F;json-20160810.jar:..&#x2F;lib&#x2F;log4j-api-2.8.2.jar:..&#x2F;lib&#x2F;log4j-core-2.8.2.jar -Dwrapper.key&#x3D;osxWGEBk6yYtP6sr -Dwrapper.backend&#x3D;pipe -Dwrapper.disable_console_input&#x3D;TRUE -Dwrapper.pid&#x3D;6019 -Dwrapper.version&#x3D;3.5.26 -Dwrapper.native_library&#x3D;wrapper -Dwrapper.arch&#x3D;x86 -Dwrapper.service&#x3D;TRUE -Dwrapper.cpu.timeout&#x3D;10 -Dwrapper.jvmid&#x3D;1 org.tanukisoftware.wrapper.WrapperSimpleApp CloudResetPwdUpdateAgent</span><br><span class="line">root      13860 76.1  0.3 796288 253072 ?       Sl   11:08   8:27 &#x2F;usr&#x2F;local&#x2F;dbappsecurity&#x2F;edr&#x2F;agent_service runservice</span><br><span class="line">root      14188  0.0  0.0  46004  6000 ?        S    11:08   0:00 &#x2F;usr&#x2F;local&#x2F;dbappsecurity&#x2F;edr&#x2F;agent_daemon</span><br><span class="line">root      17399  0.0  0.0 112712   976 pts&#x2F;0    S+   11:19   0:00 grep --color&#x3D;auto agent</span><br><span class="line">root      22206  0.0  0.0  22496  1448 ?        S    11:08   0:00 vm-agent</span><br><span class="line">root      22215  0.1  0.0 628744  4104 ?        Sl   11:08   0:01 vm-agent</span><br></pre></td></tr></table></figure><p>杀掉 <code>dbappsecurity</code> 两个进程后重建业务 A 的 pod 后就正常了。</p><p>之前也遇到过安全软件导致 pod 网络通信异常 eof 的，列举一些国产遇到过的软件软件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ds_agent  # 查下 agent 关键字</span><br><span class="line">qaxsafed  # 奇安信，查下 qax 看看有没有其他的</span><br><span class="line">secdog    # 也查下 dog 和 sec</span><br><span class="line">sangfor_watchdog # 这个不影响，但是有它基本是深信服的虚拟化环境，会和flannel的8472端口冲突</span><br><span class="line">YDservice</span><br><span class="line">Symantec</span><br><span class="line">start360su_safed   # 推荐 ps aux | grep safe 先查下</span><br><span class="line">gov_defence_service</span><br><span class="line">gov_defence_guard</span><br><span class="line">ics_agent</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;about&quot;&gt;&lt;a href=&quot;#about&quot; class=&quot;headerlink&quot; title=&quot;about&quot;&gt;&lt;/a&gt;about&lt;/h2&gt;&lt;p&gt;现场反馈客户环境上业务不正常，根据调用链去看某个业务A日志，发现无法请求另一个业务B，把业务 A 的探针取消了，加上</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubelet 和 runc 编译关闭 kmem</title>
    <link href="http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/"/>
    <id>http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/</id>
    <published>2021-04-08T17:28:30.000Z</published>
    <updated>2021-04-08T17:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提详情"><a href="#前提详情" class="headerlink" title="前提详情"></a>前提详情</h2><p>在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修复。</p><p>网上有言论说升级 Linux 内核至 <code>kernel-3.10.0-1075.el7</code> 及以上就可以修复这个问题，详细可见 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1507149#c101">slab leak causing a crash when using kmem control group</a>。但是我测试了下面的都不行：</p><ul><li>CentOS7.4</li><li>CentOS7.6</li><li>CentOS7.7的 3.10.0-1062.el7.x86_64 </li><li>CentOS Linux release 7.8.2003 (Core) - 3.10.0-1127.el7.x86_64</li></ul><p>Linux其余发行版内核如果大于等于 4.4 应该没问题。<br>这里我们编译 kubelet 关闭 kmem。</p><h2 id="准备条件"><a href="#准备条件" class="headerlink" title="准备条件"></a>准备条件</h2><p>这里我们使用的编译参数会使用容器编译的，不需要宿主机上安装 golang，安装个 docker 就行了。</p><ol><li><p><code>1c 4g</code> 的机器，这里我是使用 <code> CentOS 7.8.2003 (Core)</code><br>机器配置 2g 内存的时候编译提示 oom，升级到 4g 内存才编译成功的。</p></li><li><p>最好安装最新版本的docker</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable --now firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -ri &#39;&#x2F;^[^#]*SELINUX&#x3D;&#x2F;s#&#x3D;.+$#&#x3D;disabled#&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"></span><br><span class="line">cat&gt;&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;custom.conf&lt;&lt;EOF</span><br><span class="line">*       soft    nproc   131072</span><br><span class="line">*       hard    nproc   131072</span><br><span class="line">*       soft    nofile  131072</span><br><span class="line">*       hard    nofile  131072</span><br><span class="line">root    soft    nproc   131072</span><br><span class="line">root    hard    nproc   131072</span><br><span class="line">root    soft    nofile  131072</span><br><span class="line">root    hard    nofile  131072</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat&lt;&lt;EOF &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;docker.conf</span><br><span class="line"># 要求iptables对bridge的数据进行处理</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-arptables &#x3D; 1</span><br><span class="line"># 开启转发</span><br><span class="line">net.ipv4.ip_forward &#x3D; 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line">curl -fsSL &quot;https:&#x2F;&#x2F;get.docker.com&#x2F;&quot; | \</span><br><span class="line">  sed -r &#39;&#x2F;add-repo \$yum_repo&#x2F;a sed -i &quot;s#https:&#x2F;&#x2F;download.docker.com#http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce#&quot; &#x2F;etc&#x2F;yum.repos.d&#x2F;docker-*.repo &#39; | \</span><br><span class="line">    bash -s -- --mirror Aliyun</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker&#x2F;</span><br><span class="line">cat&gt;&#x2F;etc&#x2F;docker&#x2F;daemon.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;bip&quot;: &quot;172.17.0.1&#x2F;16&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https:&#x2F;&#x2F;fz5yth0r.mirror.aliyuncs.com&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;dockerhub.mirrors.nwafu.edu.cn&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;reg-mirror.qiniu.com&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;</span><br><span class="line">cat&gt;&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;10-docker.conf&lt;&lt;EOF</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#39;&#x2F;sbin&#x2F;iptables --wait -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#39;&#x2F;sbin&#x2F;iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y epel-release bash-completion </span><br><span class="line">cp &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;completions&#x2F;docker &#x2F;etc&#x2F;bash_completion.d&#x2F;</span><br><span class="line"></span><br><span class="line">systemctl enable --now docker</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><h3 id="确定版本"><a href="#确定版本" class="headerlink" title="确定版本"></a>确定版本</h3><p>查看下我们目前使用的 kubelet 版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"> kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>安装编译的基础依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y rsync make</span><br></pre></td></tr></table></figure><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><p>这里我们使用容器编译，所以下载到啥地方都行，也不需要安装 go。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">cd kubernetes</span><br><span class="line">git checkout v1.15.5</span><br></pre></td></tr></table></figure><h3 id="前提操作"><a href="#前提操作" class="headerlink" title="前提操作"></a>前提操作</h3><p>查看 cross 镜像的版本号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat build&#x2F;build-image&#x2F;cross&#x2F;VERSION</span><br><span class="line">v1.12.10-1</span><br></pre></td></tr></table></figure><p>拉国内的镜像，然后改名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull registry.aliyuncs.com&#x2F;k8sxio&#x2F;kube-cross:v1.12.10-1</span><br><span class="line">$ docker tag registry.aliyuncs.com&#x2F;k8sxio&#x2F;kube-cross:v1.12.10-1 k8s.gcr.io&#x2F;kube-cross:v1.12.10-1</span><br></pre></td></tr></table></figure><p>编译，这个参数测试在 v1.15.5 里可用，网上的 <code>make BUILDTAGS=&quot;nokmem&quot; WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=&quot;-N -l&quot;</code> 会无法编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在v1.15.5似乎无用</span><br><span class="line">.&#x2F;build&#x2F;run.sh make kubelet KUBE_BUILD_PLATFORMS&#x3D;linux&#x2F;amd64 BUILDTAGS&#x3D;&quot;nokmem&quot;</span><br><span class="line"># 用下面的</span><br><span class="line">.&#x2F;build&#x2F;run.sh make kubelet GOFLAGS&#x3D;&quot;-v -tags&#x3D;nokmem&quot; KUBE_BUILD_PLATFORMS&#x3D;linux&#x2F;amd64</span><br></pre></td></tr></table></figure><p>查看编译完成的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7 kubernetes]# ls -l _output&#x2F;dockerized&#x2F;bin&#x2F;linux&#x2F;amd64&#x2F;</span><br><span class="line">total 202880</span><br><span class="line">-rwxr-xr-x 1 root root   9203530 Apr  7 22:02 conversion-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9207908 Apr  7 22:02 deepcopy-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9156147 Apr  7 22:02 defaulter-gen</span><br><span class="line">-rwxr-xr-x 1 root root   4709220 Apr  7 22:02 go2make</span><br><span class="line">-rwxr-xr-x 1 root root   2894872 Apr  7 22:03 go-bindata</span><br><span class="line">-rwxr-xr-x 1 root root 157545104 Apr  7 22:13 kubelet</span><br><span class="line">-rwxr-xr-x 1 root root  15018430 Apr  7 22:03 openapi-gen</span><br></pre></td></tr></table></figure><h2 id="runc-关闭-kmem"><a href="#runc-关闭-kmem" class="headerlink" title="runc 关闭 kmem"></a>runc 关闭 kmem</h2><p>v1.0.0-rc94起 kmem 设置就被忽略了</p><h3 id="1-0-0-rc10-版本"><a href="#1-0-0-rc10-版本" class="headerlink" title="1.0.0-rc10 版本"></a>1.0.0-rc10 版本</h3><p><a href="https://cloud.tencent.com/developer/article/1743789">https://cloud.tencent.com/developer/article/1743789</a> 这个文章里说了 runc 也需要关闭</p><p>如果下面命令能成功执行则说明 runc 没关闭 kmem</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --name test --kernel-memory 100M nginx:alpine</span><br></pre></td></tr></table></figure><p><code>19.03.14</code> 测试发现可以运行，说明没有关闭，查看它的 <code>runc</code> 版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><p>根据 commit 跳转</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;opencontainers&#x2F;runc&#x2F;commit&#x2F;dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br></pre></td></tr></table></figure><p>根据这个 commit，找到了是 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10">https://github.com/opencontainers/runc/tree/v1.0.0-rc10</a> 这个 tag，</p><p>编译支持的 tag 见 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags">https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags</a><br>编译参数从 <a href="https://github.com/opencontainers/runc/blob/v1.0.0-rc10/script/release.sh#L30">release 脚本</a> 找到是下面的参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make BUILDTAGS=&quot;seccomp selinux apparmor&quot; static</span><br><span class="line"><span class="meta">#</span><span class="bash"> 后面的新版本貌似默认的 tags 是 seccomp 了</span></span><br></pre></td></tr></table></figure><p>下载源码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/opencontainers/runc.git</span><br><span class="line">cd runc</span><br><span class="line">git checkout v1.0.0-rc10</span><br></pre></td></tr></table></figure><p>直接上面的编译参数是无法编译成功的，因为很多依赖都是 <code>ubuntu</code>下面的。看了下 <code>Makefile</code> 里面提供了一个起 ubuntu 的容器，我们可以进去编译。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make shell</span><br></pre></td></tr></table></figure><p>直接 <code>make shell</code> 的话，它第一步是 <code>make runcimage</code> 会先构建镜像，然后用这个镜像起一个容器，构建的最后一步会失败，因为下面的 <code>busybox</code> 的 <code>rootfs</code> 下载地址变动了，我们得 <code>hack</code> 下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">get_busybox()&#123;</span><br><span class="line">case $(go env GOARCH) in</span><br><span class="line">arm64)</span><br><span class="line">echo &#39;https:&#x2F;&#x2F;github.com&#x2F;docker-library&#x2F;busybox&#x2F;raw&#x2F;dist-arm64v8&#x2F;glibc&#x2F;busybox.tar.xz&#39;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &#39;https:&#x2F;&#x2F;github.com&#x2F;docker-library&#x2F;busybox&#x2F;raw&#x2F;dist-amd64&#x2F;glibc&#x2F;busybox.tar.xz&#39;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>hack 之前我们先测试下，获取下输出的镜像名是<code>runc_dev:HEAD</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make shell</span></span><br><span class="line">docker build  -t runc_dev:HEAD .</span><br><span class="line">^Cmake: *** [runcimage] Interruptaemon  557.1kB</span><br></pre></td></tr></table></figure><p>因为过程会取 git 的一些信息，为了不影响，我们先拷贝文件 <code>tests/integration/multi-arch.bash</code> 和 <code>Dockerfile</code>。我们先手动构建出镜像，再删除掉这俩文件保持 git status。</p><p>先修改 <code>Dockerfile</code> 让它使用 <code>tests/integration/multi-arch.bash.new</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp Dockerfile Dockerfile.new</span><br><span class="line">vi Dockerfile.new </span><br><span class="line">tail -n2 Dockerfile.new</span><br><span class="line">RUN . tests&#x2F;integration&#x2F;multi-arch.bash.new \</span><br><span class="line">    &amp;&amp; curl -o- -sSL &#96;get_busybox&#96; | tar xfJC - $&#123;ROOTFS&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>新文件下载地址可以在 <a href="https://github.com/opencontainers/runc/blob/bb28c44f12bf24ea64590edfb4f23a4b4d2eaae8/tests/integration/get-images.sh#L59">master 分支最新的脚本里</a> 找到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get &quot;$BUSYBOX_IMAGE&quot; \</span><br><span class="line">&quot;https://github.com/docker-library/busybox/raw/dist-$&#123;arch&#125;/stable/glibc/busybox.tar.xz&quot;</span><br></pre></td></tr></table></figure><p>按照下面修改好</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp  tests/integration/multi-arch.bash tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> grep busybox tests/integration/multi-arch.bash.new</span></span><br><span class="line">get_busybox()&#123;</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-arm64v8/glibc/busybox.tar.xz&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">&#x27;https://github.com/docker-library/busybox/raw/dist-amd64/glibc/busybox.tar.xz&#x27;</span></span></span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-amd64/stable/glibc/busybox.tar.xz&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>手动编译镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build  -t runc_dev:HEAD -f Dockerfile.new .</span><br></pre></td></tr></table></figure><p>移走文件，保持 <code>git status</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv Dockerfile.new tests/integration/multi-arch.bash.new /tmp</span><br></pre></td></tr></table></figure><p>进入容器里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -ti --privileged --rm -v $PWD:/go/src/github.com/opencontainers/runc runc_dev:HEAD bash</span><br></pre></td></tr></table></figure><p>编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make BUILDTAGS=<span class="string">&quot;seccomp selinux apparmor nokmem&quot;</span> static</span></span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o runc .</span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o contrib/cmd/recvtty/recvtty ./contrib/cmd/recvtty</span><br></pre></td></tr></table></figure><p>查看信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod u+x runc</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ldd runc</span></span><br><span class="line">not a dynamic executable</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><h2 id="查看-kmem-开启"><a href="#查看-kmem-开启" class="headerlink" title="查看 kmem 开启"></a>查看 kmem 开启</h2><p>环境信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux 82.174-zh 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br><span class="line">$ cat &#x2F;etc&#x2F;redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core)</span><br></pre></td></tr></table></figure><p>判断 cgroup kernel memory 是否激活的方式。查看对应 POD container 下的 <code>memory.kmem.slabinfo</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /sys/fs/cgroup/memory/kubepods</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有内容，说明kubelet开了 kmem</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat memory.kmem.slabinfo</span></span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br></pre></td></tr></table></figure><p>pod 目录下面的容器目录或者<code>/sys/fs/cgroup/memory/docker/&lt;uuid&gt;</code>如果有 <code>memory.kmem.slabinfo</code> 则说明 <code>runc</code> 没关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -l | grep pod</span></span><br><span class="line">drwxr-xr-x  5 root root 0 4月   8 11:27 pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28</span><br><span class="line">drwxr-xr-x  4 root root 0 4月   8 11:26 pod64def35b-b44e-410d-9782-745bd47834ca</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/ | grep -E <span class="string">&#x27;^d&#x27;</span></span></span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:28 1b37f9f78f93546e3e4407f03aa84c92e95c99655467e62814ae17e0a0e68686</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 a578004f702d7b20d4b08d49c08cbb6c3ef2b3d08a62f087f5c7be0d022d9d9d</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326/memory.kmem.slabinfo</span> </span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br><span class="line">taskstats              0      0    328   24    2 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">shmem_inode_cache    216    216    680   24    4 : tunables    0    0    0 : slabdata      9      9      0</span><br><span class="line">inode_cache          162    162    592   27    4 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">Acpi-ParseExt        112    112     72   56    1 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">selinux_inode_security      0      0     40  102    1 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">RAWv6                  0      0   1216   26    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">UDP                    0      0   1088   30    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-8192           0      0   8192    4    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">net_namespace          0      0   5184    6    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">pid_namespace          0      0   2200   14    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">mqueue_inode_cache      0      0    896   36    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-2048         112    112   2048   16    8 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-32           768    768     32  128    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">kmalloc-512          128    128    512   32    4 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">kmalloc-128          256    256    128   32    1 : tunables    0    0    0 : slabdata      8      8      0</span><br><span class="line">kmalloc-8           6144   6144      8  512    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">anon_vma             306    306     80   51    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">idr_layer_cache      165    165   2112   15    8 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">vm_area_struct       259    259    216   37    2 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">mnt_cache            231    231    384   21    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">mm_struct             20     20   1600   20    8 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">signal_cache           0      0   1152   28    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">sighand_cache          0      0   2112   15    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">files_cache           50     50    640   25    4 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">kernfs_node_cache    108    108    112   36    1 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-192          273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">task_xstate          156    156    832   39    8 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">task_struct           24     24   4048    8    8 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-1024         160    160   1024   32    8 : tunables    0    0    0 : slabdata      5      5      0</span><br><span class="line">kmalloc-64           768    768     64   64    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">sock_inode_cache      75     75    640   25    4 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">proc_inode_cache     264    264    656   24    4 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">dentry               273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">kmalloc-16          2816   2816     16  256    1 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">kmalloc-96           294    294     96   42    1 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-256          352    352    256   32    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">shared_policy_node     85     85     48   85    1 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">kmalloc-4096         104    104   4096    8    8 : tunables    0    0    0 : slabdata     13     13      0</span><br></pre></td></tr></table></figure><p><code>memory.kmem.slabinfo</code>里有内容说明是开启的</p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>只有在 pod 配置了 memory limit 的时候才打开 memory accounting，即 kmem。我们下面利用 flannel pod测试下，先手动创建 cgroup</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ grep memory &#x2F;proc&#x2F;cgroups </span><br><span class="line">memory8871</span><br><span class="line"></span><br><span class="line">$ mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;test</span><br><span class="line">$ for i in &#96;seq 1 65535&#96;;do mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;test&#x2F;test-$&#123;i&#125;; done</span><br><span class="line">$ grep memory &#x2F;proc&#x2F;cgroups </span><br><span class="line">memory8655131</span><br></pre></td></tr></table></figure><p>释放出三个，删除当前节点的 flannel，可以创建出来，然后再删除新的，无法创建出来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rmdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;test&#x2F;test-&#123;1..3&#125;</span><br><span class="line">$ kubectl -n kube-system delete pod kube-flannel-ds-z2cgq</span><br><span class="line">....</span><br><span class="line">  Warning  FailedCreatePodContainer  2s (x4 over 35s)  kubelet, 10.13.82.174  unable to ensure pod container exists: failed to create container for [kubepods burstable pod5a41f53f-5ce8-4123-8199-1a865219f297] : mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;kubepods&#x2F;burstable&#x2F;pod5a41f53f-5ce8-4123-8199-1a865219f297: no space left on device</span><br></pre></td></tr></table></figure><p>替换编译好的后，先关闭 <code>kubelet</code> 和 <code>docker</code>， 关闭自启动 <code>systemctl disable docker kubelet</code>。reboot 后，查看目录 <code>/sys/fs/cgroup/memory/</code> 下的 <code>kubepods</code> 是不是不存在。然后启动 docker 和 kubelet。等带内存 limit 的 flannel 调度上来后下面命令查看。输出是 <code>Input/output error</code> 说明已经关闭了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;kubepods&#x2F; -name memory.kmem.slabinfo -exec cat &#123;&#125;  \;</span><br></pre></td></tr></table></figure><p>还有种方法是看 slab 的个数，删除 limit 的 pod后等重建看看数量增长否：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls &#x2F;sys&#x2F;kernel&#x2F;slab  | wc -l</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://eddycjy.com/posts/why-container-memory-exceed2/">https://eddycjy.com/posts/why-container-memory-exceed2/</a></li><li><a href="https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub:+unable+to+allocate+memory+on+node+-1">https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub%3A+unable+to+allocate+memory+on+node+-1</a></li><li><a href="https://github.com/kubernetes/kubernetes/blob/v1.20.6/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem_disabled.go">https://github.com/kubernetes/kubernetes/blob/v1.20.6/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem_disabled.go</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前提详情&quot;&gt;&lt;a href=&quot;#前提详情&quot; class=&quot;headerlink&quot; title=&quot;前提详情&quot;&gt;&lt;/a&gt;前提详情&lt;/h2&gt;&lt;p&gt;在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="kmem" scheme="http://zhangguanzhang.github.io/categories/k8s/kmem/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>iptables --wait -t nat -A DOCKER...: iptables NO chain/target/match by that name</title>
    <link href="http://zhangguanzhang.github.io/2021/03/23/iptables-docker-no-chain/"/>
    <id>http://zhangguanzhang.github.io/2021/03/23/iptables-docker-no-chain/</id>
    <published>2021-03-23T17:42:08.000Z</published>
    <updated>2021-03-23T17:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>我们内部有套部署的工具， 我们部署的流程是先在部署机器（部署机器可能也是node1 ）上用脚本安装好 docker，然后进容器里去起我们部署平台，有个很久的 bug 就是，部署机器上端口映射起容器会有如下报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables --wait -t nat -A DOCKER -p tcp -d 0&#x2F;8 --dport 8089 -j DNAT --to-destination 172.25.0.2:80 ! -i docker0: iptables NO chain&#x2F;target&#x2F;match by that name</span><br></pre></td></tr></table></figure><p>排查也很简单，缺少链，添加上即可:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables --wait -N DOCKER &amp;&gt;/dev/null || true</span><br><span class="line">sudo iptables --wait -t filter -N DOCKER &amp;&gt;/dev/null || true</span><br><span class="line">sudo iptables --wait -t nat  -N DOCKER &amp;&gt;/dev/null || true</span><br></pre></td></tr></table></figure><p>或者重启 docker daemon 会在启动的时候加上链。脚本安装 docker 的时候，我们执行了下面的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables --wait -P INPUT ACCEPT &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t nat &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t nat &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t raw &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t raw &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t mangle &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t mangle</span><br></pre></td></tr></table></figure><p>所以一开始是在安装 docker 前加的三个链，后面发现还是会出现。今天测试内部环境测试的时候百分之百复现了。</p><h2 id="引起原因"><a href="#引起原因" class="headerlink" title="引起原因"></a>引起原因</h2><p>和同事排查了下，确认是部署脚本里没有停止 firewalld ，后面执行 ansible 剧本的时候去停止的。整个流程是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">部署机器 ----安装docker----&gt; ansible 剧本 -----停止firewalld-----&gt; 检测到已经安装 docker 不操作</span><br><span class="line">没运行部署平台的机器    ----&gt; ansible 剧本 -----停止firewalld-----&gt; 安装 docker 并启动 docker daemon </span><br></pre></td></tr></table></figure><p>部署机器是先启动 docker daemon，然后后面剧本停止了 firewalld，而非部署机器是先停的 firewalld，再启动的 docker daemon。</p><p>手动测试了下发现停止 firewalld 会把 iptables 清空。于是部署脚本里提前把所有系统的 case 逻辑里把 firewalld 给停掉。</p><p>另外我们防止有些用户安装了 iptables 的 daemon，是这样停止的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld python-firewall firewalld-filesystem iptables &amp;&gt;&#x2F;dev&#x2F;null</span><br><span class="line">systemctl disable firewalld python-firewall firewalld-filesystem iptables &amp;&gt;&#x2F;dev&#x2F;null</span><br></pre></td></tr></table></figure><p>发现这样执行后 firewalld 还是 enabled 的状态，如果有不存在的 service （例如机器存在firewalld，后面几个都不存在）则整个 disable 会失效（全部没有 disable 成功）。改成了下面的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function Systemctl_Stop_Disable()&#123;</span><br><span class="line">    for target in $@;</span><br><span class="line">    do</span><br><span class="line">        sudo systemctl stop $target &amp;&gt;&#x2F;dev&#x2F;null || true;</span><br><span class="line">        sudo systemctl disable $target &amp;&gt;&#x2F;dev&#x2F;null || true;</span><br><span class="line">    done</span><br><span class="line">    true</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Systemctl_Stop_Disable firewalld python-firewall firewalld-filesystem iptables</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;我们内部有套部署的工具， 我们部署的流程是先在部署机器（部署机器可能也是node1 ）上用脚本安装好 docker，然后进容器里去起我们部署</summary>
      
    
    
    
    <category term="iptables" scheme="http://zhangguanzhang.github.io/categories/iptables/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Internal error occurred: jsonpatch add operation does not apply: doc is missing path: xxx</title>
    <link href="http://zhangguanzhang.github.io/2021/03/22/jsonpatch-doc-is-missing-path/"/>
    <id>http://zhangguanzhang.github.io/2021/03/22/jsonpatch-doc-is-missing-path/</id>
    <published>2021-03-22T20:42:08.000Z</published>
    <updated>2021-03-22T20:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>今天在折腾 admission webhook 注入一些属性的时候遇到了 <code>Error from server (InternalError): error when creating &quot;xxx.yml&quot;: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: &quot;/spec/template/spec/dnsConfig/options&quot;</code>。折腾半天才发现在代码里使用 jsonPatch 的话不能直接绕过结构体实例去 patch。 </p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>WebHook 接收和响应都是一个 AdmissionReview 对象，请求是里面的 AdmissionRequest，响应是里面的 AdmissionResponse。<br>比如说我们创建了个 <code>MutatingWebhookConfiguration</code> 指定让 <code>apps/v1</code> 的 <code>deploy</code> 传到我们的 webhook， 我们要修改 deploy 的一些属性，我个人是增加 dns 的 <code>single-request-reopen</code>的属性的，得在 <code>AdmissionResponse</code> 里传一个 jsonPatch的切片。<br>代码里这块是：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> patchOperation <span class="keyword">struct</span> &#123;</span><br><span class="line">Op    <span class="keyword">string</span>      <span class="string">`json:&quot;op&quot;`</span></span><br><span class="line">Path  <span class="keyword">string</span>      <span class="string">`json:&quot;path&quot;`</span></span><br><span class="line">Value <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:&quot;value,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">patch := []patchOperation&#123;</span><br><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/dnsConfig/options&quot;</span>,</span><br><span class="line">Value: []<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>&#123;</span><br><span class="line">&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;single-request-reopen&quot;</span>&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>后面发现创建 deploy 就报开头的错误。然后日志里打印了下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apiVersion:apps&#x2F;v1 resource: default&#x2F;nginx-deployment, AdmissionResponse: patch&#x3D;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;&#x2F;spec&#x2F;template&#x2F;spec&#x2F;dnsConfig&#x2F;options&quot;,&quot;value&quot;:[&#123;&quot;name&quot;:&quot;single-request-reopen&quot;&#125;]&#125;]</span><br></pre></td></tr></table></figure><p>然后把这个 patch 在 kubectl 上测试了下是可以的:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system patch deployments tiller-deploy --<span class="built_in">type</span>=json -p=<span class="string">&#x27;[&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/template/spec/dnsConfig/options&quot;,&quot;value&quot;:[&#123;&quot;name&quot;:&quot;single-request-reopen&quot;&#125;]&#125;]&#x27;</span></span></span><br><span class="line"></span><br><span class="line">deployment.extensions/tiller-deploy patched</span><br></pre></td></tr></table></figure><p>刚开始把 kube-apiserver 开 -v=8 发现 panic的信息，然后升了下版本还是没用。然后在源码里找了下这个报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> find -<span class="built_in">type</span> f -name <span class="string">&#x27;*.go&#x27;</span> -<span class="built_in">exec</span> grep -l <span class="string">&#x27;replace operation does not apply: doc is missing path&#x27;</span> &#123;&#125; \;</span></span><br><span class="line">./vendor/github.com/evanphx/json-patch/patch.go</span><br></pre></td></tr></table></figure><p>发现这个错误是引入的库抛出来的，和 k8s 无关，想了下后换个 key 试试看。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/securityContext/runAsNonRoot&quot;</span>,</span><br><span class="line">Value: <span class="literal">true</span>,</span><br><span class="line">&#125;,</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> deployment.Spec.Template.Spec.Containers &#123;</span><br><span class="line">patch = <span class="built_in">append</span>(patch, patchOperation&#123;</span><br><span class="line">Op:    <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path:  fmt.Sprintf(<span class="string">&quot;/spec/template/spec/containers/%d/lifecycle/postStart/exec/command&quot;</span>, i),</span><br><span class="line">Value: []<span class="keyword">string</span>&#123;</span><br><span class="line"><span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line"><span class="string">&quot;-c&quot;</span>,</span><br><span class="line"><span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现后面 <code>command</code> 这个也不行，explain 看了下，试试上层的看看：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> deployment.Spec.Template.Spec.Containers &#123;</span><br><span class="line">patch = <span class="built_in">append</span>(patch, patchOperation&#123;</span><br><span class="line">Op:    <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path:  fmt.Sprintf(<span class="string">&quot;/spec/template/spec/containers/%d/lifecycle&quot;</span>, i),</span><br><span class="line">Value: corev1.Lifecycle&#123;</span><br><span class="line">PostStart: &amp;corev1.Handler&#123;</span><br><span class="line">Exec: &amp;corev1.ExecAction&#123;</span><br><span class="line">Command: []<span class="keyword">string</span>&#123;</span><br><span class="line"><span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line"><span class="string">&quot;-c&quot;</span>,</span><br><span class="line"><span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现可以，应该是得给一个嵌套的结构体实例，而不是直接绕过这个结构体，去 <code>patch</code> 里面属性的 <code>value</code>，换下前面的 dnsConfig 试试:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/dnsConfig&quot;</span>,</span><br><span class="line">Value: corev1.PodDNSConfig&#123;</span><br><span class="line">Options:     []corev1.PodDNSConfigOption&#123;</span><br><span class="line">&#123;</span><br><span class="line">Name:  <span class="string">&quot;single-request-reopen&quot;</span>,</span><br><span class="line">Value: <span class="literal">nil</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>发现也可以了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook">https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;今天在折腾 admission webhook 注入一些属性的时候遇到了 &lt;code&gt;Error from server (Internal</summary>
      
    
    
    
    <category term="admission" scheme="http://zhangguanzhang.github.io/categories/admission/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>使用github action 配合 docker buildx 编译 arm64 docker-compose</title>
    <link href="http://zhangguanzhang.github.io/2021/03/12/build-arm64-docker-compose-action/"/>
    <id>http://zhangguanzhang.github.io/2021/03/12/build-arm64-docker-compose-action/</id>
    <published>2021-03-12T19:28:30.000Z</published>
    <updated>2021-03-12T19:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>git 上搜索了很多 docker-compose 的 arm64 的编译基本都是使用 <code>qemu-user-static</code> 之类的设置下后编译的，也看到过用特权容器启动 qemu-user-static 或者 <code>binfmt</code> 之类的，但是我自己机器上试了无效，貌似是因为我操作系统是低版本内核的 centos ，github 上搜了下，其他很多人的编译感觉太啰嗦了。就在 action 上整了下，测试是可用的，而且非常简单。</p><p><code>docker-practice/actions-setup-docker@master</code> 将会在在 action 的 runner 里安装 docker，创建 buildx 和 运行 <code>docker run --rm --privileged ghcr.io/dpsigs/tonistiigi-binfmt:latest --install all</code> 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">build</span> <span class="string">for</span> <span class="string">docker-compose</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span> [ <span class="string">master</span> ]</span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># - name: Check out docker/compose</span></span><br><span class="line">      <span class="comment">#   uses: actions/checkout@v2</span></span><br><span class="line">      <span class="comment">#   with:</span></span><br><span class="line">      <span class="comment">#     repository: docker/compose</span></span><br><span class="line">      <span class="comment">#     path: compose</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">install</span> <span class="string">docker</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">docker-practice/actions-setup-docker@master</span></span><br><span class="line">        <span class="comment"># this will run and buildx</span></span><br><span class="line">        <span class="comment"># docker run --rm --privileged ghcr.io/dpsigs/tonistiigi-binfmt:latest --install all</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build</span> <span class="string">docker-compose</span> </span><br><span class="line">        <span class="attr">id:</span> <span class="string">build</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">          <span class="string">ls</span> <span class="string">-l;</span> <span class="string">docker</span> <span class="string">run</span> <span class="string">--rm</span> <span class="string">arm64v8/python:3.7.10-stretch</span> <span class="string">sh</span> <span class="string">-c</span> <span class="string">&#x27;python3 -V; uname -m&#x27;</span></span><br><span class="line">          <span class="comment"># https://github.com/docker/compose/blob/master/script/build/linux</span></span><br><span class="line">          <span class="string">git</span> <span class="string">clone</span> <span class="string">https://github.com/docker/compose.git</span></span><br><span class="line">          <span class="string">cd</span> <span class="string">compose;</span></span><br><span class="line">          <span class="string">./script/clean;</span></span><br><span class="line">          <span class="string">git</span> <span class="string">checkout</span> <span class="number">1.28</span><span class="number">.5</span></span><br><span class="line">          <span class="string">DOCKER_COMPOSE_GITSHA=&quot;$(script/build/write-git-sha)&quot;;</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">----</span> <span class="string">$&#123;DOCKER_COMPOSE_GITSHA&#125;</span></span><br><span class="line">          <span class="string">docker</span> <span class="string">buildx</span> <span class="string">build</span> <span class="string">--platform</span> <span class="string">linux/arm64</span> <span class="string">.</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--target</span> <span class="string">bin</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--build-arg</span> <span class="string">DISTRO=debian</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--build-arg</span> <span class="string">GIT_COMMIT=&quot;$&#123;DOCKER_COMPOSE_GITSHA&#125;&quot;</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--output</span> <span class="string">dist/</span> <span class="string">||</span> <span class="string">:</span> <span class="string">;</span></span><br><span class="line">          <span class="string">ls</span> <span class="string">-l</span> <span class="string">dist;</span></span><br><span class="line">          <span class="string">docker</span> <span class="string">run</span> <span class="string">--platform</span> <span class="string">linux/arm64</span> <span class="string">\</span></span><br><span class="line">            <span class="string">--rm</span> <span class="string">-v</span> <span class="string">$PWD/dist:/root/</span> <span class="string">\</span></span><br><span class="line">            <span class="string">arm64v8/python:3.7.10-stretch</span> <span class="string">/root/docker-compose-linux-arm64</span> <span class="string">version;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Upload</span> <span class="string">bin</span> <span class="string">directory</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/upload-artifact@main</span></span><br><span class="line">        <span class="attr">if:</span> <span class="string">steps.build.outcome</span> <span class="string">==</span> <span class="string">&#x27;success&#x27;</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">docker-compose-linux</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">compose/dist/</span></span><br></pre></td></tr></table></figure><p>编译过程看 compose 仓库的 makefile，是运行的 <a href="https://github.com/docker/compose/blob/master/script/build/linux">https://github.com/docker/compose/blob/master/script/build/linux</a> 这个脚本。所以克隆 compose 仓库后进目录里，然后 checkout 指定 tag。官方的编译过程都是在 docker build 产生的容器里去编译的。最后有个 build –output就是直接把文件给整出来。我这里是用的 buildx 去替代 build 编译。理论上也可以编译其他架构的，我仓库已经自动化处理这个过程了。 <a href="https://github.com/zhangguanzhang/docker-compose-aarch64/releases%E3%80%82">https://github.com/zhangguanzhang/docker-compose-aarch64/releases。</a></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>银河麒麟 v10 系统，架构 arm64</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ arch</span><br><span class="line">aarch64</span><br><span class="line">$ cat &#x2F;etc&#x2F;os-release </span><br><span class="line">NAME&#x3D;&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION&#x3D;&quot;V10 (Tercel)&quot;</span><br><span class="line">ID&#x3D;&quot;kylin&quot;</span><br><span class="line">VERSION_ID&#x3D;&quot;V10&quot;</span><br><span class="line">PRETTY_NAME&#x3D;&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR&#x3D;&quot;0;31&quot;</span><br></pre></td></tr></table></figure><p>docker 版本信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">$ docker info</span><br><span class="line">Containers: 63</span><br><span class="line"> Running: 44</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 19</span><br><span class="line">Images: 24</span><br><span class="line">Server Version: 18.09.9</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line">runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.19.90-17.ky10.aarch64</span><br><span class="line">Operating System: Kylin Linux Advanced Server V10 (Tercel)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: aarch64</span><br><span class="line">CPUs: 64</span><br><span class="line">Total Memory: 62.76GiB</span><br><span class="line">Name: reg.xxx.lan</span><br><span class="line">ID: RI24:C6CM:WELZ:MQEJ:N5OY:IR74:OQPG:XV72:SFRI:NUSK:DS44:OQNQ</span><br><span class="line">Docker Root Dir: &#x2F;data&#x2F;kube&#x2F;docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> 127.0.0.0&#x2F;8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https:&#x2F;&#x2F;registry.docker-cn.com&#x2F;</span><br><span class="line"> https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&#x2F;</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><h3 id="测试运行"><a href="#测试运行" class="headerlink" title="测试运行"></a>测试运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">$ ldd .&#x2F;docker-compose-linux-arm64 </span><br><span class="line">linux-vdso.so.1 (0x0000fffd72210000)</span><br><span class="line">libdl.so.2 &#x3D;&gt; &#x2F;lib64&#x2F;libdl.so.2 (0x0000fffd721a0000)</span><br><span class="line">libz.so.1 &#x3D;&gt; &#x2F;lib64&#x2F;libz.so.1 (0x0000fffd72160000)</span><br><span class="line">libc.so.6 &#x3D;&gt; &#x2F;lib64&#x2F;libc.so.6 (0x0000fffd71fd0000)</span><br><span class="line">&#x2F;lib&#x2F;ld-linux-aarch64.so.1 (0x0000fffd72220000)</span><br><span class="line">$ ll</span><br><span class="line">总用量 10504</span><br><span class="line">drwxr-xr-x 2 root root       26  3月 13 11:11 conf.d</span><br><span class="line">-rwxr-xr-x 1 root root 10750256  3月 12 13:15 docker-compose-linux-arm64</span><br><span class="line">-rw-r--r-- 1 root root      389  3月 13 11:11 docker-compose.yml</span><br><span class="line">drwxr-xr-x 2 root root        6  3月 13 11:11 down</span><br><span class="line">$ cat conf.d&#x2F;default.conf </span><br><span class="line">server &#123;</span><br><span class="line">    listen       81;</span><br><span class="line">    server_name  localhost;</span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">        autoindex    on;</span><br><span class="line">    &#125;</span><br><span class="line">    error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">    location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">$ cat docker-compose.yml </span><br><span class="line">version: &#39;3.4&#39;</span><br><span class="line">services:</span><br><span class="line">  nginx:</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    container_name: install-nginx</span><br><span class="line">    hostname: install-nginx</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai:&#x2F;etc&#x2F;localtime:ro</span><br><span class="line">      - .&#x2F;down:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</span><br><span class="line">      - .&#x2F;conf.d&#x2F;:&#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;</span><br><span class="line">    network_mode: &quot;host&quot;</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#39;3&#39;</span><br><span class="line">        max-size: 100m</span><br><span class="line">$ mkdir conf.d</span><br><span class="line"></span><br><span class="line">$ .&#x2F;docker-compose-linux-arm64 up -d</span><br><span class="line">Pulling nginx (nginx:alpine)...</span><br><span class="line">alpine: Pulling from library&#x2F;nginx</span><br><span class="line">Digest: sha256:c2ce58e024275728b00a554ac25628af25c54782865b3487b11c21cafb7fabda</span><br><span class="line">Status: Downloaded newer image for nginx:alpine</span><br><span class="line">Creating install-nginx ... done</span><br><span class="line">$.&#x2F;docker-compose-linux-arm64 ps -a</span><br><span class="line">    Name                   Command               State   Ports</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">install-nginx   &#x2F;docker-entrypoint.sh ngin ...   Up           </span><br><span class="line">$ netstat -nlptu | grep -E &#39;:81\s&#39;</span><br><span class="line">tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      4093364&#x2F;nginx: mast </span><br></pre></td></tr></table></figure><p>页面访问了下正常，清理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;docker-compose-linux-arm64 down</span><br><span class="line">Stopping install-nginx ... done</span><br><span class="line">Removing install-nginx ... done</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/ubiquiti/docker-compose-aarch64">https://github.com/ubiquiti/docker-compose-aarch64</a></li><li><a href="https://github.com/RogerLaw/docker-compose-aarch64">https://github.com/RogerLaw/docker-compose-aarch64</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;p&gt;git 上搜索了很多 docker-compose 的 arm64 的编译基本都是使用 &lt;code&gt;qemu-user-static&lt;/co</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>集群节点关机导致dns在eviction pod之前几率不可用</title>
    <link href="http://zhangguanzhang.github.io/2021/02/02/node-shutdown-dns-unavailable/"/>
    <id>http://zhangguanzhang.github.io/2021/02/02/node-shutdown-dns-unavailable/</id>
    <published>2021-02-02T15:42:08.000Z</published>
    <updated>2021-02-02T15:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这几天我们内部在做新项目的容灾测试，业务都是在 K8S 上的。容灾里就是随便选节点 <code>shutdown -h now</code>。关机后同事便发现了（页面有错误，最终问题是）集群内 DNS 解析会有几率无法解析（导致的）。</p><p>根据 <code>SVC</code> 的流程，node 关机后，由于 kubelet 没有 update 自己。node 和 pod 在 apiserver get 的时候显示还是正常的。在 <code>kube-controller-manager</code> 的 <code>--node-monitor-grace-period</code> 时间后再过 <code>--pod-eviction-timeout</code> 时间开始 <code>eviction pod</code>，大概流程是这样。</p><p>在 <code>pod</code> 被 <code>eviction</code> 之前，默认是大概 <code>5m</code> 的时间。这段时间内，<code>node</code> 上 的所有 <code>POD</code> 的 <code>IP</code> 还在 <code>SVC</code> 的 <code>endpoint</code> 里。而同事关机的 <code>node</code> 上恰好有 <code>coredns</code> 。所以在 5m 内一直会有 coredns 副本数之一的几率解析失败。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>其实和 K8S 版本没关系，因为 <code>SVC</code> 和 <code>eviction</code> 的行为都是这样的。实际我调整了 <code>node</code> 更新自身状态的所有 <a href="https://github.com/zhangguanzhang/Kubernetes-ansible/wiki/nodeStatusUpdate">相关参数</a>，调整到在 20s 内就会 <code>eviction pod</code>，但是 20s 内还是存在几率无法解析(后续这个相关的更新时间调很快，结果出现了 svc 选中的pod还在running，但是kubelet实际更新自己 status 失败了，导致kube-controller-mananger把 pod的status patch成了非true，也就是svc的 endpoint消失了，回退更新时间后没这个bug了)。当然也问了下群友和社区群里，发现似乎大家从来没关机测试过这方面，应该是现在大伙都在用公有云了。。。。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">  kubectl  version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><h3 id="loca-dns-真的可以吗"><a href="#loca-dns-真的可以吗" class="headerlink" title="loca-dns 真的可以吗"></a>loca-dns 真的可以吗</h3><p>当然首选是 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns">local-dns 的方案</a> 了。方案搜下，很多人介绍了。简单讲下就是在每个 node 上起 <code>hostNetwork</code> 的 <code>node-cache</code> 进程做代理 ，然后利用 <code>dummy</code> 接口和 nat 来拦截发向 kube-dns <code>SVC</code> IP 的 dns 请求做缓存。</p><p><a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml">官方提供的 yaml 文件</a> 里的 <code>__PILLAR__LOCAL__DNS__,__PILLAR__DNS__SERVER__</code>需要换成<code>dummy</code>接口 IP 和 kube-dns <code>SVC</code> 的 IP，还有 <code>__PILLAR__DNS__DOMAIN__</code> 自行根据文档更换。其余几个变量会在启动的时候替换，可以启动后看日志。</p><p>然后实际测试了下还是有问题。然后捋了下流程，yaml 文件里有这个 <code>SVC</code> 和 node-cache 的启动参数</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns-upstream</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">53</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">53</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line"><span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"> <span class="attr">args:</span> [ <span class="string">...</span>, <span class="string">&quot;-upstreamsvc&quot;</span>, <span class="string">&quot;kube-dns-upstream&quot;</span> ]</span><br></pre></td></tr></table></figure><p>启动的日志里可以看到配置文件被渲染了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">cluster1.local:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    reload</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">        force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    health 169.254.20.10:8080</span><br><span class="line">&#125;</span><br><span class="line">in-addr.arpa:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    &#125;</span><br><span class="line">ip6.arpa:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    &#125;</span><br><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">    prometheus :9253</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为要 nat 去 hook 请求 kube-dns <code>SVC</code> IP（172.26.0.2）的请求，但是它自己也需要访问 kube-dns，所以 yaml 文件里创建了一个和 kube-dns 一样的属性的 svc，启动参数写了这个 SVC 名字，可以看到它代理的是走 <code>SVC</code> 的 ip 的。因为 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/connect-applications-service/#%E8%AE%BF%E9%97%AE-service">enableServiceLinks</a> 的默认开启，pod 会有如下环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> dfa env | grep KUBE_DNS_UPSTREAM_SERVICE_HOST</span></span><br><span class="line">KUBE_DNS_UPSTREAM_SERVICE_HOST=172.26.189.136</span><br></pre></td></tr></table></figure><p><a href="https://github.com/kubernetes/dns/blob/2b7b66c7824a7dd68d38568d913228e8d3d4c8c2/cmd/node-cache/app/cache_app.go#L306">代码里</a> 可以看到就是把参数的 <code>-</code> 转换成 <code>_</code> 取值然后渲染配置文件，这样就能取到 SVC 的 IP 了。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">toSvcEnv</span><span class="params">(svcName <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">envName := strings.Replace(svcName, <span class="string">&quot;-&quot;</span>, <span class="string">&quot;_&quot;</span>, <span class="number">-1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;$&quot;</span> + strings.ToUpper(envName) + <span class="string">&quot;_SERVICE_HOST&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>cluster1.local:53</code> 这个 zone 在默认配置下还是代理到 <code>SVC</code> 上，所以还是有问题。</p><p>所以只有绕过 <code>SVC</code> 才能从根本上解决这个问题。然后就把 <code>coredns</code> 改成 port 153 + <code>hostNetwork: true</code> 加 <code>nodeSelector</code> 到三个 master 上固定了。然后配置文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cluster1.local:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    reload</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">        force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    health 169.254.20.10:8080</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>然后测试还是有几率无法访问。之前看到过 <a href="https://fuckcloudnative.io/">米开朗基杨</a> 分享过 <code>coredns</code> 的一个带故障转移的插件 <a href="https://github.com/leiless/dnsredir">dnsredir</a>，尝试加这个插件去编译。</p><p>查阅文档编译后最后运行起来无法识别配置文件，因为官方不是直接基于 <code>coredns</code> 引入自己的插件开发的，而是自己的代码上来引入 <code>coredns</code> 的内置插件。</p><p>大概过程详情 issue 见链接 <a href="https://github.com/kubernetes/dns/issues/436">include coredns plugin at node-cache don’t work expect</a></p><p>官方的这个 node-cace 里的 bind 插件就是 <code>dummy</code>接口和 iptables 的 nat 部分了，这个特性蛮吸引我的，决定继续尝试下这个看看能不能配置成功。</p><h3 id="意外收获"><a href="#意外收获" class="headerlink" title="意外收获"></a>意外收获</h3><p>在测试加入插件 dnsredir 的时候米开朗基杨叫我试下最小配置段看看有干扰没，尝试了下面的配置段来回切换测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  Corefile: |</span><br><span class="line">    cluster1.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        reload</span><br><span class="line">        dnsredir . &#123;</span><br><span class="line">            to 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153</span><br><span class="line">            max_fails 1</span><br><span class="line">            health_check 1s</span><br><span class="line">            spray</span><br><span class="line">        &#125;</span><br><span class="line">        #forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">        #    max_fails 1</span><br><span class="line">        #    policy round_robin</span><br><span class="line">        #    health_check 0.4s</span><br><span class="line">        #&#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">    &#125;</span><br><span class="line">#----------</span><br><span class="line">  Corefile: |</span><br><span class="line">    cluster1.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        reload</span><br><span class="line">        #dnsredir . &#123;</span><br><span class="line">        #    to 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153</span><br><span class="line">        #    max_fails 1</span><br><span class="line">        #    health_check 1s</span><br><span class="line">        #    spray</span><br><span class="line">        #&#125;</span><br><span class="line">        forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">            max_fails 1</span><br><span class="line">            policy round_robin</span><br><span class="line">            health_check 0.4s</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>然后发现请求居然不会发生解析失败了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">function</span> <span class="function"><span class="title">d</span></span>()&#123; <span class="keyword">while</span> :;<span class="keyword">do</span> sleep 0.2; date;dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short; <span class="keyword">done</span>; &#125;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> d</span></span><br><span class="line">2021年 02月 02日 星期二 12:54:43 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST  &lt;---这个时间点关机了一个 master </span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:45 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:47 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:51 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:51 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:52 CST</span><br><span class="line">172.26.158.130</span><br></pre></td></tr></table></figure><p>然后就不打算继续折腾 dnsredir 插件了，去叫同事测试了下没问题，叫我在另一个环境上应用下修改他再测下，发现还是会发生：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @172.26.0.2 account-gateway +short</span><br><span class="line">; (1 server found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; connection timed out; no servers could be reached</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br></pre></td></tr></table></figure><p>然后我多次测试最小配置 zone，对比排查到是反向解析的问题，反向解析关闭了就不存在任何问题了，注释掉下面的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#in-addr.arpa:53 &#123;</span><br><span class="line">#    errors</span><br><span class="line">#    cache 30</span><br><span class="line">#    reload</span><br><span class="line">#    loop</span><br><span class="line">#    bind 169.254.20.10 172.26.0.2</span><br><span class="line">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">#            force_tcp</span><br><span class="line">#    &#125;</span><br><span class="line">#    prometheus :9253</span><br><span class="line">#    &#125;</span><br><span class="line">#ip6.arpa:53 &#123;</span><br><span class="line">#    errors</span><br><span class="line">#    cache 30</span><br><span class="line">#    reload</span><br><span class="line">#    loop</span><br><span class="line">#    bind 169.254.20.10 172.26.0.2</span><br><span class="line">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">#            force_tcp</span><br><span class="line">#    &#125;</span><br><span class="line">#    prometheus :9253</span><br><span class="line">#    &#125;</span><br></pre></td></tr></table></figure><p>测试解析的过程中去关机任何一台 coredns 所在 node 也没问题了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br></pre></td></tr></table></figure><h3 id="大致的yaml文件"><a href="#大致的yaml文件" class="headerlink" title="大致的yaml文件"></a>大致的yaml文件</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns-upstream</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;KubeDNSUpstream&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.3</span> <span class="comment"># &lt;---- 给他固定了得了，可以直接这个ip不走node-cache作为测试</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">cluster1.local:53</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">cache</span> &#123;</span><br><span class="line">            <span class="string">success</span> <span class="number">9984 </span><span class="number">30</span></span><br><span class="line">            <span class="string">denial</span> <span class="number">9984 </span><span class="number">5</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">bind</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.107</span><span class="string">:153</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.108</span><span class="string">:153</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.109</span><span class="string">:153</span> &#123;</span><br><span class="line">            <span class="string">force_tcp</span></span><br><span class="line">            <span class="string">max_fails</span> <span class="number">1</span></span><br><span class="line">            <span class="string">policy</span> <span class="string">round_robin</span></span><br><span class="line">            <span class="string">health_check</span> <span class="number">0.</span><span class="string">5s</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9253</span></span><br><span class="line">        <span class="string">health</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span><span class="string">:8070</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#in-addr.arpa:53 &#123;</span></span><br><span class="line">    <span class="comment">#    errors</span></span><br><span class="line">    <span class="comment">#    cache 30</span></span><br><span class="line">    <span class="comment">#    reload</span></span><br><span class="line">    <span class="comment">#    loop</span></span><br><span class="line">    <span class="comment">#    bind 169.254.20.10 172.26.0.2</span></span><br><span class="line">    <span class="comment">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span></span><br><span class="line">    <span class="comment">#            force_tcp</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#    prometheus :9253</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#ip6.arpa:53 &#123;</span></span><br><span class="line">    <span class="comment">#    errors</span></span><br><span class="line">    <span class="comment">#    cache 30</span></span><br><span class="line">    <span class="comment">#    reload</span></span><br><span class="line">    <span class="comment">#    loop</span></span><br><span class="line">    <span class="comment">#    bind 169.254.20.10 172.26.0.2</span></span><br><span class="line">    <span class="comment">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span></span><br><span class="line">    <span class="comment">#            force_tcp</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#    prometheus :9253</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="string">.:53</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">bind</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="string">__PILLAR__UPSTREAM__SERVERS__</span></span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9253</span></span><br><span class="line">      &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9253&quot;</span></span><br><span class="line">        <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span>  <span class="comment"># Don&#x27;t use cluster DNS.</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">node-cache</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">xxx.lan:5000/k8s-dns-node-cache:1.16.0</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">args:</span> [ <span class="string">&quot;-localip&quot;</span>, <span class="string">&quot;169.254.20.10,172.26.0.2&quot;</span>, <span class="string">&quot;-conf&quot;</span>, <span class="string">&quot;/etc/Corefile&quot;</span>, <span class="string">&quot;-upstreamsvc&quot;</span>, <span class="string">&quot;kube-dns-upstream&quot;</span>, <span class="string">&quot;-health-port&quot;</span>,<span class="string">&quot;8070&quot;</span> ]</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">53</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">53</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9253</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8070</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">40</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/coredns</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-dns-config</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/kube-dns</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">FileOrCreate</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-dns-config</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-dns</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">Corefile</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">Corefile.base</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods.</span></span><br><span class="line"><span class="comment"># We use this to expose metrics to Prometheus.</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9253&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">9253</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9253</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/bootstrapping:</span> <span class="string">rbac-defaults</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">services</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">namespaces</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/bootstrapping:</span> <span class="string">rbac-defaults</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">.:153</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">health</span> <span class="string">:8180</span></span><br><span class="line">        <span class="string">kubernetes</span> <span class="string">cluster1.local.</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span> &#123;</span><br><span class="line">            <span class="string">pods</span> <span class="string">insecure</span></span><br><span class="line">            <span class="string">fallthrough</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9153</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="string">/etc/resolv.conf</span></span><br><span class="line">        <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;CoreDNS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">seccomp.security.alpha.kubernetes.io/pod:</span> <span class="string">&#x27;docker/default&#x27;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">100</span></span><br><span class="line">              <span class="attr">podAffinityTerm:</span></span><br><span class="line">                <span class="attr">labelSelector:</span></span><br><span class="line">                  <span class="attr">matchExpressions:</span></span><br><span class="line">                    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">k8s-app</span></span><br><span class="line">                      <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                      <span class="attr">values:</span></span><br><span class="line">                        <span class="bullet">-</span> <span class="string">kube-dns</span></span><br><span class="line">                <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-cluster-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">coredns</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">node-role.kubernetes.io/master:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">          <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">xxxx.lan:5000/coredns:1.7.1</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">270Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">150Mi</span></span><br><span class="line">        <span class="attr">args:</span> [ <span class="string">&quot;-conf&quot;</span>, <span class="string">&quot;/etc/coredns/Corefile&quot;</span> ]</span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/coredns</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8180</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">          <span class="attr">capabilities:</span></span><br><span class="line">            <span class="attr">add:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">NET_BIND_SERVICE</span></span><br><span class="line">            <span class="attr">drop:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">all</span></span><br><span class="line">          <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">            <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">Corefile</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">Corefile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9153&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;CoreDNS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br></pre></td></tr></table></figure><h3 id="自己的方案"><a href="#自己的方案" class="headerlink" title="自己的方案"></a>自己的方案</h3><p>但是后面发现 cpu 太高了，决定自己整个方案，中途尝试了很多，最后决定自己把里面的 dummy 接口部分源码抠出来写成一个工具（这样就不用改 svc ip 了），然后高可用用其他手段。主要是替换掉 nodelocaldns 的部分</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span>  <span class="comment"># Don&#x27;t use cluster DNS.</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dummy-tool</span></span><br><span class="line">        <span class="comment">#image: registry.aliyuncs.com/zhangguanzhang/dummy-tool:v0.1</span></span><br><span class="line">        <span class="attr">image:</span> &#123;&#123; <span class="string">docker_repo_url</span> &#125;&#125;<span class="string">/dummy-tool:v0.1</span></span><br><span class="line">        <span class="attr">args:</span> </span><br><span class="line">        <span class="bullet">-</span> <span class="string">-local-ip=169.254.20.10,172.26.0.2</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-health-port=8070</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-interface-name=nodelocaldns</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8070</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">40</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dnsmasq</span></span><br><span class="line">        <span class="comment">#image: registry.aliyuncs.com/zhangguanzhang/dnsmasq:2.83</span></span><br><span class="line">        <span class="attr">image:</span> &#123;&#123; <span class="string">docker_repo_url</span> &#125;&#125;<span class="string">/dnsmasq:2.83</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">dnsmasq</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-d</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--conf-file=/etc/dnsmasq/dnsmasq.conf</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">host-localtime</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/dnsmasq</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">host-localtime</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">dnsmasq.conf:</span> <span class="string">|</span></span><br><span class="line">    <span class="literal">no</span><span class="string">-resolv</span></span><br><span class="line">    <span class="string">all-servers</span></span><br><span class="line">    <span class="string">server=10.11.86.107#153</span></span><br><span class="line">    <span class="string">server=10.11.86.108#153</span></span><br><span class="line">    <span class="string">server=10.11.86.109#153</span></span><br><span class="line">    <span class="comment">#log-queries</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://lework.github.io/2020/11/09/node-local-dns/">在 Kubernetes 集群中使用 NodeLocal DNSCache</a></li><li><a href="https://mritd.com/2019/11/05/writing-plugin-for-coredns/">writing-plugin-for-coredns</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这几天我们内部在做新项目的容灾测试，业务都是在 K8S 上的。容灾里就是随便选节点 &lt;code&gt;shutdown -h now&lt;/code&gt;</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    <category term="coredns" scheme="http://zhangguanzhang.github.io/categories/kubernetes/coredns/"/>
    
    <category term="node-cache" scheme="http://zhangguanzhang.github.io/categories/kubernetes/coredns/node-cache/"/>
    
    
    <category term="coredns" scheme="http://zhangguanzhang.github.io/tags/coredns/"/>
    
  </entry>
  
  <entry>
    <title>docker18.03 hang at &#39;restoring container&#39;</title>
    <link href="http://zhangguanzhang.github.io/2020/12/04/docker1803-hang-container-restore/"/>
    <id>http://zhangguanzhang.github.io/2020/12/04/docker1803-hang-container-restore/</id>
    <published>2020-12-04T19:42:08.000Z</published>
    <updated>2020-12-11T10:15:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>起初是 k8s 有几个 node not ready，上去看了下 kubelet 日志刷 container runtime down，重启了下 docker 后还是没用，docker ps 命令都卡住。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/redhat-release </span></span><br><span class="line">Linux xxx-disk0 3.10.0-1127.13.1.el7.x86_64 #1 SMP Tue Jun 23 15:46:38 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info</span></span><br><span class="line">Containers: 91</span><br><span class="line"> Running: 63</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 28</span><br><span class="line">Images: 539</span><br><span class="line">Server Version: 18.03.0-ce</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: cfd04396dc68220d1cecbe686a6cc3aa5ce3667c</span><br><span class="line">runc version: 4fc53a81fb7c994640722ac585fa9ca548971871</span><br><span class="line">init version: 949e6fa</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 3.10.0-1127.13.1.el7.x86_64</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 8</span><br><span class="line">Total Memory: 15.51GiB</span><br><span class="line">Name: xxx-disk0</span><br><span class="line">ID: UZRM:KRSL:TYWM:VAQY:KWCX:AVFD:NP53:TC35:YHOC:TLLO:YGXO:RMYS</span><br><span class="line">Docker Root Dir: /app/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br></pre></td></tr></table></figure><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>先停掉docker，然后前台启动加 debug 参数启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pgrep dockerd</span></span><br><span class="line">4659</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 4659 &amp;&amp; &gt; /var/run/docker.pid </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep dockerd</span></span><br><span class="line">root      5628  0.0  0.0 112708   980 pts/0    S+   22:33   0:00 grep --color=auto dockerd</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./dockerd -D</span></span><br><span class="line">WARN[0000] The "graph" config file option is deprecated. Please use "data-root" instead. </span><br><span class="line">WARN[2020-12-04T22:33:50.432804342+08:00] could not change group /var/run/docker.sock to docker: group docker not found </span><br><span class="line">DEBU[2020-12-04T22:33:50.432936283+08:00] Listener created for HTTP on unix (/var/run/docker.sock) </span><br><span class="line">INFO[2020-12-04T22:33:50.433612435+08:00] libcontainerd: started new docker-containerd process  pid=5646</span><br><span class="line">INFO[0000] starting containerd                           module=containerd revision=cfd04396dc68220d1cecbe686a6cc3aa5ce3667c version=v1.0.2</span><br><span class="line">DEBU[0000] changing OOM score to -500                    module=containerd</span><br><span class="line">INFO[0000] loading plugin "io.containerd.content.v1.content"...  module=containerd type=io.containerd.content.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.snapshotter.v1.btrfs"...  module=containerd type=io.containerd.snapshotter.v1</span><br><span class="line">WARN[0000] failed to load plugin io.containerd.snapshotter.v1.btrfs  error="path /app/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter" module=containerd</span><br><span class="line">INFO[0000] loading plugin "io.containerd.snapshotter.v1.overlayfs"...  module=containerd type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.metadata.v1.bolt"...  module=containerd type=io.containerd.metadata.v1</span><br><span class="line">WARN[0000] could not use snapshotter btrfs in metadata plugin  error="path /app/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter" module="containerd/io.containerd.metadata.v1.bolt"</span><br><span class="line">INFO[0000] loading plugin "io.containerd.differ.v1.walking"...  module=containerd type=io.containerd.differ.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.gc.v1.scheduler"...  module=containerd type=io.containerd.gc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.containers"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.content"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.diff"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.events"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.healthcheck"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.images"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.leases"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.namespaces"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.snapshots"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.monitor.v1.cgroups"...  module=containerd type=io.containerd.monitor.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.runtime.v1.linux"...  module=containerd type=io.containerd.runtime.v1</span><br><span class="line">DEBU[0000] loading tasks in namespace                    module="containerd/io.containerd.runtime.v1.linux" namespace=moby</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.tasks"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.version"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.introspection"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] serving...                                    address="/var/run/docker/containerd/docker-containerd-debug.sock" module="containerd/debug"</span><br><span class="line">INFO[0000] serving...                                    address="/var/run/docker/containerd/docker-containerd.sock" module="containerd/grpc"</span><br><span class="line">INFO[0000] containerd successfully booted in 0.009604s   module=containerd</span><br><span class="line">DEBU[2020-12-04T22:33:50.456534148+08:00] Golang's threads limit set to 113940         </span><br><span class="line">DEBU[2020-12-04T22:33:50.457345643+08:00] Using default logging driver json-file       </span><br><span class="line">DEBU[2020-12-04T22:33:50.457466912+08:00] [graphdriver] priority list: [btrfs zfs overlay2 aufs overlay devicemapper vfs] </span><br><span class="line">DEBU[2020-12-04T22:33:50.457623030+08:00] processing event stream                       module=libcontainerd namespace=plugins.moby</span><br><span class="line">DEBU[2020-12-04T22:33:50.479691287+08:00] backingFs=xfs,  projectQuotaSupported=false  </span><br><span class="line">INFO[2020-12-04T22:33:50.479712832+08:00] [graphdriver] using prior storage driver: overlay2 </span><br><span class="line">DEBU[2020-12-04T22:33:50.479724151+08:00] Initialized graph driver overlay2            </span><br><span class="line">DEBU[2020-12-04T22:33:50.510882767+08:00] Max Concurrent Downloads: 10                 </span><br><span class="line">DEBU[2020-12-04T22:33:50.510930407+08:00] Max Concurrent Uploads: 5                    </span><br><span class="line">DEBU[0000] garbage collected                             d=24.493383ms module="containerd/io.containerd.gc.v1.scheduler"</span><br><span class="line">INFO[2020-12-04T22:33:50.608483121+08:00] Graph migration to content-addressability took 0.00 seconds </span><br><span class="line">INFO[2020-12-04T22:33:50.610430840+08:00] Loading containers: start.                   </span><br><span class="line">DEBU[2020-12-04T22:33:50.610704281+08:00] processing event stream                       module=libcontainerd namespace=moby</span><br><span class="line">DEBU[2020-12-04T22:33:50.611446797+08:00] Loaded container 027a389c8c1e93629cc5f68af8d023b2ecfe350d7771ba6b87598ff705f6c19f, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.611803503+08:00] Loaded container 24735e5aea2bd91b5fa5d729ca021a09532c2ea9b8b06f5171d0da23fc3bf4cc, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612174253+08:00] Loaded container 487a8c2f30986796c3948d1469d506e1d3ab394e17533040ef7a5444a32be0fc, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612494092+08:00] Loaded container 52d32b0e03c957b6cb9b4d793c47900e689a29d9ae0d63703ea29073a352fbe5, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612816495+08:00] Loaded container 5b7b0b52c71a14164f269853679211b3823e9eecc2d3829bf2db10c9b720217d, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.613447082+08:00] Loaded container 62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be, isRunning: true </span><br><span class="line">DEBU[2020-12-04T22:33:50.613769649+08:00] Loaded container 68ba211ec7328bebd3b241631a703639447c05056ffe07ed633b72d0bc210938, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.614756585+08:00] Loaded container 73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212, isRunning: true </span><br><span class="line">DEBU[2020-12-04T22:33:50.615381990+08:00] Loaded container 7a2a75c8a0c8dac8c5773ad92fa45ac7e1d33c9be85ecb65eb147929955dca50, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616222796+08:00] Loaded container 81095c01c4b99c7d2cc9e6bee8726c11f16d27204523727e7d067d980c26ac64, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616569394+08:00] Loaded container 94098167eb466dbf1a454f5491a162488d1fdb1eebe804c5c1f403f7fce62dc4, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616981038+08:00] Loaded container 9d4cbcce43b0262d972e73b2770f26ca762e2fa86f0de88a7909b8e59c0b805a, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.617460452+08:00] Loaded container aecde8eb18924d8548d79d5e0383baa7ac3ab1cfc4c55e1f32c4089dfc153071, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.617908975+08:00] Loaded container aed0618a325b4b84363357c1830515048d23af6afd79606cbb0ad64bf5f226a2, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618252961+08:00] Loaded container b43e4995720f235c40ffd60bde1fb54e87ece3598f8bd625996042f637896687, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618557604+08:00] Loaded container c1e6a1de9b9c2fd420e718c405c114e726ec5531561a4caf662b757a3724711e, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618942417+08:00] Loaded container c5eb3c941e562153e0cf0af738f1cb43f34591f0b48ad5458ab2002f5be9e0a8, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.619380785+08:00] Loaded container e211ffccacb8f7982899097fbd0f9ce1d95f8f31f290fb10baf40d00f4980bc9, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.619831551+08:00] Loaded container ef547d238cd01ff7ec048de3442fe9293aa1d5d932ea66c5aed34bfff014182b, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.620192032+08:00] Loaded container f3bb916ec5d7847c3be4341975c47f4e2fe587fc726ca7d76e3dca15cb8dd21d, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.620438678+08:00] Loaded container fa6de6f4aa8894c18a9737bac462f57c69893eca5e4b58bc3bd793a76b252951, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:51.379861237+08:00] restoring container                           container=ef547d238cd01ff7ec048de3442fe9293aa1d5d932ea66c5aed34bfff014182b paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.379910464+08:00] restoring container                           container=e211ffccacb8f7982899097fbd0f9ce1d95f8f31f290fb10baf40d00f4980bc9 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.379994141+08:00] restoring container                           container=7a2a75c8a0c8dac8c5773ad92fa45ac7e1d33c9be85ecb65eb147929955dca50 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380029802+08:00] restoring container                           container=c1e6a1de9b9c2fd420e718c405c114e726ec5531561a4caf662b757a3724711e paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380084763+08:00] restoring container                           container=5b7b0b52c71a14164f269853679211b3823e9eecc2d3829bf2db10c9b720217d paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380127006+08:00] restoring container                           container=9d4cbcce43b0262d972e73b2770f26ca762e2fa86f0de88a7909b8e59c0b805a paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380121758+08:00] restoring container                           container=fa6de6f4aa8894c18a9737bac462f57c69893eca5e4b58bc3bd793a76b252951 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380163318+08:00] restoring container                           container=52d32b0e03c957b6cb9b4d793c47900e689a29d9ae0d63703ea29073a352fbe5 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380310029+08:00] restoring container                           container=027a389c8c1e93629cc5f68af8d023b2ecfe350d7771ba6b87598ff705f6c19f paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380382722+08:00] restoring container                           container=68ba211ec7328bebd3b241631a703639447c05056ffe07ed633b72d0bc210938 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380419320+08:00] restoring container                           container=487a8c2f30986796c3948d1469d506e1d3ab394e17533040ef7a5444a32be0fc paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380433522+08:00] restoring container                           container=73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 paused=false running=true</span><br><span class="line">DEBU[2020-12-04T22:33:51.380459224+08:00] restoring container                           container=c5eb3c941e562153e0cf0af738f1cb43f34591f0b48ad5458ab2002f5be9e0a8 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380525276+08:00] restoring container                           container=b43e4995720f235c40ffd60bde1fb54e87ece3598f8bd625996042f637896687 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380563957+08:00] restoring container                           container=81095c01c4b99c7d2cc9e6bee8726c11f16d27204523727e7d067d980c26ac64 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380586567+08:00] restoring container                           container=62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be paused=false running=true</span><br><span class="line">DEBU[2020-12-04T22:33:51.380599061+08:00] restoring container                           container=94098167eb466dbf1a454f5491a162488d1fdb1eebe804c5c1f403f7fce62dc4 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380616220+08:00] restoring container                           container=aecde8eb18924d8548d79d5e0383baa7ac3ab1cfc4c55e1f32c4089dfc153071 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380641090+08:00] restoring container                           container=f3bb916ec5d7847c3be4341975c47f4e2fe587fc726ca7d76e3dca15cb8dd21d paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380825356+08:00] restoring container                           container=24735e5aea2bd91b5fa5d729ca021a09532c2ea9b8b06f5171d0da23fc3bf4cc paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380953092+08:00] restoring container                           container=aed0618a325b4b84363357c1830515048d23af6afd79606cbb0ad64bf5f226a2 paused=false running=false</span><br></pre></td></tr></table></figure><p>然后发现卡在这，正常是会像 gin 那样启动输出支持的 http api 路由信息的。开一个窗口，发送 SIGUSR1 信号打印 goroutine 堆栈信息看看卡在哪儿：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pgrep dockerd</span></span><br><span class="line">3085</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -USR1 3085</span></span><br></pre></td></tr></table></figure><p>docker 的日志和系统日志都会有下面的类似输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dec 04 22:33:52 xxxx dockerd[3085]: time&#x3D;&quot;2020-12-33T58:15:52.906433650+08:00&quot; level&#x3D;info msg&#x3D;&quot;goroutine stacks written to &#x2F;var&#x2F;run&#x2F;docker&#x2F;goroutine-stacks-2020-12-04T223358+0800.log&quot;</span><br></pre></td></tr></table></figure><p>查看了下下面这段比较可疑，<code>daemon/daemon.go:364</code> 附近</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">goroutine 1 [semacquire, 5 minutes]:</span><br><span class="line">sync.runtime_Semacquire(0xc4204de73c)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;sema.go:56 +0x3b</span><br><span class="line">sync.(*WaitGroup).Wait(0xc4204de730)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;sync&#x2F;waitgroup.go:131 +0x74</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).restore(0xc42009a480, 0x190c3a6, 0x4)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:364 +0xfeb</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;daemon.NewDaemon(0xc42018d200, 0x2ec75c0, 0xc4201be410, 0x2ea89e0, 0xc420087d40, 0xc4201323c0, 0x0, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:894 +0x258d</span><br><span class="line">main.(*DaemonCli).start(0xc42051da40, 0xc4201c5d50, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;daemon.go:223 +0x1320</span><br><span class="line">main.runDaemon(0xc4201c5d50, 0xc42044b3b0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:78 +0x78</span><br><span class="line">main.newDaemonCommand.func1(0xc420176000, 0xc4201359e0, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:29 +0x5d</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).execute(0xc420176000, 0xc42000c090, 0x1, 0x1, 0xc420176000, 0xc42000c090)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:646 +0x44f</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).ExecuteC(0xc420176000, 0x2194e40, 0x2419c01, 0xc420135980)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:742 +0x310</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).Execute(0xc420176000, 0xc420135980, 0x190fa00)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:695 +0x2d</span><br><span class="line">main.main()</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:105 +0xe3</span><br></pre></td></tr></table></figure><p>按照docker info 的信息去找了下对应的 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364" target="_blank" rel="noopener">分支代码</a>。364 行一个<code>wg.Wait()</code>，得看前面的 goroutine 是卡在哪儿，根据前面的堆栈信息，应该是卡在 <code>github.com/docker/docker/daemon.(*Daemon).restore</code>，也就是 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L238" target="_blank" rel="noopener">238 行的 daemon.containerd.Restore 方法</a>，卡在<code>wg.Wait()</code>说明有协程没释放锁，这里<a href="https://github.com/moby/moby/blob/v18.03.0-ce/libcontainerd/client_daemon.go#L134" target="_blank" rel="noopener">containerd.Restore方法</a>的第一行就是锁，里面有个方法<code>c.remote.LoadContainer</code>，实际上是和docker-containerd通信的。</p><p>查看下 docker-containerd 进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep containerd</span><br><span class="line">root      5646  0.2  0.0 606436 14048 ?        Ssl  22:33   0:00 docker-containerd --config &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;containerd.toml</span><br><span class="line">appuser   6261  0.0  0.0 112708   984 pts&#x2F;1    S+   22:36   0:00 grep --color&#x3D;auto containerd</span><br><span class="line">root      8355  0.1  0.0   9052  4308 ?        Sl   Dec03   2:56 docker-containerd-shim -namespace moby -workdir &#x2F;app&#x2F;kube&#x2F;dockercontainerd&#x2F;daemon&#x2F;io.containerd.runtime.v1.linux&#x2F;moby&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be -address &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;docker-containerd.sock -containerd-binary &#x2F;app&#x2F;kube&#x2F;bin&#x2F;docker-containerd -runtime-root &#x2F;var&#x2F;run&#x2F;docker&#x2F;runtime-runc</span><br><span class="line">root     11171  0.0  0.0   9052  4052 ?        Sl   Dec03   0:18 docker-containerd-shim -namespace moby -workdir &#x2F;app&#x2F;kube&#x2F;dockercontainerd&#x2F;daemon&#x2F;io.containerd.runtime.v1.linux&#x2F;moby&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 -address &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;docker-containerd.sock -containerd-binary &#x2F;app&#x2F;kube&#x2F;bin&#x2F;docker-containerd -runtime-root &#x2F;var&#x2F;run&#x2F;docker&#x2F;runtime-runc</span><br></pre></td></tr></table></figure><p>有残留的，杀掉一个试试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 11171</span></span><br></pre></td></tr></table></figure><p>然后原窗口有输出了一些日志，实际上是执行了 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L244" target="_blank" rel="noopener">244行的 daemon.containerd.DeleteTask方法</a>，说明思路是对的，进程通信有问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0172] connecting to shim                            error&#x3D;&lt;nil&gt; id&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; namespace&#x3D;moby</span><br><span class="line">DEBU[2020-12-04T22:36:42.690975930+08:00] restored container                            alive&#x3D;false container&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 module&#x3D;libcontainerd namespace&#x3D;moby pid&#x3D;0</span><br><span class="line">DEBU[2020-12-04T22:36:42.701154551+08:00] Trying to unmount &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212&#x2F;mounts </span><br><span class="line">DEBU[2020-12-04T22:36:42.707909556+08:00] Unmounted &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212&#x2F;mounts </span><br><span class="line">DEBU[0172] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;exit&quot; type&#x3D;containerd.events.TaskExit</span><br><span class="line">DEBU[0172] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;delete&quot; type&#x3D;containerd.events.TaskDelete</span><br><span class="line">DEBU[2020-12-04T22:36:42.947670205+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;exit</span><br></pre></td></tr></table></figure><p>接着处理另一个：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep containerd</span></span><br><span class="line">root      5646  0.2  0.0 606692 14048 ?        Ssl  22:33   0:00 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root      6461  0.0  0.0 112708   984 pts/1    S+   22:37   0:00 grep --color=auto containerd</span><br><span class="line">root      8355  0.1  0.0   9052  4260 ?        Sl   Dec03   2:56 docker-containerd-shim -namespace moby -workdir /app/kube/dockercontainerd/daemon/io.containerd.runtime.v1.linux/moby/62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /app/kube/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 8355</span></span><br></pre></td></tr></table></figure><p>然后前台 debug 的日志没有卡住，正常启动了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0249] connecting to shim                            error&#x3D;&lt;nil&gt; id&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; namespace&#x3D;moby</span><br><span class="line">DEBU[2020-12-04T22:37:59.709825146+08:00] restored container                            alive&#x3D;false container&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be module&#x3D;libcontainerd namespace&#x3D;moby pid&#x3D;0</span><br><span class="line">DEBU[2020-12-04T22:37:59.710064357+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete</span><br><span class="line">INFO[2020-12-04T22:37:59.710093459+08:00] ignoring event                                module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">WARN[2020-12-04T22:37:59.710215638+08:00] Ignoring Exit Event, no such exec command found  container&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 exec-id&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 exec-pid&#x3D;11197</span><br><span class="line">DEBU[2020-12-04T22:37:59.719102521+08:00] Trying to unmount &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be&#x2F;mounts </span><br><span class="line">DEBU[2020-12-04T22:37:59.722934436+08:00] Unmounted &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be&#x2F;mounts </span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;containers&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;containers&#x2F;delete&quot; type&#x3D;containerd.events.ContainerDelete</span><br><span class="line">DEBU[2020-12-04T22:37:59.978450001+08:00] container mounted via layerStore: &amp;&#123;&#x2F;app&#x2F;kube&#x2F;docker&#x2F;overlay2&#x2F;97a09a97cf8c3ae835fb0ca6526c0282b26379942dfb49081189a39ce0400596&#x2F;merged 0x2f42600 0x2f42600&#125; </span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;exit&quot; type&#x3D;containerd.events.TaskExit</span><br><span class="line">DEBU[2020-12-04T22:38:00.169804015+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;exit</span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;delete&quot; type&#x3D;containerd.events.TaskDelete</span><br><span class="line">WARN[2020-12-04T22:38:00.169915440+08:00] Ignoring Exit Event, no such exec command found  container&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be exec-id&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be exec-pid&#x3D;8396</span><br><span class="line">DEBU[2020-12-04T22:38:00.170037297+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete</span><br><span class="line">INFO[2020-12-04T22:38:00.170061445+08:00] ignoring event                                module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;containers&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;containers&#x2F;delete&quot; type&#x3D;containerd.events.ContainerDelete</span><br><span class="line">DEBU[2020-12-04T22:38:00.199820461+08:00] container mounted via layerStore: &amp;&#123;&#x2F;app&#x2F;kube&#x2F;docker&#x2F;overlay2&#x2F;2abb109b107ef7f0e5c31b1a100b446234118ae38afe43977c8c718f115cdfd6&#x2F;merged 0x2f42600 0x2f42600&#125; </span><br><span class="line">DEBU[2020-12-04T22:38:00.208519823+08:00] Option Experimental: false                   </span><br><span class="line">DEBU[2020-12-04T22:38:00.208542167+08:00] Option DefaultDriver: bridge                 </span><br><span class="line">DEBU[2020-12-04T22:38:00.208549815+08:00] Option DefaultNetwork: bridge                </span><br><span class="line">DEBU[2020-12-04T22:38:00.208557480+08:00] Network Control Plane MTU: 1500              </span><br><span class="line">DEBU[2020-12-04T22:38:00.245647071+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.247719844+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0&#x2F;8 -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.249828613+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.251439314+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D PREROUTING]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>服务器上有安全狗，可能和安全狗有关系。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364" target="_blank" rel="noopener">https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;起初是 k8s 有几个 node not ready，上去看了下 kubelet 日志刷 container runtime down，重启</summary>
      
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/categories/docker/"/>
    
    
    <category term="hang" scheme="http://zhangguanzhang.github.io/tags/hang/"/>
    
  </entry>
  
  <entry>
    <title>ansible hang in docker container</title>
    <link href="http://zhangguanzhang.github.io/2020/11/23/ansible-hang-in-docker/"/>
    <id>http://zhangguanzhang.github.io/2020/11/23/ansible-hang-in-docker/</id>
    <published>2020-11-23T19:42:08.000Z</published>
    <updated>2020-11-23T19:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这几天同事发现在 docker 容器里运行 ansible 命令很卡，发来了个命令叫我试试 <code>ansible localhost -m setup -a &#39;filter=ansible_default_ipv4&#39; 2&gt;/dev/null |grep &#39;\&quot;address\&quot;&#39; |awk -F&#39;\&quot;&#39; &#39;&#123;print $4&#125;&#39;</code></p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span> </span><br><span class="line">NAME=&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION=&quot;V10 (Tercel)&quot;</span><br><span class="line">ID=&quot;kylin&quot;</span><br><span class="line">VERSION_ID=&quot;V10&quot;</span><br><span class="line">PRETTY_NAME=&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxxxx 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info</span></span><br><span class="line">Containers: 1</span><br><span class="line"> Running: 1</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 1</span><br><span class="line">Server Version: 18.09.9</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line">runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.19.90-17.ky10.aarch64</span><br><span class="line">Operating System: Kylin Linux Advanced Server V10 (Tercel)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: aarch64</span><br><span class="line">CPUs: 64</span><br><span class="line">Total Memory: 62.76GiB</span><br><span class="line">Name: xxx</span><br><span class="line">ID: 3ZQD:MZWN:FNR4:5HEE:F57N:3BLD:EP3T:LJT7:NWEJ:3TZ3:IEBD:KSHZ</span><br><span class="line">Docker Root Dir: /data/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><p>容器里的 ansible 和 python 信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ansible --version</span></span><br><span class="line">ansible 2.8.6</span><br><span class="line">  config file = None</span><br><span class="line">  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]</span><br><span class="line">  ansible python module location = /usr/local/lib/python2.7/dist-packages/ansible</span><br><span class="line">  executable location = /usr/local/bin/ansible</span><br><span class="line">  python version = 2.7.12 (default, Apr 15 2020, 17:07:12) [GCC 5.4.0 20160609]</span><br><span class="line"><span class="meta">$</span><span class="bash"> python --version</span></span><br><span class="line">Python 2.7.12</span><br></pre></td></tr></table></figure><p>他说如果用麒麟的 rpm 包安装 docker 就没问题，用我们的拷贝二进制文件安装的 docker 起的容器里就不行。一开始是怀疑 setup 模块在收集某些信息的时候阻塞了，后面我试了下这样也会卡住 <code>ansible localhost -m shell -a date</code>。</p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>带上了 <code>-vvvv</code> 发现卡在下面的输出这：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;127.0.0.1&gt; PUT &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-local-466216tkG5t&#x2F;tmpML7hBj TO &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py</span><br><span class="line">&lt;127.0.0.1&gt; EXEC &#x2F;bin&#x2F;sh -c &#39;chmod u+x &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F; &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py &amp;&amp; sleep 0&#39;</span><br><span class="line">&lt;127.0.0.1&gt; EXEC &#x2F;bin&#x2F;sh -c &#39;&#x2F;usr&#x2F;bin&#x2F;python &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py &amp;&amp; sleep 0&#39;</span><br></pre></td></tr></table></figure><p>搜了下可以 <code>export ANSIBLE_DEBUG=True</code> 打印更详细的日志，打印了下面的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">23918 1606128393.94311: ANSIBALLZ: Done creating module</span><br><span class="line">23918 1606128393.94465: _low_level_execute_command(): starting</span><br><span class="line">23918 1606128393.94493: _low_level_execute_command(): executing: &#x2F;bin&#x2F;sh -c &#39;&#x2F;usr&#x2F;bin&#x2F;python &amp;&amp; sleep 0&#39;</span><br><span class="line">23918 1606128393.94515: in local.exec_command()</span><br><span class="line">23918 1606128393.94528: opening command with Popen()</span><br><span class="line">23918 1606128393.94979: done running command with Popen()</span><br><span class="line">23918 1606128393.95005: getting output with communicate()</span><br></pre></td></tr></table></figure><p>看样子是子进程卡住了，主进程等子进程。因为容器的进程实际上也是在宿主机上的，宿主机上安装了 strace，查看下进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep ansible</span></span><br><span class="line">root     3020177  2.7  0.0 129408 48960 pts/0    Sl+  19:12   0:26 /usr/bin/python /usr/local/bin/ansible localhost -vvvvv -m shell -a ls</span><br><span class="line">root     3020189  0.0  0.0 134912 50368 pts/0    S+   19:12   0:00 /usr/bin/python /usr/local/bin/ansible localhost -vvvvv -m shell -a ls</span><br><span class="line">root     3020216  0.0  0.0   2368   768 pts/0    S+   19:12   0:00 /bin/sh -c /bin/sh -c &#x27;/usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py &amp;&amp; sleep 0&#x27;</span><br><span class="line">root     3020217  0.0  0.0   2368   768 pts/0    S+   19:12   0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py &amp;&amp; sleep 0</span><br><span class="line">root     3020218  0.0  0.0  19776 14336 pts/0    S+   19:12   0:00 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py</span><br><span class="line">root     3020219  9.3  0.0  18688 10816 pts/0    t+   19:12   1:27 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py</span><br><span class="line">root     3050341  0.3  0.0   8832  4544 ?        Ss   19:28   0:00 ssh: /root/.ansible/cp/5a0929d121 [mux]</span><br><span class="line">root     3052724  0.0  0.0 214080  1536 pts/7    S+   19:28   0:00 grep ansible</span><br></pre></td></tr></table></figure><p>strace 下 3020219 ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> strace -p 3020219</span></span><br><span class="line">close(76267956)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267957)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267958)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267959)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267960)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267961)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267962)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267963)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267964)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267965)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267966)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267967)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267968)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267969)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267970)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267971)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267972^C)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">strace: Process 3020219 detached</span><br></pre></td></tr></table></figure><p>终端一直刷上面的，看样子是文件描述符泄露，<code>错误的文件描述符</code> 英文就是 <code>Bad file descriptor</code>， 谷歌搜了下 <code>in &quot;docker&quot;  (Bad file descriptor) strace</code>，找到了 <a href="https://github.com/docker/for-linux/issues/502">Spawning PTY processes is many times slower on Docker 18.09</a> 里几位大佬排查到是某些 os 下，很多地方都有设置 limit（就是那种我以为你做了，我就没做。结果你以为我做了，你就没做，结果大家都没做的感觉了），导致容器的 nofile 会不固定，经常性的太高。而很多语言的模块会遍历所有描述符，导致会卡。如果启动容器 nofile 设置低则没问题，下面还有个大佬给了个链接在 python 层面修复这个问题 <a href="https://github.com/python/cpython/pull/11584">python/cpython#11584</a> ，还有下面的解决办法，直接设置默认的 limit：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> https://docs.docker.com/engine/reference/commandline/dockerd/<span class="comment">#daemon-configuration-file</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/docker/daemon.json-ulimits</span></span><br><span class="line">&#123;</span><br><span class="line">&quot;default-ulimits&quot;: &#123;</span><br><span class="line">&quot;nofile&quot;: &#123;</span><br><span class="line">&quot;Name&quot;: &quot;nofile&quot;,</span><br><span class="line">&quot;Hard&quot;: 1024,</span><br><span class="line">&quot;Soft&quot;: 1024</span><br><span class="line">&#125;,</span><br><span class="line">&quot;nproc&quot;: &#123;</span><br><span class="line">&quot;Name&quot;: &quot;nproc&quot;,</span><br><span class="line">&quot;Soft&quot;: 65536,</span><br><span class="line">&quot;Hard&quot;: 65536</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>改配置怕影响其他容器，就决定从 python 层面改，看了下提交的 pr <a href="https://github.com/python/cpython/commit/5626fff54ebe5863e64454c081ec585f85cf141c">python/cpython@5626fff</a> 是改的 <code>subprocess.py</code> ，在容器里查找下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">find / -type f -name subprocess.py </span><br><span class="line">/usr/lib/python2.7/subprocess.py</span><br><span class="line">/usr/lib/python3.5/asyncio/subprocess.py</span><br><span class="line">/usr/lib/python3.5/subprocess.py</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/future/moves/subprocess.py</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/gevent/subprocess.py</span><br></pre></td></tr></table></figure><p>对比了下函数名 <code>def _close_fds(self, but):</code> ，确定是 <code>/usr/lib/python2.7/subprocess.py</code>，把 pr 的内容加进去后再执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> time ansible localhost -m shell -a date</span></span><br><span class="line">[WARNING]: No inventory was parsed, only implicit localhost is available</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">localhost | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">2020年 11月 23日 星期一 19:38:14 CST</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real0m3.088s</span><br><span class="line">user0m2.860s</span><br><span class="line">sys  0m0.255s</span><br></pre></td></tr></table></figure><p>最终还是选择了起容器的时候限制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--ulimit nofile&#x3D;65536</span><br></pre></td></tr></table></figure><p>上面的几位大佬给出了其他的解决方案，也可以在 containerd 配置文件里配置把 nofile 固定住，或者 docker daemon，或者应用的软件层面修复。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/pexpect/ptyprocess/issues/50">https://github.com/pexpect/ptyprocess/issues/50</a></li><li><a href="https://github.com/docker/for-linux/issues/502">https://github.com/docker/for-linux/issues/502</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这几天同事发现在 docker 容器里运行 ansible 命令很卡，发来了个命令叫我试试 &lt;code&gt;ansible localhost </summary>
      
    
    
    
    <category term="ansible" scheme="http://zhangguanzhang.github.io/categories/ansible/"/>
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>永久关闭swap的正确姿势</title>
    <link href="http://zhangguanzhang.github.io/2020/11/20/disable-swap/"/>
    <id>http://zhangguanzhang.github.io/2020/11/20/disable-swap/</id>
    <published>2020-11-20T10:29:08.000Z</published>
    <updated>2020-11-20T10:29:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天遇到了 kylin 系统上无法关闭 swap 的情况。记录下和方便别人搜到这个知识点。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> at /etc/issue</span></span><br><span class="line">Kylin 4.0.2 \n \l</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux H-192-168-63-132 4.15.0- 58-generic #64kord1k1&#x27;SMP Thu Aug 1S15:51:97 csT 2919 aarch64 ......</span><br></pre></td></tr></table></figure><h2 id="尝试的步骤"><a href="#尝试的步骤" class="headerlink" title="尝试的步骤"></a>尝试的步骤</h2><p>fstab 里没有 swap 的挂载，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a &amp;&amp; sysctl -w vm.swappiness=0</span><br></pre></td></tr></table></figure><p>重启后，内核参数是关闭的，但是实际没有关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sysctl -a |&amp; grep vm.swappiness</span></span><br><span class="line">vm.swappiness = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           127G         48G         70G        169M        7.8G         64G</span><br><span class="line">Swap:          7.6G          0B        7.8G</span><br></pre></td></tr></table></figure><p>应该有其他的挂载，据我所知， systemd 也会负责挂载的，查找下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl list-units | grep swap</span></span><br><span class="line">dev-sda3.swap         loaded active active    Swap Partition</span><br><span class="line">swap.target           loaded active active    Swap</span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl cat dev-sda3.swap</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /run/systemd/generator.late/dev-sda3.swap</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Automatically generated by systemd-gpt-auto-generator</span></span><br><span class="line"></span><br><span class="line">[unit]</span><br><span class="line">Description=Swap Partition</span><br><span class="line">Documentation=man:systemd-gpt-auto-generator(8)</span><br><span class="line"></span><br><span class="line">[Swap]</span><br><span class="line">What=/dev/sda3</span><br></pre></td></tr></table></figure><p>发现这个无法 disable ，会报错 no such file。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>查找了下文档，<code>systemd-gpt-auto-generator</code> 是一个 GPT 分区 自动发现 与 挂载。会自动生成 mount 和 swap 的 systemd unit 文件。找到了英文文档里有下面的话:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemd-gpt-auto-generator understands the following kernel command line parameters:</span><br><span class="line"></span><br><span class="line">systemd.gpt_auto, rd.systemd.gpt_auto</span><br><span class="line">Those options take an optional boolean argument, and default to yes. The generator is enabled by default, and a negative value may be used to disable it.</span><br></pre></td></tr></table></figure><p>我们可以通过添加 kernel boot cmdline 来关闭 <code>systemd-gpt-auto-generator</code> 。</p><p>先查找到 <code>grub.cfg</code> ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> find /boot  -<span class="built_in">type</span> f -name <span class="string">&#x27;grub*cfg&#x27;</span> -<span class="built_in">exec</span> grep -l <span class="string">&#x27;/vmlinuz&#x27;</span> &#123;&#125; \;</span></span><br><span class="line">/boot/grub/grub.cfg</span><br></pre></td></tr></table></figure><p>进去备份下文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /boot/grub/</span><br><span class="line">cp grub.cfg grub.cfg-20201120</span><br></pre></td></tr></table></figure><p>找到类似下面的行:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux /boot/vmlinuz-xxx</span><br></pre></td></tr></table></figure><p>后面加上 <code>systemd.gpt_auto=false</code> ，文档是写布尔值的，不过我看到有人 <code>systemd.gpt_auto=0</code> 也行。然后重启。</p><p><code>2021/03/23</code> 测试了下面的不行。。。<br>理论上改文件 <code>/etc/default/grub</code> 里 <code>GRUB_CMDLINE_LINUX</code> 后面添加也行。</p><p>Azure Linux 的话有个 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl list-units | grep temp-disk-swapfile</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www.jinbuguo.com/systemd/systemd-gpt-auto-generator.html">systemd-gpt-auto-generator 中文手册</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/systemd-gpt-auto-generator.html">systemd-gpt-auto-generator 英文文档</a></li><li><a href="http://www.jinbuguo.com/systemd/systemd.generator.html#">systemd.generator 中文手册</a></li><li><a href="https://groups.google.com/g/shlug/c/BH11BbecodM">更改分区类型关闭 swap</a></li><li><a href="https://manpages.ubuntu.com/manpages/disco/man7/kernel-command-line.7.html">kernel-command-line man page 7</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天遇到了 kylin 系统上无法关闭 swap 的情况。记录下和方便别人搜到这个知识点。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;figure clas</summary>
      
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/categories/linux/"/>
    
    
    <category term="swap" scheme="http://zhangguanzhang.github.io/tags/swap/"/>
    
  </entry>
  
  <entry>
    <title>银河麒麟arm64系统克隆机器上k8s vxlan跨节点不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2020/11/06/kylin-arm-clone-vxlan-error/"/>
    <id>http://zhangguanzhang.github.io/2020/11/06/kylin-arm-clone-vxlan-error/</id>
    <published>2020-11-06T19:31:01.000Z</published>
    <updated>2020-11-11T02:11:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>和前一篇文章不一样，从来没遇到过这样的问题，这里记录下。实施在客户那边部署业务后，业务在浏览器上无法访问，我远程上去查看日志发现 pod 内部无法 DNS 无法解析，nginx 连不上 upsteam 报错而启动失败，实际上也是跨节点不通。实际排查过程也有往错误的方向浪费了一些时间和尝试，就不写进来了，以正确的角度写下排查过程。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>OS 是 arm64 的银河麒麟系统：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">NAME="Kylin Linux Advanced Server"</span><br><span class="line">VERSION="V10 (Tercel)"</span><br><span class="line">ID="kylin"</span><br><span class="line">VERSION_ID="V10"</span><br><span class="line">PRETTY_NAME="Kylin Linux Advanced Server V10 (Tercel)"</span><br><span class="line">ANSI_COLOR="0;31"</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux localhost.localdomain 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>集群信息（和集群版本没关系）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  "clientVersion": &#123;</span><br><span class="line">    "major": "1",</span><br><span class="line">    "minor": "15",</span><br><span class="line">    "gitVersion": "v1.15.12",</span><br><span class="line">    "gitCommit": "e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725",</span><br><span class="line">    "gitTreeState": "clean",</span><br><span class="line">    "buildDate": "2020-05-06T05:17:59Z",</span><br><span class="line">    "goVersion": "go1.12.17",</span><br><span class="line">    "compiler": "gc",</span><br><span class="line">    "platform": "linux/arm64"</span><br><span class="line">  &#125;,</span><br><span class="line">  "serverVersion": &#123;</span><br><span class="line">    "major": "1",</span><br><span class="line">    "minor": "15",</span><br><span class="line">    "gitVersion": "v1.15.12",</span><br><span class="line">    "gitCommit": "e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725",</span><br><span class="line">    "gitTreeState": "clean",</span><br><span class="line">    "buildDate": "2020-05-06T05:09:48Z",</span><br><span class="line">    "goVersion": "go1.12.17",</span><br><span class="line">    "compiler": "gc",</span><br><span class="line">    "platform": "linux/arm64"</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>node 信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o wide</span></span><br><span class="line">NAME            STATUS   ROLES         AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                   KERNEL-VERSION            CONTAINER-RUNTIME</span><br><span class="line">172.18.27.252   Ready    master,node   32h   v1.15.12   172.18.27.252   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br><span class="line">172.18.27.253   Ready    master,node   32h   v1.15.12   172.18.27.253   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br><span class="line">172.18.27.254   Ready    master,node   32h   v1.15.12   172.18.27.254   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br></pre></td></tr></table></figure><p>coredns 信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span></span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-677d9c57f-pqvfv   1/1     Running   1          21h     10.187.0.5      172.18.27.253   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-677d9c57f-zjf86   1/1     Running   1          4h45m   10.187.1.12     172.18.27.252   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>命令都是在 <code>172.18.27.252</code> 上执行的，用 <code>dig @coredns_svc_ip +short kubernetes.default.svc.cluster1.local</code> 测发现时而能解析，时而不能解析，然后发现是跨节点的问题。</p><p>在 <code>172.18.27.252</code> 上去请求 <code>172.18.27.253</code> 上的 coredns 的 metrics 接口：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -I 10.187.0.5:9153/metrics</span><br></pre></td></tr></table></figure><p>然后在 <code>172.18.27.253</code> 上抓包:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.0.5 and port 9153</span></span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">^C</span><br><span class="line">0 packets captured</span><br><span class="line">0 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure><p>没有包，在 <code>172.18.27.253</code> 上抓了下 <code>8472</code> 端口是正常能收包的。像之前的那个文章 <a href="https://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/">银河麒麟 arm64 系统上 k8s 集群跨节点不通的一次排查</a> 看了下，253 机器上查看路由也没问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.1.0</span></span><br><span class="line">10.187.1.0 via 10.187.1.0 dev flannel.1 src 10.187.0.0 uid 0</span><br><span class="line">    cache</span><br></pre></td></tr></table></figure><p>当时各种手段看了个遍，结果有点眉目了（我应该像上篇文章一样先看下 vxlan 的 vtep 信息的。。。）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.252</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.253</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.254</span><br></pre></td></tr></table></figure><p>vtep 的 mac 地址都一样，查看下，发现三台机器都是一样的，看下网卡和 flannel.1 的信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip -4 a s enp1s0</span></span><br><span class="line">2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether dc:2d:cb:17:3e:a1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.27.252/25 brd 172.18.27.255 scope global noprefixroute enp1s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">394: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 1 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.252 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">#</span><span class="bash"> 另一台机器</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -4 a s enp1s0</span></span><br><span class="line">2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether dc:2d:cb:17:3e:86 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.27.254/25 brd 172.18.27.255 scope global noprefixroute enp1s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">66: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.254 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>可以看到网卡 enp1s0 的 mac 是不一样的，查看了下几个机器的网卡配置文件的 UUID 和 HWADDR 都不一样。上面命令可以看出只有 flannel.1 的 MAC 是一样，尝试删除然后重启 flanneld 看看重建咋样：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip link <span class="built_in">set</span> flannel.1 down</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link <span class="built_in">set</span> flannel.1 up</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a |grep -m1 flanneld</span></span><br><span class="line">0aa5998260ba        122cdb7aa710                                "/opt/bin/flanneld -…"    5 hours ago          Up 3 hours                                          k8s_kube-flannel_kube-flannel-ds-2zqr9_kube-system_1ed6eba1-fa30-405e-83d0-314160c25313_1</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart 0aa</span></span><br><span class="line">0aa</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">22: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>还是一摸一样，好奇这个 mac 地址如何来的，就去看了下 <a href="https://github.com/coreos/flannel/blob/v0.11.0/backend/vxlan/vxlan.go#L104-L137" target="_blank" rel="noopener">flannel 添加 flannel.1 的源码部分</a>：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">devAttrs := vxlanDeviceAttrs&#123;</span><br><span class="line">vni:       <span class="keyword">uint32</span>(cfg.VNI),</span><br><span class="line">name:      fmt.Sprintf(<span class="string">"flannel.%v"</span>, cfg.VNI),</span><br><span class="line">vtepIndex: be.extIface.Iface.Index,</span><br><span class="line">vtepAddr:  be.extIface.IfaceAddr,</span><br><span class="line">vtepPort:  cfg.Port,</span><br><span class="line">gbp:       cfg.GBP,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dev, err := newVXLANDevice(&amp;devAttrs)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">dev.directRouting = cfg.DirectRouting</span><br><span class="line"></span><br><span class="line">subnetAttrs, err := newSubnetAttrs(be.extIface.ExtAddr, dev.MACAddr())</span><br></pre></td></tr></table></figure><p><code>dev.MACAddr()</code> 是直接 return 的 <code>dev.link.HardwareAddr</code>，goland 里 find usage 下压根没找到赋值的地方。（可以加代码打印下到底 mac 地址从哪里获取的，但是环境是远程的，得一次一次编译发过去太麻烦了我就没弄了）毫无头绪乱排查了一段时间。然后突发奇想手动按照 vxlan 创建的步骤测试添加下看看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip link add <span class="built_in">test</span> <span class="built_in">type</span> vxlan id 2 dev enp1s0 <span class="built_in">local</span> 10.186.0.0 dstport 8473 nolearning</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show <span class="built_in">test</span></span></span><br><span class="line">696: test: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ca:32:f1:0d:c6:dc brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 2 local 10.186.0.0 dev enp1s0 srcport 0 0 dstport 8473 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete <span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link add <span class="built_in">test</span> <span class="built_in">type</span> vxlan id 2 dev enp1s0 <span class="built_in">local</span> 10.186.0.0 dstport 8473 nolearning</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show <span class="built_in">test</span></span></span><br><span class="line">697: test: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ca:32:f1:0d:c6:dc brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 2 local 10.186.0.0 dev enp1s0 srcport 0 0 dstport 8473 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>发现手动创建的网络设备 mac 居然也是一样的（实际上 ide 里找上面那个 <code>HardwareAddr</code> 的赋值是在引入的一个 <code>netlink</code> 包里赋值的，也就是说 flannel 创建接口的时候和手动添加类似，没有直接设置 mac 地址，而是系统返回的），然后同样步骤在我机器上测试是不一样的，看了下客户是啥服务器，发现居然是虚机。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /sys/class/dmi/id/product_name</span></span><br><span class="line">KVM Virtual Machine</span><br></pre></td></tr></table></figure><p>三台机器上添加接口的 mac 地址都是一样的，机器是不是克隆的？询问了下同事，同事说是的。其实这就是引起故障的根源，应该内部 mac 地址默认是不随机的策略。</p><p>查看下网卡策略是咋样的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> networkctl status enp1s0</span></span><br></pre></td></tr></table></figure><p>居然都为空，ubuntu 的机器上是有个优先级低的 link 文件的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status enp10s0</span></span><br><span class="line">● 2: enp10s0</span><br><span class="line">       Link File: /lib/systemd/network/99-default.link</span><br><span class="line">    Network File: n/a</span><br><span class="line">            Type: ether</span><br><span class="line">           State: n/a (unmanaged)</span><br><span class="line">            Path: platform-80040000000.pcie-controller-pci-0000:0a:00.0</span><br><span class="line">          Driver: igb</span><br><span class="line">          Vendor: Intel Corporation</span><br><span class="line">           Model: I210 Gigabit Network Connection</span><br><span class="line">      HW Address: 00:09:06:xx:xx:xx (Esteem Networks)</span><br><span class="line">         Address: 10.226.45.23</span><br><span class="line">                  fe80::209:xxx:xxxx:xxxx</span><br><span class="line">         Gateway: 10.226.45.254</span><br></pre></td></tr></table></figure><p>一些云主机我看也没这个文件，但是上面添加的接口每次 mac 不一样，应该其他 OS 改了默认的行为。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>我们可以用 systemd 写一个 link 配置文件改变策略：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat&lt;&lt;'EOF'&gt;/etc/systemd/network/10-flannel.link</span><br><span class="line">[Match]</span><br><span class="line">OriginalName=flannel*</span><br><span class="line"></span><br><span class="line">[Link]</span><br><span class="line">MACAddressPolicy=none</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一开始只添加上面的，结果重启后文件没了，下面的文件也追加了下，然后重启还是失效。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后上下两个步骤都整就行了。如果你也遇到了，先单独上面的文件试试，不行再下面的也加上试试。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat&lt;&lt;<span class="string">'EOF'</span>&gt;&gt;/etc/systemd/networkd.conf</span></span><br><span class="line">[Match]</span><br><span class="line">OriginalName=flannel*</span><br><span class="line"></span><br><span class="line">[Link]</span><br><span class="line">MACAddressPolicy=none</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>确认生效文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status flannel.1</span></span><br><span class="line">● 36: flannel.1                                              </span><br><span class="line">             Link File: /etc/systemd/network/10-flannel.link</span><br><span class="line">          Network File: n/a                                 </span><br><span class="line">                  Type: vxlan                               </span><br><span class="line">                 State: routable (unmanaged)   </span><br><span class="line">                Driver: vxlan                               </span><br><span class="line">            HW Address: fe:22:77:eb:2f:a4                   </span><br><span class="line">                   MTU: 1450 (min: 68, max: 65535)          </span><br><span class="line">                   VNI: 1                                   </span><br><span class="line">                 Local: 172.18.27.252                       </span><br><span class="line">      Destination Port: 8472                                </span><br><span class="line">     Underlying Device: enp1s0                              </span><br><span class="line">  Queue Length (Tx/Rx): 1/1                                 </span><br><span class="line">               Address: 10.187.1.0</span><br></pre></td></tr></table></figure><p>再测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a | grep -m1 flanneld</span></span><br><span class="line">f8759c103131        122cdb7aa710                                   "/opt/bin/flanneld -…"    27 minutes ago      Up 27 minutes                                   k8s_kube-flannel_kube-flannel-ds-85kps_kube-system_127265f3-f3ea-4f89-87e1-aa6c0e0d356f_0</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart f87</span></span><br><span class="line">f87</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">36: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether 1a:e0:cc:e3:a7:04 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>果然变了，如果没变尝试把 <code>none</code> 改成 <code>random</code> 试试。然后每个节点这样操作后，查看下了下 vtep 信息正常了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"9a:1e:00:9d:0c:60"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.252</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"1a:e0:cc:e3:a7:04"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.253</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.254</span><br></pre></td></tr></table></figure><p>测试下跨节点通信：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.187.0.5:9153/metrics</span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Length: 16905</span><br><span class="line">Content-Type: text/plain; version=0.0.4; charset=utf-8</span><br><span class="line">Date: Fri, 06 Nov 2020 01:47:31 GMT</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>应该一开始就看下 vtep 信息的，其次应该是 os 的问题，缺少 link 文件。还有这个和 flannel 没关系，只要用到 Linux 自带的 vxlan 接口在这种场景上都会出现，例如 calico 新版本也有 vxlan 模式。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.jinbuguo.com/systemd/systemd.link.html" target="_blank" rel="noopener">systemd link</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;和前一篇文章不一样，从来没遇到过这样的问题，这里记录下。实施在客户那边部署业务后，业务在浏览器上无法访问，我远程上去查看日志发现 pod 内</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/categories/k8s/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>统信USO 20 hostPort 无法访问</title>
    <link href="http://zhangguanzhang.github.io/2020/10/30/uos20-nftables/"/>
    <id>http://zhangguanzhang.github.io/2020/10/30/uos20-nftables/</id>
    <published>2020-10-30T18:31:01.000Z</published>
    <updated>2021-01-06T02:03:24.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这些天陆续发现很多客户是统信的系统，部署我们业务后无法访问</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>我自己环境和客户的环境都遇到了无法访问，我自己测试的机器信息是:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">PRETTY_NAME=&quot;Uniontech OS Server 20 Enterprise&quot;</span><br><span class="line">NAME=&quot;Uniontech OS Server 20 Enterprise&quot;</span><br><span class="line">VERSION_ID=&quot;20&quot;</span><br><span class="line">VERSION=&quot;20&quot;</span><br><span class="line">ID=UOS</span><br><span class="line">HOME_URL=&quot;https://www.chinauos.com/&quot;</span><br><span class="line">BUG_REPORT_URL=&quot;http://bbs.chinauos.com&quot;</span><br><span class="line">VERSION_CODENAME=fou</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxx-PC 4.19.0-arm64-server #1760 SMP Tue Jun 30 19:51:30 CST 2020 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>客户信息是:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">PRETTY_NAME=&quot;uos 20 SP1&quot;</span><br><span class="line">NAME=&quot;uos&quot;</span><br><span class="line">VERSION_ID=&quot;20 SP1&quot;</span><br><span class="line">VERSION=&quot;20 SP1&quot;</span><br><span class="line">ID=uos</span><br><span class="line">HOME_URL=&quot;https://www.chinauos.com/&quot;</span><br><span class="line">BUG_REPORT_URL=&quot;http://bbs.chinauos.com&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux kunpeng-PC 4.19.0-arm64-server #1707 SMP Thu Mar 26 17:43:52 CST 2020 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>k8s 版本信息:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:17:59Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:09:48Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>理论上<code>v1.15</code>以后的大版本号都不会有这种问题</p><h3 id="问题现象和解决手段"><a href="#问题现象和解决手段" class="headerlink" title="问题现象和解决手段"></a>问题现象和解决手段</h3><h4 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h4><p>我们业务入口是一个用的<code>hostPort</code>的 nginx ，部署好后无法访问，在宿主机上 curl 也会无法访问，同时<code>iptables</code>的条目会异常（下面的-m mark各个系统的先后可能顺序不同）:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> iptables -S</span></span><br><span class="line">...</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">.....</span><br><span class="line"><span class="meta">$</span><span class="bash"> iptables -S | grep <span class="string">&#x27;KUBE-FIREWALL&#x27;</span> | wc -l</span></span><br><span class="line">11054</span><br></pre></td></tr></table></figure><p>实际上这个问题就是因为统信系统基于<code>ubuntu</code>改的，很多新发行版系统包括centos8都是开始使用<code>nf_tables</code>作为rule规则管理，默认的<code>iptables</code>是<code>nf_tables</code>，低版本<code>kube-proxy</code>对它的兼容性不好，需要我们切换下<code>iptables</code>到老版本。</p><p>防止相关进程更新iptables，每台机器都得这样操作，先停掉相关进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop docker kubelet kube-proxy</span><br></pre></td></tr></table></figure><p>清空iptables</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">iptables -F</span><br><span class="line">iptables -t nat -F</span><br><span class="line">iptables -t mangle -F</span><br><span class="line">iptables -X</span><br><span class="line">kube-proxy --cleanup</span><br></pre></td></tr></table></figure><p>切换到老的iptables，apt系列的<code>update-alternatives</code>是系统自带的软连接管理，下面是把老的itables做成系统PATH的软连</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">update-alternatives --set iptables /usr/sbin/iptables-legacy</span><br><span class="line">update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy</span><br><span class="line">update-alternatives --set arptables /usr/sbin/arptables-legacy</span><br><span class="line">update-alternatives --set ebtables /usr/sbin/ebtables-legacy</span><br></pre></td></tr></table></figure><p>然后启动相关的，恢复正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker kubelet kube-proxy</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这些天陆续发现很多客户是统信的系统，部署我们业务后无法访问&lt;/p&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="uos" scheme="http://zhangguanzhang.github.io/categories/k8s/uos/"/>
    
    
    <category term="uos" scheme="http://zhangguanzhang.github.io/tags/uos/"/>
    
  </entry>
  
  <entry>
    <title>银河麒麟arm64系统上k8s集群跨节点不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/"/>
    <id>http://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/</id>
    <published>2020-10-20T18:31:01.000Z</published>
    <updated>2020-10-20T18:31:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>同事在客户那边部署的集群问题频繁，先给他解决了个问题后又反映说业务 POD 由于 DNS 无法解析而启动失败，排查完发现这样的情况从没遇到过，挺有意思的，这里记录下。实际排查过程也有往错误的方向浪费了一些时间和尝试，就不写进来了，以正确的角度写下排查过程。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>集群信息:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:17:59Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:09:48Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>OS 是 arm64 的银河麒麟系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">NAME=&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION=&quot;V10 (Tercel)&quot;</span><br><span class="line">ID=&quot;kylin&quot;</span><br><span class="line">VERSION_ID=&quot;V10&quot;</span><br><span class="line">PRETTY_NAME=&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxx 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><p>先看下集群 DNS 的 SVC IP。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get svc -l k8s-app=kube-dns</span></span><br><span class="line">NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">kube-dns             ClusterIP   10.186.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP       87m</span><br></pre></td></tr></table></figure><p>手动用 dig 发 DNS 请求看看，刚开始是用的<code>cluster.local</code>，后面感觉不对劲看了下 kubelet 的参数发现<code>cluster.domain</code>是<code>cluster1.local</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @10.186.0.2 kubernetes.default.svc.cluster1.local +tcp</span></span><br><span class="line">;; Connection to 10.186.0.2#53(10.186.0.2) for kubernetes.default.svc.cluster1.local failed: timed out.</span><br><span class="line">;; Connection to 10.186.0.2#53(10.186.0.2) for kubernetes.default.svc.cluster1.local failed: timed out.</span><br></pre></td></tr></table></figure><p>超时，用 coredns 的 metrics 接口试试:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.186.0.2:9153/metrics</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>还是超时，看下 flannel 的 vtep 都正确</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;ea:77:37:86:ee:bf&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.19</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;f2:d2:28:8e:4c:61&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.20</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;2a:f1:d4:d0:32:24&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.21</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;4a:e7:02:47:20:b8&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.22</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;ce:ce:f3:fc:3f:77&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.23</span><br></pre></td></tr></table></figure><p>看下 coredns 的 pod ip，绕过集群 SVC 使用 pod ip 测试下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span></span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-677d9c57f-tdnd4               1/1     Running   0          10m   10.187.1.24     172.31.159.21   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-677d9c57f-x274j               1/1     Running   0          10m   10.187.4.24     172.31.159.22   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.187.1.24:9153/metrics</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>还是超时，继续上面的 curl ，因为是 curl 的 9153 ，它不是常见的端口，否则下文的 tcpdump 过滤条件太麻烦了。这里去目的主机上抓包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.1.24 and port 9153 -vv</span></span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:39:35.019165 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xe94e (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440201878 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:35.068097 IP (tos 0x0, ttl 64, id 39684, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.45920 &gt; 10.187.1.24.9153: Flags [S], cksum 0x80ed (correct), seq 1103099580, win 64860, options [mss 1410,sackOK,TS val 632716806 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:35.068241 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xe91c (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440201928 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:43.419197 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xc87e (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440210278 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:43.708101 IP (tos 0x0, ttl 64, id 39685, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.45920 &gt; 10.187.1.24.9153: Flags [S], cksum 0x5f2d (correct), seq 1103099580, win 64860, options [mss 1410,sackOK,TS val 632725446 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:43.708233 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xc75c (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440210568 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:54.141929 IP (tos 0x0, ttl 64, id 12300, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0x0a5a (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632735880 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:54.142080 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xeb46 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440221001 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:55.148096 IP (tos 0x0, ttl 64, id 12301, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0x066c (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632736886 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:55.148381 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xe757 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440222008 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:56.219200 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xe329 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440223078 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:57.228103 IP (tos 0x0, ttl 64, id 12302, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0xfe4b (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632738966 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:57.228247 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xdf37 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440224088 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:59.259269 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xd748 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440226119 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:40:00.059221 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0x877e (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440226918 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:40:01.308098 IP (tos 0x0, ttl 64, id 12303, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0xee5b (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632743046 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:40:01.308248 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xcf47 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440228168 ecr 632735880,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>可以看到回了包，但是报文的<code>Flags</code>都是<code>[S]</code>和<code>[S.]</code>，说明是 TCP 的 SYN 的报文重传了，回到 curl 的机器上，另开一个窗口抓包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.1.24 and port 9153 -vv</span></span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:46:20.324596 IP (tos 0x0, ttl 64, id 7952, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.53748 &gt; 10.187.1.24.9153: Flags [S], cksum 0x29f7 (correct), seq 1340604575, win 64860, options [mss 1410,sackOK,TS val 633118295 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:46:20.324636 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xdf09 (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440603416 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:21.346975 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xdb0b (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440604438 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:21.395375 IP (tos 0x0, ttl 64, id 7953, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.53748 &gt; 10.187.1.24.9153: Flags [S], cksum 0x25c8 (correct), seq 1340604575, win 64860, options [mss 1410,sackOK,TS val 633119366 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:46:21.395409 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xdada (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440604487 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:23.426969 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xd2eb (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440606518 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:23.475374 IP (tos 0x0, ttl 64, id 7954, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.53748 &gt; 10.187.1.24.9153: Flags [S], cksum 0x1da8 (correct), seq 1340604575, win 64860, options [mss 1410,sackOK,TS val 633121446 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>当时没详细的看上面的报文，这里来仔细分析下上面的报文，收到<code>10.187.1.24.9153</code>回复的报文里<code>seq</code>都是<code>1340604575</code>，从抓包现象看是这个握手包确实回来了，但是从<code>seq</code>的数字看是没有接收者，也是就是目的主机上 pod 一直 tcp 重传。查看了下路由:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /run/flannel/subnet.env</span></span><br><span class="line">FLANNEL_NETWORK=10.187.0.0/16</span><br><span class="line">FLANNEL_SUBNET=10.187.0.1/24</span><br><span class="line">FLANNEL_MTU=1450</span><br><span class="line">FLANNEL_IPMASQ=true</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip a s flannel.1</span></span><br><span class="line">542: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default </span><br><span class="line">    link/ether b6:9b:ed:b0:37:74 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.187.0.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::b49b:edff:feb0:3774/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.0.0</span></span><br><span class="line">local 10.187.0.0 dev lo src 10.187.0.0 uid 0</span><br><span class="line">    cache &lt;local&gt;</span><br></pre></td></tr></table></figure><p>绝了，居然错了，莫名奇妙的是<code>lo</code>，看了下<code>NetworkManager</code>是开启的，重启了下它。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart NetworkManager</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.0.0</span></span><br><span class="line">broadcast 10.187.0.0 dev cni0 src 10.187.0.1 uid 0</span><br><span class="line">    cache &lt;local,brd&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.31.159.254  0.0.0.0         UG    100    0        0 eno1</span><br><span class="line">10.185.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0</span><br><span class="line">10.187.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0</span><br><span class="line">172.31.159.0    0.0.0.0         255.255.255.0   U     100    0        0 eno1</span><br></pre></td></tr></table></figure><p>路由正确了，但是 flannel 到其他节点的路由消失了，得重启下 flannel。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a | grep flanneld</span></span><br><span class="line">4b3f04e62b25        122cdb7aa710                                 &quot;/opt/bin/flanneld -…&quot;    2 hours ago         Up 2 hours                                             k8s_kube-flannel_kube-flannel-ds-22bwd_kube-system_6f5ce812-c5ae-4102-9398-c4a6fee4c7ab_0</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart 4b3</span></span><br><span class="line">4b3</span><br><span class="line"><span class="meta">$</span><span class="bash"> route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.31.159.254  0.0.0.0         UG    100    0        0 eno1</span><br><span class="line">10.185.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0</span><br><span class="line">10.187.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0</span><br><span class="line">10.187.1.0      10.187.1.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">10.187.2.0      10.187.2.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">10.187.3.0      10.187.3.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">10.187.4.0      10.187.4.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">172.31.159.0    0.0.0.0         255.255.255.0   U     100    0        0 eno1</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.0.0</span></span><br><span class="line">broadcast 10.187.0.0 dev cni0 src 10.187.0.1 uid 0</span><br><span class="line">    cache &lt;local,brd&gt;</span><br></pre></td></tr></table></figure><p>再 curl 下试试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -I 10.187.4.24:9153/metrics</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Length: 19491</span><br><span class="line">Content-Type: text/plain; version=0.0.4; charset=utf-8</span><br><span class="line">Date: Tue, 20 Oct 2020 10:14:42 GMT</span><br></pre></td></tr></table></figure><p>然后每台机器上去操作了下，集群跨节点网络没有任何问题了。我们也有其他开了<code>NetworkManager</code>的 K8S 环境，但是麒麟系统上是头一次遇到这个</p><h3 id="个人对于-NetworkManager-的一些看法"><a href="#个人对于-NetworkManager-的一些看法" class="headerlink" title="个人对于 NetworkManager 的一些看法"></a>个人对于 NetworkManager 的一些看法</h3><p>这个东西我个人角度讲是感觉不成熟，之前有次同事用 nmcli 配置的掩码导致 VIP 失效，配置文件里是 PREFIX ，最后我改回 NETMASK 正常，其他的一些问题也有，这里不多说。它是一个 daemon 进程，但是现在 Linux 上的网络技术层出不穷，它并没有及时的适配好，而且更新发布缓慢。</p><p>2021/01/12 尝试了下面，在部署之前执行下面即可永久解决</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cat&gt;</span><span class="bash"> /etc/NetworkManager/conf.d/k8s.conf &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">[keyfile]</span><br><span class="line">unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:flannel*;interface-name:veth*;interface-name:cni0;interface-name:docker0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>参考 <a href="https://docs.projectcalico.org/maintenance/troubleshoot/troubleshooting#configure-networkmanager">calico文档</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;同事在客户那边部署的集群问题频繁，先给他解决了个问题后又反映说业务 POD 由于 DNS 无法解析而启动失败，排查完发现这样的情况从没遇到过</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/categories/k8s/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>openshift 4.5.9 离线安装</title>
    <link href="http://zhangguanzhang.github.io/2020/09/18/ocp-4.5-install/"/>
    <id>http://zhangguanzhang.github.io/2020/09/18/ocp-4.5-install/</id>
    <published>2020-09-18T11:14:06.000Z</published>
    <updated>2020-09-18T11:14:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是全部离线安装，也就是 UPI (User Provisioned Infrastructure) 模式安装，假设机器只能配置静态ip不能有网络配置权限和配置 dhcp 和 pxe。机器可以是物理机和虚拟机。</p><h2 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h2><h3 id="ocp介绍"><a href="#ocp介绍" class="headerlink" title="ocp介绍"></a>ocp介绍</h3><p>openshift 分为社区版本 okd 和企业版本 ocp(openshift container platform)，okd 的更新很慢，ocp 个人也是可以安装的，不购买 license 则不会享受 redhat 的官方支持。</p><p>openshift 不像其他的 dashboard 诸如 rancher，k3s 之类的，它自己实现了 cs 的三个组件，也给 k8s 贡献了 rbac 和 ingress 的代码。它的节点系统是使用 Red Hat Enterprise Linux CoreOS (RHCOS)，这是一款面向容器的操作系统，结合了 CoreOS 和 Red Hat Atomic Host 操作系统的一些最佳特性和功能。</p><p>RHCOS 是专门为从 OpenShift Container Platform 运行容器化应用程序而设计的，能够与新工具配合，提供快速安装、基于 Operator 的管理和简化的升级，ocp 里 master节点必须是 rhcos 系统，而 worker 节点除了 rhcos 以外还可以选择 rhel。并且集群里有大量的 operator，通过使用 k8s 的声明式 yaml 减少了对系统和集群底层的关注度，甚至集群升级也是通过 cluster-version-operator 完成。</p><h3 id="ocp安装流程"><a href="#ocp安装流程" class="headerlink" title="ocp安装流程"></a>ocp安装流程</h3><p>先看这个官方的图，从图来讲流程</p><p><img src="https://image-static.segmentfault.com/241/514/241514728-5ee6e815a4963_articlex" alt="ocp_install_pic"></p><p>它的安装是先在一台机器（bastion）上准备 pxe 和相关的安装集群描述信息需要的文件（Ignition）以及 dns，负载均衡，然后引导主机（<code>Bootstrap</code>）通过 dhcp 或者 人为挂载 rhcos 的 iso 在安装界面手写 boot cmdline 从 bastion 指定获取 <code>bootstrap.ign</code>和 os.raw.gz 文件完成安装， 随后 master 节点启动会获取<code>master.ign</code>文件并且从 bootstrap 节点获取 machine-config 信息，随后 node 同理。</p><p>在安装过程中 bootstrap 的 ign文件里证书是24小时过期的，因为官方这种涉及理念是 bootstrap 作用于引导集群的，安装完后会将控制平面移交到 master上，所以我们要配置负载均衡（以及代理后续集群里的 ingress controller）</p><ul><li>先在一台机器（bastion）上准备pxe和相关的安装集群描述信息需要的文件（Ignition）以及 dns，负载均衡，也就是图里的 <code>Cluster Installer</code></li><li><code>bastion</code>上手写一个集群安装的 yaml ，然后使用 oc 命令把 yaml 转换成集群部署清单和 Ignition 文件</li><li>在引导主机启动前准备好 DNS，负载均衡，镜像仓库</li><li>引导主机（<code>Bootstrap</code>）通过 dhcp 或者挂载 rhcos 的 iso 启动后在安装界面手写<code>boot cmdline</code>（包含网络配置，nameserver，install_dev,image_url,ignition_url），安装完系统后重启后，bootstrap 机器会执行 bootkube.sh 脚本，内部是 crio 和 podman 启动容器和 pod 来启动控制平面，并等待 master 加入</li><li><code>Master</code> 节点如果像我没有dhcp就手动配置boot cmdline 安装后会从引导主机远程获取资源并完成引导，会作为 node 注册。</li><li>bootstrap 节点 watch 到 node的注册后，会在集群里部署 operator 执行 一些 install-n-master的 pod，例如引导主机会让 master 节点构建 <code>Etcd</code> 集群。</li><li>引导主机使用新的 <code>Etcd</code> 集群启动临时 <code>Kubernetes</code> 控制平面。</li><li>临时控制平面在 Master 节点启动生成控制平面。</li><li>临时控制平面关闭并将控制权传递给生产控制平面。</li><li>引导主机将 OCP 组件注入生成控制平面。</li><li>整个过程主要是 bootstrap 上的 <code>bootkube.sh</code> 执行，最后执行完后会在集群里添加一个 <code>-n kube-system configmap/bootstrap</code> 作为保存状态。</li></ul><p>引导安装过程完成以后，OCP 集群部署完毕。然后集群开始下载并配置日常操作所需的其余组件，包括创建计算节点、通过 <code>Operator</code> 安装其他服务等。</p><h2 id="服务器规划"><a href="#服务器规划" class="headerlink" title="服务器规划"></a>服务器规划</h2><h3 id="资源规划"><a href="#资源规划" class="headerlink" title="资源规划"></a>资源规划</h3><p>官方文档<a href="https://docs.openshift.com/container-platform/4.5/installing/installing_bare_metal/installing-bare-metal-network-customizations.html#minimum-resource-requirements_installing-bare-metal-network-customizations">最小配置</a>信息为:</p><table><thead><tr><th>Machine</th><th>Operating System</th><th>vCPU</th><th>RAM</th><th>Storage</th></tr></thead><tbody><tr><td>Bootstrap</td><td>RHCOS</td><td>4</td><td>16GB</td><td>120 GB</td></tr><tr><td>Control plane</td><td>RHCOS</td><td>4</td><td>16 GB</td><td>120 GB</td></tr><tr><td>Compute</td><td>RHCOS or RHEL 7.6</td><td>2</td><td>8 GB</td><td>120 GB</td></tr></tbody></table><p>这里机器信息是如下，官方很多镜像都是存在<code>quay.io</code>这个仓库下，因为墙的问题会无法拉取，所以实际部署都会部署一个镜像仓库，这里我镜像仓库是使用 quay ，它会使用 443 端口(暂时没找到更改端口的方法)，负载均衡会负载 ingress controller 的 http 和 https（443端口）节点，所以镜像仓库单独一个机器，如果你用其他的仓库例如 registry 可以放在负载均衡的节点上。节点的配置和 role 如下关系，因为 bootstrap 用完后可以删除，可以后面用作 worker 节点。<strong>单 master 节点参照网上的我部署有问题，后文会说明原因</strong></p><p>服务器规划如下：</p><ul><li>三个控制平面节点，安装 <code>Etcd</code>、控制平面组件。</li><li>一个计算节点，运行实际负载。</li><li>一个引导主机，执行安装任务，集群部署完成后当作 worker 使用。</li><li>一个基础节点，用于准备上离线资源，同时用来部署 DNS 和负载均衡（负载<code>machine-config</code>、ocp 的 <code>kube-apiserver</code> 和 集群里的 <code>ingress controller</code>）。</li><li>一个镜像节点，用来部署私有镜像仓库 <code>Quay</code>，因为官方镜像在 <code>quay.io</code> 上很难拉取。</li><li>翻墙节点不在下面列表里，或者有软路由之类的可以在基础节点直接拉镜像更好</li></ul><table><thead><tr><th>Machine and hostName</th><th>OS</th><th>vCPU</th><th>RAM</th><th>Storage</th><th>IP</th><th>FQND</th><th>describe</th></tr></thead><tbody><tr><td>registry</td><td>Centos7.8</td><td>4</td><td>8GB</td><td>100 GB</td><td>10.226.45.226</td><td>registry.openshift4.example.com</td><td>镜像仓库</td></tr><tr><td>bastion</td><td>Centos7.8</td><td>8</td><td>8GB</td><td>100 GB</td><td>10.226.45.250</td><td>bastion.openshift4.example.com</td><td>dns，负载均衡(LB)，http文件下载</td></tr><tr><td>bootstrap</td><td>RHCOS</td><td>4</td><td>16GB</td><td>120 GB</td><td>10.226.45.223</td><td>bootstrap.openshift4.example.com</td><td>也叫引导节点(Bootstrap)</td></tr><tr><td>master1</td><td>RHCOS</td><td>8</td><td>16 GB</td><td>120 GB</td><td>10.226.45.251</td><td>master1.openshift4.example.com</td><td>也叫控制节点(Control plane)</td></tr><tr><td>master2</td><td>RHCOS</td><td>8</td><td>16 GB</td><td>120 GB</td><td>10.226.45.252</td><td>master2.openshift4.example.com</td><td>也叫控制节点(Control plane)</td></tr><tr><td>master3</td><td>RHCOS</td><td>8</td><td>16 GB</td><td>120 GB</td><td>10.226.45.222</td><td>master3.openshift4.example.com</td><td>也叫控制节点(Control plane)</td></tr><tr><td>worker1</td><td>RHCOS</td><td>4</td><td>16 GB</td><td>120 GB</td><td>10.226.45.223</td><td>worker1.openshift4.example.com</td><td>也叫计算节点(Compute)</td></tr></tbody></table><ul><li>这里 253 ip被抢占了，所以 master3 的ip选 222</li><li><code>openshift4</code> 是集群名，<code>example.com</code>是 basedomain</li><li>ocp内部的 kubernetes api、router、etcd通信都是使用域名，所以这里我们也给主机规定下<code>FQDN</code>，所有节点主机名都要采用三级域名格式，如 <code>master1.aa.bb.com</code>。后面会在 dns server里写入记录</li></ul><h3 id="防火墙端口"><a href="#防火墙端口" class="headerlink" title="防火墙端口"></a>防火墙端口</h3><p>接下来看一下每个节点的端口号分配。</p><p>所有节点（计算节点和控制平面）之间需要开放的端口：</p><table><thead><tr><th align="center">协议</th><th align="center">端口</th><th align="center">作用</th></tr></thead><tbody><tr><td align="center">ICMP</td><td align="center">N/A</td><td align="center">测试网络连通性</td></tr><tr><td align="center">TCP</td><td align="center"><code>9000-9999</code></td><td align="center">节点的服务端口，包括 node exporter 使用的 <code>9100-9101</code> 端口和 Cluster Version Operator 使用的 <code>9099</code> 端口</td></tr><tr><td align="center"></td><td align="center"><code>10250</code>-<code>10259</code></td><td align="center">Kubernetes 预留的默认端口</td></tr><tr><td align="center"></td><td align="center"><code>10256</code></td><td align="center">openshift-sdn</td></tr><tr><td align="center">UDP</td><td align="center"><code>4789</code></td><td align="center">VXLAN 协议或 GENEVE 协议的通信端口</td></tr><tr><td align="center"></td><td align="center"><code>6081</code></td><td align="center">VXLAN 协议或 GENEVE 协议的通信端口</td></tr><tr><td align="center"></td><td align="center"><code>9000</code>-<code>9999</code></td><td align="center">节点的服务端口，包括 node exporter 使用的 <code>9100-9101</code> 端口</td></tr><tr><td align="center"></td><td align="center"><code>30000</code>-<code>32767</code></td><td align="center">Kubernetes NodePort range</td></tr></tbody></table><p>控制平面需要向其他节点开放的端口：</p><table><thead><tr><th align="center">协议</th><th align="center">端口</th><th align="center">作用</th></tr></thead><tbody><tr><td align="center">TCP</td><td align="center"><code>2379</code>-<code>2380</code></td><td align="center">Etcd 服务端口</td></tr><tr><td align="center">TCP</td><td align="center"><code>6443</code></td><td align="center">Kubernetes API</td></tr></tbody></table><p>除此之外，还要配置两个四层负载均衡器，一个用来暴露集群 API，一个用来暴露 Ingress：</p><table><thead><tr><th align="center">端口</th><th align="center">作用</th><th align="center">内部</th><th align="center">外部</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center"><code>6443</code></td><td align="center">引导主机和控制平面使用。在引导主机初始化集群控制平面后，需从负载均衡器中手动删除引导主机</td><td align="center">x</td><td align="center">x</td><td align="center">Kubernetes API server</td></tr><tr><td align="center"><code>22623</code></td><td align="center">引导主机和控制平面使用。在引导主机初始化集群控制平面后，需从负载均衡器中手动删除引导主机</td><td align="center"></td><td align="center">x</td><td align="center">Machine Config server</td></tr><tr><td align="center"><code>443</code></td><td align="center">Ingress Controller 或 Router 使用</td><td align="center">x</td><td align="center">x</td><td align="center">HTTPS 流量</td></tr><tr><td align="center"><code>80</code></td><td align="center">Ingress Controller 或 Router 使用</td><td align="center">x</td><td align="center">x</td><td align="center">HTTP 流量</td></tr></tbody></table><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>按照官方文档，使用 UPI 基础架构的 OCP 集群需要以下的 DNS 记录。在每条记录中，<code>&lt;cluster_name&gt;</code> 是集群名称，<code>&lt;base_domain&gt;</code> 是在 <code>install-config.yaml</code> 文件中指定的集群基本域，如下表所示：</p><table><thead><tr><th align="center">组件</th><th align="center">DNS记录</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">Kubernetes API</td><td align="center"><code>api.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">此 DNS 记录必须指向控制平面节点的负载均衡器。此记录必须可由集群外部的客户端和集群中的所有节点解析。</td></tr><tr><td align="center"></td><td align="center"><code>api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">此 DNS 记录必须指向控制平面节点的负载均衡器。此记录必须可由集群外部的客户端和集群中的所有节点解析。</td></tr><tr><td align="center">Routes</td><td align="center"><code>*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">DNS 通配符记录，指向负载均衡器。这个负载均衡器的后端是 Ingress router 所在的节点，默认是计算节点。此记录必须可由集群外部的客户端和集群中的所有节点解析。</td></tr><tr><td align="center">etcd</td><td align="center"><code>etcd-&lt;index&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">OCP 要求每个 etcd 实例的 DNS 记录指向运行实例的控制平面节点。etcd 实例由 <index> 值区分，它们以 <code>0</code> 开头，以 <code>n-1</code> 结束，其中 <code>n</code> 是集群中控制平面节点的数量。集群中的所有节点必须都可以解析此记录。</td></tr><tr><td align="center"></td><td align="center"><code>_etcd-server-ssl._tcp.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">因为 etcd 使用端口 <code>2380</code> 对外服务，因此需要建立对应每台 etcd 节点的 SRV DNS 记录，优先级 0，权重 10 和端口 2380</td></tr></tbody></table><h2 id="镜像准备"><a href="#镜像准备" class="headerlink" title="镜像准备"></a>镜像准备</h2><p>因为镜像都是在<code>quay.io</code>上，国内很难拉取下来，所以参考官方文档 <a href="https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html">Creating a mirror registry for installation in a restricted network</a> 创建个镜像仓库。**要求支持 version 2 schema 2 (manifest list)**，我这里选择的是 <code>Quay 3.3</code>。quay 镜像仓库需要部署在另外一台节点，因为它需要用到 <code>443</code> 端口，与后面的负载均衡 https 端口冲突。同时因为镜像是需要翻墙拉取，所以需要自备一台能翻墙的节点处于网络边界上。</p><h3 id="本地镜像仓库-registry"><a href="#本地镜像仓库-registry" class="headerlink" title="本地镜像仓库 - registry"></a>本地镜像仓库 - registry</h3><p>仓库没必要开始搭建，在下文的 oc 同步镜像之前搭建好即可</p><p>这里使用<code>docker-compose</code>搭建 quay 仓库，自行安装 <code>docker</code> 和 <code>docker-compose</code> 设置好系统和内核参数，如果你也使用容器搭建，可以不用和我一样的操作系统。容器已经能通过<code>alias</code>互联，所以没必要的端口不用映射到宿主机上，你也可以使用 <code>podman</code> 或者其他容器工具起一个环境，甚至<code>docker run</code>起来这些容器。</p><p>设置机器的 hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname registry.openshift4.example.com</span><br></pre></td></tr></table></figure><h4 id="docker-compose-for-registry"><a href="#docker-compose-for-registry" class="headerlink" title="docker-compose for registry"></a>docker-compose for registry</h4><p>这里数据都存放在<code>/data/quay/xxx</code>，创建目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/quay/lib/mysql \</span><br><span class="line">  /data/quay/lib/redis \</span><br><span class="line">  /data/quay/config \</span><br><span class="line">  /data/quay/storage</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器权限问题，容器的user都在root组下，但是默认<span class="built_in">umask</span>下组没有w权限，所以这里加下</span></span><br><span class="line">chmod g+w /data/quay/lib/mysql/ \</span><br><span class="line">  /data/quay/lib/redis/ \</span><br><span class="line">  /data/quay/config \</span><br><span class="line">  /data/quay/storage</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建quay仓库的<code>docker-compose.yml</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cat&gt;</span><span class="bash">/data/quay/docker-compose.yml &lt;&lt; <span class="string">EOF</span></span></span><br><span class="line">version: &#x27;3.2&#x27;</span><br><span class="line">services:</span><br><span class="line">  quay:</span><br><span class="line">    #image: quay.io/redhat/quay:v3.3.1</span><br><span class="line">    image: registry.aliyuncs.com/quayx/redhat-quay:v3.3.1</span><br><span class="line">    container_name: quay</span><br><span class="line">    restart: always</span><br><span class="line">    privileged: true</span><br><span class="line">    sysctls:</span><br><span class="line">      - net.core.somaxconn=1024</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/quay/config:/conf/stack:Z</span><br><span class="line">      - /data/quay/storage:/datastorage:Z</span><br><span class="line">    ports:</span><br><span class="line">      - 443:8443</span><br><span class="line">    command: [&quot;config&quot;, &quot;redhat&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - mysql</span><br><span class="line">      - redis</span><br><span class="line">    networks:</span><br><span class="line">      quay:</span><br><span class="line">        aliases:</span><br><span class="line">          - config</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">  mysql:</span><br><span class="line">    image: registry.access.redhat.com/rhscl/mysql-57-rhel7</span><br><span class="line">    container_name: quay-mysql</span><br><span class="line">    restart: always</span><br><span class="line">    privileged: true</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/quay/lib/mysql:/var/lib/mysql/data:Z</span><br><span class="line">    # ports:</span><br><span class="line">    #   - 3306:3306</span><br><span class="line">    environment:</span><br><span class="line">      - MYSQL_ROOT_PASSWORD=redhat</span><br><span class="line">      - MYSQL_DATABASE=enterpriseregistrydb</span><br><span class="line">      - MYSQL_USER=quayuser</span><br><span class="line">      - MYSQL_PASSWORD=redhat</span><br><span class="line">    networks:</span><br><span class="line">      quay:</span><br><span class="line">        aliases:</span><br><span class="line">          - mysql</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">  redis:</span><br><span class="line">    image: registry.access.redhat.com/rhscl/redis-32-rhel7</span><br><span class="line">    container_name: quay-redis</span><br><span class="line">    restart: always</span><br><span class="line">    privileged: true</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/quay/lib/redis:/var/lib/redis/data:Z</span><br><span class="line">    networks:</span><br><span class="line">      quay:</span><br><span class="line">        aliases:</span><br><span class="line">          - redis</span><br><span class="line">    # ports:</span><br><span class="line">    #   - 6379:6379</span><br><span class="line">    depends_on:</span><br><span class="line">      - mysql</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">networks:</span><br><span class="line">  quay:</span><br><span class="line">    name: quay</span><br><span class="line">    external: false</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><strong>数据库</strong> : 主要存放镜像仓库的元数据（非镜像存储)</li><li><strong>Redis</strong> : 存放构建日志和Quay的向导</li><li><strong>Quay</strong> : 作为镜像仓库</li><li><strong>Clair</strong> : 提供镜像扫描功能</li></ul><p>注意，其中的镜像<code>quay.io/redhat/quay:v3.3.1</code>是无法拉取的，参考 <a href="https://access.redhat.com/solutions/3533201">官方链接</a> 获取Red Hat Quay v3 镜像的访问权才可以拉取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u=&quot;redhat+quay&quot; -p=&quot;O81WSHRSJR14UAZBK54GQHJS0P1V4CLWAJV1X2C4SD7KO59CQ9N3RE12612XU1HR&quot; quay.io</span><br></pre></td></tr></table></figure><p>这个镜像我已经同步到阿里云上的镜像仓库上，也方便拉取，另外这个镜像的运行命令 <code>config redhat</code>的 <code>redhat</code> 是 quay 仓库起来后的 web 里用到的密码，这里我们先拉取上面所需要的镜像。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay</span><br><span class="line">docker-compose pull</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>后续还会部署的话推荐这里使用 docker 把这三个镜像<code>docker save -o</code>成一个tar包方便以后<code>docker load -i</code>导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay</span><br><span class="line">docker save \</span><br><span class="line">  registry.access.redhat.com/rhscl/redis-32-rhel7 \</span><br><span class="line">  registry.access.redhat.com/rhscl/mysql-57-rhel7 \</span><br><span class="line">  registry.aliyuncs.com/quayx/redhat-quay:v3.3.1 | gzip - &gt; quay-img.tar.gz</span><br></pre></td></tr></table></figure><h4 id="setup-for-registry"><a href="#setup-for-registry" class="headerlink" title="setup for registry"></a>setup for registry</h4><p>起来后访问<code>https://ip</code> basic auth 信息为<code>quayconfig/redhat</code>，选择<code>Start New Registry Setup</code></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531150958.png" alt="choose an option"></p><p>选择新建配置，然后设置数据库：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531151305.png" alt="setup for rds"></p><p>连接信息按照上面的<code>docker-compose</code>里的环境变量写，<code>ssl certificate</code>先别管。</p><p>设置超级管理员，记住密码，然后下一步</p><p>下一步然后页面往下滑动，在<code>Server Configuration</code>段里设置<code>Server Hostname</code>，例如为<code>registry.openshift4.example.com</code>，往下滑动，配置redis信息</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531152811.png" alt="Server Configuration"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531152931.png" alt="redis"></p><p>点击左下角的Save，弹出的<code>Checking</code>全绿后点击<code>Next</code></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531153820.png" alt="checking your settings"></p><p>配置检查通过后，就可以保存下载下来：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531154244.png" alt="Download Configuration"></p><p>最后会导出一个 <code>quay-config.tar.gz</code>，将其上传到 Quay 所在的服务器，解压到配置文件目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp quay-config.tar.gz /data/quay/config/</span><br><span class="line">cd /data/quay/config/</span><br><span class="line">tar zxvf quay-config.tar.gz</span><br></pre></td></tr></table></figure><h4 id="ssl-for-registry"><a href="#ssl-for-registry" class="headerlink" title="ssl for registry"></a>ssl for registry</h4><p>接下来为仓库生成域名自签名证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay/config/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成私钥</span></span><br><span class="line">openssl genrsa -out ssl.key 1024</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成证书，最好使用通配符</span></span><br><span class="line">openssl req \</span><br><span class="line">  -newkey rsa:2048 -nodes -keyout ssl.key \</span><br><span class="line">  -x509 -days 36500 -out ssl.cert -subj \</span><br><span class="line">  &quot;/C=CN/ST=Wuhan/L=Wuhan/O=WPS/OU=WPS/CN=*.openshift4.example.com&quot;</span><br></pre></td></tr></table></figure><p>证书搞定后<code>PREFERRED_URL_SCHEME: http</code>修改成 https</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -ri &#x27;/^PREFERRED_URL_SCHEME:/s#\S+$#https#&#x27; config.yaml</span><br></pre></td></tr></table></figure><p>然后停掉服务，注释掉 command 后再启动，web 打开看看是不是镜像仓库，是的话本机添加下 hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay/</span><br><span class="line">docker-compose down</span><br><span class="line">sed -ri &#x27;/^\s*command: \[&quot;config&quot;/s@^@#@&#x27; docker-compose.yml</span><br><span class="line">docker-compose up -d</span><br><span class="line">grep -qw &#x27;registry.openshift4.example.com&#x27; /etc/hosts || </span><br><span class="line">    echo &#x27;127.0.0.1 registry.openshift4.example.com&#x27; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>去浏览器上web登录下镜像仓库，添加一个<code>Organization</code>，名字为<code>ocp4</code>用于存放镜像，然后添加一个<code>Repository</code>名字为<code>openshift4</code></p><h2 id="纵云梯节点配置"><a href="#纵云梯节点配置" class="headerlink" title="纵云梯节点配置"></a>纵云梯节点配置</h2><h3 id="配置registry-的-hosts"><a href="#配置registry-的-hosts" class="headerlink" title="配置registry 的 hosts"></a>配置registry 的 hosts</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -qw &#x27;registry.openshift4.example.com&#x27; /etc/hosts || </span><br><span class="line">    echo &#x27;10.226.45.226 registry.openshift4.example.com&#x27; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="二进制文件和-secret-json-准备"><a href="#二进制文件和-secret-json-准备" class="headerlink" title="二进制文件和 secret.json 准备"></a>二进制文件和 secret.json 准备</h3><p>拉取镜像是在翻墙的节点上使用的<code>openshift client</code>的 oc 二进制 cli ，该命令大部分的子命令在操作集群的时候和 kubectl 是一致的。此节需要我们在纵云梯节点上执行oc 命令执行去把镜像拉取推送到本地的 registry 上。我们先准备拉取镜像用到的一些前置文件</p><p>首先是 oc 下载，很多机器都需要，官方说 这个<a href="https://cloud.redhat.com/openshift/install">页面</a>下载，我们也可以去<a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-4.5/">mirror页面</a>  下载。解压后放到系统的 <code>$PATH</code> 里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-4.5/openshift-client-linux-4.5.9.tar.gz</span><br><span class="line">tar zxvf openshift-client-linux-4.5.9.tar.gz</span><br><span class="line">mv oc kubectl /usr/local/bin</span><br></pre></td></tr></table></figure><p>安装一些基础小工具，jq 用来格式化 json 文件，chrony 用于时间同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release &amp;&amp; \</span><br><span class="line">  yum install \</span><br><span class="line">  jq \</span><br><span class="line">  bind-utils \</span><br><span class="line">  tcpdump \</span><br><span class="line">  chrony \</span><br><span class="line">  httpd-tools \</span><br><span class="line">  dos2unix \</span><br><span class="line">  strace</span><br></pre></td></tr></table></figure><p>准备拉取镜像权限认证文件，很多镜像在好几个镜像仓库上，有授权信息才能拉取。 从 Red Hat OpenShift Cluster Manager 站点的 Pull Secret 页面下载 <a href="https://cloud.redhat.com/openshift/install/pull-secret">registry.redhat.io</a> 的 pull secret</p><p>把文件或者内容整上去后，格式化下json</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jq . pull-secret.txt &gt; pull-secret.json</span><br></pre></td></tr></table></figure><p>大致下面的内容</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;auths&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;cloud.openshift.com&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;b3BlbnNo...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;quay.io&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;b3BlbnNo...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;registry.connect.redhat.com&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;NTE3Njg5Nj...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;registry.redhat.io&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;NTE3Njg5Nj...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>把前面 quay 的用户名和密码按照<code>user:pass</code> base64 加密了，例如<code>echo -n admin:openshift | base64</code>， 然后符合 json 的格式要求下把镜像仓库和 auth 信息追加到<code>pull-secret.json</code>里</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    &quot;registry.openshift4.example.com&quot;: &#123;</span><br><span class="line">      &quot;auth&quot;: &quot;......==&quot;</span><br><span class="line">    &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>增加后用 <code>jq . pull-secret.json</code> 检验下 json 格式是否正确</p><p>下面利用变量拼接一些镜像tag来同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9</span></span><br><span class="line">OCP_VERSION=$(oc version -o json | jq -r .releaseClientVersion)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9-x86_64</span></span><br><span class="line">OCP_RELEASE=$OCP_VERSION-$(arch)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4</span></span><br><span class="line">OCP_MAJOR=$&#123;OCP_VERSION%%.*&#125;</span><br><span class="line"></span><br><span class="line">LOCAL_REGISTRY=&#x27;registry.openshift4.example.com&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 对应前面的 ocp4/openshift4</span></span><br><span class="line">LOCAL_REPOSITORY=&quot;ocp$&#123;OCP_MAJOR&#125;/openshift$&#123;OCP_MAJOR&#125;&quot;</span><br><span class="line"></span><br><span class="line">PRODUCT_REPO=&#x27;openshift-release-dev&#x27;</span><br><span class="line">RELEASE_NAME=&quot;ocp-release&quot;</span><br><span class="line"></span><br><span class="line">LOCAL_SECRET_JSON=&#x27;pull-secret.json&#x27;</span><br></pre></td></tr></table></figure><ul><li><strong>OCP_RELEASE</strong> : OCP 版本，可以在<a href="https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags">这个页面</a>查看。如果版本不对，下面执行 <code>oc adm</code> 时会提示 <code>image does not exist</code>。</li><li><strong>LOCAL_REGISTRY</strong> : 本地仓库的域名和端口。</li><li><strong>LOCAL_REPOSITORY</strong> : 镜像存储库名称，使用 <code>ocp4/openshift4</code>。</li><li><code>PRODUCT_REPO</code> 和 <code>RELEASE_NAME</code> 都不需要改，这些都是一些版本特征，保持不变即可。</li><li><strong>LOCAL_SECRET_JSON</strong> : 密钥路径，就是上面 <code>pull-secret.json</code> 的存放路径。</li></ul><h3 id="同步镜像"><a href="#同步镜像" class="headerlink" title="同步镜像"></a>同步镜像</h3><p>同步镜像分为两种方式，一种是实时使用 oc 命令把镜像转发到 <code>LOCAL_REGISTRY</code>，一种是离线存储为文件，然后把文件拿到内网去用 oc 命令推送到内网的仓库。</p><p>一些后续可研究的东西:<br>如果后续有需求研究同步镜像到国内，每个版本的镜像列表可以查看<a href="https://github.com/openshift/cluster-version-operator/blob/master/docs/user/reconciliation.md">官方这个文档</a>里的命令解开镜像去查看镜像列表：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看镜像info</span></span><br><span class="line">oc adm release info -a $&#123;LOCAL_SECRET_JSON&#125;\</span><br><span class="line"> quay.io/openshift-release-dev/ocp-release:4.5.9-x86_64</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解开到本地查看</span></span><br><span class="line">oc image extract -a $&#123;LOCAL_SECRET_JSON&#125; \</span><br><span class="line">  quay.io/openshift-release-dev/ocp-release:4.5.9-x86_64 \</span><br><span class="line">  --path /:/tmp/release</span><br></pre></td></tr></table></figure><p>如果想解开这个镜像研究的话，我已经把这个镜像同步到阿里上了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skopeo copy  docker://quay.io/openshift-release-dev/ocp-release:4.5.9-x86_64 \</span><br><span class="line">  docker://registry.aliyuncs.com/openshift-release-dev/ocp-release:4.5.9-x86_64   \</span><br><span class="line">  --insecure-policy</span><br></pre></td></tr></table></figure><h4 id="实时转发到-LOCAL-REGISTRY"><a href="#实时转发到-LOCAL-REGISTRY" class="headerlink" title="实时转发到 LOCAL_REGISTRY"></a>实时转发到 LOCAL_REGISTRY</h4><p>添加registry的hosts到同步的机器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -qw &#x27;registry.openshift4.example.com&#x27; /etc/hosts || </span><br><span class="line">    echo &#x27;10.226.45.226 registry.openshift4.example.com&#x27; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>执行同步命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里之前打算同步到阿里云仓库上，但是一直报错，以后有空的时候再研究下看怎么同步到阿里的镜像仓库上</span></span><br><span class="line">oc adm -a $&#123;LOCAL_SECRET_JSON&#125; release mirror \</span><br><span class="line">  --from=quay.io/$&#123;PRODUCT_REPO&#125;/$&#123;RELEASE_NAME&#125;:$&#123;OCP_RELEASE&#125; \</span><br><span class="line">  --to=$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125; \</span><br><span class="line">  --to-release-image=$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125;:$&#123;OCP_RELEASE&#125; \</span><br><span class="line">  --insecure # 镜像仓库是自签名证书，所以加这个选项</span><br></pre></td></tr></table></figure><p>下面是输出，可以临时保存下:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">info: Mirroring 110 images to registry.openshift4.example.com/ocp4/openshift4 ...</span><br><span class="line">registry.openshift4.example.com/</span><br><span class="line">  ocp4/openshift4</span><br><span class="line">    manifests:</span><br><span class="line">      sha256:00edb6c1dae03e1870e1819b4a8d29b655fb6fc40a396a0db2d7c8a20bd8ab8d -&gt; 4.5.9-local-storage-static-provisioner</span><br><span class="line">      sha256:0259aa5845ce43114c63d59cedeb71c9aa5781c0a6154fe5af8e3cb7bfcfa304 -&gt; 4.5.9-machine-api-operator</span><br><span class="line">      sha256:07f11763953a2293bac5d662b6bd49c883111ba324599c6b6b28e9f9f74112be -&gt; 4.5.9-cluster-kube-storage-version-migrator-operator</span><br><span class="line">      sha256:08068503746a2dfd4dcaf61dca44680a9a066960cac7b1e81ea83e1f157e7e8c -&gt; 4.5.9-service-ca-operator</span><br><span class="line">      sha256:09a7dea10cd584c6048f8df3dcec67dd9a8432eb44051353e180dfeb350c6310 -&gt; 4.5.9-cluster-node-tuning-operator</span><br><span class="line">      sha256:09c763b2ad50711c2b1b5f26c9c70875f821f460e7214f1b11c4be8374424f42 -&gt; 4.5.9-kube-client-agent</span><br><span class="line">      sha256:1140eff8442d5ac679a3a85655723cd51f12a63c0c8d61251214ff952cc40b06 -&gt; 4.5.9-container-networking-plugins</span><br><span class="line">      sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd -&gt; 4.5.9-cluster-etcd-operator</span><br><span class="line">      sha256:14f97b5e9195c8f9b682523dc1febbb1f75fd60d408ac2731cdfa047aee0f43d -&gt; 4.5.9-must-gather</span><br><span class="line">      sha256:15be0e6de6e0d7bec726611f1dcecd162325ee57b993e0d886e70c25a1faacc3 -&gt; 4.5.9-openshift-controller-manager</span><br><span class="line">      sha256:167c14d56714982360fbaa72b3eb3f8d4fb6c5ad10bbbb7ea396606d8412b7eb -&gt; 4.5.9-sdn</span><br><span class="line">      sha256:1960c492ce80345e03d50c6cada3d37341ca604d891c48b03109de525e99997a -&gt; 4.5.9-tests</span><br><span class="line">      sha256:1c24c01e807700d30430ce4ff2f77b54d55910cfbf9ee0cbe5c1c64979547463 -&gt; 4.5.9-prometheus-operator</span><br><span class="line">      sha256:1cf4b3ce90933a7dbef9d01ebcb00a7f490dce7c44de6ddb16ddebad07de8eda -&gt; 4.5.9-k8s-prometheus-adapter</span><br><span class="line">      sha256:1d73207be4fbba735dbb53facd23c2359886a9194df3267fd91a843a7ba10316 -&gt; 4.5.9-console-operator</span><br><span class="line">      sha256:203ab5955dbcbc5f3334e1d7f2ba9fac5f7a1e187370e88c08ad2996a8507711 -&gt; 4.5.9-aws-pod-identity-webhook</span><br><span class="line">      sha256:21fdd7329f0d9d85acb935223915fbb3af07d4ea051678fc8ec85e47ab95edb1 -&gt; 4.5.9-mdns-publisher</span><br><span class="line">      sha256:22e47de98549ac9e792f06d4083b5d99db3ecc8dc4c94c8950dba9756a75dc28 -&gt; 4.5.9-grafana</span><br><span class="line">      sha256:2527b9f6712b8551cbcec637fc87bb9640973e9f9f4972d489c5308ef25eeed0 -&gt; 4.5.9-jenkins</span><br><span class="line">      sha256:273585703f8f56c980ff77f9a71e6e380d8caa645c4101525572906e0820e8fc -&gt; 4.5.9-docker-builder</span><br><span class="line">      sha256:27774942e605c4d4b13da7721c75135aac27db8409862625b76f5b31eb2e0d63 -&gt; 4.5.9-keepalived-ipfailover</span><br><span class="line">      sha256:2899afae1d0d9baa0309944dd55975a195a2cbd211f9ecf574e39287827fa684 -&gt; 4.5.9-ovirt-machine-controllers</span><br><span class="line">      sha256:289d792eada5d1460d69a927dff31328b5fcb1f1d2e31eb7a3fd69afd9a118fc -&gt; 4.5.9-kuryr-cni</span><br><span class="line">      sha256:2b40c4a5cb2a9586ec6c8e43e3013e6ee97781399b2fa836cf8d6c181ff8430c -&gt; 4.5.9-jenkins-agent-maven</span><br><span class="line">      sha256:2be07ebc1d005decbb8e624e9beb4d282900f42eb4f8ec0b76e31daedd8874cc -&gt; 4.5.9-installer-artifacts</span><br><span class="line">      sha256:3161e52e7bbbf445170c4992d5a47ed87c1d026a7f94b8d0cd30c4508fb48643 -&gt; 4.5.9-oauth-server</span><br><span class="line">      sha256:36e7c99acba49c22c7b84a339bfdab820fae17dec07cb4a17db18617b1d46a37 -&gt; 4.5.9-kube-etcd-signer-server</span><br><span class="line">      sha256:39f8b615e82b3a3a087d6f7d301303d8a0862e4ca13e8f99e6ad968a04985f80 -&gt; 4.5.9-cli</span><br><span class="line">      sha256:3b893950a9db5aa3653d80b05b236e72c168ff368d1c2e0192dd6036486eb715 -&gt; 4.5.9-cluster-version-operator</span><br><span class="line">      sha256:3bbdc631a9271d45960e9db061bde9bc87c6b1df3ec58a17f81dd7326065b4dd -&gt; 4.5.9-prom-label-proxy</span><br><span class="line">      sha256:3c45fb79851e8498666470739c452535d16fde408f54461cc3e239608ae55943 -&gt; 4.5.9-ironic-ipa-downloader</span><br><span class="line">      sha256:3ec062eef4a908c7ce7ee01222a641505f9ccb45e4864f4d3c40a2c3160928f2 -&gt; 4.5.9-azure-machine-controllers</span><br><span class="line">      sha256:416ad8f3ddd49adae4f7db66f8a130319084604296c076c2a6d22264a5688d65 -&gt; 4.5.9-cluster-bootstrap</span><br><span class="line">      sha256:43fdf850ddbbad7b727eff2850d1886a69b6efca837ff8a35d097c1f15f0922c -&gt; 4.5.9-aws-machine-controllers</span><br><span class="line">      sha256:45ec998707309535437f03a2f361ea4c660744c41926258f6a42b566228fe59a -&gt; 4.5.9-jenkins-agent-nodejs</span><br><span class="line">      sha256:486ed094bac19bd3be37eff728596319f73e5648cfb49742ef8c1a859db15e55 -&gt; 4.5.9-operator-lifecycle-manager</span><br><span class="line">      sha256:4a547f79252b06c0fd9fa112d4bca0cd2e5958f471e9a872e33f4e917c1ebccd -&gt; 4.5.9-cluster-update-keys</span><br><span class="line">      sha256:4b15cf622173dedc8ab30dd1d81c7a4ffe2317fce9fd179bb26b04167604bfab -&gt; 4.5.9-kube-state-metrics</span><br><span class="line">      sha256:4d10b546ad2eb8bf67b50217e441d3b01d219ceb96bdf31855a5199fc23c29c7 -&gt; 4.5.9-cluster-autoscaler-operator</span><br><span class="line">      sha256:4d4f72c1556d2085c55691bddf5fadea7bb14ba980fdc28f02146b090134f3a6 -&gt; 4.5.9-cluster-svcat-apiserver-operator</span><br><span class="line">      sha256:4f99f1a5d8a5b7789016316ab33319e5d8b1b27b42ccfb974eb98b27968683b5 -&gt; 4.5.9-prometheus-alertmanager</span><br><span class="line">      sha256:521721a7d0d298eb361d550c08f682942323a68d07e580450e2a64cbfab54880 -&gt; 4.5.9-baremetal-machine-controllers</span><br><span class="line">      sha256:52a566dabf19f82f3fba485807784bd34b2b6a03539d0a2c471ea283ee601e62 -&gt; 4.5.9-openstack-machine-controllers</span><br><span class="line">      sha256:5f77b35d6095068e685cd389f5aeea4b3fad82e771957663839baaa331859eee -&gt; 4.5.9-libvirt-machine-controllers</span><br><span class="line">      sha256:62b44f524d9b820855629cb01bc8c7fff79a039a72f3b421761e1a65ea621b13 -&gt; 4.5.9-baremetal-runtimecfg</span><br><span class="line">      sha256:65dc7fc90223061deec270ecdfa365d509950a1efa4b9025c9355d02b8c22f9b -&gt; 4.5.9-cluster-machine-approver</span><br><span class="line">      sha256:66e8675a73707d519467b3598259587de325315e186a839c233f3659f58be534 -&gt; 4.5.9-ironic</span><br><span class="line">      sha256:6b4bc1c8fa3e762d70a837f63f660e4ff3e129015d35c82a7b6da0c6fb7919da -&gt; 4.5.9-kube-rbac-proxy</span><br><span class="line">      sha256:708de132d6a5a6c15c0b7f04f572cd85ff67b1733c5235d74b74f3a690a98ce7 -&gt; 4.5.9-ironic-machine-os-downloader</span><br><span class="line">      sha256:70c3c46df383aa123ffd0a09f18afcc92e894b01cb9edc4e42cdac9b4a092fa1 -&gt; 4.5.9-cluster-svcat-controller-manager-operator</span><br><span class="line">      sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4 -&gt; 4.5.9-cluster-ingress-operator</span><br><span class="line">      sha256:755c6cd68730ad7f72950be4110c79b971403864cb9bf8f211edf4d35858b2e6 -&gt; 4.5.9-multus-cni</span><br><span class="line">      sha256:792a91df9bb7c4fb49f01c0a70479cc356aa324082ad869bf141a7ff98f51f5a -&gt; 4.5.9-deployer</span><br><span class="line">      sha256:7a56d91d1e0aeddc46dce7fcfda56d4430321e14563e10207893a360a87d3111 -&gt; 4.5.9-ironic-inspector</span><br><span class="line">      sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425 -&gt; 4.5.9-x86_64</span><br><span class="line">      sha256:80cb8dd15ae02c81ba09387560eb3edb8a12645b3a3179620038df340f51bd54 -&gt; 4.5.9-thanos</span><br><span class="line">      sha256:84cb2378e2115c1fba70f6cb7ccf053ad848a2038a38c2f5ab45a2e5f21d871c -&gt; 4.5.9-console</span><br><span class="line">      sha256:87037e2988ca6fee330729017c4ecb164f1a6f68e9eb4edacc976c3efb9515fd -&gt; 4.5.9-insights-operator</span><br><span class="line">      sha256:8bd68e0f18cd6ba0a6f12848f3edb630fcca9090a272aa58e3818a6382067e52 -&gt; 4.5.9-kuryr-controller</span><br><span class="line">      sha256:8c25f463d04079a8a93791c307ff893264855d60bbb8ba7a54179d9d44fc2f9f -&gt; 4.5.9-openshift-state-metrics</span><br><span class="line">      sha256:8ee14d942d7a971f52147b15fbd706844e250bd6c72c42150d92fc53cc826111 -&gt; 4.5.9-prometheus-config-reloader</span><br><span class="line">      sha256:95e63453871b0aca40375be741597363cb8b4393a2d59b2924bb4a4123b4835e -&gt; 4.5.9-csi-snapshot-controller</span><br><span class="line">      sha256:9bf5e6781444c43939ccf52f4d69fa163dd0faf874bff3c0adb2f259780f2b47 -&gt; 4.5.9-ovn-kubernetes</span><br><span class="line">      sha256:9d7cf54cc50ab837ec797954ff49c0d6c9989e1b3079431199026203f512daf9 -&gt; 4.5.9-cluster-openshift-controller-manager-operator</span><br><span class="line">      sha256:a14dc3297e6ea3098118c920275fc54aeb29d2f490e5023f9cbb37b6af05be81 -&gt; 4.5.9-cluster-dns-operator</span><br><span class="line">      sha256:a25790af4a004de3183a44b240dd3a3138749fae3ffa4ac41cca94319f614213 -&gt; 4.5.9-openshift-apiserver</span><br><span class="line">      sha256:a48d986d1731609226d8f6876c7cef21e8cefccd4e04c000f255b1f6f908a5ea -&gt; 4.5.9-cluster-csi-snapshot-controller-operator</span><br><span class="line">      sha256:a51adfc033493814df1a193b0de547749e052cb0811edacc05b878e1fef167b8 -&gt; 4.5.9-etcd</span><br><span class="line">      sha256:a7433426912aa9ff24b63105c5799d47614978ee80a4a241f5e9f31e8e588710 -&gt; 4.5.9-cluster-network-operator</span><br><span class="line">      sha256:aa73c07c74838b11b6150c6907ab2c40c86a40eb63bf2c59db881815ab3553c5 -&gt; 4.5.9-coredns</span><br><span class="line">      sha256:adc3e7c9ab16095165261f0feef990bb829ccf57ae506dadf3734cee0801057a -&gt; 4.5.9-cluster-image-registry-operator</span><br><span class="line">      sha256:af0f67519dbd7ffe2732d89cfa342ee55557f0dc5e8ee8c674eed5ff209bb15a -&gt; 4.5.9-machine-os-content</span><br><span class="line">      sha256:b05f9e685b3f20f96fa952c7c31b2bfcf96643e141ae961ed355684d2d209310 -&gt; 4.5.9-baremetal-installer</span><br><span class="line">      sha256:b1f97ac926075c15a45aac7249e03ff583159b6e03c429132c3111ed5302727b -&gt; 4.5.9-multus-admission-controller</span><br><span class="line">      sha256:b7261a317a4bdd681f8812c2022ff7f391a606f2b80a1c068363c5675abd4363 -&gt; 4.5.9-cluster-samples-operator</span><br><span class="line">      sha256:bb64c463c4b999fbd124a772e3db56f27c74b1e033052c737b69510fc1b9e590 -&gt; 4.5.9-pod</span><br><span class="line">      sha256:bc6c8fd4358d3a46f8df4d81cd424e8778b344c368e6855ed45492815c581438 -&gt; 4.5.9-hyperkube</span><br><span class="line">      sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597 -&gt; 4.5.9-machine-config-operator</span><br><span class="line">      sha256:bcd799425dbdbc7361a44c8bfdc7e6cc3f6f31d59ac18cbea14f3dce8de12d62 -&gt; 4.5.9-cluster-policy-controller</span><br><span class="line">      sha256:bf0d5fd64ab53dfdd477c90a293f3ec90379a22e8b356044082e807565699863 -&gt; 4.5.9-cloud-credential-operator</span><br><span class="line">      sha256:c011c86ee352f377bbcaa46521b731677e81195240cdd4faba7a25a679f00530 -&gt; 4.5.9-cluster-autoscaler</span><br><span class="line">      sha256:c3fc77b6c7ab6e1e2593043532d2a9076831915f2da031fa10aa62bdfbbd7ba8 -&gt; 4.5.9-prometheus-node-exporter</span><br><span class="line">      sha256:c492193a82adfb303b1ac27e48a12cc248c13d120909c98ea19088debe47fd8d -&gt; 4.5.9-kube-storage-version-migrator</span><br><span class="line">      sha256:c4fee4a9d551caa8bfa8cca0a1fb919408a86451bccb3649965c09dcab068a88 -&gt; 4.5.9-operator-marketplace</span><br><span class="line">      sha256:c516f6fa1dd2bf3cca769a32eae08fd027250d56361abb790910c9a18f9fcf07 -&gt; 4.5.9-cluster-node-tuned</span><br><span class="line">      sha256:c93a0de1e4cb3f04e37d547c1b81a2a22c4d9d01c013374c466ed3fd0416215a -&gt; 4.5.9-cluster-config-operator</span><br><span class="line">      sha256:ca556d4515818e3e10d2e771d986784460509146ea9dd188fb8ba6f6ac694132 -&gt; 4.5.9-cluster-kube-controller-manager-operator</span><br><span class="line">      sha256:cca683b844f3e73bb8160e27b986762e77601831eca77e2eea6d94a499959869 -&gt; 4.5.9-multus-route-override-cni</span><br><span class="line">      sha256:cd1f7e40de6a170946faea98f94375f5aae2d9868c049f9ed00fb4f07b32d775 -&gt; 4.5.9-kube-proxy</span><br><span class="line">      sha256:d014f98f1d9a5a6f7e70294830583670d1b17892d38bc8c009ec974f12599bff -&gt; 4.5.9-tools</span><br><span class="line">      sha256:d0d21ae3e27140e1fa13b49d6b2883a0f1466d8e47a2a4839f22de80668d5c95 -&gt; 4.5.9-haproxy-router</span><br><span class="line">      sha256:d11eff4148b733de49e588cfe4c002b5fdd3dea5caea4a7a3a1087390782e56a -&gt; 4.5.9-cluster-openshift-apiserver-operator</span><br><span class="line">      sha256:d72cbba07de1dacfe3669e11fa4f4efa2c7b65664899923622b0ca0715573671 -&gt; 4.5.9-telemeter</span><br><span class="line">      sha256:d8725a2f93d0df184617a41995ee068914ca5a155cdf11c304553abcc4c3100a -&gt; 4.5.9-cli-artifacts</span><br><span class="line">      sha256:db0794d279028179c45791b0dc7ff22f2172d2267d85e0a37ef2a26ffda9c642 -&gt; 4.5.9-cluster-storage-operator</span><br><span class="line">      sha256:dbca81e9b4055763f422c22df01a77efba1dca499686a24210795f2ffddf20c9 -&gt; 4.5.9-docker-registry</span><br><span class="line">      sha256:ddb26e047d0e0d7b11bdb625bd7a941ab66f7e1ef5a5f7455d8694e7ba48989d -&gt; 4.5.9-cluster-kube-scheduler-operator</span><br><span class="line">      sha256:e1d1cea1cf52358b3e91ce8176b83ad36b6b28a226ee6f356fa11496396d93ec -&gt; 4.5.9-cluster-authentication-operator</span><br><span class="line">      sha256:e34e4f3b0097db265b40d627c2cc7b4ddd953a30b7dd283e2ec4bbe5c257a49e -&gt; 4.5.9-configmap-reloader</span><br><span class="line">      sha256:e4758039391099dc1b0265099474113dcd8bcce84a1c92d02c1ef760793079e6 -&gt; 4.5.9-cluster-kube-apiserver-operator</span><br><span class="line">      sha256:e4aae960b36e292b9807f9dc6f3feb57b62cc6a91f9349f983468a1102dc01a7 -&gt; 4.5.9-multus-whereabouts-ipam-cni</span><br><span class="line">      sha256:e8e8da1a4d743770940708c84408dc6188e4df2104c014160e051ef4a8c51b04 -&gt; 4.5.9-baremetal-operator</span><br><span class="line">      sha256:ea61c3e635bbaacd3b0833cf1983f25d3fc6eddc306230bf2703334ba61c1c26 -&gt; 4.5.9-gcp-machine-controllers</span><br><span class="line">      sha256:ec29896354cb1f651142e1c3793890fb59e5252955cb5706cca0e6917e888a61 -&gt; 4.5.9-oauth-proxy</span><br><span class="line">      sha256:ec3a914142ee8dfa01b7a407f7909ff766c210c44a33d4fa731f5547fdb65796 -&gt; 4.5.9-ironic-static-ip-manager</span><br><span class="line">      sha256:ee00cecd8084aac097d0b0ee2033d7931467d92f00643c6434ce6620d643cf35 -&gt; 4.5.9-operator-registry</span><br><span class="line">      sha256:eee108b303058e74c75b5cf16aeefaeb9d0972cd25cf09e0d33ef3213dcd50d0 -&gt; 4.5.9-cluster-monitoring-operator</span><br><span class="line">      sha256:f6b9d00a5dbde8a772b0709aa4ae7d686aa2b645a2a6fbee85eebe15925f8fbb -&gt; 4.5.9-prometheus</span><br><span class="line">      sha256:f70fdff00e09230af556265af72a87fbc22de2cf0f6447ebc8205baaca02fae2 -&gt; 4.5.9-installer</span><br><span class="line">      sha256:fbc84979fec952728014a8551f6cb0deb436f19aa288b8e03803ae370fc3c911 -&gt; 4.5.9-ironic-hardware-inventory-recorder</span><br><span class="line">  stats: shared=0 unique=0 size=0B</span><br><span class="line"></span><br><span class="line">phase 0:</span><br><span class="line">  registry.openshift4.example.com ocp4/openshift4 blobs=0 mounts=0 manifests=110 shared=0</span><br><span class="line"></span><br><span class="line">info: Planning completed in 22.31s</span><br><span class="line">sha256:ec3a914142ee8dfa01b7a407f7909ff766c210c44a33d4fa731f5547fdb65796 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-static-ip-manager</span><br><span class="line">sha256:f6b9d00a5dbde8a772b0709aa4ae7d686aa2b645a2a6fbee85eebe15925f8fbb registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus</span><br><span class="line">sha256:65dc7fc90223061deec270ecdfa365d509950a1efa4b9025c9355d02b8c22f9b registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-machine-approver</span><br><span class="line">sha256:9d7cf54cc50ab837ec797954ff49c0d6c9989e1b3079431199026203f512daf9 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-openshift-controller-manager-operator</span><br><span class="line">sha256:07f11763953a2293bac5d662b6bd49c883111ba324599c6b6b28e9f9f74112be registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-storage-version-migrator-operator</span><br><span class="line">sha256:3bbdc631a9271d45960e9db061bde9bc87c6b1df3ec58a17f81dd7326065b4dd registry.openshift4.example.com/ocp4/openshift4:4.5.9-prom-label-proxy</span><br><span class="line">sha256:bc6c8fd4358d3a46f8df4d81cd424e8778b344c368e6855ed45492815c581438 registry.openshift4.example.com/ocp4/openshift4:4.5.9-hyperkube</span><br><span class="line">sha256:167c14d56714982360fbaa72b3eb3f8d4fb6c5ad10bbbb7ea396606d8412b7eb registry.openshift4.example.com/ocp4/openshift4:4.5.9-sdn</span><br><span class="line">sha256:486ed094bac19bd3be37eff728596319f73e5648cfb49742ef8c1a859db15e55 registry.openshift4.example.com/ocp4/openshift4:4.5.9-operator-lifecycle-manager</span><br><span class="line">sha256:c011c86ee352f377bbcaa46521b731677e81195240cdd4faba7a25a679f00530 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-autoscaler</span><br><span class="line">sha256:e4aae960b36e292b9807f9dc6f3feb57b62cc6a91f9349f983468a1102dc01a7 registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-whereabouts-ipam-cni</span><br><span class="line">sha256:ea61c3e635bbaacd3b0833cf1983f25d3fc6eddc306230bf2703334ba61c1c26 registry.openshift4.example.com/ocp4/openshift4:4.5.9-gcp-machine-controllers</span><br><span class="line">sha256:bf0d5fd64ab53dfdd477c90a293f3ec90379a22e8b356044082e807565699863 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cloud-credential-operator</span><br><span class="line">sha256:ddb26e047d0e0d7b11bdb625bd7a941ab66f7e1ef5a5f7455d8694e7ba48989d registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-scheduler-operator</span><br><span class="line">sha256:cd1f7e40de6a170946faea98f94375f5aae2d9868c049f9ed00fb4f07b32d775 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-proxy</span><br><span class="line">sha256:adc3e7c9ab16095165261f0feef990bb829ccf57ae506dadf3734cee0801057a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-image-registry-operator</span><br><span class="line">sha256:eee108b303058e74c75b5cf16aeefaeb9d0972cd25cf09e0d33ef3213dcd50d0 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-monitoring-operator</span><br><span class="line">sha256:09c763b2ad50711c2b1b5f26c9c70875f821f460e7214f1b11c4be8374424f42 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-client-agent</span><br><span class="line">sha256:4d10b546ad2eb8bf67b50217e441d3b01d219ceb96bdf31855a5199fc23c29c7 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-autoscaler-operator</span><br><span class="line">sha256:80cb8dd15ae02c81ba09387560eb3edb8a12645b3a3179620038df340f51bd54 registry.openshift4.example.com/ocp4/openshift4:4.5.9-thanos</span><br><span class="line">sha256:21fdd7329f0d9d85acb935223915fbb3af07d4ea051678fc8ec85e47ab95edb1 registry.openshift4.example.com/ocp4/openshift4:4.5.9-mdns-publisher</span><br><span class="line">sha256:87037e2988ca6fee330729017c4ecb164f1a6f68e9eb4edacc976c3efb9515fd registry.openshift4.example.com/ocp4/openshift4:4.5.9-insights-operator</span><br><span class="line">sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-ingress-operator</span><br><span class="line">sha256:d8725a2f93d0df184617a41995ee068914ca5a155cdf11c304553abcc4c3100a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cli-artifacts</span><br><span class="line">sha256:a25790af4a004de3183a44b240dd3a3138749fae3ffa4ac41cca94319f614213 registry.openshift4.example.com/ocp4/openshift4:4.5.9-openshift-apiserver</span><br><span class="line">sha256:e8e8da1a4d743770940708c84408dc6188e4df2104c014160e051ef4a8c51b04 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-operator</span><br><span class="line">sha256:521721a7d0d298eb361d550c08f682942323a68d07e580450e2a64cbfab54880 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-machine-controllers</span><br><span class="line">sha256:d11eff4148b733de49e588cfe4c002b5fdd3dea5caea4a7a3a1087390782e56a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-openshift-apiserver-operator</span><br><span class="line">sha256:4b15cf622173dedc8ab30dd1d81c7a4ffe2317fce9fd179bb26b04167604bfab registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-state-metrics</span><br><span class="line">sha256:cca683b844f3e73bb8160e27b986762e77601831eca77e2eea6d94a499959869 registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-route-override-cni</span><br><span class="line">sha256:70c3c46df383aa123ffd0a09f18afcc92e894b01cb9edc4e42cdac9b4a092fa1 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-svcat-controller-manager-operator</span><br><span class="line">sha256:a48d986d1731609226d8f6876c7cef21e8cefccd4e04c000f255b1f6f908a5ea registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-csi-snapshot-controller-operator</span><br><span class="line">sha256:5f77b35d6095068e685cd389f5aeea4b3fad82e771957663839baaa331859eee registry.openshift4.example.com/ocp4/openshift4:4.5.9-libvirt-machine-controllers</span><br><span class="line">sha256:203ab5955dbcbc5f3334e1d7f2ba9fac5f7a1e187370e88c08ad2996a8507711 registry.openshift4.example.com/ocp4/openshift4:4.5.9-aws-pod-identity-webhook</span><br><span class="line">sha256:aa73c07c74838b11b6150c6907ab2c40c86a40eb63bf2c59db881815ab3553c5 registry.openshift4.example.com/ocp4/openshift4:4.5.9-coredns</span><br><span class="line">sha256:ee00cecd8084aac097d0b0ee2033d7931467d92f00643c6434ce6620d643cf35 registry.openshift4.example.com/ocp4/openshift4:4.5.9-operator-registry</span><br><span class="line">sha256:e1d1cea1cf52358b3e91ce8176b83ad36b6b28a226ee6f356fa11496396d93ec registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-authentication-operator</span><br><span class="line">sha256:c4fee4a9d551caa8bfa8cca0a1fb919408a86451bccb3649965c09dcab068a88 registry.openshift4.example.com/ocp4/openshift4:4.5.9-operator-marketplace</span><br><span class="line">sha256:08068503746a2dfd4dcaf61dca44680a9a066960cac7b1e81ea83e1f157e7e8c registry.openshift4.example.com/ocp4/openshift4:4.5.9-service-ca-operator</span><br><span class="line">sha256:ec29896354cb1f651142e1c3793890fb59e5252955cb5706cca0e6917e888a61 registry.openshift4.example.com/ocp4/openshift4:4.5.9-oauth-proxy</span><br><span class="line">sha256:4f99f1a5d8a5b7789016316ab33319e5d8b1b27b42ccfb974eb98b27968683b5 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-alertmanager</span><br><span class="line">sha256:e4758039391099dc1b0265099474113dcd8bcce84a1c92d02c1ef760793079e6 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-apiserver-operator</span><br><span class="line">sha256:ca556d4515818e3e10d2e771d986784460509146ea9dd188fb8ba6f6ac694132 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-controller-manager-operator</span><br><span class="line">sha256:b05f9e685b3f20f96fa952c7c31b2bfcf96643e141ae961ed355684d2d209310 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-installer</span><br><span class="line">sha256:22e47de98549ac9e792f06d4083b5d99db3ecc8dc4c94c8950dba9756a75dc28 registry.openshift4.example.com/ocp4/openshift4:4.5.9-grafana</span><br><span class="line">sha256:84cb2378e2115c1fba70f6cb7ccf053ad848a2038a38c2f5ab45a2e5f21d871c registry.openshift4.example.com/ocp4/openshift4:4.5.9-console</span><br><span class="line">sha256:0259aa5845ce43114c63d59cedeb71c9aa5781c0a6154fe5af8e3cb7bfcfa304 registry.openshift4.example.com/ocp4/openshift4:4.5.9-machine-api-operator</span><br><span class="line">sha256:1d73207be4fbba735dbb53facd23c2359886a9194df3267fd91a843a7ba10316 registry.openshift4.example.com/ocp4/openshift4:4.5.9-console-operator</span><br><span class="line">sha256:27774942e605c4d4b13da7721c75135aac27db8409862625b76f5b31eb2e0d63 registry.openshift4.example.com/ocp4/openshift4:4.5.9-keepalived-ipfailover</span><br><span class="line">sha256:2be07ebc1d005decbb8e624e9beb4d282900f42eb4f8ec0b76e31daedd8874cc registry.openshift4.example.com/ocp4/openshift4:4.5.9-installer-artifacts</span><br><span class="line">sha256:15be0e6de6e0d7bec726611f1dcecd162325ee57b993e0d886e70c25a1faacc3 registry.openshift4.example.com/ocp4/openshift4:4.5.9-openshift-controller-manager</span><br><span class="line">sha256:c3fc77b6c7ab6e1e2593043532d2a9076831915f2da031fa10aa62bdfbbd7ba8 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-node-exporter</span><br><span class="line">sha256:36e7c99acba49c22c7b84a339bfdab820fae17dec07cb4a17db18617b1d46a37 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-etcd-signer-server</span><br><span class="line">sha256:3c45fb79851e8498666470739c452535d16fde408f54461cc3e239608ae55943 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-ipa-downloader</span><br><span class="line">sha256:b7261a317a4bdd681f8812c2022ff7f391a606f2b80a1c068363c5675abd4363 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-samples-operator</span><br><span class="line">sha256:2899afae1d0d9baa0309944dd55975a195a2cbd211f9ecf574e39287827fa684 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ovirt-machine-controllers</span><br><span class="line">sha256:95e63453871b0aca40375be741597363cb8b4393a2d59b2924bb4a4123b4835e registry.openshift4.example.com/ocp4/openshift4:4.5.9-csi-snapshot-controller</span><br><span class="line">sha256:e34e4f3b0097db265b40d627c2cc7b4ddd953a30b7dd283e2ec4bbe5c257a49e registry.openshift4.example.com/ocp4/openshift4:4.5.9-configmap-reloader</span><br><span class="line">sha256:45ec998707309535437f03a2f361ea4c660744c41926258f6a42b566228fe59a registry.openshift4.example.com/ocp4/openshift4:4.5.9-jenkins-agent-nodejs</span><br><span class="line">sha256:1c24c01e807700d30430ce4ff2f77b54d55910cfbf9ee0cbe5c1c64979547463 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-operator</span><br><span class="line">sha256:d0d21ae3e27140e1fa13b49d6b2883a0f1466d8e47a2a4839f22de80668d5c95 registry.openshift4.example.com/ocp4/openshift4:4.5.9-haproxy-router</span><br><span class="line">sha256:3161e52e7bbbf445170c4992d5a47ed87c1d026a7f94b8d0cd30c4508fb48643 registry.openshift4.example.com/ocp4/openshift4:4.5.9-oauth-server</span><br><span class="line">sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-etcd-operator</span><br><span class="line">sha256:d014f98f1d9a5a6f7e70294830583670d1b17892d38bc8c009ec974f12599bff registry.openshift4.example.com/ocp4/openshift4:4.5.9-tools</span><br><span class="line">sha256:792a91df9bb7c4fb49f01c0a70479cc356aa324082ad869bf141a7ff98f51f5a registry.openshift4.example.com/ocp4/openshift4:4.5.9-deployer</span><br><span class="line">sha256:8bd68e0f18cd6ba0a6f12848f3edb630fcca9090a272aa58e3818a6382067e52 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kuryr-controller</span><br><span class="line">sha256:62b44f524d9b820855629cb01bc8c7fff79a039a72f3b421761e1a65ea621b13 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-runtimecfg</span><br><span class="line">sha256:dbca81e9b4055763f422c22df01a77efba1dca499686a24210795f2ffddf20c9 registry.openshift4.example.com/ocp4/openshift4:4.5.9-docker-registry</span><br><span class="line">sha256:52a566dabf19f82f3fba485807784bd34b2b6a03539d0a2c471ea283ee601e62 registry.openshift4.example.com/ocp4/openshift4:4.5.9-openstack-machine-controllers</span><br><span class="line">sha256:8c25f463d04079a8a93791c307ff893264855d60bbb8ba7a54179d9d44fc2f9f registry.openshift4.example.com/ocp4/openshift4:4.5.9-openshift-state-metrics</span><br><span class="line">sha256:43fdf850ddbbad7b727eff2850d1886a69b6efca837ff8a35d097c1f15f0922c registry.openshift4.example.com/ocp4/openshift4:4.5.9-aws-machine-controllers</span><br><span class="line">sha256:708de132d6a5a6c15c0b7f04f572cd85ff67b1733c5235d74b74f3a690a98ce7 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-machine-os-downloader</span><br><span class="line">sha256:f70fdff00e09230af556265af72a87fbc22de2cf0f6447ebc8205baaca02fae2 registry.openshift4.example.com/ocp4/openshift4:4.5.9-installer</span><br><span class="line">sha256:c516f6fa1dd2bf3cca769a32eae08fd027250d56361abb790910c9a18f9fcf07 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-node-tuned</span><br><span class="line">sha256:bcd799425dbdbc7361a44c8bfdc7e6cc3f6f31d59ac18cbea14f3dce8de12d62 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-policy-controller</span><br><span class="line">sha256:8ee14d942d7a971f52147b15fbd706844e250bd6c72c42150d92fc53cc826111 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-config-reloader</span><br><span class="line">sha256:09a7dea10cd584c6048f8df3dcec67dd9a8432eb44051353e180dfeb350c6310 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-node-tuning-operator</span><br><span class="line">sha256:3b893950a9db5aa3653d80b05b236e72c168ff368d1c2e0192dd6036486eb715 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-version-operator</span><br><span class="line">sha256:4a547f79252b06c0fd9fa112d4bca0cd2e5958f471e9a872e33f4e917c1ebccd registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-update-keys</span><br><span class="line">sha256:2527b9f6712b8551cbcec637fc87bb9640973e9f9f4972d489c5308ef25eeed0 registry.openshift4.example.com/ocp4/openshift4:4.5.9-jenkins</span><br><span class="line">sha256:1cf4b3ce90933a7dbef9d01ebcb00a7f490dce7c44de6ddb16ddebad07de8eda registry.openshift4.example.com/ocp4/openshift4:4.5.9-k8s-prometheus-adapter</span><br><span class="line">sha256:416ad8f3ddd49adae4f7db66f8a130319084604296c076c2a6d22264a5688d65 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-bootstrap</span><br><span class="line">sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425 registry.openshift4.example.com/ocp4/openshift4:4.5.9-x86_64</span><br><span class="line">sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597 registry.openshift4.example.com/ocp4/openshift4:4.5.9-machine-config-operator</span><br><span class="line">sha256:6b4bc1c8fa3e762d70a837f63f660e4ff3e129015d35c82a7b6da0c6fb7919da registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-rbac-proxy</span><br><span class="line">sha256:a51adfc033493814df1a193b0de547749e052cb0811edacc05b878e1fef167b8 registry.openshift4.example.com/ocp4/openshift4:4.5.9-etcd</span><br><span class="line">sha256:1960c492ce80345e03d50c6cada3d37341ca604d891c48b03109de525e99997a registry.openshift4.example.com/ocp4/openshift4:4.5.9-tests</span><br><span class="line">sha256:66e8675a73707d519467b3598259587de325315e186a839c233f3659f58be534 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic</span><br><span class="line">sha256:755c6cd68730ad7f72950be4110c79b971403864cb9bf8f211edf4d35858b2e6 registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-cni</span><br><span class="line">sha256:a7433426912aa9ff24b63105c5799d47614978ee80a4a241f5e9f31e8e588710 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-network-operator</span><br><span class="line">sha256:a14dc3297e6ea3098118c920275fc54aeb29d2f490e5023f9cbb37b6af05be81 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-dns-operator</span><br><span class="line">sha256:d72cbba07de1dacfe3669e11fa4f4efa2c7b65664899923622b0ca0715573671 registry.openshift4.example.com/ocp4/openshift4:4.5.9-telemeter</span><br><span class="line">sha256:7a56d91d1e0aeddc46dce7fcfda56d4430321e14563e10207893a360a87d3111 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-inspector</span><br><span class="line">sha256:db0794d279028179c45791b0dc7ff22f2172d2267d85e0a37ef2a26ffda9c642 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-storage-operator</span><br><span class="line">sha256:3ec062eef4a908c7ce7ee01222a641505f9ccb45e4864f4d3c40a2c3160928f2 registry.openshift4.example.com/ocp4/openshift4:4.5.9-azure-machine-controllers</span><br><span class="line">sha256:273585703f8f56c980ff77f9a71e6e380d8caa645c4101525572906e0820e8fc registry.openshift4.example.com/ocp4/openshift4:4.5.9-docker-builder</span><br><span class="line">sha256:4d4f72c1556d2085c55691bddf5fadea7bb14ba980fdc28f02146b090134f3a6 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-svcat-apiserver-operator</span><br><span class="line">sha256:9bf5e6781444c43939ccf52f4d69fa163dd0faf874bff3c0adb2f259780f2b47 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ovn-kubernetes</span><br><span class="line">sha256:289d792eada5d1460d69a927dff31328b5fcb1f1d2e31eb7a3fd69afd9a118fc registry.openshift4.example.com/ocp4/openshift4:4.5.9-kuryr-cni</span><br><span class="line">sha256:af0f67519dbd7ffe2732d89cfa342ee55557f0dc5e8ee8c674eed5ff209bb15a registry.openshift4.example.com/ocp4/openshift4:4.5.9-machine-os-content</span><br><span class="line">sha256:14f97b5e9195c8f9b682523dc1febbb1f75fd60d408ac2731cdfa047aee0f43d registry.openshift4.example.com/ocp4/openshift4:4.5.9-must-gather</span><br><span class="line">sha256:c93a0de1e4cb3f04e37d547c1b81a2a22c4d9d01c013374c466ed3fd0416215a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-config-operator</span><br><span class="line">sha256:b1f97ac926075c15a45aac7249e03ff583159b6e03c429132c3111ed5302727b registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-admission-controller</span><br><span class="line">sha256:bb64c463c4b999fbd124a772e3db56f27c74b1e033052c737b69510fc1b9e590 registry.openshift4.example.com/ocp4/openshift4:4.5.9-pod</span><br><span class="line">sha256:39f8b615e82b3a3a087d6f7d301303d8a0862e4ca13e8f99e6ad968a04985f80 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cli</span><br><span class="line">sha256:c492193a82adfb303b1ac27e48a12cc248c13d120909c98ea19088debe47fd8d registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-storage-version-migrator</span><br><span class="line">sha256:2b40c4a5cb2a9586ec6c8e43e3013e6ee97781399b2fa836cf8d6c181ff8430c registry.openshift4.example.com/ocp4/openshift4:4.5.9-jenkins-agent-maven</span><br><span class="line">sha256:00edb6c1dae03e1870e1819b4a8d29b655fb6fc40a396a0db2d7c8a20bd8ab8d registry.openshift4.example.com/ocp4/openshift4:4.5.9-local-storage-static-provisioner</span><br><span class="line">sha256:1140eff8442d5ac679a3a85655723cd51f12a63c0c8d61251214ff952cc40b06 registry.openshift4.example.com/ocp4/openshift4:4.5.9-container-networking-plugins</span><br><span class="line">sha256:fbc84979fec952728014a8551f6cb0deb436f19aa288b8e03803ae370fc3c911 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-hardware-inventory-recorder</span><br><span class="line">info: Mirroring completed in 2.27s (0B/s)</span><br><span class="line"></span><br><span class="line">Success</span><br><span class="line">Update image:  registry.openshift4.example.com/ocp4/openshift4:4.5.9-x86_64</span><br><span class="line">Mirror prefix: registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line"></span><br><span class="line">To use the new mirrored repository to install, add the following section to the install-config.yaml:</span><br><span class="line"></span><br><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-release</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy:</span><br><span class="line"></span><br><span class="line">apiVersion: operator.openshift.io/v1alpha1</span><br><span class="line">kind: ImageContentSourcePolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: example</span><br><span class="line">spec:</span><br><span class="line">  repositoryDigestMirrors:</span><br><span class="line">  - mirrors:</span><br><span class="line">    - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">  - mirrors:</span><br><span class="line">    - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">    source: quay.io/openshift-release-dev/ocp-release</span><br></pre></td></tr></table></figure><p>梯子不稳定的话多执行几次，<code>oc adm release mirror</code> 命令执行完成后会输出下面类似的信息，保存下来，将来会用在 <code>install-config.yaml</code> 文件中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">imageContentSources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">registry.openshift4.example.com/ocp4/openshift4</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-release</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">registry.openshift4.example.com/ocp4/openshift4</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-v4.0-art-dev</span></span><br></pre></td></tr></table></figure><h4 id="同步到本地目录"><a href="#同步到本地目录" class="headerlink" title="同步到本地目录"></a>同步到本地目录</h4><p>这里说下同步到本地，假如云梯的节点和内网是不通的，所以我们需要先在纵云梯节点上把镜像存为文件，后面文件拷贝内网里去推送到内网仓库上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">mkdir mirror</span><br><span class="line">oc adm -a $&#123;LOCAL_SECRET_JSON&#125; release mirror \</span><br><span class="line">  --from=quay.io/$&#123;PRODUCT_REPO&#125;/$&#123;RELEASE_NAME&#125;:$&#123;OCP_RELEASE&#125; \</span><br><span class="line">  --to-dir=mirror/</span><br></pre></td></tr></table></figure><p>梯子不稳定的话多执行几次，这个命令支持继续上次的位置下载，下载完成后会有下面类似内容</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">phase 0:</span><br><span class="line">   openshift/release blobs=238 mounts=0 manifests=110 shared=5</span><br><span class="line"></span><br><span class="line">info: Planning completed in 22.61s</span><br><span class="line">uploading: file://openshift/release sha256:740e93718e387c4611372639d025bcde65fa084f10cd64b2fea59ac3edc3b5c2 823.3MiB</span><br><span class="line">uploading: file://openshift/release sha256:4908e3220585a526b87e77f88ee7ddd06c502447269792ea4013e1b2f414f41e 383.1MiB</span><br><span class="line">uploading: file://openshift/release sha256:b9cb5a1468a5d3b235df159d5f795d2f44bc14fbe6c3d36b28ce8a69eb545771 515.5MiB</span><br><span class="line">sha256:15be0e6de6e0d7bec726611f1dcecd162325ee57b993e0d886e70c25a1faacc3 file://openshift/release:4.5.9-openshift-controller-manager</span><br><span class="line">sha256:bc6c8fd4358d3a46f8df4d81cd424e8778b344c368e6855ed45492815c581438 file://openshift/release:4.5.9-hyperkube</span><br><span class="line">sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4 file://openshift/release:4.5.9-cluster-ingress-operator</span><br><span class="line">sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597 file://openshift/release:4.5.9-machine-config-operator</span><br><span class="line">sha256:9d7cf54cc50ab837ec797954ff49c0d6c9989e1b3079431199026203f512daf9 file://openshift/release:4.5.9-cluster-openshift-controller-manager-operator</span><br><span class="line">sha256:c492193a82adfb303b1ac27e48a12cc248c13d120909c98ea19088debe47fd8d file://openshift/release:4.5.9-kube-storage-version-migrator</span><br><span class="line">sha256:2527b9f6712b8551cbcec637fc87bb9640973e9f9f4972d489c5308ef25eeed0 file://openshift/release:4.5.9-jenkins</span><br><span class="line">....</span><br><span class="line">info: Mirroring completed in 1h27m3.41s (345.7kB/s)</span><br><span class="line"></span><br><span class="line">Success</span><br><span class="line">Update image:  openshift/release:4.5.9</span><br><span class="line"></span><br><span class="line">To upload local images to a registry, run:</span><br><span class="line"></span><br><span class="line">    oc image mirror --from-dir=mirror/ &#x27;file://openshift/release:4.5.9*&#x27; REGISTRY/REPOSITORY</span><br><span class="line"></span><br><span class="line">Configmap signature file mirror/config/signature-sha256-7ad540594e2a6673.yaml created</span><br></pre></td></tr></table></figure><p>结尾输出了把 mirror 目录下文件推送到镜像仓库的命令 <code>oc image mirror --from-dir=mirror/ &#39;file://openshift/release:4.5.9*&#39; REGISTRY/REPOSITORY</code>，后面会用到这个命令，目录为下面情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tree -L 4 mirror/</span></span><br><span class="line">mirror/</span><br><span class="line">├── config</span><br><span class="line">│   └── signature-sha256-7ad540594e2a6673.yaml</span><br><span class="line">└── v2</span><br><span class="line">    └── openshift</span><br><span class="line">        └── release</span><br><span class="line">            ├── blobs</span><br><span class="line">            └── manifests</span><br></pre></td></tr></table></figure><p>把这些镜像目录打包成压缩包，压缩后的大概5-6G大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zcvf mirror.tar.gz mirror/</span><br></pre></td></tr></table></figure><p>把前面的<code>mirror.tar.gz</code>压缩包传过来，这里传到内网的<code>LOCAL_REGISTRY</code>机器上，还有<code>pull-secret.json</code>文件和<code>oc</code>命令也记得拷贝过来。文件准备好后设置下变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9</span></span><br><span class="line">OCP_VERSION=$(oc version -o json | jq -r .releaseClientVersion)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9-x86_64</span></span><br><span class="line">OCP_RELEASE=$OCP_VERSION-$(arch)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4</span></span><br><span class="line">OCP_MAJOR=$&#123;OCP_VERSION%%.*&#125;</span><br><span class="line"></span><br><span class="line">LOCAL_REGISTRY=&#x27;registry.openshift4.example.com&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 对应前面的 ocp4/openshift4</span></span><br><span class="line">LOCAL_REPOSITORY=&quot;ocp$&#123;OCP_MAJOR&#125;/openshift$&#123;OCP_MAJOR&#125;&quot;</span><br><span class="line"></span><br><span class="line">PRODUCT_REPO=&#x27;openshift-release-dev&#x27;</span><br><span class="line">RELEASE_NAME=&quot;ocp-release&quot;</span><br><span class="line"></span><br><span class="line">LOCAL_SECRET_JSON=&#x27;pull-secret.json&#x27;</span><br></pre></td></tr></table></figure><p>然后用之前下载镜像到目录的时候结尾输出的命令导入下，因为证书是非权威机构ca签署的，所以加<code>--insecure</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf mirror.tar.gz</span><br><span class="line"></span><br><span class="line">oc image mirror -a $&#123;LOCAL_SECRET_JSON&#125; \</span><br><span class="line">  --from-dir=mirror/ &#x27;file://openshift/release:4.5.9*&#x27; \</span><br><span class="line"><span class="meta">  $</span><span class="bash">&#123;LOCAL_REGISTRY&#125;/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span> \</span></span><br><span class="line"><span class="bash">  --insecure</span></span><br></pre></td></tr></table></figure><p>导入后去镜像仓库的 web 上查看有没有下面名字的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span></span></span><br><span class="line">registry.openshift4.example.com/ocp4/openshift4:4.5.9-x86_64</span><br></pre></td></tr></table></figure><h2 id="bastion"><a href="#bastion" class="headerlink" title="bastion"></a>bastion</h2><p>bastion 提供了下列功能：</p><ul><li>dns server: 由 coredns + etcd 提供</li><li>负载均衡: haproxy</li><li>http file download: nginx</li><li>openshift-install 二进制文件: 用它转换部署清单成 ignition 文件</li><li>oc: ocp 的 client cli，和 kubectl 一样操作 ocp 集群</li></ul><p>如果你机器数量多，或者内网有 dns server 和负载均衡，上面这些服务没必要耦合部署在一台上，同时这些实现手段看自己掌握的工具，没必要和我用一模一样</p><h3 id="cert-for-registry"><a href="#cert-for-registry" class="headerlink" title="cert for registry"></a>cert for registry</h3><p>bastion 节点推送镜像由于非权威 ca 签署证书会报错 <code>x509: certificate signed by unknown authority</code>，把 registry 机器上 quay 的 <code>ssl.cert</code> 复制到 bastion 上，执行下面操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp ssl.cert /etc/pki/ca-trust/source/anchors/ssl.crt</span><br><span class="line">update-ca-trust extract</span><br></pre></td></tr></table></figure><p>如果使用 Docker 登录，需要将证书复制到 docker 的信任证书路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker/certs.d/registry.openshift4.example.com</span><br><span class="line">cp ssl.cert /etc/docker/certs.d/registry.openshift4.example.com/ssl.crt</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="oc-amp-amp-openshift-install"><a href="#oc-amp-amp-openshift-install" class="headerlink" title="oc &amp;&amp; openshift-install"></a>oc &amp;&amp; openshift-install</h3><p>把 registry 上的 oc 命令拷贝过来。为了保证安装版本一致性，需要从镜像库中提取 <code>openshift-install</code> 二进制文件，不能直接从 <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/</a> 下载，不然后面会有 <code>sha256</code> 匹配不上的问题。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这一步需要用到上面的 <span class="built_in">export</span> 变量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者提前在 纵云梯节点执行然后拷贝过来</span></span><br><span class="line">oc adm release extract \</span><br><span class="line">  -a $&#123;LOCAL_SECRET_JSON&#125; \</span><br><span class="line">  --command=openshift-install \</span><br><span class="line">  &quot;$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125;:$&#123;OCP_RELEASE&#125;&quot;</span><br></pre></td></tr></table></figure><p>如果提示 <code>error: image dose not exist</code>，说明拉取的镜像不全，或者版本不对。</p><p>把文件移动到 <code>$PATH</code> 并确认版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">chmod +x openshift-install</span><br><span class="line">mv openshift-install /usr/local/bin/</span><br><span class="line"></span><br><span class="line">openshift-install version</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面是输出</span></span><br><span class="line"></span><br><span class="line">openshift-install 4.5.9</span><br><span class="line">built from commit 0d5c871ce7d03f3d03ab4371dc39916a5415cf5c</span><br><span class="line">release image registry.openshift4.example.com/ocp4/openshift4@sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425</span><br></pre></td></tr></table></figure><h3 id="dns-server"><a href="#dns-server" class="headerlink" title="dns server"></a>dns server</h3><p>按照官方文档要求，集群里大部分组件之间通信都是用 dns ，所以我们这里得部署一个 dns server，官方以前 3.x.x 的时候使用 ansible 部署的<code>named</code>提供，网上也有人用 dnsmasq，这里使用 <code>docker-compose</code> 起 coredns 作为 dns server，由于这里需要添加 <code>SRV</code> 记录，所以需要 CoreDNS 结合 etcd 插件使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/coredns</span><br><span class="line">cd /data/coredns</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">docker-compose.yml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">version: &#x27;3.2&#x27;</span><br><span class="line">services:</span><br><span class="line">  coredns:</span><br><span class="line">    image: coredns/coredns:1.7.0</span><br><span class="line">    container_name: coredns</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;53:53/udp&quot;</span><br><span class="line">      - &quot;53:53/tcp&quot;</span><br><span class="line">      - &quot;9153:9153/tcp&quot;</span><br><span class="line">    cap_drop:</span><br><span class="line">      - ALL</span><br><span class="line">    cap_add:</span><br><span class="line">      - NET_BIND_SERVICE</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/coredns/config/:/etc/coredns/</span><br><span class="line">      - /etc/localtime:/etc/localtime:ro</span><br><span class="line">    command: [&quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - coredns-etcd</span><br><span class="line">    networks:</span><br><span class="line">      coredns:</span><br><span class="line">        aliases:</span><br><span class="line">          - coredns</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">  coredns-etcd:</span><br><span class="line">    #image: quay.io/coreos/etcd:v3.4.13</span><br><span class="line">    image: registry.aliyuncs.com/k8sxio/etcd:3.4.13-0</span><br><span class="line">    container_name: coredns-etcd</span><br><span class="line">    restart: always</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/coredns/etcd/data:/var/lib/etcd:Z</span><br><span class="line">      - /data/coredns/etcd/conf:/etc/etcd:Z</span><br><span class="line">      - /etc/localtime:/etc/localtime:ro</span><br><span class="line">    command: [&quot;/usr/local/bin/etcd&quot;, &quot;--config-file=/etc/etcd/etcd.config.yml&quot;]</span><br><span class="line">    networks:</span><br><span class="line">      coredns:</span><br><span class="line">        aliases:</span><br><span class="line">          - etcd</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">networks:</span><br><span class="line">  coredns:</span><br><span class="line">    name: coredns</span><br><span class="line">    external: false</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建相关目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/coredns/config/ \</span><br><span class="line">    /data/coredns/etcd/data \</span><br><span class="line">    /data/coredns/etcd/conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> etcd 3.4.10后data目录权限必须是0700</span></span><br><span class="line">chmod 0700 /data/coredns/etcd/data</span><br></pre></td></tr></table></figure><h4 id="coredns"><a href="#coredns" class="headerlink" title="coredns"></a>coredns</h4><p>创建 coredns 的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /data/coredns/config/Corefile &lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">.:53 &#123;  # 监听 TCP 和 UDP 的 53 端口</span><br><span class="line">    template IN A apps.openshift4.example.com &#123;</span><br><span class="line">    match .*apps\.openshift4\.example\.com # 匹配请求 DNS 名称的正则表达式</span><br><span class="line">    answer &quot;&#123;&#123; .Name &#125;&#125; 60 IN A 10.226.45.250&quot; # DNS 应答</span><br><span class="line">    fallthrough</span><br><span class="line">    &#125;</span><br><span class="line">    etcd &#123;   # 配置启用 etcd 插件,后面可以指定域名,例如 etcd test.com &#123;</span><br><span class="line">        path /skydns # etcd 里面的路径 默认为 /skydns，以后所有的 dns 记录都存储在该路径下</span><br><span class="line">        endpoint http://etcd:2379 # etcd 访问地址，这里是容器里，所以写alias域名，多个则空格分开</span><br><span class="line">        fallthrough # 如果区域匹配但不能生成记录，则将请求传递给下一个插件</span><br><span class="line">        # tls CERT KEY CACERT # 可选参数，etcd 认证证书设置</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus  # 监控插件，开启metrics</span><br><span class="line">    cache 160</span><br><span class="line">    reload</span><br><span class="line">    loadbalance   # 负载均衡，开启 DNS 记录轮询策略</span><br><span class="line">    forward . 114.114.114.114 #上游 dns server，多个的话空格隔开</span><br><span class="line">    log # 打印日志</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>这里配置了一个通配符解析就是给 router 使用的（ocp的 ingress controller），有条件可以硬件F5，real server 则写所有 router 所在的 node ip，然后 vhost的通配符域名解析的 ip 写 F5 的 ip。也可以 keepalived 漂个 VIP 做，这里偷懒使用 基础节点，因为基础节点上有 haproxy 作为负载均衡，会反向代理 集群里 hostNetwork 的 router-default 的端口</li><li>forward 的上游可以看宿主机 <code>/etc/resolv.conf</code> 上的 nameserver 段填写</li></ul><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>创建 etcd 的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /data/coredns/etcd/conf/etcd.config.yml &lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">name: coredns-etcd</span><br><span class="line">data-dir: /var/lib/etcd</span><br><span class="line">wal-dir: /var/lib/etcd/wal</span><br><span class="line">auto-compaction-mode: periodic</span><br><span class="line">auto-compaction-retention: &quot;1&quot;</span><br><span class="line">snapshot-count: 5000</span><br><span class="line">heartbeat-interval: 100</span><br><span class="line">election-timeout: 1000</span><br><span class="line">quota-backend-bytes: 0</span><br><span class="line">listen-peer-urls: &#x27;http://127.0.0.1:2380&#x27;</span><br><span class="line">listen-client-urls: &#x27;http://0.0.0.0:2379&#x27;</span><br><span class="line">max-snapshots: 3</span><br><span class="line">max-wals: 5</span><br><span class="line">cors:</span><br><span class="line">initial-advertise-peer-urls: &#x27;http://127.0.0.1:2380&#x27;</span><br><span class="line">advertise-client-urls: &#x27;http://0.0.0.0:2379&#x27;</span><br><span class="line">discovery:</span><br><span class="line">discovery-fallback: &#x27;proxy&#x27;</span><br><span class="line">discovery-proxy:</span><br><span class="line">discovery-srv:</span><br><span class="line">initial-cluster: &#x27;coredns-etcd=http://127.0.0.1:2380&#x27; #和上面的name一致</span><br><span class="line">initial-cluster-token: &#x27;etcd-coredns&#x27;</span><br><span class="line">initial-cluster-state: &#x27;new&#x27;</span><br><span class="line">strict-reconfig-check: false</span><br><span class="line">enable-v2: false</span><br><span class="line">enable-pprof: true</span><br><span class="line">proxy: &#x27;off&#x27;</span><br><span class="line">proxy-failure-wait: 5000</span><br><span class="line">proxy-refresh-interval: 30000</span><br><span class="line">proxy-dial-timeout: 1000</span><br><span class="line">proxy-write-timeout: 5000</span><br><span class="line">proxy-read-timeout: 0</span><br><span class="line">force-new-cluster: false</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="config-for-dns"><a href="#config-for-dns" class="headerlink" title="config for dns"></a>config for dns</h4><p>确保宿主机上的53没有其他 dns server 进程使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>验证下解析</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig +short apps.openshift4.example.com @127.0.0.1</span></span><br><span class="line">10.226.45.250</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig +short x.apps.openshift4.example.com @127.0.0.1</span></span><br><span class="line">10.226.45.250</span><br></pre></td></tr></table></figure><p>然后根据集群的 ip 添加解析记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">alias etcdctlv3=&#x27;docker exec -e ETCDCTL_API=3 coredns-etcd etcdctl&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/api &#x27;&#123;&quot;host&quot;:&quot;10.226.45.250&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/api-int &#x27;&#123;&quot;host&quot;:&quot;10.226.45.250&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里默认etcd是部署在master节点上</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/etcd-0 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.251&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/etcd-1 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.252&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/etcd-2 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.222&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加密的etcd域名SRV记录，这里我是只有一个master，所以这里只写一个</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/_tcp/_etcd-server-ssl/x1 &#x27;&#123;&quot;host&quot;:&quot;etcd-0.openshift4.example.com&quot;,&quot;ttl&quot;:60,&quot;priority&quot;:0,&quot;weight&quot;:10,&quot;port&quot;:2380&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/_tcp/_etcd-server-ssl/x2 &#x27;&#123;&quot;host&quot;:&quot;etcd-1.openshift4.example.com&quot;,&quot;ttl&quot;:60,&quot;priority&quot;:0,&quot;weight&quot;:10,&quot;port&quot;:2380&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/_tcp/_etcd-server-ssl/x3 &#x27;&#123;&quot;host&quot;:&quot;etcd-2.openshift4.example.com&quot;,&quot;ttl&quot;:60,&quot;priority&quot;:0,&quot;weight&quot;:10,&quot;port&quot;:2380&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 除此之外再添加各节点主机名记录</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/bootstrap &#x27;&#123;&quot;host&quot;:&quot;10.226.45.223&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/master1 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.251&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/master2 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.252&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/master3 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.222&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里bootstrap后面用来当作worker用，所以ip写bootstrap</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/worker1 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.223&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 镜像节点的域名</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/registry &#x27;&#123;&quot;host&quot;:&quot;10.226.45.226&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br></pre></td></tr></table></figure><p>查看所有记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">etcdctlv3 get /skydns --prefix</span><br></pre></td></tr></table></figure><h4 id="validate-for-dns"><a href="#validate-for-dns" class="headerlink" title="validate for dns"></a>validate for dns</h4><p>验证dns，自己比对输出看ip是否正确</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">yum install -y bind-utils</span><br><span class="line"></span><br><span class="line">dig +short api.openshift4.example.com @127.0.0.1</span><br><span class="line"></span><br><span class="line">dig +short api-int.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short etcd-0.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short etcd-1.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short etcd-2.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short bootstrap.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short master1.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short master2.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short master3.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short worker1.openshift4.example.com @127.0.0.1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dig +short -t SRV _etcd-server-ssl._tcp.openshift4.example.com @127.0.0.1</span><br><span class="line">10 33 2380 etcd-0.openshift4.example.com.</span><br><span class="line">10 33 2380 etcd-1.openshift4.example.com.</span><br><span class="line">10 33 2380 etcd-2.openshift4.example.com.</span><br></pre></td></tr></table></figure><p>然后我们改下系统的<code>resolv.conf</code>，使用 coredns 作为dns，这样我们不用去写 hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp -a /etc/resolv.conf /etc/resolv.conf.bak</span><br><span class="line">cat &gt;/etc/resolv.conf&lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">search openshift4.example.com</span><br><span class="line">nameserver 10.226.45.250</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>这里因为安装流程是 bootstrap 机器起来后，bootstrap 运行 machine-config 进程，安装 master 和 worker 的时候会访问 bootstrap 上的 <code>mcahine-config</code> 下面的 http 接口（后面有兴趣在 bootstrap 起来后去执行试试）:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sk https://api-int.openshift4.example.com:22623/config/master</span><br></pre></td></tr></table></figure><p>安装完成后 bootstrap 会交出控制平面到 master 上（machine-config 和 kubernetes api），所以无论 master 有几个，都必须要配置负载均衡。使用工具不限，nginx，envoy 啥的均可。nginx 配置四层 mode 七层 check rs 的时候要安装插件来7层 healthz check，挺麻烦的，所以这里我使用 haproxy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/haproxy</span><br><span class="line">cd /data/haproxy</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">docker-compose.yml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">version: &#x27;3.2&#x27;</span><br><span class="line">services:</span><br><span class="line">  haproxy:</span><br><span class="line">    image: haproxy:lts</span><br><span class="line">    container_name: haproxy</span><br><span class="line">    restart: always</span><br><span class="line">    network_mode: host</span><br><span class="line">    sysctls:</span><br><span class="line">      - net.core.somaxconn=2000</span><br><span class="line">    cap_drop:</span><br><span class="line">      - ALL</span><br><span class="line">    cap_add:</span><br><span class="line">      - NET_BIND_SERVICE</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/haproxy/config/:/etc/haproxy/</span><br><span class="line">      - /etc/localtime:/etc/localtime:ro</span><br><span class="line">      - /etc/hosts:/etc/hosts:ro</span><br><span class="line">    command: [&quot;-f&quot;, &quot;/etc/haproxy/haproxy.cfg&quot;]</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="config-for-haproxy"><a href="#config-for-haproxy" class="headerlink" title="config for haproxy"></a>config for haproxy</h4><p>配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/haproxy/config/</span><br><span class="line">cat &gt;/data/haproxy/config/haproxy.cfg&lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">global</span><br><span class="line">  maxconn  2000</span><br><span class="line">  ulimit-n  16384</span><br><span class="line">  log  127.0.0.1 local0 err</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  log global</span><br><span class="line">  mode  http</span><br><span class="line">  option  httplog</span><br><span class="line">  timeout connect 5000</span><br><span class="line">  timeout client  50000</span><br><span class="line">  timeout server  50000</span><br><span class="line">  timeout http-request 15s</span><br><span class="line">  timeout http-keep-alive 15s</span><br><span class="line"></span><br><span class="line">listen stats</span><br><span class="line">    bind         :9000</span><br><span class="line">    mode         http</span><br><span class="line">    stats        enable</span><br><span class="line">    stats        uri /</span><br><span class="line">    stats        refresh   30s</span><br><span class="line">    stats        auth      admin:openshift #web页面登录</span><br><span class="line">    monitor-uri  /healthz</span><br><span class="line"></span><br><span class="line">frontend openshift-api-server</span><br><span class="line">    bind :6443</span><br><span class="line">    default_backend openshift-api-server</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend openshift-api-server</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    option httpchk GET /healthz</span><br><span class="line">    http-check expect string ok</span><br><span class="line">    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100</span><br><span class="line">    server bootstrap 10.226.45.223:6443 check check-ssl verify none #安装结束后删掉此行</span><br><span class="line">    server master1 10.226.45.251:6443 check check-ssl verify none</span><br><span class="line">    server master2 10.226.45.252:6443 check check-ssl verify none</span><br><span class="line">    server master3 10.226.45.222:6443 check check-ssl verify none</span><br><span class="line"></span><br><span class="line">frontend machine-config-server</span><br><span class="line">    bind :22623</span><br><span class="line">    default_backend machine-config-server</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend machine-config-server</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    server bootstrap 10.226.45.223:22623 check #安装结束后删掉此行</span><br><span class="line">    server master1 10.226.45.251:22623 check</span><br><span class="line">    server master2 10.226.45.252:22623 check</span><br><span class="line">    server master3 10.226.45.222:22623 check</span><br><span class="line"></span><br><span class="line">frontend ingress-http</span><br><span class="line">    bind :80</span><br><span class="line">    default_backend ingress-http</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend ingress-http</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    server master1 10.226.45.251:80 check</span><br><span class="line">    server master2 10.226.45.252:80 check</span><br><span class="line">    server master3 10.226.45.222:80 check</span><br><span class="line"></span><br><span class="line">frontend ingress-https</span><br><span class="line">    bind :443</span><br><span class="line">    default_backend ingress-https</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend ingress-https</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    server master1 10.226.45.251:443 check</span><br><span class="line">    server master2 10.226.45.252:443 check</span><br><span class="line">    server master3 10.226.45.222:443 check</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>因为前文使用的 quay 仓库无法更改 443 端口，这会和 haproxy 代理的 ingress-https 的 443 冲突，所以 quay 仓库单独部署。启动 haproxy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/haproxy</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>可以访问 <a href="http://ip:9000/">http://ip:9000</a> 查看 haproxy 的状态，basic auth 为上面 haproxy 配置文件里的<code>admin:openshift</code></p><h3 id="http-file-download"><a href="#http-file-download" class="headerlink" title="http file download"></a>http file download</h3><p>由于这里不是 pxe 安装，我们还得提供 http server，让 master 和 bootstrap 从 installer iso 启动后下载 raw.gz 写入到硬盘里。这里使用 nginx 提供</p><p>如果有配置 dhcp 的网络环境，这里需要起 dhcp server 和 tftp 之类的服务让 pxe 自动安装，但是我这里没这个网络条件，所以是开机器后挂载 installer iso，然后开机的时候按 tab 按键输入一堆 boot 选项参数，让它从指定 http 的地方下载和安装（也就是前面我门下载的rhcos的raw.gz文件）。这里 http 用容器 nginx 提供</p><p>创建相关目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/install-nginx/&#123;install/ignition,conf.d&#125;</span><br></pre></td></tr></table></figure><p>创建 docker-compose 文件和配置文件，因为宿主机有负载均衡 80 端口在运行，所以这里 nginx 使用 8080 端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cd /data/install-nginx</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">docker-compose.yml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">version: &#x27;3.4&#x27;</span><br><span class="line">services:</span><br><span class="line">  nginx:</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    container_name: install-nginx</span><br><span class="line">    hostname: install-nginx</span><br><span class="line">    volumes:</span><br><span class="line">      - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro</span><br><span class="line">      - /data/install-nginx/install:/usr/share/nginx/html</span><br><span class="line">      - /data/install-nginx/conf.d/:/etc/nginx/conf.d/</span><br><span class="line">    network_mode: &quot;host&quot;</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">EOF</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash"> /data/install-nginx/conf.d/default.conf &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">server &#123;</span><br><span class="line">    listen       8080;</span><br><span class="line">    server_name  localhost;</span><br><span class="line">    location / &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">        autoindex    on;</span><br><span class="line">    &#125;</span><br><span class="line">    error_page   500 502 503 504  /50x.html;</span><br><span class="line">    location = /50x.html &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>启动<code>install-nginx</code>，起来后访问 <a href="http://ip:8080/">http://ip:8080</a> 看到目录就正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><h4 id="rhcos-iso"><a href="#rhcos-iso" class="headerlink" title="rhcos iso"></a>rhcos iso</h4><p>下载 rhcos 的 ISO ， <a href="https://cloud.redhat.com/openshift/install/metal/user-provisioned">下载链接地址</a> 或者 <a href="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/">这里下载</a> 。我们下载下面俩个即可，<strong>版本号必须小于等于ocp的版本号</strong>，<code>installer</code>是给机器挂载，会在内存里运行，根据用户输入的 boot cmdline 去从 http server 下载 <code>metal.x86_64.raw.gz</code> 和 ignition 文件安装和配置</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rhcos-4.5.6-x86_64-installer.x86_64.iso</span><br><span class="line">rhcos-4.5.6-x86_64-metal.x86_64.raw.gz</span><br></pre></td></tr></table></figure><p>installer是给机器挂载从光驱启动的，raw.gz存放在 nginx的http目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/install-nginx/install/</span><br><span class="line">wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/latest/latest/rhcos-4.5.6-x86_64-metal.x86_64.raw.gz</span><br></pre></td></tr></table></figure><p>installer 的 iso 如果是物理机或者虚机的话不要在 bmc 或者类似 vshpere 上远程挂载你自己pc上的 iso，否则会遇到 bootstrap 和 master 在 logo 的安装界面输入boot cmdline 后回车无响应的情况，建议传到远端上，相对于机器近的存储上。例如是 exsi 和 vshpere 则上传到上面的数据存储上。</p><h3 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h3><h4 id="生成ssh密钥对"><a href="#生成ssh密钥对" class="headerlink" title="生成ssh密钥对"></a>生成ssh密钥对</h4><p>在安装过程中，我们会在基础节点上执行 OCP 安装调试和灾难恢复，因此必须在基础节点上配置 SSH key，ssh-agent 将会用它来执行安装程序。</p><p>基础节点上的 core 用户可以使用该私钥登录到 Master 节点。同时部署集群时，该私钥会被添加到 core 用户的 <code>~/.ssh/authorized_keys</code> 列表中。</p><p>创建无密码验证的 SSH key：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -N &#x27;&#x27; -f ~/.ssh/new_rsa</span><br></pre></td></tr></table></figure><p>后续 ssh 到 master 和 node 使用下面命令即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -i ~/.ssh/new_rsa core@10.226.45.251</span><br></pre></td></tr></table></figure><h4 id="创建安装配置文件"><a href="#创建安装配置文件" class="headerlink" title="创建安装配置文件"></a>创建安装配置文件</h4><p>ocp的一些 yaml 和安装文件存放在<code>/data/ocpinstall</code>下，自定义 <code>install-config.yaml</code> 配置文件必须命名为 <code>install-config.yaml</code>。配置文件内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/ocpinstall</span><br><span class="line">cd /data/ocpinstall</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">install-config.yaml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">apiVersion: v1</span><br><span class="line">baseDomain: example.com</span><br><span class="line">compute:</span><br><span class="line">- hyperthreading: Enabled</span><br><span class="line">  name: worker</span><br><span class="line">  replicas: 0</span><br><span class="line">controlPlane:</span><br><span class="line">  hyperthreading: Enabled</span><br><span class="line">  name: master</span><br><span class="line">  replicas: 3</span><br><span class="line">metadata:</span><br><span class="line">  name: openshift4</span><br><span class="line">networking:</span><br><span class="line">  clusterNetwork:</span><br><span class="line">  - cidr: 10.128.0.0/14</span><br><span class="line">    hostPrefix: 23</span><br><span class="line">  networkType: OpenShiftSDN</span><br><span class="line">  serviceNetwork:</span><br><span class="line">  - 172.30.0.0/16</span><br><span class="line">platform:</span><br><span class="line">  none: &#123;&#125;</span><br><span class="line">fips: false</span><br><span class="line">pullSecret: &#x27;&#123;&quot;auths&quot;: ...&#125;&#x27;</span><br><span class="line">sshKey: &#x27;ssh-rsa ...&#x27;</span><br><span class="line">additionalTrustBundle: |</span><br><span class="line">  -----BEGIN CERTIFICATE-----</span><br><span class="line">  注意这里要前面空个两格用于yaml对齐</span><br><span class="line">  -----END CERTIFICATE-----</span><br><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-release</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><strong>baseDomain</strong> : 所有 Openshift 内部的 DNS 记录必须是此基础的子域，并包含集群名称。</li><li><strong>compute</strong> : 计算节点配置。这是一个数组，每一个元素必须以连字符 - 开头。</li><li><strong>hyperthreading</strong> : Enabled 表示启用同步多线程或超线程。默认启用同步多线程，可以提高机器内核的性能。如果要禁用，则控制平面和计算节点都要禁用。</li><li><strong>compute.replicas</strong> : 计算节点数量。因为我们要手动创建计算节点，所以这里要设置为 0。</li><li><strong>controlPlane.replicas</strong> : 控制平面节点数量。控制平面节点数量必须和 etcd 节点数量一致，为了实现高可用，所以设置为 3。</li><li><strong>metadata.name</strong> : 集群名称。即前面 DNS 记录中的 <code>&lt;cluster_name&gt;</code>。</li><li><strong>cidr</strong> : 定义了分配 Pod IP 的 IP 地址段，不能和物理网络重叠。</li><li><strong>hostPrefix</strong> : 分配给每个节点的子网前缀长度，等同于 k8s 的 node-max-mask。例如，如果将 <code>hostPrefix</code> 设置为 <code>23</code>，则为每一个节点分配一个给定 cidr 的 <code>/23</code> 子网，允许510个 Pod IP 地址，24位的话每个 node 的 pod 最多254个ip后期可能会太少了。</li><li><strong>serviceNetwork</strong> : Service IP 的地址池，只能设置一个。</li><li><strong>pullSecret</strong> : 上篇文章使用的 pull secret，可通过命令 <code>jq . /root/pull-secret.json</code> 来压缩成一行。</li><li><strong>sshKey</strong> : 上面创建的公钥，可通过命令 <code>cat ~/.ssh/new_rsa.pub</code> 查看。</li><li><strong>additionalTrustBundle</strong> : 私有镜像仓库 Quay 的信任证书，可在镜像节点上通过命令 <code>cat /data/quay/config/ssl.cert</code> 查看。</li><li><strong>imageContentSources</strong> : 来自前面 <code>oc adm release mirror</code> 的输出结果。</li></ul><p><strong>platform</strong> 实际上支持 gcp，aws，vshpere，bmc之类的自动创建机器，有兴趣可以去去研究下</p><p>修改好上面的文件后接着生成 Ignition 配置文件，创建后<code>install-config.yaml</code>会被删除，所以备份下，同时该文件实际上会被生成 configmap 的 yaml 放在<code>manifests/cluster-config.yaml</code>)里，在集群起来后会存在<code>kube-system cm/cluster-config-v1</code>里</p><h4 id="生成部署配置文件"><a href="#生成部署配置文件" class="headerlink" title="生成部署配置文件"></a>生成部署配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/ocpinstall</span><br><span class="line">cp install-config.yaml  install-config.yaml.$(date +%Y%m%d)</span><br></pre></td></tr></table></figure><p>创建部署清单manifests</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/ocpinstall</span><br><span class="line">openshift-install create manifests</span><br></pre></td></tr></table></figure><p>如果你的 worker 节点很多，不希望 master 节点上有pod被调度，修改 <code>manifests/cluster-scheduler-02-config.yml</code> 文件，将 <code>mastersSchedulable</code> 的值设为 flase，以防止 Pod 调度到控制节点。</p><p>后面的install create会转换manifest目录下所有文件，这里备份下manifest目录，有兴趣后面可以研究下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp -a manifests manifests-bak</span><br><span class="line"><span class="meta">#</span><span class="bash"> 把部署清单转换成 Ignition 配置文件</span></span><br><span class="line">openshift-install create ignition-configs</span><br></pre></td></tr></table></figure><p><code>ll manifests/</code>目录是一堆yaml</p><p>Ignition 生成的文件如下，也有隐藏的文件，有兴趣可以去看看</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── auth</span><br><span class="line">│   ├── kubeadmin-password</span><br><span class="line">│   └── kubeconfig</span><br><span class="line">├── bootstrap.ign</span><br><span class="line">├── master.ign</span><br><span class="line">├── metadata.json</span><br><span class="line">└── worker.ign</span><br></pre></td></tr></table></figure><p>把所有 ignition 文件复制到 http server 的目录里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\cp -a /data/ocpinstall/*.ign /data/install-nginx/install/ignition/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 由于nginx容器用户是nginx，上面的ign文件的o是0，增加下o的r的权限</span></span><br><span class="line">chmod o+r /data/install-nginx/install/ignition/*.ign</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 改名下raw.gz的名字，以便在 boot cmdline的时候少输入url</span></span><br><span class="line">cd /data/install-nginx/install/</span><br><span class="line">mv rhcos-4.*-x86_64-metal.x86_64.raw.gz rhcos.raw.gz</span><br></pre></td></tr></table></figure><h2 id="安装集群"><a href="#安装集群" class="headerlink" title="安装集群"></a>安装集群</h2><p>先安装 bootstrap 机器，是因为 master 机器起来的时候会请求<code>curl -sk https://api-int.openshift4.example.com:22623/config/master</code>，如果 bootstrap 机器起来后上面的 容器和 pod 服务没就绪。master 节点安装完系统开机后会一直 retry 这个接口</p><h3 id="bootstrap"><a href="#bootstrap" class="headerlink" title="bootstrap"></a>bootstrap</h3><p>机器创建好后给机器的光驱挂载上前面的 installer 的 iso 文件，开机后会停留在菜单选择界面</p><ol><li>在 RHCOS Installer 安装界面按 <code>Tab</code> 键进入引导参数配置选项</li><li>在默认选项 <code>coreos.inst=yes</code> 之后添加（由于无法拷贝粘贴，请输入<strong>仔细核对</strong>后再回车进行）：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=10.226.45.223::10.226.45.254:255.255.255.0:bootstrap.openshift4.example.com:ens192:none nameserver=10.226.45.250 coreos.inst.install_dev=sda coreos.inst.image_url=http://10.226.45.250:8080/rhcos.raw.gz coreos.inst.ignition_url=http://10.226.45.250:8080/ignition/bootstrap.ign</span><br></pre></td></tr></table></figure><p>参数说明，参数为<code>key=value key1=value1</code> 每个得空格隔开</p><table><thead><tr><th align="center">key</th><th align="center">value</th><th align="center">注意事项</th></tr></thead><tbody><tr><td align="center"><code>ip</code></td><td align="center"><code>$IPADDRESS::$DEFAULTGW:$NETMASK:$HOSTNAMEFQDN:$IFACE:none</code></td><td align="center"><code>IFACE</code>在rhcos里似乎是固定的<code>ens192</code>，我试过了可以不写</td></tr><tr><td align="center"><code>nameserver</code></td><td align="center"><code>$&#123;bastion_ip&#125;</code></td><td align="center">集群的dns server，这里是<code>bastion</code></td></tr><tr><td align="center"><code>coreos.inst.install_dev</code></td><td align="center"><code>sda</code></td><td align="center">rhcos 安装在哪块儿硬盘上，测试过只有一块硬盘也不会自动识别，所以不能省略，我的环境是 vsphere，硬盘是 sda，没有在 openstack 的虚机下试过(这样可能写vda？)</td></tr><tr><td align="center"><code>coreos.inst.image_url</code></td><td align="center"><code>http://$&#123;bastion_ip&#125;:8080/rhcos-metal.raw.gz</code></td><td align="center"><code>rhcos-metal</code>的 url，根据自己http server的实际 url 填写</td></tr><tr><td align="center"><code>coreos.inst.ignition_url</code></td><td align="center"><code>http://$&#123;bastion_ip&#125;:8080/ignition/$&#123;type&#125;.ign</code></td><td align="center">ignition 的 url 链接，bootstrap 则结尾是 <code>bootstrap.ign</code>，master 则是 <code>master.gin</code></td></tr></tbody></table><p><img src="https://cdn.jsdelivr.net/gh/zhangguanzhang/Image-Hosting/picgo/rhcos_install_boot_cmdline.png" alt="boot_cmdline"></p><ol start="3"><li>如果安装有问题会进入 <code>emergency shell</code>，检查网络、域名解析是否正常，如果不正常一般是以上参数输入有误，reboot 退出 shell 回到第一步重新开始。</li></ol><p>安装成功后从基础节点通过命令 <code>ssh -i ~/.ssh/new_rsa core@10.226.45.250</code> 登录 bootstrap 节点，然后<code>sudo su</code>切换到 root 后验证：</p><ul><li>网络配置是否符合自己的设定：<ul><li><code>hostname</code></li><li><code>ip route</code></li><li><code>cat /etc/resolv.conf</code></li></ul></li><li>验证是否成功启动 bootstrap 相应服务：<ul><li><code>podman ps -a</code> 查看服务是否以容器方式运行</li><li>使用 <code>ss -tulnp</code> 查看 6443 和 22623 端口是否启用。</li></ul></li></ul><p>这里简单介绍一下 bootstrap 节点的启动流程，它启动后运行<code>bootkube.service</code>，systemd 的 <code>ConditionPathExists</code> 可以看到用文件作为防止成功后再次运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@bootstrap core]# systemctl cat bootkube</span><br><span class="line"><span class="meta">#</span><span class="bash"> /etc/systemd/system/bootkube.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Bootstrap a Kubernetes cluster</span><br><span class="line">Requires=crio-configure.service</span><br><span class="line">Wants=kubelet.service</span><br><span class="line">After=kubelet.service crio-configure.service</span><br><span class="line">ConditionPathExists=!/opt/openshift/.bootkube.done</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/opt/openshift</span><br><span class="line">ExecStart=/usr/local/bin/bootkube.sh</span><br></pre></td></tr></table></figure><p>查看上面的脚本内容，我们会发现它会先通过 <code>podman</code> 跑一些容器，然后在容器里面启动临时控制平面，这个临时控制平面是通过 <code>CRIO</code> 跑在容器里的，有点绕。。看下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@bootstrap core]# podman ps -a</span><br><span class="line">CONTAINER ID  IMAGE                                                                                                                    COMMAND               CREATED         STATUS                     PORTS  NAMES</span><br><span class="line">7a9a800c6819  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:416ad8f3ddd49adae4f7db66f8a130319084604296c076c2a6d22264a5688d65   start --tear-down...  12 minutes ago  Up 12 minutes ago                 strange_yonath</span><br><span class="line">8997b001e324  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bf0d5fd64ab53dfdd477c90a293f3ec90379a22e8b356044082e807565699863   render --dest-dir...  12 minutes ago  Exited (0) 12 minutes ago         stoic_williamson</span><br><span class="line">7c45d60ecd39  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597   bootstrap --etcd-...  12 minutes ago  Exited (0) 12 minutes ago         lucid_heisenberg</span><br><span class="line">fe300339df9b  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4   render --prefix=c...  12 minutes ago  Exited (0) 12 minutes ago         suspicious_kepler</span><br><span class="line">102c20d247d7  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ddb26e047d0e0d7b11bdb625bd7a941ab66f7e1ef5a5f7455d8694e7ba48989d   /usr/bin/cluster-...  12 minutes ago  Exited (0) 12 minutes ago         optimistic_euler</span><br><span class="line">1b7c759792c0  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ca556d4515818e3e10d2e771d986784460509146ea9dd188fb8ba6f6ac694132   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         nice_lehmann</span><br><span class="line">8dab8ea23ebd  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e4758039391099dc1b0265099474113dcd8bcce84a1c92d02c1ef760793079e6   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         clever_meitner</span><br><span class="line">8bd804fb88bf  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c93a0de1e4cb3f04e37d547c1b81a2a22c4d9d01c013374c466ed3fd0416215a   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         recursing_ramanujan</span><br><span class="line">11dc7e222bf9  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         wizardly_easley</span><br><span class="line">b60208539287  registry.openshift4.example.com/ocp4/openshift4@sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425  render --output-d...  13 minutes ago  Exited (0) 13 minutes ago         goofy_bassi</span><br><span class="line">428dec4624e1  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd   /usr/bin/grep -oP...  13 minutes ago  Exited (0) 13 minutes ago         pedantic_northcutt</span><br><span class="line">[root@bootstrap core]# crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                                                                  NAMESPACE                             ATTEMPT</span><br><span class="line">ec6155d87b866       13 minutes ago      Ready               bootstrap-kube-scheduler-bootstrap.openshift4.example.com             kube-system                           0</span><br><span class="line">6808217a146b7       13 minutes ago      Ready               bootstrap-kube-controller-manager-bootstrap.openshift4.example.com    kube-system                           0</span><br><span class="line">7f1ae27edd3fe       13 minutes ago      Ready               bootstrap-kube-apiserver-bootstrap.openshift4.example.com             kube-system                           0</span><br><span class="line">cc92475639870       13 minutes ago      Ready               cloud-credential-operator-bootstrap.openshift4.example.com            openshift-cloud-credential-operator   0</span><br><span class="line">08c45b635d5c1       13 minutes ago      Ready               bootstrap-cluster-version-operator-bootstrap.openshift4.example.com   openshift-cluster-version             0</span><br><span class="line">69fc31911a89f       14 minutes ago      Ready               bootstrap-machine-config-operator-bootstrap.openshift4.example.com    default                               0</span><br><span class="line">7d2598ac710e6       14 minutes ago      Ready               etcd-bootstrap-member-bootstrap.openshift4.example.com                openshift-etcd                        0</span><br></pre></td></tr></table></figure><p>如果你快速查看上面的可能一时半会儿没起来，可以通过下面命令持续观察脚本运行的日志，<code>bootkube.sh</code> 正常运行完（所有master也上来后）会出现<code>bootkube.service complete</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> journalctl -b -f -u bootkube.service</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Jun 05 00:24:12 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:12.108179       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:21 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:21.595680       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:26 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:26.250214       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:26 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:26.306421       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:29.097072       1 waitforceo.go:64] Cluster etcd operator bootstrapped successfully</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:29.097306       1 waitforceo.go:58] cluster-etcd-operator bootstrap etcd</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com podman[16531]: 2020-06-05 00:24:29.120864426 +0000 UTC m=+17.965364064 container died 77971b6ca31755a89b279fab6f9c04828c4614161c2e678c7cba48348e684517 (image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9f7a02df3a5d91326d95e444e2e249f8205632ae986d6dccc7f007ec65c8af77, name=recursing_cerf)</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com bootkube.sh[12571]: bootkube.service complete</span><br></pre></td></tr></table></figure><h3 id="master"><a href="#master" class="headerlink" title="master"></a>master</h3><p>在 bootstrap 的 crictl pods 没问题后，我们来启动 master节点，参考 bootstrap，同样配置 boot cmdline 启动，注意 ip，hostname，ign结尾的<code>类型.ign</code>，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=10.226.45.251::10.226.45.254:255.255.255.0:master1.openshift4.example.com:ens192:none nameserver=10.226.45.250 coreos.inst.install_dev=sda coreos.inst.image_url=http://10.226.45.250:8080/rhcos.raw.gz coreos.inst.ignition_url=http://10.226.45.250:8080/ignition/master.ign</span><br></pre></td></tr></table></figure><h3 id="worker"><a href="#worker" class="headerlink" title="worker"></a>worker</h3><p>此时你worker也可以按照相关参数启动，下面是参考，我这里是后面 bootstrap 完成后重装它去启动作为worker的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=10.226.45.223::10.226.45.254:255.255.255.0:worker1.openshift4.example.com:ens192:none nameserver=10.226.45.250 coreos.inst.install_dev=sda coreos.inst.image_url=http://10.226.45.250:8080/rhcos.raw.gz coreos.inst.ignition_url=http://10.226.45.250:8080/ignition/worker.ign</span><br></pre></td></tr></table></figure><p>一样的安装，注意<code>ingition_url</code>结尾是<code>worker.ign</code>，起来后 node 的 csr 得批准</p><p>查看挂起的证书签名请求（CSR），并确保添加到集群的每台节点都能看到具有 <code>Pending</code> 或 <code>Approved</code> 状态的客户端和服务端请求。针对 Pending 状态的 CSR 批准请求：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc get csr</span><br><span class="line">oc adm certificate approve xxx</span><br></pre></td></tr></table></figure><p>或者执行以下命令批准所有 CSR：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CSR_NAMES=`oc get csr -o json | jq -r <span class="string">&#x27;.items[] | select(.status == &#123;&#125; ) | .metadata.name&#x27;</span>`</span><br><span class="line">[ -n <span class="string">&quot;<span class="variable">$CSR_NAMES</span>&quot;</span> ] &amp;&amp;  oc adm certificate approve <span class="variable">$CSR_NAMES</span></span><br></pre></td></tr></table></figure><p>后续添加 worker 重复此步骤即可</p><h3 id="验证集群"><a href="#验证集群" class="headerlink" title="验证集群"></a>验证集群</h3><h4 id="配置-oc-kubeconfig"><a href="#配置-oc-kubeconfig" class="headerlink" title="配置 oc kubeconfig"></a>配置 oc kubeconfig</h4><p>在 bastion 节点上,我们生成部署清单的时候会产生一个<code>kubeconfig</code>，目录<code>/data/ocpinstall/auth</code></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">├── auth</span><br><span class="line">│   ├── kubeadmin-password</span><br><span class="line">│   └── kubeconfig</span><br></pre></td></tr></table></figure><p>像 k8s 那样，我们需要拷贝到默认读取目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.kube</span><br><span class="line">\cp /data/ocpinstall/auth/kubeconfig ~/.kube/config</span><br><span class="line">oc whoami</span><br></pre></td></tr></table></figure><p>配置下 oc 的自动补全</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cat&gt;</span><span class="bash">/etc/bash_completion.d/oc &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">source &lt;(oc completion bash)</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="验证集群节点"><a href="#验证集群节点" class="headerlink" title="验证集群节点"></a>验证集群节点</h4><p>查看集群节点是否上来</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oc  get node</span></span><br><span class="line">NAME                             STATUS   ROLES           AGE     VERSION</span><br><span class="line">master1.openshift4.example.com   Ready    master,worker   3d14h   v1.18.3+6c42de8</span><br><span class="line">master2.openshift4.example.com   Ready    master,worker   3d14h   v1.18.3+6c42de8</span><br><span class="line">master3.openshift4.example.com   Ready    master,worker   3d14h   v1.18.3+6c42de8</span><br></pre></td></tr></table></figure><p>查看集群的 pod，如果很久之后某些 pod 还是一直 crash 的话看下面<code>troubleshooting</code>的，因为<a href="https://github.com/openshift/cluster-bootstrap/blob/master/pkg/start/status.go#L46-L106">源码里会等待所有pod不再crash</a>  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc get po -A</span><br></pre></td></tr></table></figure><p>使用 openshift-install 命令查看 bootstrap 完成否，会需要等很久，有问题看下文的 troubleshooting</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openshift-install --dir=/data/ocpinstall wait-for bootstrap-complete --log-level=debug</span></span><br><span class="line">DEBUG OpenShift Installer 4.5.9</span><br><span class="line">DEBUG Built from commit 0d5c871ce7d03f3d03ab4371dc39916a5415cf5c</span><br><span class="line">INFO Waiting up to 20m0s for the Kubernetes API at https://api.openshift4.example.com:6443...</span><br><span class="line">INFO API v1.18.3+6c42de8 up</span><br><span class="line">INFO Waiting up to 40m0s for bootstrapping to complete...</span><br><span class="line">DEBUG Bootstrap status: complete</span><br><span class="line">INFO It is now safe to remove the bootstrap resources</span><br><span class="line">DEBUG Time slapsed per state:</span><br><span class="line">DEBUG Bootstrap Complete: 24m13s</span><br><span class="line">INFO Time elapsed: 24m13s</span><br></pre></td></tr></table></figure><p>这里<a href="https://github.com/openshift/cluster-bootstrap/blob/master/pkg/start/start.go#L105-L129">查看源码</a>，实际上 bootkube.sh 最后阶段会向集群里注入一个 cm <code>kube-system/bootstrap</code>存放状态。而上面的<code>wait-for bootstrap-complete</code>实际上就是等待这个cm创建，可以<a href="https://github.com/openshift/installer/blob/master/cmd/openshift-install/create.go#L312">查看源码里的waitForBootstrapConfigMap方法</a></p><h3 id="web-console"><a href="#web-console" class="headerlink" title="web console"></a>web console</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /data/ocpinstall</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> openshift-install wait-for install-complete</span></span><br><span class="line">INFO Waiting up to 30m0s for the cluster at https://api.openshift4.example.com:6443 to initialize...</span><br><span class="line">INFO Waiting up to 10m0s for the openshift-console route to be created...</span><br><span class="line">INFO Install complete!</span><br><span class="line">INFO To access the cluster as the system:admin user when using &#x27;oc&#x27;, run &#x27;export KUBECONFIG=/data/ocpinstall/auth/kubeconfig&#x27;</span><br><span class="line">INFO Access the OpenShift web-console here: https://console-openshift-console.apps.openshift4.example.com</span><br><span class="line">INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;xxxx-xxxx-zKu8J-W7QRi&quot;</span><br><span class="line">INFO Time elapsed: 1s  </span><br></pre></td></tr></table></figure><p>注意最后提示访问 <code>Web Console</code> 的网址及用户密码。如果密码忘了也没关系，可以查看文件 <code>/data/ocpinstall/auth/kubeadmin-password</code> 来获得密码。</p><p>本地访问 Web Console，因为集群里已经有 ingress controller 了，并且我们的入口是 10.226.45.250 ，所以本地需要添加下列 hosts：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.226.45.250 console-openshift-console.apps.openshift4.example.com</span><br><span class="line">10.226.45.250 oauth-openshift.apps.openshift4.example.com</span><br></pre></td></tr></table></figure><p>浏览器访问 <code>https://console-openshift-console.apps.openshift4.example.com</code>，输入上面输出的用户名(kubeadm)和密码登录。首次登录后顶部会提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">You are logged <span class="keyword">in</span> as a temporary administrative user. Update the Cluster OAuth configuration to allow others to <span class="built_in">log</span> <span class="keyword">in</span>.</span><br></pre></td></tr></table></figure><p>我们可以通过 htpasswd 自定义管理员账号，步骤如下：</p><p>1 <code>cd /data/ocpinstall/auth &amp;&amp; htpasswd -c -B -b users.htpasswd admin xxxxx</code></p><p>2 将 <code>users.htpasswd</code> 文件下载到本地。</p><p>3 在 Web Console 页面打开 <code>Global Configuration</code>：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605150947.png" alt="Global Configuration"></p><p>然后找到 <code>OAuth</code>，点击进入，然后添加 <code>HTPasswd</code> 类型的 <code>Identity Providers</code>，并上传 <code>users.htpasswd</code> 文件。</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605151307.png" alt="htpasswd"></p><p>4 退出当前用户，要注意退出到如下界面：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605151646.png" alt="login"></p><p>选择 <code>htpasswd</code>，然后输入之前创建的用户名密码登录。</p><p>如果退出后出现的就是用户密码输入窗口，实际还是 <code>kube:admin</code> 的校验，如果未出现如上提示，可以手动输入 Web Console 地址来自动跳转。</p><p>5 登录后貌似能看到 <code>Administrator</code> 菜单项，但访问如 <code>OAuth Details</code> 仍然提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oauths.config.openshift.io <span class="string">&quot;cluster&quot;</span> is forbidden: User <span class="string">&quot;admin&quot;</span> cannot get resource <span class="string">&quot;oauths&quot;</span> <span class="keyword">in</span> API group <span class="string">&quot;config.openshift.io&quot;</span> at the cluster scope</span><br></pre></td></tr></table></figure><p>因此需要授予集群管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc adm policy add-cluster-role-to-user cluster-admin admin</span><br></pre></td></tr></table></figure><p>Web Console 部分截图：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605152528.png" alt="operatorhub"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605152729.png" alt="routes"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605152911.png" alt="dashboards"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605153048.png" alt="metrics"></p><p>如果想删除默认账号，可以执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc -n kube-system delete secrets kubeadmin</span><br></pre></td></tr></table></figure><h2 id="集群善后的一些配置"><a href="#集群善后的一些配置" class="headerlink" title="集群善后的一些配置"></a>集群善后的一些配置</h2><h3 id="ingress-controller"><a href="#ingress-controller" class="headerlink" title="ingress controller"></a>ingress controller</h3><p>github 地址为<a href="https://github.com/openshift/cluster-ingress-operator">cluster-ingress-operator</a><br>这里我集群是3个 master，发现ingress controller的一些属性没配置对，自行先<code>get deploy router-default -o yaml</code>查看，可能后续修复了。根据推测排查到是 operator 部署的，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ~]# oc get ingresscontrollers -A</span><br><span class="line">NAMESPACE                    NAME      AGE</span><br><span class="line">openshift-ingress-operator   default   3d15h</span><br></pre></td></tr></table></figure><p>但是看了下属性<code>oc explain ingresscontrollers.spec.endpointPublishingStrategy.hostNetwork</code>发现不支持配置互斥和<code>dnsPolicy</code>，这会导致无法解析集群内的svc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@master2 ~]# crictl pods --name router-default-fb744fb7f-hmmn5 -q</span><br><span class="line">0a0c7cc6d1ad6815f7613fd758c5329c4265ddb6607f568b69e30fdafdfc0a52</span><br><span class="line">[root@master2 ~]# crictl ps --pod=0a0c7cc6d1ad6815f7613fd758c5329c4265ddb6607f568b69e30fdafdfc0a52</span><br><span class="line">CONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID</span><br><span class="line">b04c17fb1c58d       dd7aaceb9081f88c9ba418708f32a66f5de4e527a00c7f6ede50d55c93eb04ed   3 days ago          Running             router              1                   0a0c7cc6d1ad6</span><br><span class="line">[root@master2 ~]# crictl exec b04 cat /etc/resolv.conf</span><br><span class="line">search openshift4.example.com</span><br><span class="line">nameserver 10.226.45.250</span><br><span class="line">[root@master2 ~]# crictl exec b04 curl  kubernetes.default.svc.cluster.local</span><br><span class="line"><span class="meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span></span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: kubernetes.default.svc.cluster.local; Unknown error</span><br><span class="line">FATA[0000] execing command in container failed: command terminated with exit code 6</span><br></pre></td></tr></table></figure><p>这个我已经提<a href="https://github.com/openshift/cluster-ingress-operator/issues/464">issue</a> 了</p><p>调整下 ingress controller 的数量，因为 deployment 由 crd 纳管，最好不要直接去操作 deploy 的属性，同时这里我们是把 ingress controller 部署在 master 上，所以得用 nodeSelector 固定，先打 label</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">oc label node master1.openshift4.example.com ingressControllerDeploy=true</span><br><span class="line">oc label node master2.openshift4.example.com ingressControllerDeploy=true</span><br><span class="line">oc label node master3.openshift4.example.com ingressControllerDeploy=true</span><br><span class="line"></span><br><span class="line">oc -n openshift-ingress-operator patch ingresscontroller default --type=&#x27;merge&#x27; -p &quot;$(cat &lt;&lt;- EOF</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  nodePlacement:</span><br><span class="line">    nodeSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        ingressControllerDeploy: &quot;true&quot;</span><br><span class="line">EOF</span><br><span class="line">)&quot;</span><br></pre></td></tr></table></figure><h3 id="移除haproxy-里-bootstrap-的配置"><a href="#移除haproxy-里-bootstrap-的配置" class="headerlink" title="移除haproxy 里 bootstrap 的配置"></a>移除haproxy 里 bootstrap 的配置</h3><p>自行操作，移除负载均衡里的无用配置</p><h3 id="Local-Storage-Operator"><a href="#Local-Storage-Operator" class="headerlink" title="Local Storage Operator"></a>Local Storage Operator</h3><p>参考 <a href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.5/html/storage/persistent-storage-using-local-volume#local-storage-install_persistent-storage-local">Installing the Local Storage Operator</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc oc new-project local-storage</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="clusteroperator"><a href="#clusteroperator" class="headerlink" title="clusteroperator"></a>clusteroperator</h3><p>查看 clusteroperator，如果有个别有问题，可以等worker部署完了慢慢排查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ~]# oc get clusteroperator</span><br><span class="line">NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE</span><br><span class="line">authentication                             4.5.9     True        False         False      118m</span><br><span class="line">cloud-credential                           4.5.9     True        False         False      3d19h</span><br><span class="line">cluster-autoscaler                         4.5.9     True        False         False      3d18h</span><br><span class="line">config-operator                            4.5.9     True        False         False      3d18h</span><br><span class="line">console                                    4.5.9     True        False         False      8m22s</span><br><span class="line">csi-snapshot-controller                    4.5.9     True        False         False      8m27s</span><br><span class="line">dns                                        4.5.9     True        False         False      8m22s</span><br><span class="line">etcd                                       4.5.9     True        False         False      3d19h</span><br><span class="line">image-registry                             4.5.9     True        False         False      3d19h</span><br><span class="line">ingress                                    4.5.9     True        False         False      8m24s</span><br><span class="line">insights                                   4.5.9     True        False         False      3d19h</span><br><span class="line">kube-apiserver                             4.5.9     True        False         False      3d19h</span><br><span class="line">kube-controller-manager                    4.5.9     True        False         False      3d19h</span><br><span class="line">kube-scheduler                             4.5.9     True        False         False      3d19h</span><br><span class="line">kube-storage-version-migrator              4.5.9     True        False         False      8m23s</span><br><span class="line">machine-api                                4.5.9     True        False         False      3d19h</span><br><span class="line">machine-approver                           4.5.9     True        False         False      3d19h</span><br><span class="line">machine-config                             4.5.9     True        False         False      3d19h</span><br><span class="line">marketplace                                4.5.9     True        False         False      3d19h</span><br><span class="line">monitoring                                 4.5.9     True        False         False      3d18h</span><br><span class="line">network                                    4.5.9     True        False         False      3d19h</span><br><span class="line">node-tuning                                4.5.9     True        False         False      8m27s</span><br><span class="line">openshift-apiserver                        4.5.9     True        False         False      8m20s</span><br><span class="line">openshift-controller-manager               4.5.9     True        False         False      8m5s</span><br><span class="line">openshift-samples                          4.5.9     True        False         False      3d18h</span><br><span class="line">operator-lifecycle-manager                 4.5.9     True        False         False      3d19h</span><br><span class="line">operator-lifecycle-manager-catalog         4.5.9     True        False         False      3d19h</span><br><span class="line">operator-lifecycle-manager-packageserver   4.5.9     True        False         False      7m56s</span><br><span class="line">service-ca                                 4.5.9     True        False         False      3d19h</span><br><span class="line">storage                                    4.5.9     True        False         False      3d19h</span><br></pre></td></tr></table></figure><h2 id="troubleshooting"><a href="#troubleshooting" class="headerlink" title="troubleshooting"></a>troubleshooting</h2><p>参考<a href="https://github.com/openshift/installer/blob/master/docs/user/troubleshooting.md">官方troubleshooting文档</a>，很多理念还是 k8s，所以多用 oc 命令排查</p><p>这里是我单 master 时候的错误，可以参考下排查思路</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ocpinstall]# oc get po -A</span><br><span class="line">NAMESPACE                                          NAME                                                      READY   STATUS             RESTARTS   AGE</span><br><span class="line">openshift-apiserver-operator                       openshift-apiserver-operator-6ddb679b87-478mq             1/1     Running            1          65m</span><br><span class="line">openshift-authentication-operator                  authentication-operator-f9c8c4d9-gqkhl                    1/1     Running            6          65m</span><br><span class="line">openshift-cluster-machine-approver                 machine-approver-55ccb88c4-cfxtm                          2/2     Running            0          65m</span><br><span class="line">openshift-cluster-node-tuning-operator             cluster-node-tuning-operator-576df548b-mbzcz              1/1     Running            0          65m</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-8vv2c                                               1/1     Running            0          57m</span><br><span class="line">openshift-cluster-storage-operator                 csi-snapshot-controller-operator-59cfdb9dc6-8qsm2         1/1     Running            0          65m</span><br><span class="line">openshift-cluster-version                          cluster-version-operator-6fc5bf7855-bjdbg                 1/1     Running            0          65m</span><br><span class="line">openshift-controller-manager-operator              openshift-controller-manager-operator-6d85b6f94-zxdc4     1/1     Running            1          65m</span><br><span class="line">openshift-dns-operator                             dns-operator-b47fff57d-4vlbs                              2/2     Running            0          65m</span><br><span class="line">openshift-dns                                      dns-default-qq6lt                                         3/3     Running            0          56m</span><br><span class="line">openshift-etcd-operator                            etcd-operator-5bcff88f49-mqvdh                            1/1     Running            1          65m</span><br><span class="line">openshift-kube-apiserver-operator                  kube-apiserver-operator-79b99c5564-c24z8                  1/1     Running            1          65m</span><br><span class="line">openshift-kube-apiserver                           installer-2-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master1.openshift4.example.com             3/4     CrashLoopBackOff   14         55m</span><br><span class="line">openshift-kube-controller-manager-operator         kube-controller-manager-operator-656db94888-gxlh8         1/1     Running            1          65m</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-controller-manager                  installer-4-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master1.openshift4.example.com    4/4     Running            0          55m</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master1.openshift4.example.com          0/1     Completed          0          55m</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-4-master1.openshift4.example.com          0/1     Completed          0          54m</span><br><span class="line">openshift-kube-scheduler-operator                  openshift-kube-scheduler-operator-5db8ddbd86-zgc2j        1/1     Running            1          65m</span><br><span class="line">openshift-kube-scheduler                           installer-2-master1.openshift4.example.com                0/1     Completed          0          57m</span><br><span class="line">openshift-kube-scheduler                           installer-3-master1.openshift4.example.com                0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           installer-4-master1.openshift4.example.com                0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           installer-5-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master1.openshift4.example.com   2/2     Running            0          55m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-2-master1.openshift4.example.com          0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-3-master1.openshift4.example.com          0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master1.openshift4.example.com          0/1     Completed          0          55m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master1.openshift4.example.com          0/1     Completed          0          54m</span><br><span class="line">openshift-kube-storage-version-migrator-operator   kube-storage-version-migrator-operator-57c5ccbffc-rsfgm   1/1     Running            1          65m</span><br><span class="line">openshift-kube-storage-version-migrator            migrator-84f6d56959-7fjrb                                 1/1     Running            0          57m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-5gx99                         0/1     Pending            0          56m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-dpjf8                         0/1     Pending            0          56m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-whsdq                         0/1     Running            0          56m</span><br><span class="line">openshift-machine-config-operator                  machine-config-controller-c5f979c7c-jrr7h                 1/1     Running            0          56m</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-hqfns                               2/2     Running            0          57m</span><br><span class="line">openshift-machine-config-operator                  machine-config-operator-7f9fc9fb9-6zh9j                   1/1     Running            0          65m</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-w84sq                               1/1     Running            0          56m</span><br><span class="line">openshift-multus                                   multus-admission-controller-z9hc4                         2/2     Running            0          57m</span><br><span class="line">openshift-multus                                   multus-jfwbz                                              1/1     Running            0          58m</span><br><span class="line">openshift-network-operator                         network-operator-84bb7765bc-7c2jm                         1/1     Running            0          65m</span><br><span class="line">openshift-operator-lifecycle-manager               catalog-operator-5ff486f6c8-tplsr                         1/1     Running            0          65m</span><br><span class="line">openshift-operator-lifecycle-manager               olm-operator-b4fd47678-wmpxt                              1/1     Running            0          65m</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-b68fc7b76-g8fxb                             1/1     Running            0          72s</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-b68fc7b76-z9t72                             1/1     Running            0          79s</span><br><span class="line">openshift-sdn                                      ovs-r69fg                                                 1/1     Running            0          57m</span><br><span class="line">openshift-sdn                                      sdn-6js24                                                 1/1     Running            0          57m</span><br><span class="line">openshift-sdn                                      sdn-controller-sk2f9                                      1/1     Running            0          57m</span><br><span class="line">openshift-service-ca-operator                      service-ca-operator-57cd4bc97f-t2mnq                      1/1     Running            1          65m</span><br><span class="line">openshift-service-ca                               service-ca-c75f9fb85-grcsm                                1/1     Running            0          57m</span><br></pre></td></tr></table></figure><p>发现<code>kube-apiserver-master1.openshift4.example.com</code>的 pod 有个容器一直 crash，从 bastion 上 ssh 到 master1 上去后查看日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 apiserver 的容器组</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> crictl ps --pod=$(crictl pods --name=kube-apiserver-master1.openshift4.example.com --quiet)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到失败的容器，查看<span class="built_in">log</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> crictl logs xxx</span></span><br><span class="line">...</span><br><span class="line">W0917 06:31:23.240165    1 clientconn.go:1208] grpc: addrConn.createTransport failed to connect to &#123;https://10.226.45.251:2379 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err :connection error: desc = &quot;transport: Error while dialing dial tcp 10.226.45.251:2379: connect: connection refused&quot;. Reconnecting...</span><br></pre></td></tr></table></figure><p>查看发现没有etcd的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | grep etcd</span><br></pre></td></tr></table></figure><p>回到 bastion 上，查看 etcd 来源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> oc get po -A | grep etcd</span></span><br><span class="line">openshift-etcd-operator                            etcd-operator-5bcff88f49-jf4rk                            1/1     Running            1          100m</span><br><span class="line"><span class="meta">#</span><span class="bash"> 有operator，查看deloy</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> oc get deployment -A | grep etcd</span></span><br><span class="line">openshift-etcd-operator                            etcd-operator                            1/1     1            1           100m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard                        0/1     1            0           91m</span><br></pre></td></tr></table></figure><p>从时间上看，operator 先部署，然后operator 部署了 etcd-quorum-guard 的pod，但是没就绪。这个 etcd 来源于 crd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oc -n openshift-etcd get etcd</span></span><br><span class="line">NAME     AGE</span><br><span class="line">cluster  138m</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> oc -n openshift-etcd get etcd cluster -o yaml</span></span><br><span class="line">...</span><br><span class="line">- lastTransitionTime: &quot;2020-09-17T05:33:17Z&quot;</span><br><span class="line">  message: still wating for trhee healthz etcd members</span><br><span class="line">  reason：NotEnoughEtcdMembers</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="单-master-的无法部署的处理手段"><a href="#单-master-的无法部署的处理手段" class="headerlink" title="单 master 的无法部署的处理手段"></a>单 master 的无法部署的处理手段</h4><p>看输出是期待 3 个 etcd member，我们需要更改成非HA的，执行下面命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">oc patch etcd cluster -p=&#x27;</span><br><span class="line">&#123;&quot;spec&quot;: &#123;&quot;unsupportedConfigOverrides&quot;: &#123;&quot;useUnsupportedUnsafeNonHANonProductionUnstableEtcd&quot;: true&#125;&#125;&#125;</span><br><span class="line">&#x27; </span><br><span class="line">--type=merge</span><br></pre></td></tr></table></figure><p>ocp4.6.+ 除了要应用以上<code>etcd</code>配置更新外，还需要配置更改<code>oauth</code>对于APIServer HA 的要求，参<a href="https://github.com/openshift/okd/issues/465">https://github.com/openshift/okd/issues/465</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc patch authentication cluster -p=&#x27;&#123;&quot;spec&quot;:&#123;&quot;unsupportedConfigOverrides&quot;:&#123;&quot;useUnsupportedUnsafeNonHANonProductionUnstableOAuthServer&quot;:true&#125;&#125;&#125;&#x27; --type=merge</span><br></pre></td></tr></table></figure><h4 id="一些参考信息"><a href="#一些参考信息" class="headerlink" title="一些参考信息"></a>一些参考信息</h4><p>master 上 manifests 目录正常参考如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 core]# ll /etc/kubernetes/manifests/</span><br><span class="line">total 48</span><br><span class="line">-rw-r--r--. 1 root root 22092 Sep 17 11:54 etcd-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root  5431 Sep 18 10:17 kube-apiserver-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root  5903 Sep 17 12:01 kube-controller-manager-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root  3531 Sep 17 12:00 kube-scheduler-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root   697 Sep 17 11:19 recycler-pod.yaml</span><br></pre></td></tr></table></figure><p>具体哪一步骤有问题可以看下面目录的done文件结合<code>/usr/local/bin/bootkube.sh</code>脚本查看，正常完成后是下面的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@bootstrap kubernetes]# ll /opt/openshift/</span><br><span class="line">total 28</span><br><span class="line">drwxr-xr-x. 2 root root   77 Sep 17 05:21 auth</span><br><span class="line">drwxr-xr-x. 2 root root  176 Sep 17 05:22 bootstrap-manifests</span><br><span class="line">drwxr-xr-x. 4 root root   50 Sep 17 05:22 cco-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 cco-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 config-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 config-bootstrap.done</span><br><span class="line">drw-r--r--. 4 root root   40 Sep 17 05:21 cvo-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:21 cvo-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:21 etcd-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:21 etcd-bootstrap.done</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 ingress-operator-bootstrap.done</span><br><span class="line">drwxr-x---. 2 root root  105 Sep 17 05:22 ingress-operator-manifests</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 kube-apiserver-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 kube-apiserver-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 kube-controller-manager-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 kube-controller-manager-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 kube-scheduler-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 kube-scheduler-bootstrap.done</span><br><span class="line">drwxr-xr-x. 2 root root 8192 Sep 17 05:22 manifests</span><br><span class="line">drw-r-xr-x. 4 root root   40 Sep 17 05:22 mco-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 mco-bootstrap.done</span><br><span class="line">drwxr-xr-x. 2 root root 4096 Sep 17 05:21 openshift</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:21 openshift-manifests.done</span><br><span class="line">drwxr-xr-x. 2 root root 8192 Sep 17 05:21 tls</span><br></pre></td></tr></table></figure><p>3个 master 和 1个 node的所有pod情况参考:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ocpinstall]# oc get po -A</span><br><span class="line">NAMESPACE                                          NAME                                                         READY   STATUS      RESTARTS   AGE</span><br><span class="line">openshift-apiserver-operator                       openshift-apiserver-operator-6ddb679b87-48rfk                1/1     Running     2          3d21h</span><br><span class="line">openshift-apiserver                                apiserver-559b848b77-2x22v                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-apiserver                                apiserver-559b848b77-bctpw                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-apiserver                                apiserver-559b848b77-fwks6                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-authentication-operator                  authentication-operator-f9c8c4d9-n94nf                       1/1     Running     4          3d21h</span><br><span class="line">openshift-authentication                           oauth-openshift-8555c79cc8-hksfc                             1/1     Running     0          145m</span><br><span class="line">openshift-authentication                           oauth-openshift-8555c79cc8-nlmxb                             1/1     Running     0          145m</span><br><span class="line">openshift-cloud-credential-operator                cloud-credential-operator-5b67677b7d-h5kfg                   1/1     Running     0          3d20h</span><br><span class="line">openshift-cluster-machine-approver                 machine-approver-55ccb88c4-xdnkl                             2/2     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             cluster-node-tuning-operator-576df548b-7xb6r                 1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-bw8ql                                                  1/1     Running     0          47m</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-mxpz2                                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-pblld                                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-w5pct                                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-samples-operator                 cluster-samples-operator-6bdbc4ffc5-jq7zn                    2/2     Running     0          3d20h</span><br><span class="line">openshift-cluster-storage-operator                 cluster-storage-operator-56f4c88949-844js                    1/1     Running     0          3d20h</span><br><span class="line">openshift-cluster-storage-operator                 csi-snapshot-controller-7d764ffdf8-wnp7h                     1/1     Running     0          3d20h</span><br><span class="line">openshift-cluster-storage-operator                 csi-snapshot-controller-operator-59cfdb9dc6-58ntn            1/1     Running     1          3d21h</span><br><span class="line">openshift-cluster-version                          cluster-version-operator-6fc5bf7855-96p2p                    1/1     Running     0          3d21h</span><br><span class="line">openshift-config-operator                          openshift-config-operator-b79c8b76-5x8c5                     1/1     Running     0          3d20h</span><br><span class="line">openshift-console-operator                         console-operator-887dd965b-28cj2                             1/1     Running     0          3d20h</span><br><span class="line">openshift-console                                  console-7cbd7657d-56bkb                                      1/1     Running     910        3d20h</span><br><span class="line">openshift-console                                  console-7cbd7657d-dqvjz                                      1/1     Running     0          3h28m</span><br><span class="line">openshift-console                                  downloads-54fc476767-59qbt                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-console                                  downloads-54fc476767-hjxhq                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-controller-manager-operator              openshift-controller-manager-operator-6d85b6f94-4t2f7        1/1     Running     2          3d21h</span><br><span class="line">openshift-controller-manager                       controller-manager-2wbpf                                     1/1     Running     0          2d22h</span><br><span class="line">openshift-controller-manager                       controller-manager-pnnk9                                     1/1     Running     0          2d22h</span><br><span class="line">openshift-controller-manager                       controller-manager-qrtg9                                     1/1     Running     0          2d22h</span><br><span class="line">openshift-dns-operator                             dns-operator-b47fff57d-rmnl5                                 2/2     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-h7scc                                            3/3     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-mdsq5                                            3/3     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-r24lg                                            3/3     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-sm5zl                                            3/3     Running     0          47m</span><br><span class="line">openshift-etcd-operator                            etcd-operator-5bcff88f49-jf4rk                               1/1     Running     1          3d21h</span><br><span class="line">openshift-etcd                                     etcd-master1.openshift4.example.com                          4/4     Running     0          3d20h</span><br><span class="line">openshift-etcd                                     etcd-master2.openshift4.example.com                          4/4     Running     0          3d20h</span><br><span class="line">openshift-etcd                                     etcd-master3.openshift4.example.com                          4/4     Running     0          3d20h</span><br><span class="line">openshift-etcd                                     installer-2-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-etcd                                     installer-2-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-etcd                                     installer-2-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-etcd                                     installer-3-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     installer-3-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     installer-3-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-2-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-2-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-2-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-3-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-3-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-3-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-image-registry                           cluster-image-registry-operator-574467db97-hgcl6             2/2     Running     0          3d20h</span><br><span class="line">openshift-image-registry                           image-pruner-1600473600-ps7rq                                0/1     Completed   0          2d8h</span><br><span class="line">openshift-image-registry                           image-pruner-1600560000-pzqzt                                0/1     Completed   0          32h</span><br><span class="line">openshift-image-registry                           image-pruner-1600646400-bbghl                                0/1     Completed   0          8h</span><br><span class="line">openshift-image-registry                           node-ca-5s66m                                                1/1     Running     0          3d20h</span><br><span class="line">openshift-image-registry                           node-ca-mn8wr                                                1/1     Running     0          3d20h</span><br><span class="line">openshift-image-registry                           node-ca-w22br                                                1/1     Running     0          47m</span><br><span class="line">openshift-image-registry                           node-ca-wcj74                                                1/1     Running     0          3d20h</span><br><span class="line">openshift-ingress-operator                         ingress-operator-69cc5476dc-scjjx                            2/2     Running     0          3d20h</span><br><span class="line">openshift-ingress                                  router-default-5f5d6bf574-67bbd                              1/1     Running     0          134m</span><br><span class="line">openshift-ingress                                  router-default-5f5d6bf574-d6z84                              1/1     Running     0          133m</span><br><span class="line">openshift-ingress                                  router-default-5f5d6bf574-kcf7v                              1/1     Running     0          135m</span><br><span class="line">openshift-insights                                 insights-operator-6cd58d5859-cf9lg                           1/1     Running     0          3d20h</span><br><span class="line">openshift-kube-apiserver-operator                  kube-apiserver-operator-79b99c5564-tlr67                     1/1     Running     2          3d21h</span><br><span class="line">openshift-kube-apiserver                           installer-10-master1.openshift4.example.com                  0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           installer-10-master2.openshift4.example.com                  0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           installer-10-master3.openshift4.example.com                  0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           installer-11-master1.openshift4.example.com                  0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           installer-11-master2.openshift4.example.com                  0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           installer-11-master3.openshift4.example.com                  0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           installer-3-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-apiserver                           installer-4-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-apiserver                           installer-5-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-5-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-5-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-6-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-7-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-8-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-8-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-8-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-9-master1.openshift4.example.com                   0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           installer-9-master2.openshift4.example.com                   0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           installer-9-master3.openshift4.example.com                   0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master1.openshift4.example.com                4/4     Running     0          2d22h</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master2.openshift4.example.com                4/4     Running     0          2d22h</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master3.openshift4.example.com                4/4     Running     0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-10-master1.openshift4.example.com            0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-10-master2.openshift4.example.com            0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-10-master3.openshift4.example.com            0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-11-master1.openshift4.example.com            0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-11-master2.openshift4.example.com            0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-11-master3.openshift4.example.com            0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-3-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-4-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-5-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-5-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-5-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-6-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-7-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-8-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-8-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-8-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-9-master1.openshift4.example.com             0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-9-master2.openshift4.example.com             0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-9-master3.openshift4.example.com             0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-controller-manager-operator         kube-controller-manager-operator-656db94888-9twxm            1/1     Running     2          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-2-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-5-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-5-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-5-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-6-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-6-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-6-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-7-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-7-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-7-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master1.openshift4.example.com       4/4     Running     0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master2.openshift4.example.com       4/4     Running     1          3d20h</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master3.openshift4.example.com       4/4     Running     0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-2-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master2.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-5-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-5-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-5-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-6-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-6-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-6-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-7-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-7-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-7-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler-operator                  openshift-kube-scheduler-operator-5db8ddbd86-b4npw           1/1     Running     1          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-3-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-4-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-4-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-4-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-5-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-5-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-5-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-6-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-6-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-7-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-7-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-7-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master1.openshift4.example.com      2/2     Running     1          3d20h</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master2.openshift4.example.com      2/2     Running     0          3d20h</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master3.openshift4.example.com      2/2     Running     2          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-3-master2.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master2.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-6-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-6-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-7-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-7-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-7-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-storage-version-migrator-operator   kube-storage-version-migrator-operator-57c5ccbffc-7x2tv      1/1     Running     1          3d21h</span><br><span class="line">openshift-kube-storage-version-migrator            migrator-84f6d56959-hnhlb                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-api                              cluster-autoscaler-operator-568855ff87-mxpfm                 2/2     Running     0          3d20h</span><br><span class="line">openshift-machine-api                              machine-api-operator-7744b46cbc-frbmx                        2/2     Running     0          3d20h</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-dgk9t                            1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-g495s                            1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-pbwq9                            1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-controller-c5f979c7c-vkmdd                    1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-79k88                                  2/2     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-h6fxj                                  2/2     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-qgdvd                                  2/2     Running     0          47m</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-t9gsq                                  2/2     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-operator-7f9fc9fb9-9whkf                      1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-66vdl                                  1/1     Running     0          3d20h</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-x7zw6                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-xfs4q                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-marketplace                              certified-operators-77d7996cd8-59qhh                         1/1     Running     82         3d20h</span><br><span class="line">openshift-marketplace                              community-operators-7c466b6dc4-87q7n                         1/1     Running     104        3d20h</span><br><span class="line">openshift-marketplace                              marketplace-operator-66f9cd98-c5psh                          1/1     Running     0          3d20h</span><br><span class="line">openshift-marketplace                              redhat-marketplace-799fdd5f6c-pwhb6                          1/1     Running     36         3d20h</span><br><span class="line">openshift-marketplace                              redhat-operators-6c568f687f-t494g                            1/1     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               alertmanager-main-0                                          5/5     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               alertmanager-main-1                                          5/5     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               alertmanager-main-2                                          5/5     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               cluster-monitoring-operator-db8666945-pvsdq                  2/2     Running     2          3d20h</span><br><span class="line">openshift-monitoring                               grafana-6db66b6b79-glzvk                                     2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               kube-state-metrics-6495bd567c-cp47h                          3/3     Running     2          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-6bc5z                                          2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-hv2kb                                          2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-lh4rs                                          2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-sgrh8                                          2/2     Running     0          47m</span><br><span class="line">openshift-monitoring                               openshift-state-metrics-84c7687db5-6dkcq                     3/3     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               prometheus-adapter-674b6c66dc-hmwwt                          1/1     Running     0          2d22h</span><br><span class="line">openshift-monitoring                               prometheus-adapter-674b6c66dc-vwvcq                          1/1     Running     0          2d22h</span><br><span class="line">openshift-monitoring                               prometheus-k8s-0                                             7/7     Running     1          3d20h</span><br><span class="line">openshift-monitoring                               prometheus-k8s-1                                             7/7     Running     1          3d20h</span><br><span class="line">openshift-monitoring                               prometheus-operator-78769c95bb-2w2sc                         2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               telemeter-client-654456d57f-grq8w                            3/3     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               thanos-querier-657fff7b4-p2zb7                               4/4     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               thanos-querier-657fff7b4-tnkxm                               4/4     Running     0          3d20h</span><br><span class="line">openshift-multus                                   multus-9ljk9                                                 1/1     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-admission-controller-2wxgw                            2/2     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-admission-controller-srz97                            2/2     Running     0          3d20h</span><br><span class="line">openshift-multus                                   multus-admission-controller-wh6g8                            2/2     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-nf2vz                                                 1/1     Running     0          47m</span><br><span class="line">openshift-multus                                   multus-pnvbh                                                 1/1     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-qw5j4                                                 1/1     Running     0          3d20h</span><br><span class="line">openshift-network-operator                         network-operator-84bb7765bc-92p6g                            1/1     Running     0          3d21h</span><br><span class="line">openshift-operator-lifecycle-manager               catalog-operator-5ff486f6c8-2pjdd                            1/1     Running     0          3d21h</span><br><span class="line">openshift-operator-lifecycle-manager               olm-operator-b4fd47678-c5kvt                                 1/1     Running     0          3d21h</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-c8c74bbcb-64wf2                                1/1     Running     0          102m</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-c8c74bbcb-skbpd                                1/1     Running     0          100m</span><br><span class="line">openshift-sdn                                      ovs-cnmcx                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      ovs-kftfb                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      ovs-qglgb                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      ovs-tczlx                                                    1/1     Running     0          47m</span><br><span class="line">openshift-sdn                                      sdn-7dxts                                                    1/1     Running     0          3d20h</span><br><span class="line">openshift-sdn                                      sdn-controller-9xkqp                                         1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      sdn-controller-bwcc2                                         1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      sdn-controller-pfq7c                                         1/1     Running     2          3d21h</span><br><span class="line">openshift-sdn                                      sdn-crgmz                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      sdn-kwtss                                                    1/1     Running     0          47m</span><br><span class="line">openshift-sdn                                      sdn-vqzp9                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-service-ca-operator                      service-ca-operator-57cd4bc97f-9hll7                         1/1     Running     2          3d21h</span><br><span class="line">openshift-service-ca                               service-ca-c75f9fb85-xs454                                   1/1     Running     0          3d21h</span><br><span class="line">openshift-service-catalog-removed                  openshift-service-catalog-apiserver-remover-j7nft            0/1     Completed   0          3h28m</span><br><span class="line">openshift-service-catalog-removed                  openshift-service-catalog-controller-manager-remover-tp4ff   0/1     Completed   0          3h28m</span><br></pre></td></tr></table></figure><h2 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h2><p>2021-06-08 推荐 24 小时后再备份，<a href="https://docs.openshift.com/container-platform/4.7/backup_and_restore/backing-up-etcd.html">官方备份文档</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.5/html/installing_on_bare_metal/installing-restricted-networks-bare-metal#installation-about-restricted-networks_installing-restricted-networks-bare-metal">官方文档 [zh]在受限网络中的裸机上安装集群</a></li><li><a href="https://fuckcloudnative.io/posts/openshift4.4-install-offline-static-1-requirement/">4.4.9部署</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/welcome/index.html">学习文档</a></li><li><a href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.5/pdf/architecture/OpenShift_Container_Platform-4.5-Architecture-zh-CN.pdf">pdf</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/architecture/architecture-rhcos.html">rhcos</a></li></ul><p>其他的 单master 参考:</p><ul><li><a href="https://gist.github.com/williamcaban/7d4fa16c91cf597517e5778428e74658">https://gist.github.com/williamcaban/7d4fa16c91cf597517e5778428e74658</a></li><li><a href="https://misa.gitbook.io/k8s-ocp-yaml/openshift-docs/2020-02-25-openshift4.4-install-online-staticip-allinone">https://misa.gitbook.io/k8s-ocp-yaml/openshift-docs/2020-02-25-openshift4.4-install-online-staticip-allinone</a></li><li><a href="https://cgruver.github.io/okd4-single-node-cluster/">https://cgruver.github.io/okd4-single-node-cluster/</a></li><li><a href="https://github.com/cai11745/k8s-ocp-yaml/blob/master/ocp4/2020-02-25-openshift4.4-install-online-staticIP-allinone.md#%E5%AE%89%E8%A3%85%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6">https://github.com/cai11745/k8s-ocp-yaml/blob/master/ocp4/2020-02-25-openshift4.4-install-online-staticIP-allinone.md#%E5%AE%89%E8%A3%85%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文是全部离线安装，也就是 UPI (User Provisioned Infrastructure) 模式安装，假设机器只能配置静态ip不能有网络配置权限和配置 dhcp 和 pxe。机器可以是物理机和虚拟机。&lt;/p&gt;
&lt;h2 id=&quot;前言介绍&quot;&gt;&lt;a href=&quot;#前言</summary>
      
    
    
    
    
    <category term="openshift" scheme="http://zhangguanzhang.github.io/tags/openshift/"/>
    
    <category term="ocp" scheme="http://zhangguanzhang.github.io/tags/ocp/"/>
    
  </entry>
  
  <entry>
    <title>个人办公用 wireguard 组网笔记</title>
    <link href="http://zhangguanzhang.github.io/2020/08/05/wireguard-for-personal/"/>
    <id>http://zhangguanzhang.github.io/2020/08/05/wireguard-for-personal/</id>
    <published>2020-08-05T18:29:08.000Z</published>
    <updated>2020-08-05T18:29:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>作为 IT 人员，经常需要连到办公网工作，并不是每个公司都有 vpn，自己搭建的话 openvpn 之类的配置麻烦啰嗦。这里写下 wireguard 的简单搭建。它比 IPSec 更快，更简单，更精简，更有用。它比 OpenVPN 更高效。WireGuard 设计为通用 VPN，适用于多种不同情况。它是跨平台的，可大规模部署。</p><p>通常如下图的部署: 一台 ECS 主机，得有公网 IP，下图就是 <code>pc ----&gt; ECS &lt;------ 公司的 pc</code></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">            +----------+</span><br><span class="line">            |          |</span><br><span class="line">   +-------&gt;+ ECS      +&lt;-----+</span><br><span class="line">   |        +----------+      |</span><br><span class="line">   |                          |</span><br><span class="line">   |                          |</span><br><span class="line">   |                          |</span><br><span class="line">   |                          |          company</span><br><span class="line">   |                      +---+------------------+</span><br><span class="line">+--++                     |                      |</span><br><span class="line">|PC |                     |            +---+     |</span><br><span class="line">+---+                     |            |PC |     |</span><br><span class="line">                          |            +---+     |</span><br><span class="line">                          |                      |</span><br><span class="line">                          +----------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当然，如果你会折腾的话 pc 可以是软路由，有兴趣和条件的可以看我博客 <a href="https://zhangguanzhang.github.io/2020/05/13/x86-router-flash/">proxmox x86软路由笔记</a>。</p><h2 id="登录到-ECS-上"><a href="#登录到-ECS-上" class="headerlink" title="登录到 ECS 上"></a>登录到 ECS 上</h2><p>得益于 wireguard 中没有 client/server 的概念，只要所有 nat 中的某台机器能够和 gateway 主机建立连接，即可实现共享所有节点的网络资源。这里 ECS 有公网ip，所以担当 gateway</p><h3 id="安装-wireguard"><a href="#安装-wireguard" class="headerlink" title="安装 wireguard"></a>安装 wireguard</h3><p><a href="https://www.wireguard.com/install/">官方安装文档</a> ，或者查看 <a href="https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&mid=2247488853&idx=1&sn=38acb5689db9d9d69ab1ebc78248e0ed&chksm=fbee5598cc99dc8ee81dc6e2a6ed12bb1fd61efd19f152c75e6e41aadb79a15562d7a6c9cb81&mpshare=1&scene=1&srcid=1118udSysN19LYkQxZEVWFTY&sharer_sharetime=1605681632258&sharer_shareid=8eaca72194dae7b3d51d5c708436eee4&key=8236791ccb71351070dff27fe2ad7a9f146455609c8c7a4bc57532e008e6e2a92c27e10b673a090dd88e54740c3391dbc2623a4128ba12f4ebfc9f83dbaf4ec0e6f01195f693765eb5690757359f4eaecfd37a78bb722773f7c6fa6a83cfbe73fa5273902c5aa16b765ece15a9130e8b12a3496d7bf2ae684ac9200cc5f39a31&ascene=1&uin=MzA1MzI4OTMzMQ==&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=AREByymqoZ6jfJZckbtVD7I=&pass_ticket=Jm9uDmvylBr7yM4ArNVQwkHhP3TB921kMFgCmo8A4uq+xezPGCG3aYKbPKyDMclJ&wx_header=0">如何在五分钟内装好 WireGuard？</a> ECS是 linux 系统的话内核要5.x以上，没有就升级下内核，其他个人 pc 电脑则下载客户端，当然软路由的话则去找个带 wireguard的固件。</p><h4 id="centos7"><a href="#centos7" class="headerlink" title="centos7"></a>centos7</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">yum -y install https:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-7.0-4.el7.elrepo.noarch.rpm</span><br><span class="line">rpm --import https:&#x2F;&#x2F;www.elrepo.org&#x2F;RPM-GPG-KEY-elrepo.org</span><br><span class="line">yum --enablerepo&#x3D;elrepo-kernel install -y kernel-lt</span><br><span class="line"></span><br><span class="line">yum -y --enablerepo&#x3D;elrepo-kernel install kernel-lt-&#123;devel,headers,perf&#125;</span><br><span class="line"># 失败就加   --skip-broken </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">awk -F\&#39; &#39;$1&#x3D;&#x3D;&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#39; &#x2F;etc&#x2F;grub2.cfg</span><br><span class="line"></span><br><span class="line">#看数字</span><br><span class="line">grub2-set-default 2</span><br><span class="line">grub2-mkconfig -o &#x2F;etc&#x2F;grub2.cfg</span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line">yum install dkms kmod-wireguard wireguard-tools</span><br><span class="line"></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line">modprobe wireguard</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>开启转发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.ip_forward=1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/wireguard</span><br></pre></td></tr></table></figure><h3 id="生成密钥对"><a href="#生成密钥对" class="headerlink" title="生成密钥对"></a>生成密钥对</h3><p>wg 的每个互相之间要一对密钥，例如 A 连 gateway， A 需要 gateway的公钥，gateway 需要 A 的公钥，不能共用一套密钥对。</p><p>生成 gateway 的密钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wg genkey | tee gw-privatekey | wg pubkey &gt; gw-publickey</span><br></pre></td></tr></table></figure><p>生成个人电脑的密钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wg genkey | tee pc-privatekey | wg pubkey &gt; pc-publickey</span><br></pre></td></tr></table></figure><p>生成公司电脑的密钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wg genkey | tee cm-pc-privatekey | wg pubkey &gt; cm-pc-publickey</span><br></pre></td></tr></table></figure><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>wg 的组网得定义一个网段，这个网段和你所有运行了wg的局域网的ip不能一样，例如我定义的是 <code>10.1.0.1/24</code>，ecs 上配置文件为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; wg0.conf &lt;&lt;EOF</span><br><span class="line">[Interface]</span><br><span class="line">ListenPort = 16000 # 客户端连过来填写的端口，安全组的tcp和udp都要放行</span><br><span class="line">Address = 10.1.0.1/24  #wg之前通信组网的内网ip和段</span><br><span class="line">PrivateKey = $(cat gw-privatekey)   # 使用 shell 读取gateway的私钥到这里</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面两条是放行的iptables和MASQUERADE</span></span><br><span class="line">PostUp   = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span><br><span class="line">PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> pc</span></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat pc-publickey)</span><br><span class="line">AllowedIPs = 10.1.0.2/32</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> company router</span></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat cm-pc-publickey)</span><br><span class="line">AllowedIPs = 10.1.0.3/32, 192.168.2.0/24, 10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后是每个客户端的配置文件，下面是我笔记本 wg 的客户端软件配置文件内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; pc.conf &lt;&lt;EOF</span><br><span class="line">[Interface]</span><br><span class="line">PrivateKey = $(cat pc-privatekey)</span><br><span class="line">Address = 10.1.0.2/24 #wg之前通信组网的内网ip和段，主机位每个得不一样</span><br><span class="line"><span class="meta">#</span><span class="bash"> DNS = 192.168.2.3</span></span><br><span class="line"></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat gw-publickey)   # gateway的公钥</span><br><span class="line">AllowedIPs = 10.1.0.0/24, 192.168.2.0/24, 10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</span><br><span class="line">Endpoint = $(curl -s ip.sb):16000 #gateway 公网ip和端口</span><br><span class="line">PersistentKeepalive = 10 # 心跳时间</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>公司的电脑 wg 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; cm-pc.conf &lt;&lt;EOF</span><br><span class="line">[Interface]</span><br><span class="line">PrivateKey = $(cat cm-pc-privatekey)</span><br><span class="line">Address = 10.1.0.3/24 #wg之前通信组网的内网ip和段，主机位每个得不一样</span><br><span class="line"></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat gw-publickey)   # gateway的公钥</span><br><span class="line">AllowedIPs = 10.1.0.0/24</span><br><span class="line">Endpoint = $(curl -s ip.sb):16000  # gateway 公网ip和端口</span><br><span class="line">PersistentKeepalive = 10 # 心跳时间</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后把 <code>pc.conf</code> 和 <code>cm-pc.conf</code> 的内容拷贝到对应的 wg 客户端软件里。</p><p>讲解下配置文件，我办公室是台式机proxmox里的软路由，并不是上面我说的办公室pc，这样我接入的设备也可以访问。我个人是推荐办公室搞个 proxmox 整虚拟机和软路由。</p><p>办公网的路由器网段是 <code>192.168.2.0/24</code>，<code>192.168.2.3</code>是软路由，主要是上面有dns server（adguard home），办公网内添加 hosts 我是直接在dns server上添加的。所以我个人PC那里写了 <code>DNS = 192.168.2.3</code>，这样 dns 解析都走到办公网的软路由上，家里不需要本地配置 hosts。</p><p><code>10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</code>的网段都是办公网的内网网段。<code>AllowedIPs</code>意思就是把请求目的IP是这些网段的，都发到 wg0 这个接口上，也就是添加路由表。这样我在家里，我个人 pc 打开 wg后，就能访问办公网了。</p><p>ECS上启动 wg 和停止 wg</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wg-quick up wg0 #默认取 /etc/wireguard/$name.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定配置文件启动 wg-quick up /etc/wireguard/wg0.conf</span></span><br><span class="line">wg-quick down wg0</span><br></pre></td></tr></table></figure><p>PostUp 和 PostDown 就是启动后和停止后的命令，是 Linux 的话就推荐写 iptables 放行转发和做 NAT。</p><p>查看组网状态，shell 上 wg 回车即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wg</span></span><br><span class="line">interface: wg0</span><br><span class="line">  public key: FZcFhf0eq2yFgXPNBqYnpoZHnzmgFI7JCLp/5vn1DG0=</span><br><span class="line">  private key: (hidden)</span><br><span class="line">  listening port: 16000</span><br><span class="line"></span><br><span class="line">peer: OtydRPJDt+H8upZDz5zJueRjUQ0tS4tr9P6w4BL2+w0=</span><br><span class="line">  endpoint: xxxxxxxxxxx:53956</span><br><span class="line">  allowed ips: 10.1.0.3/32, 192.168.2.0/24, 10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</span><br><span class="line">  latest handshake: 1 minute, 2 seconds ago</span><br><span class="line">  transfer: 485.48 MiB received, 55.88 MiB sent</span><br><span class="line"></span><br><span class="line">peer: VkhLdmaPS2KmhlSOrPk1XS1MWZrhb+00BdsC0swUBhk=</span><br><span class="line">  endpoint: xxxxxxxxxx:13545</span><br><span class="line">  allowed ips: 10.1.0.2/32</span><br><span class="line">  latest handshake: 21 minutes, 25 seconds ago</span><br><span class="line">  transfer: 56.11 MiB received, 476.83 MiB sent</span><br></pre></td></tr></table></figure><h4 id="一些注意点"><a href="#一些注意点" class="headerlink" title="一些注意点"></a>一些注意点</h4><ul><li>如果是软路由，开了 pxsswxll 代理之类的，记得把 ECS 的 公网IP 设置为不走代理。</li><li>openwrt 运行 wireguard 的话：<code>网络</code> – <code>防火墙</code> – <code>常规设置</code> – <code>常规设置</code> – <code>转发</code> 设置为<code>接受</code></li><li>openwrt 的 wireguard 多次配置可能会有问题，gateway上无法看到它上线，重启下后再试试。也可能需要下面规则<ul><li><code>网络</code> – <code>防火墙</code> – <code>自定义规则</code>：<code>iptables -I FORWARD -i wg0 -j ACCEPT</code> <code>iptables -I FORWARD -o wg0 -j ACCEPT</code> <code>iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</code></li><li>如果 openwrt 下面的设备比如电脑和手机和组网的段不通，但是openwrt ssh上去ping和curl测试可以访问组网的段，可以加下nat规则<code>iptables -t nat -I POSTROUTING -o wg0 -j MASQUERADE</code></li></ul></li></ul><h2 id="udp被-qos-下配合-udp2raw-使用"><a href="#udp被-qos-下配合-udp2raw-使用" class="headerlink" title="udp被 qos 下配合 udp2raw 使用"></a>udp被 qos 下配合 udp2raw 使用</h2><p>实际使用中很大几率遇到 udp 被 qos 了，导致连接经常断开，这里使用 <a href="https://github.com/wangyu-/udp2raw-tunnel">udp2raw</a> 把 udp 报文伪装成 tcp 避免被 qos。这里我使用 docker 部署的，实体进程和相关文档见 <a href="https://github.com/wangyu-/udp2raw-tunnel/blob/master/doc/README.zh-cn.md#%E8%BF%90%E8%A1%8C">udp2raw运行</a></p><p>Linux server端 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 监听86的tcp端口，把86端口收到的伪装成tcp的udp报文转发到 127.0.0.1:16000 上</span></span><br><span class="line">docker run \</span><br><span class="line">    -d --name udp2raw  \</span><br><span class="line">    --restart always \</span><br><span class="line">    --net host \</span><br><span class="line">    --cap-add NET_RAW \</span><br><span class="line">    --cap-add NET_ADMIN  \</span><br><span class="line">    zhangguanzhang/udp2raw \</span><br><span class="line">    -s -l 0.0.0.0:86 \</span><br><span class="line">    -r 127.0.0.1:16000 \</span><br><span class="line">    -k passwd123 \</span><br><span class="line">    --raw-mode faketcp  \</span><br><span class="line">    --cipher-mode xor  -a</span><br></pre></td></tr></table></figure><p>Linux或者软路由系统 client 端 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 监听16000的 udp 端口，把16000端口收到的udp报文伪装成tcp发到 &lt;public_ip&gt;:86 上</span></span><br><span class="line">docker run --net host \</span><br><span class="line">    -d --name udp2raw  \</span><br><span class="line">    --restart always \</span><br><span class="line">    --cap-add NET_RAW \</span><br><span class="line">    --cap-add NET_ADMIN    \</span><br><span class="line">    zhangguanzhang/udp2raw \</span><br><span class="line">    -c -l 0.0.0.0:16000 \</span><br><span class="line">    -r &lt;public_ip&gt;:86 \</span><br><span class="line">    -k passwd123 \</span><br><span class="line">    --raw-mode faketcp   \</span><br><span class="line">    --cipher-mode xor  -a</span><br></pre></td></tr></table></figure><p><a href="https://github.com/wangyu-/udp2raw-multiplatform">windows 客户端下载</a>，运行命令参考:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./udp2raw_mp.exe -c -l 0.0.0.0:16000 -r &lt;public_ip&gt;:86 -k passwd123 --raw-mode faketcp --cipher-mode xor</span><br></pre></td></tr></table></figure><p>所有 client 端的 <code>[peer]</code> 部分里之前连云主机的 ip 都写成<code>127.0.0.1:16000</code>，这样 wg 客户端是先向本地的 udp2raw 客户端发 udp 报文，然后报文被封装成 tcp 发往云主机上的 udp2raw server，再到 wg server 上。</p><p>** 客户端和云主机上 ** 的 wg 的 mtu 设置成 <code>1280</code>(网上有写1200的，但是 windows 的 wg 客户端无法启动，邮件询问作者说最小 <code>1280</code> 才能启动)。例如我路由器配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[Interface]</span><br><span class="line">...</span><br><span class="line">MTU = 1280</span><br><span class="line">[Peer]</span><br><span class="line">...</span><br><span class="line">Endpoint = 127.0.0.1:16000</span><br><span class="line">PersistentKeepalive = 10</span><br></pre></td></tr></table></figure><p>windows的 wg 目前 <code>Endpoint</code>必须写本机的 ip（ipconfig命令查看），不能写<code>127.0.0.1</code>，否则无法连 peer（日志会一直刷<code>Failed to send handshake initiation write udp4 0.0.0.0:xxx-&gt;127.0.0.1:16000: wsasendto: The requested address is not valid in its context</code>），这个 bug 已经反馈给作者了。</p><p>udp2raw 的 client 连上 server 后，双方都会打印下面日志:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># server</span><br><span class="line">changed state to server_ready</span><br><span class="line"># client</span><br><span class="line">changed state from to client_handshake2 to client_ready</span><br></pre></td></tr></table></figure><h3 id="一个注意点"><a href="#一个注意点" class="headerlink" title="一个注意点"></a>一个注意点</h3><p>openwrt 上在接口 添加 wireguard 接口，然后 peer 那里的 ip 写 127.0.0.1(也就是openwrt上的udp2raw的ip)可能不行，换成 openwrt 的 局域网 ip试下</p><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://anyisalin.github.io/2018/11/21/fast-flexible-nat-to-nat-vpn-wireguard/">https://anyisalin.github.io/2018/11/21/fast-flexible-nat-to-nat-vpn-wireguard/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为 IT 人员，经常需要连到办公网工作，并不是每个公司都有 vpn，自己搭建的话 openvpn 之类的配置麻烦啰嗦。这里写下 wireguard 的简单搭建。它比 IPSec 更快，更简单，更精简，更有用。它比 OpenVPN 更高效。WireGuard 设计为通用 V</summary>
      
    
    
    
    <category term="wireguard" scheme="http://zhangguanzhang.github.io/categories/wireguard/"/>
    
    
    <category term="wireguard" scheme="http://zhangguanzhang.github.io/tags/wireguard/"/>
    
  </entry>
  
  <entry>
    <title>prometheus的rate与irate内部是如何计算的</title>
    <link href="http://zhangguanzhang.github.io/2020/07/30/prometheus-rate-and-irate/"/>
    <id>http://zhangguanzhang.github.io/2020/07/30/prometheus-rate-and-irate/</id>
    <published>2020-07-30T10:58:10.000Z</published>
    <updated>2020-08-21T09:58:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>市面上的翻译误导人，压根不是啥<code>平均增长率</code>，看了下源码和实际算下来让大家好理解</p><h3 id="rate"><a href="#rate" class="headerlink" title="rate"></a>rate</h3><p>主要代码是在 <a href="https://github.com/prometheus/prometheus/blob/master/promql/functions.go" target="_blank" rel="noopener">https://github.com/prometheus/prometheus/blob/master/promql/functions.go</a> 的<code>extrapolatedRate</code> 和 <code>funcRate</code>，funcRate为</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">funcRate</span><span class="params">(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper)</span> <span class="title">Vector</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> extrapolatedRate(vals, args, enh, <span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它的前后还有<code>funcDelta</code>和<code>funcIncrease</code>对应promql的<code>delta</code>和<code>increase</code>，这俩函数内部都是调用的<code>extrapolatedRate</code>，主要区别是通过向<code>extrapolatedRate</code>函数传递最后的两个布尔标志位的差异，来在<code>extrapolatedRate</code>内部进行差异化计算，也就是说<code>rate</code>、<code>delta</code>和<code>increase</code>的部分数学计算逻辑是一样的。</p><p><code>funcRate</code>里<code>extrapolatedRate</code>最后俩实参格式为<code>isCounter bool, isRate bool</code>，所以<code>rate</code>只能用在<code>counter</code>的 metrics 类型上进行计算。</p><h3 id="数据点的选取"><a href="#数据点的选取" class="headerlink" title="数据点的选取"></a>数据点的选取</h3><p>先看这段代码</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> (</span><br><span class="line">  counterCorrection <span class="keyword">float64</span></span><br><span class="line">  lastValue         <span class="keyword">float64</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> _, sample := <span class="keyword">range</span> samples.Points &#123;</span><br><span class="line">  <span class="keyword">if</span> isCounter &amp;&amp; sample.V &lt; lastValue &#123;</span><br><span class="line">    counterCorrection += lastValue</span><br><span class="line">  &#125;</span><br><span class="line">  lastValue = sample.V</span><br><span class="line">&#125;</span><br><span class="line">resultValue := lastValue - samples.Points[<span class="number">0</span>].V + counterCorrection</span><br></pre></td></tr></table></figure><p>counterCorrection是字面意思修正数值，counter会reset，例如exporter重启了。例如60秒内有下面6数值，在第四个数字后面发生了重置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2 4 6 8 2 4</span><br></pre></td></tr></table></figure><p>2小于lastValue 8，所以<code>counterCorrection = 8</code></p><p>最后的 <code>resultValue = 4 + 8 - 2</code>，当然，重置的情况很少，这里如果不重置用数据<code>2 4 6 8 10 12</code>算就是最后一个值减去第一个值<code>resultValue = 12 - 2 + 0</code>和重置算得一样</p><h3 id="计算的算式"><a href="#计算的算式" class="headerlink" title="计算的算式"></a>计算的算式</h3><p>是结果除以时间的秒数</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> isRate &#123;</span><br><span class="line">  resultValue = resultValue / ms.Range.Seconds()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="对比下irate"><a href="#对比下irate" class="headerlink" title="对比下irate"></a>对比下irate</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 取最后一个数据点</span></span><br><span class="line">lastSample := samples.Points[<span class="built_in">len</span>(samples.Points)<span class="number">-1</span>]</span><br><span class="line"><span class="comment">// 取倒数第二个数据点</span></span><br><span class="line">previousSample := samples.Points[<span class="built_in">len</span>(samples.Points)<span class="number">-2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> resultValue <span class="keyword">float64</span></span><br><span class="line"><span class="keyword">if</span> isRate &amp;&amp; lastSample.V &lt; previousSample.V &#123;</span><br><span class="line">  <span class="comment">// counter重置则取最后一个值.</span></span><br><span class="line">  resultValue = lastSample.V</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 最后一个点数值 - 倒数第二个数值</span></span><br><span class="line">  resultValue = lastSample.V - previousSample.V</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 最后两个点的时间间隔</span></span><br><span class="line">sampledInterval := lastSample.T - previousSample.T</span><br><span class="line"><span class="keyword">if</span> sampledInterval == <span class="number">0</span> &#123;</span><br><span class="line">  <span class="comment">// Avoid dividing by 0.</span></span><br><span class="line">  <span class="keyword">return</span> out</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> isRate &#123;</span><br><span class="line">    <span class="comment">// 转换成秒，然后结果除以秒数</span></span><br><span class="line">  resultValue /= <span class="keyword">float64</span>(sampledInterval) / <span class="number">1000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>官方文档和市面上的 gitbook 都是把<code>rate</code>翻译成<code>增长率</code>是错误的，应该是<code>平均每秒增长了多少数值</code>。按照实践来算下，同时查询<code>node_time_seconds[1m]</code>和<code>rate(node_time_seconds[1m])</code>。我们手动计算下看看是否和rate的结果一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ node_time_seconds[1m]</span><br><span class="line">node_time_seconds&#123;instance&#x3D;&quot;exporter:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;</span><br><span class="line">1596077182.3093214 @1596077182.307 &#x2F;&#x2F; 第一个点</span><br><span class="line">1596077192.3132203 @1596077192.307</span><br><span class="line">1596077202.311446 @1596077202.307</span><br><span class="line">1596077212.309673 @1596077212.307</span><br><span class="line">1596077222.316771 @1596077222.307</span><br><span class="line">1596077232.3151288 @1596077232.307 &#x2F;&#x2F; 最后一个点</span><br><span class="line">node_time_seconds&#123;instance&#x3D;&quot;10.0.23.29:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;</span><br><span class="line">1596077178.6314309 @1596077178.633  &#x2F;&#x2F; 第一个点</span><br><span class="line">1596077188.6312084 @1596077188.633</span><br><span class="line">1596077198.633293 @1596077198.634</span><br><span class="line">1596077208.6332283 @1596077208.634</span><br><span class="line">1596077218.6320524 @1596077218.633</span><br><span class="line">1596077228.635078 @1596077228.633  &#x2F;&#x2F; 最后一个点</span><br><span class="line"></span><br><span class="line">$ rate(node_time_seconds[1m])</span><br><span class="line">&#123;instance&#x3D;&quot;exporter:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;    1.0001161479949952</span><br><span class="line">&#123;instance&#x3D;&quot;10.0.23.29:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;  1.0000729417800902</span><br></pre></td></tr></table></figure><p>先用<code>10.0.23.29</code>这个 instance 算，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1596077228.635078 - 1596077178.6314309) &#x2F; (1596077228.633 - 1596077178.633)</span><br><span class="line">&#x2F;&#x2F; web上的时间是秒数的，go的time是多了三个单位，所以代码里&#x2F;1000转换成秒这里不需要除以1000</span><br><span class="line">上面式子左边和右边算是下面结果:</span><br><span class="line">50.003647089       &#x2F;  50    &#x3D;  1.00007294178</span><br></pre></td></tr></table></figure><p>谷歌搜的在线计算器算的(比windows的calc精度高一些)，由于是float64，所以精度丢失了一些。结果一样。再算下另一个 instance</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1596077232.3151288 - 1596077182.3093214) &#x2F;</span><br><span class="line">    (1596077232.307 - 1596077182.307)</span><br><span class="line">            50.0058073997     &#x2F;    50 &#x3D; 1.00011614799</span><br></pre></td></tr></table></figure><p><code>increase</code>是最后一个点减去第一个点，不除以秒数。所以在 counter 没发生重置情况下，下面两个是相等的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">increase(node_time_seconds[1m]) &#x2F; 60 &#x3D;&#x3D; rate(node_time_seconds[1m])</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;市面上的翻译误导人，压根不是啥&lt;code&gt;平均增长率&lt;/code&gt;，看了下源码和实际算下来让大家好理解&lt;/p&gt;
&lt;h3 id=&quot;rate&quot;&gt;</summary>
      
    
    
    
    <category term="Prometheus" scheme="http://zhangguanzhang.github.io/categories/Prometheus/"/>
    
    
    <category term="Prometheus" scheme="http://zhangguanzhang.github.io/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>k8s master机器文件系统故障的一次恢复过程</title>
    <link href="http://zhangguanzhang.github.io/2020/07/23/fs-error-fix-k8s-master/"/>
    <id>http://zhangguanzhang.github.io/2020/07/23/fs-error-fix-k8s-master/</id>
    <published>2020-07-23T13:58:10.000Z</published>
    <updated>2020-07-23T13:58:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>研发反馈他们那边一套集群有台master文件系统损坏无法开机，他们是三台openstack上的虚机，是虚拟化宿主机故障导致的虚机文件系统损坏。三台机器是master+node，指导他修复后开机，修复过程和我之前文章<a href="https://zhangguanzhang.github.io/2019/12/05/suse-fix-data-but-device-busy/">opensuse的一次救援</a>步骤一样</p><p>起来后我上去看，因为做了 HA 的，所以只有这个node有问题，集群没影响</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# kubectl get node -o wide</span><br><span class="line">NAME             STATUS     ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">10.252.146.104   NotReady   &lt;none&gt;   30d   v1.16.9   10.252.146.104   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker://19.3.11</span><br><span class="line">10.252.146.105   Ready      &lt;none&gt;   30d   v1.16.9   10.252.146.105   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker://19.3.11</span><br><span class="line">10.252.146.106   Ready      &lt;none&gt;   30d   v1.16.9   10.252.146.106   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker://19.3.11</span><br></pre></td></tr></table></figure><p>启动docker试试</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl start docker</span><br><span class="line">Job for docker.service canceled.</span><br></pre></td></tr></table></figure><p>无法启动，查看下启动失败的服务</p><h3 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl --failed</span><br><span class="line">  UNIT               LOAD   ACTIVE SUB    DESCRIPTION                 </span><br><span class="line">● containerd.service loaded failed failed containerd container runtime</span><br></pre></td></tr></table></figure><p>如果没有<code>containerd.service</code>。可能被 service 纳管了，尝试下<code>service containerd start</code>试试</p><p>查看下<code>containerd</code>的日志</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# journalctl -xe -u containerd</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.481459735+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.service.v1.snapshots-service&quot;...&quot; type=io.containerd.service.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.481472223+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.runtime.v1.linux&quot;...&quot; type=io.containerd.runtime.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.481517630+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.runtime.v2.task&quot;...&quot; type=io.containerd.runtime.v2</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.481562176+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.monitor.v1.cgroups&quot;...&quot; type=io.containerd.monitor.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.481964349+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.service.v1.tasks-service&quot;...&quot; type=io.containerd.service.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.481996158+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.internal.v1.restart&quot;...&quot; type=io.containerd.internal.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482048208+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.containers&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482081110+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.content&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482096598+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.diff&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482112263+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.events&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482123307+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.healthcheck&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482133477+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.images&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482142943+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.leases&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482151644+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482160741+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.internal.v1.opt&quot;...&quot; type=io.containerd.internal.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482184201+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482194643+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482206871+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.version&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482215454+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482365838+08:00&quot; level=info msg=serving... address=&quot;/run/containerd/containerd.sock&quot;</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time=&quot;2020-07-23T11:20:11.482404139+08:00&quot; level=info msg=&quot;containerd successfully booted in 0.003611s&quot;</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: panic: runtime error: invalid memory address or nil pointer dereference</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x5626b983c259]</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: goroutine 55 [running]:</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Bucket).Cursor(...)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/bucket.go:84</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Bucket).Get(0x0, 0x5626bb7e3f10, 0xb, 0xb, 0x0, 0x2, 0x4)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/bucket.go:260 +0x39</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.scanRoots.func6(0x7fe557c63020, 0x2, 0x2, 0x0, 0x0, 0x0, 0x0, 0x5626b95eec72)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/gc.go:222 +0xcb</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*Bucket).ForEach(0xc0003d1780, 0xc00057b640, 0xa, 0xa)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/bucket.go:388 +0x100</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.scanRoots(0x5626bacedde0, 0xc0003d1680, 0xc0002ee2a0, 0xc00031a3c0, 0xc000527a60, 0x7fe586a43fff)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/gc.go:216 +0x4df</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.(*DB).getMarked.func1(0xc0002ee2a0, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/db.go:359 +0x165</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/vendor/go.etcd.io/bbolt.(*DB).View(0xc00000c1e0, 0xc00008b860, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/vendor/go.etcd.io/bbolt/db.go:701 +0x92</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.(*DB).getMarked(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x203000, 0x203000, 0x400)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/db.go:342 +0x7e</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.(*DB).GarbageCollect(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/db.go:257 +0xa3</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/gc/scheduler.(*gcScheduler).run(0xc0000a0b40, 0x5626bacede20, 0xc0000d6010)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:310 +0x511</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: created by github.com/containerd/containerd/gc/scheduler.init.0.func1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:132 +0x462</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Main process exited, code=exited, status=2/INVALIDARGUMENT</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Failed with result &#x27;exit-code&#x27;.</span><br></pre></td></tr></table></figure><p>这个问题从panic抛出的堆栈信息看和我之前文章<a href="https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/">docker启动panic</a>很类似，都是 boltdb 文件出错，找下 git 信息去看看代码路径在哪</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl cat containerd | grep ExecStart</span><br><span class="line">ExecStartPre=-/sbin/modprobe overlay</span><br><span class="line">ExecStart=/usr/bin/containerd</span><br><span class="line"></span><br><span class="line">[root@k8s-m1 ~]# /usr/bin/containerd --version</span><br><span class="line">containerd containerd.io 1.2.13 7ad184331fa3e55e52b890ea95e65ba581ae3429</span><br></pre></td></tr></table></figure><p>按照这个blob去用github的url访问是404，只有去按照tag版本查看了，根据相关代码找到了 boltdb 的文件名是<code>meta.db</code><br><a href="https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L257">https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L257</a><br><a href="https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L79">https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L79</a><br><a href="https://github.com/containerd/containerd/blob/v1.2.13/services/server/server.go#L261-L268">https://github.com/containerd/containerd/blob/v1.2.13/services/server/server.go#L261-L268</a></p><p>查找下<code>ic.Root</code>路径是多少</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# /usr/bin/containerd --help | grep config</span><br><span class="line">     config    information on the containerd config</span><br><span class="line">   --config value, -c value     path to the configuration file (default: &quot;/etc/containerd/config.toml&quot;)</span><br><span class="line"></span><br><span class="line">[root@k8s-m1 ~]# grep root /etc/containerd/config.toml</span><br><span class="line">#root = &quot;/var/lib/containerd&quot;</span><br><span class="line">[root@k8s-m1 ~]]# find /var/lib/containerd -type f -name meta.db</span><br><span class="line">/var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db</span><br></pre></td></tr></table></figure><p>找到boltdb文件，改名启动</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]]# mv /var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db&#123;,.bak&#125;</span><br><span class="line">[root@k8s-m1 ~]# systemctl status containerd.service</span><br><span class="line">● containerd.service - containerd container runtime</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/containerd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: failed (Result: exit-code) since Thu 2020-07-23 11:20:11 CST; 17min ago</span><br><span class="line">     Docs: https://containerd.io</span><br><span class="line">  Process: 9186 ExecStart=/usr/bin/containerd (code=exited, status=2)</span><br><span class="line">  Process: 9182 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 9186 (code=exited, status=2)</span><br><span class="line"></span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.(*DB).getMarked(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x203000, 0x203000, 0x400)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/db.go:342 +0x7e</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/metadata.(*DB).GarbageCollect(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/metadata/db.go:257 +0xa3</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com/containerd/containerd/gc/scheduler.(*gcScheduler).run(0xc0000a0b40, 0x5626bacede20, 0xc0000d6010)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:310 +0x511</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: created by github.com/containerd/containerd/gc/scheduler.init.0.func1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         /go/src/github.com/containerd/containerd/gc/scheduler/scheduler.go:132 +0x462</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Main process exited, code=exited, status=2/INVALIDARGUMENT</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Failed with result &#x27;exit-code&#x27;.</span><br><span class="line">[root@k8s-m1 ~]# systemctl restart containerd.service</span><br><span class="line">[root@k8s-m1 ~]# systemctl status containerd.service</span><br><span class="line">● containerd.service - containerd container runtime</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/containerd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu 2020-07-23 11:25:37 CST; 1s ago</span><br><span class="line">     Docs: https://containerd.io</span><br><span class="line">  Process: 15661 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 15663 (containerd)</span><br><span class="line">    Tasks: 16</span><br><span class="line">   Memory: 28.6M</span><br><span class="line">   CGroup: /system.slice/containerd.service</span><br><span class="line">           └─15663 /usr/bin/containerd</span><br><span class="line"></span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496725460+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.images&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496734129+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.leases&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496742793+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496751740+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.internal.v1.opt&quot;...&quot; type=io.containerd.internal.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496775185+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496785498+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496794873+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.version&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496803178+08:00&quot; level=info msg=&quot;loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...&quot; type=io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496944458+08:00&quot; level=info msg=serving... address=&quot;/run/containerd/containerd.sock&quot;</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time=&quot;2020-07-23T11:25:37.496958031+08:00&quot; level=info msg=&quot;containerd successfully booted in 0.003994s&quot;</span><br></pre></td></tr></table></figure><h3 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h3><p>containerd 起来后，启动下 docker</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/docker.service.d</span><br><span class="line">           └─10-docker.conf</span><br><span class="line">   Active: inactive (dead) since Thu 2020-07-23 11:20:13 CST; 18min ago</span><br><span class="line">     Docs: https://docs.docker.com</span><br><span class="line">  Process: 9398 ExecStopPost=/bin/bash -c /sbin/iptables -D FORWARD -s 0.0.0.0/0 -j ACCEPT &amp;&gt; /dev/null || : (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 9187 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 9187 (code=exited, status=0/SUCCESS)</span><br><span class="line"></span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.956503485+08:00&quot; level=error msg=&quot;Stop container error: Failed to stop container 68860c8d16b9ce7e74e8efd9db00e70a57eef1b752c2e6c703073c0bce5517d3 with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.954347116+08:00&quot; level=error msg=&quot;Stop container error: Failed to stop container 5ec9922beed1276989f1866c3fd911f37cc26aae4e4b27c7ce78183a9a4725cc with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.953615411+08:00&quot; level=info msg=&quot;Container failed to stop after sending signal 15 to the process, force killing&quot;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.956557179+08:00&quot; level=error msg=&quot;Stop container error: Failed to stop container 6d0096fbcd4055f8bafb6b38f502a0186cd1dfca34219e9dd6050f512971aef5 with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.954601191+08:00&quot; level=info msg=&quot;Container failed to stop after sending signal 15 to the process, force killing&quot;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.956600790+08:00&quot; level=error msg=&quot;Stop container error: Failed to stop container 6d1175ba6c55cb05ad89f4134ba8e9d3495c5acb5f07938dc16339b7cca013bf with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.957188989+08:00&quot; level=info msg=&quot;Daemon shutdown complete&quot;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.957212655+08:00&quot; level=info msg=&quot;stopping event stream following graceful shutdown&quot; error=&quot;context canceled&quot; module=libcontainerd namespace=plugins.moby</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time=&quot;2020-07-23T11:20:13.957209679+08:00&quot; level=info msg=&quot;stopping event stream following graceful shutdown&quot; error=&quot;context canceled&quot; module=libcontainerd namespace=moby</span><br><span class="line">Jul 23 11:20:13 k8s-m1 systemd[1]: Stopped Docker Application Container Engine.</span><br><span class="line">[root@k8s-m1 ~]# systemctl start docker</span><br><span class="line">[root@k8s-m1 ~]# systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/docker.service.d</span><br><span class="line">           └─10-docker.conf</span><br><span class="line">   Active: active (running) since Thu 2020-07-23 11:26:11 CST; 1s ago</span><br><span class="line">     Docs: https://docs.docker.com</span><br><span class="line">  Process: 9398 ExecStopPost=/bin/bash -c /sbin/iptables -D FORWARD -s 0.0.0.0/0 -j ACCEPT &amp;&gt; /dev/null || : (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 16156 ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 15974 (dockerd)</span><br><span class="line">    Tasks: 62</span><br><span class="line">   Memory: 89.1M</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           └─15974 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock</span><br><span class="line"></span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:10.851106564+08:00&quot; level=error msg=&quot;cb4e16249cd8eac48ed734c71237195f04d63c56c55c0199b3cdf3d49461903d cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:10.860456898+08:00&quot; level=error msg=&quot;d9bbcab186ccb59f96c95fc886ec1b66a52aa96e45b117cf7d12e3ff9b95db9f cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:10.872405757+08:00&quot; level=error msg=&quot;07eb7a09bc8589abcb4d79af4b46798327bfb00624a7b9ceea457de392ad8f3d cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:10.877896618+08:00&quot; level=error msg=&quot;f5867657025bd7c3951cbd3e08ad97338cf69df2a97967a419e0e78eda869b73 cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:11.143661583+08:00&quot; level=info msg=&quot;Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:11.198200760+08:00&quot; level=info msg=&quot;Loading containers: done.&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:11.219959208+08:00&quot; level=info msg=&quot;Docker daemon&quot; commit=42e35e61f3 graphdriver(s)=overlay2 version=19.03.11</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:11.220049865+08:00&quot; level=info msg=&quot;Daemon has completed initialization&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time=&quot;2020-07-23T11:26:11.232373131+08:00&quot; level=info msg=&quot;API listen on /var/run/docker.sock&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 systemd[1]: Started Docker Application Container Engine.</span><br></pre></td></tr></table></figure><h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><p>etcd启动也失败，journal 查看下 etcd 状态</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# journalctl -xe -u etcd</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Loading server configuration from &quot;/etc/etcd/etcd.config.yml&quot;</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: etcd Version: 3.3.20</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Git SHA: 9fd7e2b80</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Go Version: go1.12.17</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Go OS/Arch: linux/amd64</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: setting maximum number of CPUs to 16, total number of available CPUs is 16</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: found invalid file/dir wal under data dir /var/lib/etcd (Ignore this if you are upgrading etcd)</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: ignoring peer auto TLS since certs given</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: peerTLS: cert = /etc/kubernetes/pki/etcd/peer.crt, key = /etc/kubernetes/pki/etcd/peer.key, ca = /etc/kubernetes/pki/etcd/ca.crt, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = fals&gt;</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for peers on https://10.252.146.104:2380</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: ignoring client auto TLS since certs given</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: pprof is enabled under /debug/pprof</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: The scheme of client url http://127.0.0.1:2379 is HTTP while peer key/cert files are presented. Ignored key/cert files.</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: The scheme of client url http://127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for client requests on 127.0.0.1:2379</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for client requests on 10.252.146.104:2379</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: skipped unexpected non snapshot file 000000000000002e-000000000052f2be.snap.broken</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: recovered store from snapshot at index 5426092</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: restore compact to 3967425</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: cannot unmarshal event: proto: KeyValue: illegal tag 0 (wire type 0)</span><br><span class="line">Jul 23 11:26:15 k8s-m1 systemd[1]: etcd.service: Main process exited, code=exited, status=1/FAILURE</span><br><span class="line">Jul 23 11:26:15 k8s-m1 systemd[1]: etcd.service: Failed with result &#x27;exit-code&#x27;.</span><br><span class="line">Jul 23 11:26:15 k8s-m1 systemd[1]: Failed to start Etcd Service.</span><br><span class="line">[root@k8s-m1 ~]# ll /var/lib/etcd/member/snap/</span><br><span class="line">total 8560</span><br><span class="line">-rw-r--r-- 1 root root   13499 Jul 20 13:36 000000000000002e-000000000052cbac.snap</span><br><span class="line">-rw-r--r-- 2 root root  128360 Jul 20 13:01 000000000000002e-000000000052f2be.snap.broken</span><br><span class="line">-rw------- 1 root root 8617984 Jul 23 11:26 db</span><br></pre></td></tr></table></figure><p>这套集群是使用我的<a href="https://github.com/zhangguanzhang/Kubernetes-ansible">ansible部署的，求star</a>，自带了<a href="https://github.com/zhangguanzhang/Kubernetes-ansible-base/tree/roles/etcd/templates">备份脚本</a>，但是是三天前坏的</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# ll /opt/etcd_bak/</span><br><span class="line">total 41524</span><br><span class="line">-rw-r--r-- 1 root root 8618016 Jul 17 02:00 etcd-2020-07-17-02:00:01.db</span><br><span class="line">-rw-r--r-- 1 root root 8618016 Jul 18 02:00 etcd-2020-07-18-02:00:01.db</span><br><span class="line">-rw-r--r-- 1 root root 8323104 Jul 19 02:00 etcd-2020-07-19-02:00:01.db</span><br><span class="line">-rw-r--r-- 1 root root 8618016 Jul 20 02:00 etcd-2020-07-20-02:00:01.db</span><br></pre></td></tr></table></figure><h4 id="etcd备份文件恢复"><a href="#etcd备份文件恢复" class="headerlink" title="etcd备份文件恢复"></a>etcd备份文件恢复</h4><p>有恢复剧本，但是前提是etcd的v2和v3不能共存，否则无法恢复备份，我们线上都是把v2的存储关闭了的。主要是<a href="https://github.com/zhangguanzhang/Kubernetes-ansible-base/blob/roles/restoreETCD/tasks/main.yml">这个tasks里的26到42行步骤</a>，这里复制了其他机器master上的 07/23 号的etcd备份文件，然后改了下host跑了下</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# cp -a /var/lib/etcd&#123;,.bak&#125; #先备份下etcd目录</span><br><span class="line">[root@k8s-m1 ~]# cd Kubernetes-ansible</span><br><span class="line">[root@k8s-m1 Kubernetes-ansible]# ansible-playbook restoreETCD.yml -e &#x27;db=/opt/etcd_bak/etcd-bak.db&#x27;</span><br><span class="line"></span><br><span class="line">PLAY [10.252.146.104] **********************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [Gathering Facts] *********************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : fail] ******************************************************************************************************************************************************************************************************************</span><br><span class="line">skipping: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 检测备份文件存在否] *************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : fail] ******************************************************************************************************************************************************************************************************************</span><br><span class="line">skipping: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : set_fact] **************************************************************************************************************************************************************************************************************</span><br><span class="line">skipping: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : set_fact] **************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 停止etcd] ****************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 删除etcd数据目录] ************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104] =&gt; (item=/var/lib/etcd)</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 分发备份文件] ****************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 恢复备份] ******************************************************************************************************************************************************************************************************************</span><br><span class="line">changed: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 启动etcd] ****************************************************************************************************************************************************************************************************************</span><br><span class="line">fatal: [10.252.146.104]: FAILED! =&gt; &#123;&quot;changed&quot;: false, &quot;msg&quot;: &quot;Unable to start service etcd: Job for etcd.service failed because the control process exited with error code.\nSee \&quot;systemctl status etcd.service\&quot; and \&quot;journalctl -xe\&quot; for details.\n&quot;&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP *********************************************************************************************************************************************************************************************************************************</span><br><span class="line">10.252.146.104             : ok=7    changed=1    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0</span><br></pre></td></tr></table></figure><p>查看下日志</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# journalctl -xe -u etcd</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Loading server configuration from &quot;/etc/etcd/etcd.config.yml&quot;</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: etcd Version: 3.3.20</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Git SHA: 9fd7e2b80</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Go Version: go1.12.17</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Go OS/Arch: linux/amd64</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: setting maximum number of CPUs to 16, total number of available CPUs is 16</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: ignoring peer auto TLS since certs given</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: peerTLS: cert = /etc/kubernetes/pki/etcd/peer.crt, key = /etc/kubernetes/pki/etcd/peer.key, ca = /etc/kubernetes/pki/etcd/ca.crt, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = fals&gt;</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for peers on https://10.252.146.104:2380</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: ignoring client auto TLS since certs given</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: pprof is enabled under /debug/pprof</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: The scheme of client url http://127.0.0.1:2379 is HTTP while peer key/cert files are presented. Ignored key/cert files.</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: The scheme of client url http://127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for client requests on 127.0.0.1:2379</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for client requests on 10.252.146.104:2379</span><br><span class="line">Jul 23 11:27:47 k8s-m1 etcd[58954]: member ac2dcf6aed12e8f1 has already been bootstrapped</span><br><span class="line">Jul 23 11:27:47 k8s-m1 systemd[1]: etcd.service: Main process exited, code=exited, status=1/FAILURE</span><br><span class="line">Jul 23 11:27:47 k8s-m1 systemd[1]: etcd.service: Failed with result &#x27;exit-code&#x27;.</span><br><span class="line">Jul 23 11:27:47 k8s-m1 systemd[1]: Failed to start Etcd Service.</span><br></pre></td></tr></table></figure><p>这个<code>member xxxx has already been bootstrapped</code>解决办法就是把配置文件的下面修改，后面启动完记得改回来</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initial-cluster-state: &#x27;new&#x27; 改成 initial-cluster-state: &#x27;existing&#x27;</span><br></pre></td></tr></table></figure><p>然后成功启动</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# systemctl start etcd</span><br><span class="line">[root@k8s-m1 Kubernetes-ansible]# journalctl -xe -u etcd</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Loading server configuration from &quot;/etc/etcd/etcd.config.yml&quot;</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: etcd Version: 3.3.20</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Git SHA: 9fd7e2b80</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Go Version: go1.12.17</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Go OS/Arch: linux/amd64</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: setting maximum number of CPUs to 16, total number of available CPUs is 16</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: found invalid file/dir wal under data dir /var/lib/etcd (Ignore this if you are upgrading etcd)</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: ignoring peer auto TLS since certs given</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: peerTLS: cert = /etc/kubernetes/pki/etcd/peer.crt, key = /etc/kubernetes/pki/etcd/peer.key, ca = /etc/kubernetes/pki/etcd/ca.crt, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = fals&gt;</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for peers on https://10.252.146.104:2380</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: ignoring client auto TLS since certs given</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: pprof is enabled under /debug/pprof</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: The scheme of client url http://127.0.0.1:2379 is HTTP while peer key/cert files are presented. Ignored key/cert files.</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: The scheme of client url http://127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for client requests on 127.0.0.1:2379</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for client requests on 10.252.146.104:2379</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: recovered store from snapshot at index 5952463</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: restore compact to 4369703</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: name = etcd-001</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: data dir = /var/lib/etcd</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: member dir = /var/lib/etcd/member</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: dedicated WAL dir = /var/lib/etcd/wal</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: heartbeat = 100ms</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: election = 1000ms</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: snapshot count = 5000</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: advertise client URLs = https://10.252.146.104:2379</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: restarting member ac2dcf6aed12e8f1 in cluster 367e2aebc6430cbe at commit index 5952491</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: ac2dcf6aed12e8f1 became follower at term 47</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: newRaft ac2dcf6aed12e8f1 [peers: [1e713be314744d53,8b1621b475555fd9,ac2dcf6aed12e8f1], term: 47, commit: 5952491, applied: 5952463, lastindex: 5952491, lastterm: 47]</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: enabled capabilities for version 3.3</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: added member 1e713be314744d53 [https://10.252.146.105:2380] to cluster 367e2aebc6430cbe from store</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: added member 8b1621b475555fd9 [https://10.252.146.106:2380] to cluster 367e2aebc6430cbe from store</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: added member ac2dcf6aed12e8f1 [https://10.252.146.104:2380] to cluster 367e2aebc6430cbe from store</span><br></pre></td></tr></table></figure><p>查看集群状态</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# etcd-ha</span><br><span class="line">+-----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|          ENDPOINT           |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |</span><br><span class="line">+-----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">| https://10.252.146.104:2379 | ac2dcf6aed12e8f1 |  3.3.20 |  8.3 MB |     false |        47 |    5953557 |</span><br><span class="line">| https://10.252.146.105:2379 | 1e713be314744d53 |  3.3.20 |  8.6 MB |     false |        47 |    5953557 |</span><br><span class="line">| https://10.252.146.106:2379 | 8b1621b475555fd9 |  3.3.20 |  8.3 MB |      true |        47 |    5953557 |</span><br><span class="line">+-----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br></pre></td></tr></table></figure><p>然后给kube-apiserver三个组件和kubelet起来后</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# kubectl get node -o wide</span><br><span class="line">NAME             STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">10.252.146.104   Ready    &lt;none&gt;   30d   v1.16.9   10.252.146.104   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker://19.3.11</span><br><span class="line">10.252.146.105   Ready    &lt;none&gt;   30d   v1.16.9   10.252.146.105   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker://19.3.11</span><br><span class="line">10.252.146.106   Ready    &lt;none&gt;   30d   v1.16.9   10.252.146.106   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker://19.3.11</span><br></pre></td></tr></table></figure><p>pod也在慢慢自愈了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;研发反馈他们那边一套集群有台master文件系统损坏无法开机，他们是三台openstack上的虚机，是虚拟化宿主机故障导致的虚机文件系统损坏</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
    <category term="etcd" scheme="http://zhangguanzhang.github.io/tags/etcd/"/>
    
  </entry>
  
</feed>
