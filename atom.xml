<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhangguanzhang</title>
  
  <subtitle>站在巨人的肩膀上</subtitle>
  <link href="http://zhangguanzhang.github.io/atom.xml" rel="self"/>
  
  <link href="http://zhangguanzhang.github.io/"/>
  <updated>2021-08-25T15:08:06.000Z</updated>
  <id>http://zhangguanzhang.github.io/</id>
  
  <author>
    <name>Zhangguanzhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>flannel下集群有个节点网络不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2021/08/25/flannel-a-host-net-tmout/"/>
    <id>http://zhangguanzhang.github.io/2021/08/25/flannel-a-host-net-tmout/</id>
    <published>2021-08-25T15:08:06.000Z</published>
    <updated>2021-08-25T15:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障"><a href="#故障" class="headerlink" title="故障"></a>故障</h2><p>问题和版本没关系，客户的 node 信息啥的后面排错里有。有个节点通信有问题，其余节点都没问题。</p><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><h3 id="惯例信息"><a href="#惯例信息" class="headerlink" title="惯例信息"></a>惯例信息</h3><p>先看下 <code>flannel</code> 的 <code>vxlan</code> 的 <code>vtep</code> 信息，客户是双网卡的，但是默认路由是这个网卡，不用管另外的网卡了。下面信息看了下 <code>VtepMAC</code> 和 <code>public-ip</code> 都正常。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node -o yaml | grep -B4 public</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;76:21:69:41:de:fe&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.51</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;b6:61:5c:8d:d9:eb&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.52</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;1e:8c:3e:12:fc:0f&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.53</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;ba:fe:64:36:6e:a1&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.54</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;8e:c1:4d:18:e5:d6&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.55</span><br><span class="line">--</span><br><span class="line">    annotations:</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;fe:95:e6:bf:a0:62&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 10.25.1.56</span><br></pre></td></tr></table></figure><p>coredns 的 pod ip 和 node 分布情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get pod -o wide</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5757945748-cr67w              1/1     Running   0          19h   172.27.2.7   10.25.1.56   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-krwfd              1/1     Running   0          19h   172.27.1.4   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-zf4zm              1/1     Running   0          19h   172.27.3.7   10.25.1.54   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="排查-1"><a href="#排查-1" class="headerlink" title="排查"></a>排查</h3><p>curl 下 coredns 的 metrics 接口试试，只有 <code>10.25.1.51</code> 和其他节点无法通信。会导致下面的 curl 卡住。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl  172.27.1.4:9153</span><br></pre></td></tr></table></figure><p>目标机器 <code>10.25.1.55</code> 上通过 flannel.1 接口抓我们的 curl 包:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i flannel.1 host 172.27.1.4 and port 9153 -vv</span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:07:46.203094 IP (tos 0x0, ttl 64, id 56025, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57888 &gt; 172.27.1.4.9153: Flags [S], cksum 0x6804 (correct), seq 879302783, win 28200, options [mss 1410,sackOK,TS val 56279718 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:07:46.203173 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.1.4.9153 &gt; 172.27.0.0.57888: Flags [S.], cksum 0x5969 (incorrect -&gt; 0x163b), seq 4197245653, ack 879302784, win 27960, options [mss 1410,sackOK,TS val 431774697 ecr 56279718,nop,wscale 7], length 0</span><br><span class="line">10:07:47.204797 IP (tos 0x0, ttl 64, id 56026, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57888 &gt; 172.27.1.4.9153: Flags [S], cksum 0x641a (correct), seq 879302783, win 28200, options [mss 1410,sackOK,TS val 56280720 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:07:47.204880 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.1.4.9153 &gt; 172.27.0.0.57888: Flags [S.], cksum 0x5969 (incorrect -&gt; 0x1251), seq 4197245653, ack 879302784, win 27960, options [mss 1410,sackOK,TS val 431775699 ecr 56279718,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>看着是回复了报文 <code>172.27.1.4.9153 &gt; 172.27.0.0.57888</code>，在我们 curl 的机器 <code>10.25.1.51</code>上 <code>lsof -nPi :57888</code> 看到的确实是卡住的 curl 命令 pid 。<code>10.25.1.51</code> 上也同时抓包看下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i flannel.1 host 172.27.1.4 and port 9153 -vv</span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:08:57.241129 IP (tos 0x0, ttl 64, id 34444, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57966 &gt; 172.27.1.4.9153: Flags [S], cksum 0x5969 (incorrect -&gt; 0x2fb2), seq 276913734, win 28200, options [mss 1410,sackOK,TS val 56350922 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:08:58.242423 IP (tos 0x0, ttl 64, id 34445, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57966 &gt; 172.27.1.4.9153: Flags [S], cksum 0x5969 (incorrect -&gt; 0x2bc8), seq 276913734, win 28200, options [mss 1410,sackOK,TS val 56351924 ecr 0,nop,wscale 7], length 0</span><br><span class="line">10:09:00.246423 IP (tos 0x0, ttl 64, id 34446, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.27.0.0.57966 &gt; 172.27.1.4.9153: Flags [S], cksum 0x5969 (incorrect -&gt; 0x23f4), seq 276913734, win 28200, options [mss 1410,sackOK,TS val 56353928 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>没收到包，从 <code>eth1</code> 抓下 <code>flannel</code> 的 <code>8475</code> 端口(配置里我们改了 flannel 的端口)试试:</p><p>目标机器 <code>10.25.1.55</code> 上抓包</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth1 host 10.25.1.51 and port 8475 -vvv</span><br><span class="line">tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:09:40.966705 IP (tos 0x0, ttl 64, id 50110, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.42770 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:40.966869 IP (tos 0x0, ttl 64, id 46192, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:41.968322 IP (tos 0x0, ttl 64, id 50327, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.42770 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:41.968440 IP (tos 0x0, ttl 64, id 46957, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:43.099646 IP (tos 0x0, ttl 64, id 47316, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:43.972322 IP (tos 0x0, ttl 64, id 51119, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.42770 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:09:43.972454 IP (tos 0x0, ttl 64, id 47934, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.55.48472 &gt; 10.25.1.51.8475: [no cksum] UDP, length 82</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>目标机器 <code>10.25.1.55</code> 上抓包:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -nn -i eth1 host 10.25.1.55 and port 8475 -vvv</span><br><span class="line">tcpdump: listening on eth1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">10:10:21.702308 IP (tos 0x0, ttl 64, id 6079, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.59558 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:10:22.702441 IP (tos 0x0, ttl 64, id 6117, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.59558 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br><span class="line">10:10:24.706444 IP (tos 0x0, ttl 64, id 7699, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    10.25.1.51.59558 &gt; 10.25.1.55.8475: [no cksum] UDP, length 82</span><br></pre></td></tr></table></figure><p>完全没报文过来，看了下 <code>flannel</code> 的接口流量压根就没收到任何包:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ifconfig flannel.1</span><br><span class="line">flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1450</span><br><span class="line">        inet 172.27.0.0  netmask 255.255.255.255  broadcast 0.0.0.0</span><br><span class="line">        inet6 fe80::7421:69ff:fe41:defe  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 76:21:69:41:de:fe  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 28900  bytes 2113052 (2.0 MiB)</span><br><span class="line">        TX errors 0  dropped 8 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><p>说明报文从 <code>10.25.1.55</code> 发出后没到 51 上，让客户开通 <code>udp 8475 10.25.1.0/24</code> 整个段的东西向安全组后就正常了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl  172.27.1.4:9153</span><br><span class="line">^C</span><br><span class="line">$ curl  172.27.1.4:9153</span><br><span class="line">404 page not found</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;故障&quot;&gt;&lt;a href=&quot;#故障&quot; class=&quot;headerlink&quot; title=&quot;故障&quot;&gt;&lt;/a&gt;故障&lt;/h2&gt;&lt;p&gt;问题和版本没关系，客户的 node 信息啥的后面排错里有。有个节点通信有问题，其余节点都没问题。&lt;/p&gt;
&lt;h2 id=&quot;排查&quot;&gt;&lt;a hr</summary>
      
    
    
    
    
    <category term="flannel" scheme="http://zhangguanzhang.github.io/tags/flannel/"/>
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>一次 cni-plugins 导致集群 dns 无法解析的排错</title>
    <link href="http://zhangguanzhang.github.io/2021/08/24/cni-plugins-bridge-err/"/>
    <id>http://zhangguanzhang.github.io/2021/08/24/cni-plugins-bridge-err/</id>
    <published>2021-08-24T13:08:06.000Z</published>
    <updated>2021-08-24T13:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>环境是 1.15.5 的 x86_64 的 k8s 。命令输出被我查看日志给冲掉了，大致描述下。<br>中间件 kafka 无法连上 zookeeper ，看了下日志报错域名无法解析。看了下 coredns 都挂了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span><br><span class="line">NAME                       READY   STATUS             RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5757945748-l2d2g   0/1     CrashLoopBackOff   254        3d11h   172.27.0.2   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-w5pfx   0/1     CrashLoopBackOff   254        3d11h   172.27.0.5   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-wfndd   0/1     CrashLoopBackOff   254        3d11h   172.27.0.3   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>查看了日志是报错无法连 kubernetes svc <code>https://172.26.0.1:443/xxxx</code> ，报错 <code>No route to host</code></p><h2 id="排错"><a href="#排错" class="headerlink" title="排错"></a>排错</h2><h3 id="基本排查"><a href="#基本排查" class="headerlink" title="基本排查"></a>基本排查</h3><p>去节点 <code>10.25.1.55</code> 上 <code>docker ps -a | grep coredns</code> 找 pause 的容器 id ，<code>docker inspect xxxxx | grep -m1 -i pid</code> 取进程 pid。然后 nsenter 进去</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ nsenter --net --target 14659 </span><br><span class="line">$ ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 42:7d:b0:83:a9:aa brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.27.0.5/16 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>curl 了下 svc ip <code>https://172.26.0.1</code> 发现报错 <code>No route to host</code>。然后直接用 ep 也就是 kube-apiserver 的真实 ip和端口访问下，发现不通</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curk -kvL https://10.25.1.51:6443</span><br></pre></td></tr></table></figure><p>然后 ping 下宿主机发现也不通。看了下转发都开了。然后也看了下也没安全软件。</p><h3 id="桥接"><a href="#桥接" class="headerlink" title="桥接"></a>桥接</h3><p>然后看了下桥接发现了问题所在，先写下正常环境桥接信息。我们 flannel，不是二进制，容器都是在 cni-plugins 下桥接在 cni0 上的。下面找个正常环境的 coredns 做下信息展示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 取容器 pid</span><br><span class="line">$ docker inspect d30 | grep -m1 -i pid</span><br><span class="line">            &quot;Pid&quot;: 9079,</span><br><span class="line"></span><br><span class="line">$ nsenter --net --target 9079 ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if210: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 06:3e:42:00:91:33 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.27.0.74/24 brd 172.27.0.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>注意看 <code>if</code> 后面的数字，宿主机上查看下是哪个 <code>veth</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ip link | grep -E &#x27;^210&#x27;</span><br><span class="line">210: vetha1ca1d55@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default</span><br></pre></td></tr></table></figure><p>使用 brctl 看下 cni0 下是有这个的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brctl show cni0 | grep vetha1ca1d55</span><br><span class="line">vetha1ca1d55</span><br></pre></td></tr></table></figure><h4 id="机器异常的桥接信息"><a href="#机器异常的桥接信息" class="headerlink" title="机器异常的桥接信息"></a>机器异常的桥接信息</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ nsenter --net --target 14659 ip a s </span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 42:7d:b0:83:a9:aa brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.27.0.5/16 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">$ ip -o link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT qlen 1\    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc pfifo_fast state UP mode DEFAULT qlen 1000\    link/ether fa:16:3e:35:09:13 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000\    link/ether fa:16:3e:e2:4d:8d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT \    link/ether 02:42:3c:a2:d7:3b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">6: vethda399ca1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether a2:95:74:65:29:6d brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">7: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT \    link/ether 0a:e5:a5:9a:66:8b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: veth96d8f326@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether 92:a5:9c:7c:dd:cb brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line">9: vethdf8eb371@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether 4a:fb:37:c3:f2:7e brd ff:ff:ff:ff:ff:ff link-netnsid 2</span><br><span class="line">10: veth44ce32f4@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT \    link/ether 6e:96:73:2b:4e:52 brd ff:ff:ff:ff:ff:ff link-netnsid 3</span><br><span class="line">11: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT qlen 1000\    link/ether 2a:f9:74:b7:11:b8 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">12: veth0f91b3a3@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT \    link/ether 0a:36:90:dc:0a:97 brd ff:ff:ff:ff:ff:ff link-netnsid 4</span><br></pre></td></tr></table></figure><p>查看了下 cni0 压根对不上 <code>veth44ce32f4</code>:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ip a s cni0</span><br><span class="line">11: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP qlen 1000</span><br><span class="line">    link/ether 2a:f9:74:b7:11:b8 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.27.1.1/24 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::28f9:74ff:feb7:11b8/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">$ brctl show</span><br><span class="line">bridge name     bridge id               STP enabled     interfaces</span><br><span class="line">cni0            8000.2af974b711b8       no              veth0f91b3a3</span><br><span class="line">docker0         8000.02423ca2d73b       no</span><br></pre></td></tr></table></figure><p>应该是桥接错了，重新创建下发现好了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system delete pod -l k8s-app=kube-dns</span><br><span class="line">pod &quot;coredns-5757945748-l2d2g&quot; deleted</span><br><span class="line">pod &quot;coredns-5757945748-wfndd&quot; deleted</span><br><span class="line">pod &quot;coredns-5757945748-wjrdf&quot; deleted</span><br><span class="line">$ kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5757945748-4smll   1/1     Running   0          29s   172.27.1.3   10.25.1.55   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-d9wqk   1/1     Running   0          29s   172.27.3.6   10.25.1.54   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-5757945748-dtvfl   1/1     Running   0          29s   172.27.2.6   10.25.1.56   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现场拿了下 cni-plugins 的校验值去查了下，我们使用的是 <code>v0.7.0</code> 版本。有必要升级下了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h2&gt;&lt;p&gt;环境是 1.15.5 的 x86_64 的 k8s 。命令输出被我查看日志给冲掉了，大致描述下。&lt;br&gt;中间件 kafka 无法连上 zoo</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/tags/kubernetes/"/>
    
    <category term="cni-plugins" scheme="http://zhangguanzhang.github.io/tags/cni-plugins/"/>
    
  </entry>
  
  <entry>
    <title>kubelet 为系统配置预留资源</title>
    <link href="http://zhangguanzhang.github.io/2021/08/16/reserve-compute-resources/"/>
    <id>http://zhangguanzhang.github.io/2021/08/16/reserve-compute-resources/</id>
    <published>2021-08-16T10:08:06.000Z</published>
    <updated>2021-08-16T10:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>我们环境有部分 pod 特殊，单独节点部署，oom 的时候会搞挂一些系统进程，这几天折腾了下配置了下 kubelet 相关的 <code>reserved</code>。主要是 kubelet 的配置文件一些参数，不写 systemd 里，全部写配置文件里。版本是如下，因为我们不单单是 <code>x86_64</code> ，由于还有其他的架构以及会部署在客户的现场，为了减少维护，所以我们都是除了 <code>flanneld</code> 和 <code>coredns</code> 以外。k8s 相关的二进制的形式部署的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;20&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.20.6&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;8a62859e515889f07e3e3be6a1080413f17cf2c3&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2021-04-15T03:28:42Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.15.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;20&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.20.6&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;8a62859e515889f07e3e3be6a1080413f17cf2c3&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2021-04-15T03:19:55Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.15.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>阅读本篇文章之前，推荐先浏览器同时打开这两篇官方文档后稍微看完再看本篇文章:</p><ul><li><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/">官方文档</a> 和 </li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">最初的设计文档</a></li></ul><h3 id="相关说明"><a href="#相关说明" class="headerlink" title="相关说明"></a>相关说明</h3><p>相关术语就是 <code>enforceNodeAllocatable</code> ，它的默认值是 <code>[&quot;pods&quot;]</code> ，也就是 pod 能够使用节点上所有资源。但是节点上除了自己以外还有 kubelet ，kube 的三个组件，container runtime engine，以及 systemd 纳管的一些系统进程。如果有个 node 达到资源满了被驱逐，可能会漂移到其他节点上，把其他节点也搞挂了，形成连锁雪崩的情况。根据 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">官方文档最开始的设计</a> 一个 node 的 allocate 为下面的情况，<code>Allocatable</code> 为 pod 的：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Allocatable] = [Node Capacity] - [Kube-Reserved] - [System-Reserved] - [Hard-Eviction-Threshold]</span><br></pre></td></tr></table></figure><p>转换下就是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[Node Capacity] = [Allocatable] + [Kube-Reserved] + [System-Reserved] + [Hard-Eviction-Threshold]</span><br></pre></td></tr></table></figure><p>节点上的 <code>Allocatable</code> 被定义为 pod 的可用计算资源量。 调度器不会超额申请 Allocatable。 目前支持 <code>CPU</code>, <code>memory</code> 和 <code>ephemeral-storage</code> 这几个参数。上面的 <code>Hard-Eviction</code> 是有默认值的。而由于下面默认值，我们需要加上 kube 和 system 的 reserved 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br></pre></td></tr></table></figure><h2 id="尝试"><a href="#尝试" class="headerlink" title="尝试"></a>尝试</h2><p>加了上面俩后发现不生效，最后去看 yaml 里相关设置的参考后以及部分源码后摸索出来了。但是其实官方这块是有文档的: <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/">官方文档</a> 和 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">最初的设计文档</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;15%&quot;</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">inventory_hostname</span> <span class="string">in</span> <span class="string">groups</span>[<span class="string">&#x27;kube_master&#x27;</span>] <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">400m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">896Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">256Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">500Mi</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">2Gi</span></span><br></pre></td></tr></table></figure><p>这个模板判断的灵感是来源于 kubespray ，<a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml">defaults/main.yml</a> 和 <a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/templates/kubelet-config.v1beta1.yaml.j2">templates/kubelet-config.v1beta1.yaml.j2</a><br>我们环境都是二进制，所以 master 上 kube 会多配置些。但是这样配置了看了下无法生效，看了下必须要配置 cgroup path。也就是下面的:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;15%&quot;</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">inventory_hostname</span> <span class="string">in</span> <span class="string">groups</span>[<span class="string">&#x27;kube_master&#x27;</span>] <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">400m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">896Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">256Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">500Mi</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kubeReservedCgroup:</span> <span class="string">/kube.slice</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">2Gi</span></span><br><span class="line"><span class="attr">systemReservedCgroup:</span> <span class="string">/system.slice</span></span><br></pre></td></tr></table></figure><p>根据官方文档的示例值是俩不同的 path，但是市面上有不少人这方面的文章互相抄袭，他们会把 <code>kubeReservedCgroup: /system.slice/kube.slice</code> 嵌套下。配置了上面的后会发现依然无法启动报错下面的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to start ContainerManager Failed to enforce Kube Reserved Cgroup Limits on &quot;/kube.slice&quot;: [&quot;kubelet&quot;] cgroup does not exist</span><br></pre></td></tr></table></figure><p>最后找了下相关源码 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/cgroup_manager_linux.go#L257">pkg/kubelet/cm/cgroup_manager_linux.go 的 func (m *cgroupManagerImpl) Exists(name CgroupName) bool </a> 方法，我们只关心下面的几个 cgroup 就行了：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">allowlistControllers := sets.NewString(&quot;cpu&quot;, &quot;cpuacct&quot;, &quot;cpuset&quot;, &quot;memory&quot;, &quot;systemd&quot;, &quot;pids&quot;)</span><br><span class="line"></span><br><span class="line">if _, ok := m.subsystems.MountPoints[&quot;hugetlb&quot;]; ok &#123;</span><br><span class="line">allowlistControllers.Insert(&quot;hugetlb&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>市面上都是手动创建的不推荐，推荐在 kubelet 的 service 加个 <code>ExecStartPre</code> 和脚本判断处理。</p><h2 id="最终配置"><a href="#最终配置" class="headerlink" title="最终配置"></a>最终配置</h2><p>kubelet 的 service 文件参考:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=&#123;&#123; data_dir &#125;&#125;/kube/kubelet</span><br><span class="line">ExecStartPre=/bin/bash &#123;&#123; data_dir &#125;&#125;/kube/kubelet/kubelet-cg.sh</span><br><span class="line">ExecStart=&#123;&#123; bin_dir &#125;&#125;/kubelet \</span><br><span class="line">  --config=&#123;&#123; data_dir &#125;&#125;/kube/kubelet/kubelet-config.yaml \</span><br><span class="line">  --root-dir=&#123;&#123; data_dir &#125;&#125;/kube/kubelet \</span><br><span class="line">  --docker-root=&#123;&#123; data_dir &#125;&#125;/kube/docker \</span><br><span class="line">  --cni-bin-dir=&#123;&#123; bin_dir &#125;&#125; \</span><br><span class="line">  --cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">  --hostname-override=&#123;&#123; inventory_hostname &#125;&#125; \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">  --network-plugin=cni \</span><br><span class="line">  --experimental-dockershim-root-directory=&#123;&#123; data_dir &#125;&#125;/kube/dockershim \</span><br><span class="line">  --pod-infra-container-image=registry.aliyuncs.com/k8sxio/pause:3.5 \</span><br><span class="line">  --register-node=true \</span><br><span class="line">  --v=2 \</span><br><span class="line">  --node-ip=&#123;&#123; inventory_hostname &#125;&#125;</span><br><span class="line"></span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>我们的环境目前还是 cgroupfs ， systemd 的可能需要你自己去摸索了。下面是 <code>kubelet-cg.sh</code> 和 <code>kubelet-config.yaml</code>:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="comment"># Default: []</span></span><br><span class="line"><span class="attr">allowedUnsafeSysctls:</span> []</span><br><span class="line"><span class="attr">address:</span> &#123;&#123; <span class="string">inventory_hostname</span> &#125;&#125;</span><br><span class="line"><span class="attr">authentication:</span></span><br><span class="line">  <span class="attr">anonymous:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">webhook:</span></span><br><span class="line">    <span class="attr">cacheTTL:</span> <span class="string">2m0s</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">x509:</span></span><br><span class="line">    <span class="attr">clientCAFile:</span> &#123;&#123; <span class="string">ca_dir</span> &#125;&#125;<span class="string">/ca.pem</span></span><br><span class="line"><span class="attr">authorization:</span></span><br><span class="line">  <span class="attr">mode:</span> <span class="string">Webhook</span></span><br><span class="line">  <span class="attr">webhook:</span></span><br><span class="line">    <span class="attr">cacheAuthorizedTTL:</span> <span class="string">5m0s</span></span><br><span class="line">    <span class="attr">cacheUnauthorizedTTL:</span> <span class="string">30s</span></span><br><span class="line"><span class="attr">tlsCertFile:</span> &#123;&#123; <span class="string">ca_dir</span> &#125;&#125;<span class="string">/kubelet.pem</span></span><br><span class="line"><span class="attr">tlsPrivateKeyFile:</span> &#123;&#123; <span class="string">ca_dir</span> &#125;&#125;<span class="string">/kubelet-key.pem</span></span><br><span class="line"><span class="attr">cgroupDriver:</span> <span class="string">cgroupfs</span></span><br><span class="line"><span class="attr">cgroupsPerQOS:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">clusterDNS:</span></span><br><span class="line">  <span class="bullet">-</span> &#123;&#123; <span class="string">CLUSTER_DNS_SVC_IP</span> &#125;&#125;</span><br><span class="line"><span class="attr">clusterDomain:</span> &#123;&#123; <span class="string">CLUSTER_DNS_DOMAIN</span> &#125;&#125;</span><br><span class="line"><span class="attr">configMapAndSecretChangeDetectionStrategy:</span> <span class="string">Watch</span></span><br><span class="line"><span class="attr">containerLogMaxFiles:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">containerLogMaxSize:</span> <span class="string">10Mi</span></span><br><span class="line"><span class="attr">contentType:</span> <span class="string">application/vnd.kubernetes.protobuf</span></span><br><span class="line"><span class="attr">cpuCFSQuota:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: &quot;100ms&quot; The value must be between 1 us and 1 second</span></span><br><span class="line"><span class="attr">cpuCFSQuotaPeriod:</span> <span class="string">100ms</span></span><br><span class="line"><span class="attr">cpuManagerPolicy:</span> <span class="string">none</span></span><br><span class="line"><span class="attr">cpuManagerReconcilePeriod:</span> <span class="string">10s</span></span><br><span class="line"><span class="attr">enableControllerAttachDetach:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: true</span></span><br><span class="line"><span class="attr">enableDebuggingHandlers:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: true</span></span><br><span class="line"><span class="attr">enableSystemLogHandler:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: [&quot;pods&quot;]</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md</span></span><br><span class="line"><span class="comment"># pkg/kubelet/cm/node_container_manager_linux.go:67</span></span><br><span class="line"><span class="attr">enforceNodeAllocatable:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-reserved</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">system-reserved</span></span><br><span class="line"><span class="comment"># Default: 10</span></span><br><span class="line"><span class="attr">eventBurst:</span> <span class="number">100</span></span><br><span class="line"><span class="comment"># Default: 5</span></span><br><span class="line"><span class="attr">eventRecordQPS:</span> <span class="number">50</span></span><br><span class="line"><span class="comment"># Default:</span></span><br><span class="line"><span class="comment">#   memory.available:  &quot;100Mi&quot;</span></span><br><span class="line"><span class="comment">#   nodefs.available:  &quot;10%&quot;</span></span><br><span class="line"><span class="comment">#   nodefs.inodesFree: &quot;5%&quot;</span></span><br><span class="line"><span class="comment">#   imagefs.available: &quot;15%&quot;</span></span><br><span class="line"><span class="attr">evictionHard:</span></span><br><span class="line">  <span class="attr">imagefs.available:</span> <span class="string">&quot;15%&quot;</span></span><br><span class="line">  <span class="attr">memory.available:</span> <span class="string">&quot;200Mi&quot;</span></span><br><span class="line">  <span class="attr">nodefs.available:</span> <span class="string">&quot;10%&quot;</span></span><br><span class="line">  <span class="attr">nodefs.inodesFree:</span> <span class="string">&quot;5%&quot;</span></span><br><span class="line"><span class="attr">evictionPressureTransitionPeriod:</span> <span class="string">5m0s</span></span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: &quot;20s&quot;</span></span><br><span class="line"><span class="attr">fileCheckFrequency:</span> <span class="string">10s</span></span><br><span class="line"><span class="comment"># Default: &quot;promiscuous-bridge&quot;</span></span><br><span class="line"><span class="attr">hairpinMode:</span> <span class="string">promiscuous-bridge</span></span><br><span class="line"><span class="attr">healthzPort:</span> <span class="number">10248</span></span><br><span class="line"><span class="comment"># Default: &quot;127.0.0.1&quot;</span></span><br><span class="line"><span class="attr">healthzBindAddress:</span> &#123;&#123; <span class="string">inventory_hostname</span> &#125;&#125;</span><br><span class="line"><span class="comment"># Default: &quot;20s&quot;, staticPodUrl 才有用</span></span><br><span class="line"><span class="attr">httpCheckFrequency:</span> <span class="string">0s</span></span><br><span class="line"><span class="attr">imageGCHighThresholdPercent:</span> <span class="number">85</span></span><br><span class="line"><span class="attr">imageGCLowThresholdPercent:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">imageMinimumGCAge:</span> <span class="string">2m0s</span></span><br><span class="line"><span class="comment"># Default: 15</span></span><br><span class="line"><span class="attr">iptablesDropBit:</span> <span class="number">15</span></span><br><span class="line"><span class="comment"># Default: 14</span></span><br><span class="line"><span class="attr">iptablesMasqueradeBit:</span> <span class="number">14</span></span><br><span class="line"><span class="comment"># Default: 10</span></span><br><span class="line"><span class="attr">kubeAPIBurst:</span> <span class="number">100</span></span><br><span class="line"><span class="comment"># Default: 5</span></span><br><span class="line"><span class="attr">kubeAPIQPS:</span> <span class="number">50</span></span><br><span class="line"><span class="comment"># https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml</span></span><br><span class="line"><span class="comment"># https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/templates/kubelet-config.v1beta1.yaml.j2</span></span><br><span class="line"><span class="comment"># Default: nil</span></span><br><span class="line"><span class="attr">kubeReserved:</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">inventory_hostname</span> <span class="string">in</span> <span class="string">groups</span>[<span class="string">&#x27;kube_master&#x27;</span>] <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">400m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">896Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">256Mi</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">500Mi</span></span><br><span class="line"><span class="comment"># pkg/kubelet/cm/cgroup_manager_linux.go:257 func (m *cgroupManagerImpl) Exists(name CgroupName) bool &#123;</span></span><br><span class="line"><span class="attr">kubeReservedCgroup:</span> <span class="string">/kube.slice</span></span><br><span class="line"><span class="attr">systemReserved:</span></span><br><span class="line">  <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">  <span class="attr">ephemeral-storage:</span> <span class="string">2Gi</span></span><br><span class="line"><span class="attr">systemReservedCgroup:</span> <span class="string">/system.slice</span></span><br><span class="line"><span class="attr">makeIPTablesUtilChains:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: 1000000</span></span><br><span class="line"><span class="attr">maxOpenFiles:</span> <span class="number">1000000</span></span><br><span class="line"><span class="comment"># Default: 110</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">set</span> <span class="string">nodeLen</span> <span class="string">=</span> <span class="string">groups</span>[<span class="string">&#x27;kube_node&#x27;</span>] <span class="string">|</span> <span class="string">length</span> <span class="string">%</span>&#125;</span><br><span class="line">&#123;<span class="string">%</span> <span class="string">if</span> <span class="string">nodeLen</span> <span class="string">==</span> <span class="number">1</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">253</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">elif</span> <span class="string">nodeLen</span> <span class="string">&lt;</span> <span class="number">3</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">200</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">elif</span> <span class="string">nodeLen</span> <span class="string">&gt;=</span> <span class="number">3</span> <span class="string">and</span> <span class="string">nodeLen</span> <span class="string">&lt;=6</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">150</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">maxPods:</span> <span class="number">110</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="comment"># Default: 40</span></span><br><span class="line"><span class="attr">nodeLeaseDurationSeconds:</span> <span class="number">40</span> <span class="comment"># 看源码乘以了0.25 作为更新间隔了</span></span><br><span class="line"><span class="comment"># Default: &quot;5m&quot; # 节点状态没有更改时候的上报频率，如果有更改就立即更新。NodeLease 启用下它才有用。如果设置了 nodeStatusUpdateFrequency 则它的默认值等于它来向后兼容</span></span><br><span class="line"><span class="attr">nodeStatusReportFrequency:</span> <span class="string">1m0s</span></span><br><span class="line"><span class="attr">nodeStatusUpdateFrequency:</span> <span class="string">10s</span></span><br><span class="line"><span class="attr">oomScoreAdj:</span> <span class="number">-999</span></span><br><span class="line"><span class="attr">podPidsLimit:</span> <span class="number">-1</span></span><br><span class="line"><span class="attr">port:</span> <span class="number">10250</span></span><br><span class="line"><span class="attr">readOnlyPort:</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># Default: 10</span></span><br><span class="line"><span class="attr">registryBurst:</span> <span class="number">20</span></span><br><span class="line"><span class="comment"># Default: 5</span></span><br><span class="line"><span class="attr">registryPullQPS:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">resolvConf:</span> &#123;<span class="string">%</span> <span class="string">if</span> <span class="string">ansible_distribution</span> <span class="string">==</span> <span class="string">&quot;Ubuntu&quot;</span> <span class="string">and</span> <span class="string">ansible_distribution_major_version|int</span> <span class="string">&gt;</span> <span class="number">16</span> <span class="string">%</span>&#125;<span class="string">/run/systemd/resolve/resolv.conf</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">else</span> <span class="string">%</span>&#125;<span class="string">/etc/resolv.conf</span></span><br><span class="line">&#123;<span class="string">%</span> <span class="string">endif</span> <span class="string">%</span>&#125;</span><br><span class="line"><span class="attr">rotateCertificates:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># Default: &quot;2m&quot;</span></span><br><span class="line"><span class="attr">runtimeRequestTimeout:</span> <span class="string">2m0s</span></span><br><span class="line"><span class="attr">serializeImagePulls:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">staticPodPath:</span> <span class="string">/etc/kubernetes/manifests</span></span><br><span class="line"><span class="comment"># Default: &quot;4h&quot;</span></span><br><span class="line"><span class="attr">streamingConnectionIdleTimeout:</span> <span class="string">20m0s</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># shutdownGracePeriod: 30s</span></span><br><span class="line"><span class="comment"># shutdownGracePeriodCriticalPods: 10s    # 1.21后的特性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Default: &quot;1m&quot; # sync for ConfigMaps and Secrets.</span></span><br><span class="line"><span class="attr">syncFrequency:</span> <span class="string">1m0s</span></span><br><span class="line"><span class="attr">volumeStatsAggPeriod:</span> <span class="string">1m0s</span></span><br><span class="line"><span class="attr">volumePluginDir:</span> <span class="string">/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</span></span><br><span class="line"><span class="attr">tlsCipherSuites:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_128_GCM_SHA256</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_256_GCM_SHA384</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_128_CBC_SHA</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">TLS_RSA_WITH_AES_256_CBC_SHA</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_and_create</span></span>()&#123;</span><br><span class="line"><span class="comment"># pkg/kubelet/cm/cgroup_manager_linux.go:257  func (m *cgroupManagerImpl) Exists(name CgroupName) bool &#123;</span></span><br><span class="line">    <span class="built_in">local</span> cg_controller=<span class="variable">$1</span></span><br><span class="line">    <span class="keyword">if</span> mountpoint -q /sys/fs/cgroup/<span class="variable">$&#123;cg_controller&#125;</span>;<span class="keyword">then</span></span><br><span class="line">        mkdir -p /sys/fs/cgroup/<span class="variable">$&#123;cg_controller&#125;</span>/system.slice</span><br><span class="line">        mkdir -p /sys/fs/cgroup/<span class="variable">$&#123;cg_controller&#125;</span>/kube.slice</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">check_and_create cpu</span><br><span class="line">check_and_create cpuacct</span><br><span class="line">check_and_create cpuset</span><br><span class="line">check_and_create memory</span><br><span class="line">check_and_create systemd</span><br><span class="line">check_and_create pids</span><br><span class="line">check_and_create hugetlb</span><br></pre></td></tr></table></figure><p>关于 pod 数量这块和大佬讨论了下，<code>maxPods</code> 大了的话实际上例如 docker 撑不住，所以没必要太大，我的判断逻辑是节点数量少的时候也就是我们内部的测试环境下，pod 数量调大，客户现场还是推荐的 110。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/">官方文档</a> 和 </li><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/node-allocatable.md">最初的设计文档</a></li><li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/defaults/main.yml">defaults/main.yml</a> 和 </li><li><a href="https://github.com/kubernetes-sigs/kubespray/blob/master/roles/kubernetes/node/templates/kubelet-config.v1beta1.yaml.j2">templates/kubelet-config.v1beta1.yaml.j2</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;我们环境有部分 pod 特殊，单独节点部署，oom 的时候会搞挂一些系统进程，这几天折腾了下配置了下 kubelet 相关的</summary>
      
    
    
    
    
    <category term="kubelet" scheme="http://zhangguanzhang.github.io/tags/kubelet/"/>
    
  </entry>
  
  <entry>
    <title>鲲鹏920的麒麟v10物理服务器断电后无法启动处理</title>
    <link href="http://zhangguanzhang.github.io/2021/07/26/kylin-v10-boot-hang/"/>
    <id>http://zhangguanzhang.github.io/2021/07/26/kylin-v10-boot-hang/</id>
    <published>2021-07-26T12:08:06.000Z</published>
    <updated>2021-07-26T12:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>珠海园区升压前置检查，上周六整个园区关电检查，然后今天来后连不上我们在 鲲鹏920的麒麟v10机器上开的虚拟机了，进 bmc 的 web 看了下是开机进入后卡住。</p><h2 id="信息同步"><a href="#信息同步" class="headerlink" title="信息同步"></a>信息同步</h2><p>是当初安装系统的同事去处理这个事情的，他 bmc 的 web 上去重启在菜单那里按 e 编辑准备改 boot cmdline 进单用户，结果按 e 后要输入用户名和密码，询问了麒麟他们。很久也没给答复。然后就在那干等，上面的虚机有我的环境，我就过去看了下。</p><h3 id="尝试的处理"><a href="#尝试的处理" class="headerlink" title="尝试的处理"></a>尝试的处理</h3><p>麒麟那边的人员没有回复，我打算这边同步尝试下其他手段，而且不只一台无法开机，哪怕麒麟的回复了密码也能同步尝试不同手段。Linux 无法开机的就搞个 yum 系列的新系统镜像挂载到光驱，然后设置成光驱启动，然后开机后进入 iso 的安装界面，<code>Troubleshooting –&gt; Rescue a CentOS Linux system</code>。先进 bmc web 上用java的远程窗口（之前华三的 h5 挂载镜像失败了），挂载本地的 麒麟v10 镜像，然后 bmc web 上设置下次从光驱启动。然后重启服务器，进 <code>Rescue</code> 模式里后，界面是显示乱的。</p><p>这个时候问了下珠海那边有同事能去机房看下吗，同事说正在赶去。然后我赶紧去找了下 centos 7的arm64镜像，后面下载完了进 <code>Rescue</code> 模式里一直卡住。估计没适配国产的服务器。后面尝试了其他国产的系统镜像，没 <code>Rescue</code> 模式。机房现场也有人去了，麒麟v10 镜像<code>Rescue</code> 模式进去后，他那边现场看了下也是显示乱的。</p><h3 id="单用户"><a href="#单用户" class="headerlink" title="单用户"></a>单用户</h3><p>后面麒麟回复了修改 boot 的时候用户名是 <code>root</code> 密码是 <code>Kylin123123</code>，确实能进入编辑 boot 的界面了，但是加了相关选项后还是无法进入单用户，左上角只有一个光标然后一直卡住。询问安装系统的同事，分区是咋搞的，他说 lvm 和有个大盘。然后问他有没有其他正常能开机的机器，他说有一个，我 ssh 上去后看到下面类似的分区：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0 893.1G  0 disk </span><br><span class="line">├─sda1            8:1    0   200M  0 part /boot/efi</span><br><span class="line">├─sda2            8:2    0     1G  0 part /boot</span><br><span class="line">└─sda3            8:3    0   892G  0 part </span><br><span class="line">  ├─klas-root   252:0    0   838G  0 lvm  /</span><br><span class="line">  ├─klas-swap   252:1    0     4G  0 lvm  [SWAP]</span><br><span class="line">  └─klas-backup 252:2    0    50G  0 lvm  </span><br><span class="line">sdb               8:16   0  14.6T  0 disk </span><br><span class="line">└─sdb1            8:17   0     2T  0 part /oskvm</span><br></pre></td></tr></table></figure><p>正常机器的 lvm 信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vgs</span><br><span class="line">  VG   #PV #LV #SN Attr   VSize    VFree</span><br><span class="line">  klas   1   3   0 wz--n- &lt;891.94g    0 </span><br><span class="line">$ lvs</span><br><span class="line">  LV     VG   Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  backup klas -wi-a-----   50.00g                                                    </span><br><span class="line">  root   klas -wi-ao---- &lt;837.94g                                                    </span><br><span class="line">  swap   klas -wi-ao----    4.00g</span><br></pre></td></tr></table></figure><p>推测了下，无法开机的机器是因为把 sdb 搞成 lvm 的原因，所以单用户肯定别想了。</p><h3 id="Rescue-模式"><a href="#Rescue-模式" class="headerlink" title="Rescue 模式"></a>Rescue 模式</h3><p>目前的解决办法只有重装，这个时候快到中午了，其他同事催得急，重装代价太高了，安装慢，而且上面的虚拟机都没了，Rescue 模式后的乱码能解决就有办法了。看看正常机器上有没有显示的相关内核启动参数。</p><p>查找下 <code>grub</code> 的配置文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd /boot</span><br><span class="line">$ find -type f -name &#x27;grub*&#x27;</span><br><span class="line">./efi/EFI/kylin/grubaa64.efi</span><br><span class="line">./efi/EFI/kylin/grubenv</span><br><span class="line">./efi/EFI/kylin/grub.cfg</span><br></pre></td></tr></table></figure><p>看了下果然有</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ./efi/EFI/kylin/grub.cfg</span><br><span class="line">linux/vmlinuz-4.19.90-17.ky10.aarch64 root=/dev/mapper/klas-root ro crashkernel=auto rd.lvm.lv=klas/root rd.lvm.lv=klas/swap smmu.bypassdev=0x1000:0x17 smmu.bypassdev=0x1000:0x15 crashkernel=1024M,high video=efifb:off video=VGA-1:640x480-32@60me</span><br></pre></td></tr></table></figure><p>然后在 麒麟v10 的 iso 进入了 <code>Troubleshooting –&gt; Rescue</code> 选中 <code>Rescue</code> 先别回车，直接按 e 编辑，<code>linux</code>那行后面加了上面的<code> video=efifb:off video=VGA-1:640x480-32@60me</code>，然后 <code>ctrl + x</code> 启动，显示正常了。和 Centos 的 <code>Rescue</code> 有点不一样，进去后类似进入 emergency 模式里，然后回车继续，进入到了一个 bash。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vgchange -a y</span><br><span class="line">$ lvs</span><br><span class="line">  LV     VG   Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  18     18   -wi-ao----   14.55t                                                    </span><br><span class="line">  backup klas -wi-a-----   50.00g                                                    </span><br><span class="line">  root   klas -wi-ao---- &lt;837.94g                                                    </span><br><span class="line">  swap   klas -wi-ao----    4.00g     </span><br><span class="line">$ mkdir /mnt/sysimage</span><br></pre></td></tr></table></figure><p>然后挂载 root 分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mount /dev/mapper/klas-root /mnt/sysimage</span><br><span class="line">$ chroot /mnt/sysimage</span><br></pre></td></tr></table></figure><p>其实也没必要 chroot 进去，无非就是 vi 多了个前面的路径。取消最后一行的挂载，然后 reboot 后重启就好了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 没chroot进去就是编辑 vi /mnt/sysimage/etc/fstab</span><br><span class="line">$ vi /etc/fstab</span><br><span class="line">...</span><br><span class="line">#/dev/mapper/18-18     /oskvm                    xfs     default   0 0</span><br></pre></td></tr></table></figure><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>可能是分区太大了，lvm 在国产系统开机上有问题。另外也不推荐物理机使用 lvm，能不查资料笔记熟悉 lvm 的命令的人实际上目前不多，会修复的人更少了。lvm 扩容一时爽，恢复火葬场。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ pvs</span><br><span class="line">  PV         VG   Fmt  Attr PSize    PFree </span><br><span class="line">  /dev/sda3  klas lvm2 a--  &lt;891.94g     0 </span><br><span class="line">  /dev/sdb   18   lvm2 a--    14.55t &lt;1.16g</span><br><span class="line">l$ vgs</span><br><span class="line">  VG   #PV #LV #SN Attr   VSize    VFree </span><br><span class="line">  18     1   1   0 wz--n-   14.55t &lt;1.16g</span><br><span class="line">  klas   1   3   0 wz--n- &lt;891.94g     0 </span><br><span class="line">$ lvs</span><br><span class="line">  LV     VG   Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert</span><br><span class="line">  18     18   -wi-ao----   14.55t                                                    </span><br><span class="line">  backup klas -wi-a-----   50.00g                                                    </span><br><span class="line">  root   klas -wi-ao---- &lt;837.94g                                                    </span><br><span class="line">  swap   klas -wi-ao----    4.00g </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;珠海园区升压前置检查，上周六整个园区关电检查，然后今天来后连不上我们在 鲲鹏920的麒麟v10机器上开的虚拟机了，进 bmc</summary>
      
    
    
    
    
    <category term="kylin" scheme="http://zhangguanzhang.github.io/tags/kylin/"/>
    
    <category term="arm64" scheme="http://zhangguanzhang.github.io/tags/arm64/"/>
    
  </entry>
  
  <entry>
    <title>dlv命令行的远程调试 golang 进程步骤(包含容器进程)</title>
    <link href="http://zhangguanzhang.github.io/2021/07/20/dlv-remote/"/>
    <id>http://zhangguanzhang.github.io/2021/07/20/dlv-remote/</id>
    <published>2021-07-20T20:08:06.000Z</published>
    <updated>2021-07-20T20:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><p>记录下 dlv 的远程调试，建议不要在代码里加 fmt 去调试。不谈 goland 啥的远程调试，本文章目前只写 dlv 的命令行配合远端调试。</p><h2 id="一些前提须知"><a href="#一些前提须知" class="headerlink" title="一些前提须知"></a>一些前提须知</h2><h3 id="符号链接路径"><a href="#符号链接路径" class="headerlink" title="符号链接路径"></a>符号链接路径</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line"><span class="string">&quot;os&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">f, _ := os.Open(<span class="string">&quot;asdasdasd&quot;</span>)</span><br><span class="line">fmt.Println(f.Name())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码你编译了后，在其他机器上运行，panic 的堆栈信息会是你机器上的路径信息，路径信息是保留的，例如下面的是我在 windows 上交叉编译仍到 Linux 上执行的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">panic: runtime error: invalid memory address or nil pointer dereference</span><br><span class="line">[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x497d50]</span><br><span class="line"></span><br><span class="line">goroutine 1 [running]:</span><br><span class="line">os.(*File).Name(...)</span><br><span class="line">D:/Install/Go/src/os/file.go:55</span><br><span class="line">main.main()</span><br><span class="line">D:/github_dir/go/dlv-test/main.go:10 +0x50</span><br></pre></td></tr></table></figure><p>可以通过下面的编译选项去掉（项目路径也就是<code>$PWD</code>显示的不要带空格，否则会编译报错）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go build -gcflags=&quot;all=-trimpath=$PWD&quot; -asmflags &quot;all=-trimpath=$PWD&quot;  main.go</span><br></pre></td></tr></table></figure><p>使用上面的参数编译完后的，这里注意下下面的 <code>D:/Install/Go</code>，后面文章会用到。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">panic: runtime error: invalid memory address or nil pointer dereference</span><br><span class="line">[signal 0xc0000005 code=0x0 addr=0x0 pc=0x649857]</span><br><span class="line"></span><br><span class="line">goroutine 1 [running]:</span><br><span class="line">os.(*File).Name(...)</span><br><span class="line">        D:/Install/Go/src/os/file.go:55</span><br><span class="line">main.main()</span><br><span class="line">        main2.go:10 +0x57</span><br></pre></td></tr></table></figure><h2 id="dlv-命令行远端调试"><a href="#dlv-命令行远端调试" class="headerlink" title="dlv 命令行远端调试"></a>dlv 命令行远端调试</h2><p>很多时候线上机器都是 Linux ，源码在本地，而且机器上不一定会有 golang，也就是说 <code>dlv debug main.go</code>满足的条件实际上并不多。这里主要讲下 <code>dlv exec</code> 和 <code>dlv attach</code>。<br>exec 是用 dlv 运行编译完的二进制文件，golang 关闭 cgo 编译的就是静态编译了，运行机器上有无 golang 都能运行。attach 是调试一个已经运行的进程。<br>这里我是在linux上运行一个编译好的 gin-demo，然后在目标机器上准备一个 dlv 的二进制文件，用 <code>dlv attach</code> 开一个 server，然后我们在本地有源码的 windows 上 <code>dlv connect</code> 连上去调试。</p><p>这里以我本地的 windows 和 远端的 Linux 做测试。windows 上和 linux 上都已经安装了 dlv 了，并把路径加到 PATH 里了，windows 我下了 git bash。</p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>根据实际开发流程来，windows 上项目编辑文件 <code>main.go</code>，代码随便写的，不要吐槽。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;github.com/gin-gonic/gin&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> count <span class="keyword">int</span></span><br><span class="line">r := gin.Default()</span><br><span class="line">r.GET(<span class="string">&quot;/ping&quot;</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">count++</span><br><span class="line">c.JSON(<span class="number">200</span>, gin.H&#123;</span><br><span class="line"><span class="string">&quot;message&quot;</span>: <span class="string">&quot;pong&quot;</span>,</span><br><span class="line"><span class="string">&quot;count&quot;</span>:   count,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;)</span><br><span class="line">r.Run()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go mod init test</span><br></pre></td></tr></table></figure><p>然后推送到代码仓库上，手动或者 robot 触发 CI 构建</p><h3 id="ci-的编译"><a href="#ci-的编译" class="headerlink" title="ci 的编译"></a>ci 的编译</h3><p>Dockerfile 如下，为了避免 dlv 调试出现 <code>Warning: debugging optimized function</code>，我们需要在 <code>-gcflags=</code> 里加 <code>-N -l</code> ，为了防止变量被 <code>Dockerfile</code> 解析，<code>$PWD</code> 全部换成了<code>pwd</code>。<code>$&#123;LDFLAGS&#125;</code>是注入一些 version 信息之类的，可以看我文章<a href="https://zhangguanzhang.github.io/2020/04/27/go-version-ifno/">git工作流下golang项目的version信息该如何处理</a></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> golang:<span class="number">1.16</span>.<span class="number">6</span> as mod</span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> stage=mod</span></span><br><span class="line"><span class="keyword">ARG</span> GOPROXY=https://goproxy.cn,https://mirrors.aliyun.com/goproxy/,https://goproxy.io,direct</span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /root/myapp/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> go.mod ./</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> go.sum ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> go mod download</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> mod as builder</span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> stage=intermediate0</span></span><br><span class="line"><span class="keyword">ARG</span> LDFLAGS</span><br><span class="line"><span class="keyword">ARG</span> GOARCH=amd64</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./ ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> CGO_ENABLED=0 GOOS=linux GOARCH=<span class="variable">$&#123;GOARCH&#125;</span> \</span></span><br><span class="line"><span class="bash">   go build -o gin-demo \</span></span><br><span class="line"><span class="bash">   -gcflags=<span class="string">&quot;all=-trimpath=`pwd` -N -l&quot;</span> \</span></span><br><span class="line"><span class="bash">   -asmflags <span class="string">&quot;all=-trimpath=`pwd`&quot;</span> \</span></span><br><span class="line"><span class="bash">   -ldflags <span class="string">&quot;<span class="variable">$&#123;LDFLAGS&#125;</span>&quot;</span> main.go</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> alpine:<span class="number">3.13</span>.<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> MAINTAINER=<span class="string">&quot;zhangguanzhang zhangguanzhang@qq.com&quot;</span> \</span></span><br><span class="line"><span class="bash">    URL=<span class="string">&quot;https://github.com/zhangguanzhang/xxxx&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> --from=builder /root/myapp/gin-demo /gin-demo</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> TZ Asia/Shanghai</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> sed -i <span class="string">&#x27;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&#x27;</span> /etc/apk/repositories &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apk update &amp;&amp; \</span></span><br><span class="line"><span class="bash">    apk add --no-cache \</span></span><br><span class="line"><span class="bash">      curl \</span></span><br><span class="line"><span class="bash">      ca-certificates \</span></span><br><span class="line"><span class="bash">      bash \</span></span><br><span class="line"><span class="bash">      iproute2 \</span></span><br><span class="line"><span class="bash">      tzdata &amp;&amp; \</span></span><br><span class="line"><span class="bash">    ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="built_in">echo</span> Asia/Shanghai &gt; /etc/timezone &amp;&amp; \</span></span><br><span class="line"><span class="bash">    <span class="keyword">if</span> [ ! -e /etc/nsswitch.conf ];<span class="keyword">then</span> <span class="built_in">echo</span> <span class="string">&#x27;hosts: files dns myhostname&#x27;</span> &gt; /etc/nsswitch.conf; <span class="keyword">fi</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">   rm -rf /var/cache/apk/* /tmp/*</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;/gin-demo&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>然后编译完了，模拟下 CD 部署在 Linux 机器上运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name t1 --rm -p 8080:8080 test</span><br></pre></td></tr></table></figure><p>这里我用容器演示，虽然容器的 pid namespaces 隔离了，但是实际上容器里的所有进程还是会在宿主机上有 pid 对应的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep gin-dem[o]</span><br><span class="line">root     15598  0.0  0.1 708540  5128 ?        Ssl  21:10   0:00 /gin-demo</span><br></pre></td></tr></table></figure><h3 id="Linux-上的准备"><a href="#Linux-上的准备" class="headerlink" title="Linux 上的准备"></a>Linux 上的准备</h3><p>拷贝 Linux 的 dlv 可执行文件放到目标机器的 PATH 下，推荐按照 LFS 的规范放 /usr/local/bin/ 下</p><h4 id="http-接口之类的-server-类调试"><a href="#http-接口之类的-server-类调试" class="headerlink" title="http 接口之类的 server 类调试"></a>http 接口之类的 server 类调试</h4><p>比如我这个 demo 就是要调试接口内部的，也就是运行后触发的主要运行段是在接口内，所以我们不需要 <code>dlv run</code>， 而是下面这样 attach 执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dlv attach $(pgrep gin-demo) --listen=:2345 --headless=true --log=true  \</span><br><span class="line">  --log-output=debugger,debuglineerr,gdbwire,lldbout,rpc --accept-multiclient --api-version=2</span><br></pre></td></tr></table></figure><p><code>$(pgrep gin-demo)</code> 也可以换成具体的 pid 。</p><h4 id="非接口类服务容器内调试"><a href="#非接口类服务容器内调试" class="headerlink" title="非接口类服务容器内调试"></a>非接口类服务容器内调试</h4><p>可能我们的服务一开始就要打断点，也就是 main 开始之类的地方就打断点，用 attach 就不现实了。我们的思路是先把容器起来，然后把 dlv 拷贝进去，<code>dlv exec</code> 执行二进制文件<br>如果是 <code>docker-compose</code> 可以把容器的命令替换了，类似下面</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">entrypoint:</span> [<span class="string">&#x27;sh&#x27;</span>]</span><br><span class="line">    <span class="attr">tty:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>k8s 的话也和上面差不多，然后把探针啥的给取消或者 initDelay 调久点。下面我用 <code>docker run</code> 模拟容器下操作，需要添加 <code>ptrace</code> 权限，否则 dlv 执行会报错 <code>could not launch process: fork/exec /gin-demo: operation not permitted</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --name t1 --rm -tid  --cap-add=SYS_PTRACE --entrypoint sh test</span><br><span class="line">$ docker cp $(which dlv) t1:/bin/</span><br><span class="line">$ docker exec t1 ip a s  eth0</span><br><span class="line">96: eth0@if97: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default </span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>容器里执行 dlv ，注意添加 <code>--allow-non-terminal-interactive</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker exec -ti t1  \</span><br><span class="line">  dlv exec /gin-demo --listen=:2345 --allow-non-terminal-interactive --headless=true --log=true  \</span><br><span class="line">  --log-output=debugger,debuglineerr,gdbwire,lldbout,rpc --accept-multiclient --api-version=2</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>，然后在另一个 tty 窗口我们利用 socat 转发成宿主机的端口，没有就安装下。开启 socat 转发：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socat  -d -d TCP-LISTEN:2345,reuseaddr,fork,bind=0.0.0.0 TCP:172.17.0.2:2345</span><br></pre></td></tr></table></figure><h3 id="本机上开始调试"><a href="#本机上开始调试" class="headerlink" title="本机上开始调试"></a>本机上开始调试</h3><p>和 os 无关，我这里是 windows 而已，dlv 也要放在 PATH 里，或者绝对路径运行 dlv。要在源码目录运行，因为 <code>docker build</code> 里容器编译的时候去掉源码的前缀路径，所以 dlv 调试按照链接找文件都会按照相对路径找，所以我们需要在windows 上的源码路径里执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dlv connect 192.168.2.111:2345</span><br></pre></td></tr></table></figure><p>连上之后，没有 Linux 的<code>(dlv) </code>这样的 format，这是 dlv 引用的库在 windows 上的 bug，只要出现了<code>Type &#39;help&#39; for list of commands.</code>说明成功连上了。</p><p>执行下 <code>sources</code> 看下链接路径：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/go/src/...</span><br><span class="line">/usr/local/go/src/...</span><br><span class="line">/usr/local/go/src/..</span><br><span class="line">...</span><br><span class="line">main.go</span><br></pre></td></tr></table></figure><p>前面的是 golang 的内部包，最后面的是我们项目路径的，因为我们 build 的时候 trim 掉前缀路径了，所以看着都是相对路径。<code>/usr/local/go</code> 是 golang 的 docker 镜像里的 <code>GOROOT</code>，我们本地 windows 的 go root 可以通过 cmd 或者 git bash 里使用下面的命令查看:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ go env GOROOT</span><br><span class="line">D:\Install\Go</span><br></pre></td></tr></table></figure><p>源码包已经相对路径存在了，但是 golang 的自带包还没有，我们需要配置映射路径，接着在 dlv 的交互里执行下面的:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config substitute-path /usr/local/go D:\Install\Go</span><br></pre></td></tr></table></figure><p>然后我们在 <code>main.go:10</code> 打个断点（也可以包名.方法内的相对第几行打断点），然后 <code>c</code> 执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b main.go:10</span><br><span class="line">Breakpoint 1 (enabled) set at 0x891be0 for main.main.func1() main.go:10</span><br><span class="line">c</span><br></pre></td></tr></table></figure><p>然后我们触发下请求，例如在 Linux 上直接用 curl 触发请求：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:8080/ping</span><br></pre></td></tr></table></figure><p>命令会阻塞住，因为 server 端没回复，我们回到我们的 windows 上，界面有打印下面的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; main.main.func1() main.go:10 (hits goroutine(33):1 total:2) (PC: 0x891be0)</span><br><span class="line">     5: func main() &#123;</span><br><span class="line">     6:         var count int</span><br><span class="line">     7:         r := gin.Default()</span><br><span class="line">     8:         r.GET(&quot;/ping&quot;, func(c *gin.Context) &#123;</span><br><span class="line">     9:                 count++</span><br><span class="line">=&gt;  10:                 c.JSON(200, gin.H&#123;</span><br><span class="line">    11:                         &quot;message&quot;: &quot;pong&quot;,</span><br><span class="line">    12:                         &quot;count&quot;:   count,</span><br><span class="line">    13:                 &#125;)</span><br><span class="line">    14:         &#125;)</span><br><span class="line">    15:         r.Run()</span><br></pre></td></tr></table></figure><p>我们打印下 count 的值</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p count</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>然后 c，让代码 continue 。然后能看到我们的 Linux 上的 curl 命令收到 server 端的请求了。</p><p>dlv 里常用的命令如下:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config max-string-len 1000  # 配置打印变量的输出长度，防止被折叠显示</span><br><span class="line">b file.go:数字              # 文件行数打断点</span><br><span class="line">c                           # 执行到下一个断点</span><br><span class="line">so                          # 直接执行完所在的当前函数</span><br><span class="line">s                           # 单步执行</span><br><span class="line">n  数字                     # 执行到后面的n行那里</span><br></pre></td></tr></table></figure><p>dlv 运行的时候会读取配置文件路径，像上面的<code>config max-string-len 1000</code> 和 <code>config substitute-path /usr/local/go D:\Install\Go</code> 都可以配置在文件里。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/go-delve/delve/issues/2583">https://github.com/go-delve/delve/issues/2583</a></li><li><a href="https://jvns.ca/blog/2020/04/29/why-strace-doesnt-work-in-docker/">https://jvns.ca/blog/2020/04/29/why-strace-doesnt-work-in-docker/</a></li><li><a href="https://github.com/go-delve/delve/issues/515#issuecomment-214911481">https://github.com/go-delve/delve/issues/515#issuecomment-214911481</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前情提要&quot;&gt;&lt;a href=&quot;#前情提要&quot; class=&quot;headerlink&quot; title=&quot;前情提要&quot;&gt;&lt;/a&gt;前情提要&lt;/h2&gt;&lt;p&gt;记录下 dlv 的远程调试，建议不要在代码里加 fmt 去调试。不谈 goland 啥的远程调试，本文章目前只写 dlv 的</summary>
      
    
    
    
    
    <category term="golang" scheme="http://zhangguanzhang.github.io/tags/golang/"/>
    
    <category term="dlv" scheme="http://zhangguanzhang.github.io/tags/dlv/"/>
    
  </entry>
  
  <entry>
    <title>编译mips64le架构的consul</title>
    <link href="http://zhangguanzhang.github.io/2021/07/16/consul-mips64le/"/>
    <id>http://zhangguanzhang.github.io/2021/07/16/consul-mips64le/</id>
    <published>2021-07-16T13:08:06.000Z</published>
    <updated>2021-07-16T13:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>建议使用容器编译，否则建议 clone 进 GOPATH 里</p><h3 id="clone"><a href="#clone" class="headerlink" title="clone"></a>clone</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/hashicorp/consul.git</span><br><span class="line"><span class="built_in">cd</span> consul</span><br></pre></td></tr></table></figure><p>线上使用的是 <code>v1.8</code> 版本，这里我以 <code>v1.8.14</code> (2021/07/19 发布的)搞的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout v1.8.14</span><br></pre></td></tr></table></figure><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>拉取需要的镜像。貌似 golang 1.16 更好的支持 mips64 了，所以条件允许的话，这里可以下改下 golang 的版本试试</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ head -n2 build-support/docker/Build-Go.dockerfile</span><br><span class="line">ARG GOLANG_VERSION=1.14.11</span><br><span class="line">FROM golang:$&#123;GOLANG_VERSION&#125;</span><br><span class="line">$ docker pull golang:1.14.11</span><br></pre></td></tr></table></figure><h3 id="开始编译"><a href="#开始编译" class="headerlink" title="开始编译"></a>开始编译</h3><p>相关变量可以查看 <code>GNUmakefile</code> ，实际会在容器里运行 <code>build-support/functions/20-build.sh</code> 里的 <code>function build_consul</code> ：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">local container_id=$(docker create -it \</span><br><span class="line">   $&#123;volume_mount&#125; \</span><br><span class="line">   -e CGO_ENABLED=0 \</span><br><span class="line">   -e GOLDFLAGS=&quot;$&#123;GOLDFLAGS&#125;&quot; \</span><br><span class="line">   -e GOTAGS=&quot;$&#123;GOTAGS&#125;&quot; \</span><br><span class="line">   $&#123;image_name&#125; \</span><br><span class="line">   ./build-support/scripts/build-local.sh -o &quot;$&#123;XC_OS&#125;&quot; -a &quot;$&#123;XC_ARCH&#125;&quot;)</span><br></pre></td></tr></table></figure><p>而在容器里执行<code>build-support/scripts/build-local.sh  -o &quot;$&#123;XC_OS&#125;&quot; -a &quot;$&#123;XC_ARCH&#125;&quot;</code> 会执行 <code>build-support/functions/20-build.sh</code> 里的 <code>function build_consul_local</code>，主要编译是下面的这行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">debug_run env CGO_ENABLED=0 GOOS=$&#123;os&#125; GOARCH=$&#123;arch&#125; go install -ldflags &quot;$&#123;GOLDFLAGS&#125;&quot; -tags &quot;$&#123;GOTAGS&#125;&quot; &amp;&amp; cp &quot;$&#123;MAIN_GOPATH&#125;/bin/$&#123;GOBIN_EXTRA&#125;$&#123;binname&#125;&quot; &quot;$&#123;outdir&#125;/$&#123;binname&#125;&quot;</span><br></pre></td></tr></table></figure><p>开始编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">export DOCKER_BUILD_QUIET=0</span><br><span class="line">export XC_OS=linux XC_ARCH=mips64le</span><br><span class="line">make consul-docker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">==&gt; Building Consul - OSes: linux, Architectures: mips64le</span><br><span class="line">Building sequentially with go install</span><br><span class="line">---&gt;   linux/mips64le</span><br><span class="line"># github.com/boltdb/bolt</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/bolt_unix.go:62:15: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/bucket.go:135:15: undefined: brokenUnaligned</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:101:13: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:317:12: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:335:10: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/db.go:336:8: undefined: maxMapSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:169:19: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:176:14: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:204:17: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:207:7: undefined: maxAllocSize</span><br><span class="line">/go/pkg/mod/github.com/boltdb/bolt@v1.3.1/freelist.go:207:7: too many errors</span><br><span class="line">ERROR: Failed to build Consul for linux/mips64le</span><br><span class="line">make: *** [consul-docker] Error 1</span><br></pre></td></tr></table></figure><p>编译报错，是因为 boltdb 的库现在 archive 了。而且相关文件用 golang 的 build tag 区分架构了，没有预置 mips64le 的，搜了几个 issue 后搞了下编译还是失败了，仔细看上面的 path，没有读取 vendor 目录，得改编译容器里的 mod 目录下的 boltdb 文件。所以得 hack 下。</p><h3 id="hack"><a href="#hack" class="headerlink" title="hack"></a>hack</h3><p>先产生临时文件<code>/tmp/hack_consul.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /tmp/hack_consul.sh &lt;&lt; <span class="string">&#x27;consul_eof&#x27;</span></span><br><span class="line">         <span class="keyword">if</span> [ <span class="variable">$arch</span> == <span class="string">&quot;mips64le&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">cat &gt; $(go env GOPATH)/pkg/mod/github.com/boltdb/bolt@v1.3.1/bolt_mips64x.go &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">// +build mips64 mips64le</span><br><span class="line"></span><br><span class="line">package bolt</span><br><span class="line"></span><br><span class="line">// maxMapSize represents the largest mmap size supported by Bolt.</span><br><span class="line">const maxMapSize = 0x8000000000 // 512GB</span><br><span class="line"></span><br><span class="line">// maxAllocSize is the size used when creating array pointers.</span><br><span class="line">const maxAllocSize = 0x7FFFFFFF</span><br><span class="line"></span><br><span class="line">// Are unaligned load/stores broken on this arch?</span><br><span class="line">var brokenUnaligned = <span class="literal">false</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; $(go env GOPATH)/pkg/mod/github.com/boltdb/bolt@v1.3.1/bolt_mips.go &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">// +build mips mipsle</span><br><span class="line"></span><br><span class="line">package bolt</span><br><span class="line"></span><br><span class="line">// maxMapSize represents the largest mmap size supported by Bolt.</span><br><span class="line">const maxMapSize = 0x40000000 // 1GB</span><br><span class="line"></span><br><span class="line">// maxAllocSize is the size used when creating array pointers.</span><br><span class="line">const maxAllocSize = 0xFFFFFFF</span><br><span class="line"></span><br><span class="line">// Are unaligned load/stores broken on this arch?</span><br><span class="line">var brokenUnaligned = <span class="literal">false</span></span><br><span class="line">EOF</span><br><span class="line">         <span class="keyword">fi</span></span><br><span class="line">consul_eof</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>取 go install 命令的行数，在前面一行插入 hack 的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">CONSUL_HACK_NUM=$(grep -Pn <span class="string">&#x27;debug_run.+?go install&#x27;</span> build-support/<span class="built_in">functions</span>/20-build.sh | cut -d: -f 1)</span><br><span class="line"></span><br><span class="line">sed -i <span class="string">&quot;$[CONSUL_HACK_NUM-1]r /tmp/hack_consul.sh&quot;</span> build-support/<span class="built_in">functions</span>/20-build.sh</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="继续编译"><a href="#继续编译" class="headerlink" title="继续编译"></a>继续编译</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ make consul-docker</span><br><span class="line">Building Golang build container</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/5 : ARG GOLANG_VERSION=1.14.11</span><br><span class="line">Step 2/5 : FROM golang:$&#123;GOLANG_VERSION&#125;</span><br><span class="line"> ---&gt; f93db70cba35</span><br><span class="line">Step 3/5 : ARG GOTOOLS=&quot;github.com/elazarl/go-bindata-assetfs/...    github.com/hashicorp/go-bindata/...    golang.org/x/tools/cmd/cover    golang.org/x/tools/cmd/stringer    github.com/axw/gocov/gocov    gopkg.in/matm/v1/gocov-html&quot;</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; e0979c09e72f</span><br><span class="line">Step 4/5 : RUN GO111MODULE=on go get -v $&#123;GOTOOLS&#125; &amp;&amp; mkdir -p /consul</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 9cb7abf49b86</span><br><span class="line">Step 5/5 : WORKDIR /consul</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; fb1467e4623c</span><br><span class="line">Successfully built fb1467e4623c</span><br><span class="line">Successfully tagged consul-build-go:latest</span><br><span class="line">==&gt; Building Consul</span><br><span class="line">Ensuring Go modules are up to date</span><br><span class="line">Creating the Go Build Container with image: consul-build-go</span><br><span class="line">Copying the source from &#x27;/root/go/src/github.com/hashicorp/consul&#x27; to /consul</span><br><span class="line">Running build in container</span><br><span class="line">==&gt; Building Consul - OSes: linux, Architectures: mips64le</span><br><span class="line">Building sequentially with go install</span><br><span class="line">---&gt;   linux/mips64le</span><br><span class="line">Copying back artifacts</span><br><span class="line"></span><br><span class="line">$ ls -l pkg/bin/linux_mips64le/</span><br><span class="line">total 112192</span><br><span class="line">-rwxr-xr-x 1 root root 114881061 Jul 16 12:34 consul</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>拷贝到龙芯机器上测试下可否运行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ lscpu</span><br><span class="line">Architecture:          mips64el</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                8</span><br><span class="line">On-line CPU(s) list:   0-7</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    4</span><br><span class="line">座：                 2</span><br><span class="line">NUMA 节点：         2</span><br><span class="line">型号名称：        Loongson-3A R4 (Loongson-3B4000)</span><br><span class="line">CPU max MHz:           1800.0000</span><br><span class="line">CPU min MHz:           900.0000</span><br><span class="line">BogoMIPS：            3594.02</span><br><span class="line">L1d 缓存：          64K</span><br><span class="line">L1i 缓存：          64K</span><br><span class="line">L2 缓存：           256K</span><br><span class="line">L3 缓存：           2048K</span><br><span class="line">NUMA 节点0 CPU：    0-3</span><br><span class="line">NUMA 节点1 CPU：    4-7</span><br><span class="line">$ /tmp/consul --version</span><br><span class="line">Consul v1.8.14</span><br><span class="line">Revision 94c1bdd3b+CHANGES</span><br><span class="line">Protocol 2 spoken by default, understands 2 to 3 (agent will automatically use protocol &gt;2 when speaking to compatible agents)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/boltdb/bolt/issues/656">https://github.com/boltdb/bolt/issues/656</a></li><li><a href="https://github.com/boltdb/bolt/pull/663/files">https://github.com/boltdb/bolt/pull/663/files</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;编译&quot;&gt;&lt;a href=&quot;#编译&quot; class=&quot;headerlink&quot; title=&quot;编译&quot;&gt;&lt;/a&gt;编译&lt;/h2&gt;&lt;p&gt;建议使用容器编译，否则建议 clone 进 GOPATH 里&lt;/p&gt;
&lt;h3 id=&quot;clone&quot;&gt;&lt;a href=&quot;#clone&quot; cla</summary>
      
    
    
    
    
    <category term="consul" scheme="http://zhangguanzhang.github.io/tags/consul/"/>
    
    <category term="mips64le" scheme="http://zhangguanzhang.github.io/tags/mips64le/"/>
    
  </entry>
  
  <entry>
    <title>机器重启后 kube-apiserver 无法启动，etcd刷(error &quot;EOF&quot;, ServerName &quot;&quot;)</title>
    <link href="http://zhangguanzhang.github.io/2021/07/06/kube-apiserver-cannot-start-after-reboot/"/>
    <id>http://zhangguanzhang.github.io/2021/07/06/kube-apiserver-cannot-start-after-reboot/</id>
    <published>2021-07-06T14:46:06.000Z</published>
    <updated>2021-07-06T14:46:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>三个 master （etcd 也在 master 上，master上也有 kubelet）和 n 个 node。master 上组件(kube-controller-manager,kube-scheduler,kubelet)的 apiserver 的ip 都是 127.0.0.1:6443。kube-apiserver的 etcd 地址写了三个 etcd 的。k8s 版本为 <code>v1.15.5</code></p><h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>93 这台 master 机器重启后，发现 93 节点 <code>NotReady</code>，上去看了下 kubelet 无法连上本机的 kube-apiserver。kube-apiserver 运行个十几秒后才退出，<code>etcd</code> 一直刷如下日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">etcd: rejected connection from &quot;10.129.173.93:47566&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47614&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47714&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47874&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br><span class="line">etcd: rejected connection from &quot;10.129.173.93:47948&quot; (error &quot;EOF&quot;, &quot;ServerName &quot;&quot;)</span><br></pre></td></tr></table></figure><h2 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h2><h3 id="etcd的错误"><a href="#etcd的错误" class="headerlink" title="etcd的错误"></a>etcd的错误</h3><p>这个 EOF 是 etcd 的客户端没正常关闭造成的，etcd 之间也会互相连，先查看下 etcd 状态，因为 k8s 版本是 <code>v1.15.5</code>。默认使用的 etcd v3 api。etcd 的<code>--listen-client-urls</code>里我们包含了一个<code>http://127.0.0.1:2379</code>。所以下面命令不需要带<code>--endpoints=xxxx</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> ETCDCTL_API=3</span><br><span class="line">$ etcdctl member list</span><br><span class="line">172cf33e1b47c1c8, started, etcd2, https://10.129.173.93:2380, https://10.129.173.93:2379</span><br><span class="line">35f3e39e5dd3195e, started, etcd3, https://10.129.173.94:2380, https://10.129.173.94:2379</span><br><span class="line">47a96a577fe753d1, started, etcd1, https://10.129.173.92:2380, https://10.129.173.92:2379</span><br></pre></td></tr></table></figure><p>看下 dbSize，推荐使用 <code>--write-out=table</code> 看，会自动换算人性化的 size。如果需要压缩的 revision 推荐使用<code>-w=json</code> 来获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl endpoint status -w&#x3D;table</span><br></pre></td></tr></table></figure><p>看了下没达到默认的 2G，如果满了需要压缩执行下面的命令。每台的 revision 不同，每台都要执行下面的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 压缩旧版本</span><br><span class="line">etcdctl compact $revision</span><br><span class="line"># 清理碎片</span><br><span class="line">etcdctl defrag</span><br><span class="line"># 忽略告警</span><br><span class="line">etcdctl alarm disarm</span><br></pre></td></tr></table></figure><p>确认 etcd 没有问题，然后其他两个机器的 kube-apiserver 都正常，说明触发 EOF 的是 93 这台上面的 kube-apiserver。停止它后手动前台下。</p><h3 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop kube-apiserver</span><br><span class="line">$ systemctl cat kube-apiserver</span><br></pre></td></tr></table></figure><p>然后终端上用<code>ExecStart</code>的部分，把<code>--v=2</code>改成<code>--v=5</code>启动，下面是输出。全部放出来，方便其他遇到的人搜到这篇文章。太长了，我就先放到最后吧。</p><p>启动后 34 秒后刷了一堆<code>Get https://localhost:6443/apis/xxxxxxx: dial tcp 127.0.0.1:6443: i/o timeout</code> 就退出了。参照之前的<a href="https://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/">文章</a>查了下进程，确认没有安全软件安全狗。</p><p>除去 panic 和明确的 error 和 flag 相关的报错，k8s 相关的二进制无法启动基本是和 ipv6 有关系。查看下 ipv6 情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sysctl -a |&amp; grep -E <span class="string">&#x27;(all|default)\.disable_ipv6&#x27;</span></span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br></pre></td></tr></table></figure><p>无论是显示的关还是开，反转下这俩参数，然后再试试发现启动后不会退出了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv6.conf.all.disable_ipv6=0</span><br><span class="line">sysctl -w net.ipv6.conf.default.disable_ipv6=0</span><br></pre></td></tr></table></figure><p>然后参数固化下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;&gt; &#x2F;etc&#x2F;sysctl.conf &lt;&lt;EOF</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 &#x3D; 0</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 &#x3D; 0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="启动异常时候的日志"><a href="#启动异常时候的日志" class="headerlink" title="启动异常时候的日志"></a>启动异常时候的日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br><span class="line">923</span><br><span class="line">924</span><br><span class="line">925</span><br><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br><span class="line">936</span><br><span class="line">937</span><br><span class="line">938</span><br><span class="line">939</span><br><span class="line">940</span><br><span class="line">941</span><br><span class="line">942</span><br><span class="line">943</span><br><span class="line">944</span><br><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br><span class="line">954</span><br><span class="line">955</span><br><span class="line">956</span><br><span class="line">957</span><br><span class="line">958</span><br><span class="line">959</span><br><span class="line">960</span><br><span class="line">961</span><br><span class="line">962</span><br><span class="line">963</span><br><span class="line">964</span><br><span class="line">965</span><br><span class="line">966</span><br><span class="line">967</span><br><span class="line">968</span><br><span class="line">969</span><br><span class="line">970</span><br><span class="line">971</span><br><span class="line">972</span><br><span class="line">973</span><br><span class="line">974</span><br><span class="line">975</span><br><span class="line">976</span><br><span class="line">977</span><br><span class="line">978</span><br><span class="line">979</span><br><span class="line">980</span><br><span class="line">981</span><br><span class="line">982</span><br><span class="line">983</span><br><span class="line">984</span><br><span class="line">985</span><br><span class="line">986</span><br><span class="line">987</span><br><span class="line">988</span><br><span class="line">989</span><br><span class="line">990</span><br><span class="line">991</span><br><span class="line">992</span><br><span class="line">993</span><br><span class="line">994</span><br><span class="line">995</span><br><span class="line">996</span><br><span class="line">997</span><br><span class="line">998</span><br><span class="line">999</span><br><span class="line">1000</span><br><span class="line">1001</span><br><span class="line">1002</span><br><span class="line">1003</span><br><span class="line">1004</span><br><span class="line">1005</span><br><span class="line">1006</span><br><span class="line">1007</span><br><span class="line">1008</span><br><span class="line">1009</span><br><span class="line">1010</span><br><span class="line">1011</span><br><span class="line">1012</span><br><span class="line">1013</span><br><span class="line">1014</span><br><span class="line">1015</span><br><span class="line">1016</span><br><span class="line">1017</span><br><span class="line">1018</span><br><span class="line">1019</span><br><span class="line">1020</span><br><span class="line">1021</span><br><span class="line">1022</span><br><span class="line">1023</span><br><span class="line">1024</span><br><span class="line">1025</span><br><span class="line">1026</span><br><span class="line">1027</span><br><span class="line">1028</span><br><span class="line">1029</span><br><span class="line">1030</span><br><span class="line">1031</span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">1041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line">1051</span><br><span class="line">1052</span><br><span class="line">1053</span><br><span class="line">1054</span><br><span class="line">1055</span><br><span class="line">1056</span><br><span class="line">1057</span><br><span class="line">1058</span><br><span class="line">1059</span><br><span class="line">1060</span><br><span class="line">1061</span><br><span class="line">1062</span><br><span class="line">1063</span><br><span class="line">1064</span><br><span class="line">1065</span><br><span class="line">1066</span><br><span class="line">1067</span><br><span class="line">1068</span><br><span class="line">1069</span><br><span class="line">1070</span><br><span class="line">1071</span><br><span class="line">1072</span><br><span class="line">1073</span><br><span class="line">1074</span><br><span class="line">1075</span><br><span class="line">1076</span><br><span class="line">1077</span><br><span class="line">1078</span><br><span class="line">1079</span><br><span class="line">1080</span><br><span class="line">1081</span><br><span class="line">1082</span><br><span class="line">1083</span><br><span class="line">1084</span><br><span class="line">1085</span><br><span class="line">1086</span><br><span class="line">1087</span><br><span class="line">1088</span><br><span class="line">1089</span><br><span class="line">1090</span><br><span class="line">1091</span><br><span class="line">1092</span><br><span class="line">1093</span><br><span class="line">1094</span><br><span class="line">1095</span><br><span class="line">1096</span><br><span class="line">1097</span><br><span class="line">1098</span><br><span class="line">1099</span><br><span class="line">1100</span><br><span class="line">1101</span><br><span class="line">1102</span><br><span class="line">1103</span><br><span class="line">1104</span><br><span class="line">1105</span><br><span class="line">1106</span><br><span class="line">1107</span><br><span class="line">1108</span><br><span class="line">1109</span><br><span class="line">1110</span><br><span class="line">1111</span><br><span class="line">1112</span><br><span class="line">1113</span><br><span class="line">1114</span><br><span class="line">1115</span><br><span class="line">1116</span><br><span class="line">1117</span><br><span class="line">1118</span><br><span class="line">1119</span><br><span class="line">1120</span><br><span class="line">1121</span><br><span class="line">1122</span><br><span class="line">1123</span><br><span class="line">1124</span><br><span class="line">1125</span><br><span class="line">1126</span><br><span class="line">1127</span><br><span class="line">1128</span><br><span class="line">1129</span><br><span class="line">1130</span><br><span class="line">1131</span><br><span class="line">1132</span><br><span class="line">1133</span><br><span class="line">1134</span><br><span class="line">1135</span><br><span class="line">1136</span><br><span class="line">1137</span><br><span class="line">1138</span><br><span class="line">1139</span><br><span class="line">1140</span><br><span class="line">1141</span><br><span class="line">1142</span><br><span class="line">1143</span><br><span class="line">1144</span><br><span class="line">1145</span><br><span class="line">1146</span><br><span class="line">1147</span><br><span class="line">1148</span><br><span class="line">1149</span><br><span class="line">1150</span><br><span class="line">1151</span><br><span class="line">1152</span><br><span class="line">1153</span><br><span class="line">1154</span><br><span class="line">1155</span><br><span class="line">1156</span><br><span class="line">1157</span><br><span class="line">1158</span><br><span class="line">1159</span><br><span class="line">1160</span><br><span class="line">1161</span><br><span class="line">1162</span><br><span class="line">1163</span><br><span class="line">1164</span><br><span class="line">1165</span><br><span class="line">1166</span><br><span class="line">1167</span><br><span class="line">1168</span><br><span class="line">1169</span><br><span class="line">1170</span><br><span class="line">1171</span><br><span class="line">1172</span><br><span class="line">1173</span><br><span class="line">1174</span><br><span class="line">1175</span><br><span class="line">1176</span><br><span class="line">1177</span><br><span class="line">1178</span><br><span class="line">1179</span><br><span class="line">1180</span><br><span class="line">1181</span><br><span class="line">1182</span><br><span class="line">1183</span><br><span class="line">1184</span><br><span class="line">1185</span><br><span class="line">1186</span><br><span class="line">1187</span><br><span class="line">1188</span><br><span class="line">1189</span><br><span class="line">1190</span><br><span class="line">1191</span><br><span class="line">1192</span><br><span class="line">1193</span><br><span class="line">1194</span><br><span class="line">1195</span><br><span class="line">1196</span><br><span class="line">1197</span><br><span class="line">1198</span><br><span class="line">1199</span><br><span class="line">1200</span><br><span class="line">1201</span><br><span class="line">1202</span><br><span class="line">1203</span><br><span class="line">1204</span><br><span class="line">1205</span><br><span class="line">1206</span><br><span class="line">1207</span><br><span class="line">1208</span><br><span class="line">1209</span><br><span class="line">1210</span><br><span class="line">1211</span><br><span class="line">1212</span><br><span class="line">1213</span><br><span class="line">1214</span><br><span class="line">1215</span><br><span class="line">1216</span><br><span class="line">1217</span><br><span class="line">1218</span><br><span class="line">1219</span><br><span class="line">1220</span><br><span class="line">1221</span><br><span class="line">1222</span><br><span class="line">1223</span><br><span class="line">1224</span><br><span class="line">1225</span><br><span class="line">1226</span><br><span class="line">1227</span><br><span class="line">1228</span><br><span class="line">1229</span><br><span class="line">1230</span><br><span class="line">1231</span><br><span class="line">1232</span><br><span class="line">1233</span><br><span class="line">1234</span><br><span class="line">1235</span><br><span class="line">1236</span><br><span class="line">1237</span><br><span class="line">1238</span><br><span class="line">1239</span><br><span class="line">1240</span><br><span class="line">1241</span><br><span class="line">1242</span><br><span class="line">1243</span><br><span class="line">1244</span><br><span class="line">1245</span><br><span class="line">1246</span><br><span class="line">1247</span><br><span class="line">1248</span><br><span class="line">1249</span><br><span class="line">1250</span><br><span class="line">1251</span><br><span class="line">1252</span><br><span class="line">1253</span><br><span class="line">1254</span><br><span class="line">1255</span><br><span class="line">1256</span><br><span class="line">1257</span><br><span class="line">1258</span><br><span class="line">1259</span><br><span class="line">1260</span><br><span class="line">1261</span><br><span class="line">1262</span><br><span class="line">1263</span><br><span class="line">1264</span><br><span class="line">1265</span><br><span class="line">1266</span><br><span class="line">1267</span><br><span class="line">1268</span><br><span class="line">1269</span><br><span class="line">1270</span><br><span class="line">1271</span><br><span class="line">1272</span><br><span class="line">1273</span><br><span class="line">1274</span><br><span class="line">1275</span><br><span class="line">1276</span><br><span class="line">1277</span><br><span class="line">1278</span><br><span class="line">1279</span><br><span class="line">1280</span><br><span class="line">1281</span><br><span class="line">1282</span><br><span class="line">1283</span><br><span class="line">1284</span><br><span class="line">1285</span><br><span class="line">1286</span><br><span class="line">1287</span><br><span class="line">1288</span><br><span class="line">1289</span><br><span class="line">1290</span><br><span class="line">1291</span><br><span class="line">1292</span><br><span class="line">1293</span><br><span class="line">1294</span><br><span class="line">1295</span><br><span class="line">1296</span><br><span class="line">1297</span><br><span class="line">1298</span><br><span class="line">1299</span><br><span class="line">1300</span><br><span class="line">1301</span><br><span class="line">1302</span><br><span class="line">1303</span><br><span class="line">1304</span><br><span class="line">1305</span><br><span class="line">1306</span><br><span class="line">1307</span><br><span class="line">1308</span><br><span class="line">1309</span><br><span class="line">1310</span><br><span class="line">1311</span><br><span class="line">1312</span><br><span class="line">1313</span><br><span class="line">1314</span><br><span class="line">1315</span><br><span class="line">1316</span><br><span class="line">1317</span><br><span class="line">1318</span><br><span class="line">1319</span><br><span class="line">1320</span><br><span class="line">1321</span><br><span class="line">1322</span><br><span class="line">1323</span><br><span class="line">1324</span><br><span class="line">1325</span><br><span class="line">1326</span><br><span class="line">1327</span><br><span class="line">1328</span><br><span class="line">1329</span><br><span class="line">1330</span><br><span class="line">1331</span><br><span class="line">1332</span><br><span class="line">1333</span><br><span class="line">1334</span><br><span class="line">1335</span><br><span class="line">1336</span><br><span class="line">1337</span><br><span class="line">1338</span><br><span class="line">1339</span><br><span class="line">1340</span><br><span class="line">1341</span><br><span class="line">1342</span><br><span class="line">1343</span><br><span class="line">1344</span><br><span class="line">1345</span><br><span class="line">1346</span><br><span class="line">1347</span><br><span class="line">1348</span><br><span class="line">1349</span><br><span class="line">1350</span><br><span class="line">1351</span><br><span class="line">1352</span><br><span class="line">1353</span><br><span class="line">1354</span><br><span class="line">1355</span><br><span class="line">1356</span><br><span class="line">1357</span><br><span class="line">1358</span><br><span class="line">1359</span><br><span class="line">1360</span><br><span class="line">1361</span><br><span class="line">1362</span><br><span class="line">1363</span><br><span class="line">1364</span><br><span class="line">1365</span><br><span class="line">1366</span><br><span class="line">1367</span><br><span class="line">1368</span><br><span class="line">1369</span><br><span class="line">1370</span><br><span class="line">1371</span><br><span class="line">1372</span><br><span class="line">1373</span><br><span class="line">1374</span><br><span class="line">1375</span><br><span class="line">1376</span><br><span class="line">1377</span><br><span class="line">1378</span><br><span class="line">1379</span><br><span class="line">1380</span><br><span class="line">1381</span><br><span class="line">1382</span><br><span class="line">1383</span><br><span class="line">1384</span><br><span class="line">1385</span><br><span class="line">1386</span><br><span class="line">1387</span><br><span class="line">1388</span><br><span class="line">1389</span><br><span class="line">1390</span><br><span class="line">1391</span><br><span class="line">1392</span><br><span class="line">1393</span><br><span class="line">1394</span><br><span class="line">1395</span><br><span class="line">1396</span><br><span class="line">1397</span><br><span class="line">1398</span><br><span class="line">1399</span><br><span class="line">1400</span><br><span class="line">1401</span><br><span class="line">1402</span><br><span class="line">1403</span><br><span class="line">1404</span><br><span class="line">1405</span><br><span class="line">1406</span><br><span class="line">1407</span><br><span class="line">1408</span><br><span class="line">1409</span><br><span class="line">1410</span><br><span class="line">1411</span><br><span class="line">1412</span><br><span class="line">1413</span><br><span class="line">1414</span><br><span class="line">1415</span><br><span class="line">1416</span><br><span class="line">1417</span><br><span class="line">1418</span><br><span class="line">1419</span><br><span class="line">1420</span><br><span class="line">1421</span><br><span class="line">1422</span><br><span class="line">1423</span><br><span class="line">1424</span><br><span class="line">1425</span><br><span class="line">1426</span><br><span class="line">1427</span><br><span class="line">1428</span><br><span class="line">1429</span><br><span class="line">1430</span><br><span class="line">1431</span><br><span class="line">1432</span><br><span class="line">1433</span><br><span class="line">1434</span><br><span class="line">1435</span><br><span class="line">1436</span><br><span class="line">1437</span><br><span class="line">1438</span><br><span class="line">1439</span><br><span class="line">1440</span><br><span class="line">1441</span><br><span class="line">1442</span><br><span class="line">1443</span><br><span class="line">1444</span><br><span class="line">1445</span><br><span class="line">1446</span><br><span class="line">1447</span><br><span class="line">1448</span><br><span class="line">1449</span><br><span class="line">1450</span><br><span class="line">1451</span><br><span class="line">1452</span><br><span class="line">1453</span><br><span class="line">1454</span><br><span class="line">1455</span><br><span class="line">1456</span><br><span class="line">1457</span><br><span class="line">1458</span><br><span class="line">1459</span><br><span class="line">1460</span><br><span class="line">1461</span><br><span class="line">1462</span><br><span class="line">1463</span><br><span class="line">1464</span><br><span class="line">1465</span><br><span class="line">1466</span><br><span class="line">1467</span><br><span class="line">1468</span><br><span class="line">1469</span><br><span class="line">1470</span><br><span class="line">1471</span><br><span class="line">1472</span><br><span class="line">1473</span><br><span class="line">1474</span><br><span class="line">1475</span><br><span class="line">1476</span><br><span class="line">1477</span><br><span class="line">1478</span><br><span class="line">1479</span><br><span class="line">1480</span><br><span class="line">1481</span><br><span class="line">1482</span><br><span class="line">1483</span><br><span class="line">1484</span><br><span class="line">1485</span><br><span class="line">1486</span><br><span class="line">1487</span><br><span class="line">1488</span><br><span class="line">1489</span><br><span class="line">1490</span><br><span class="line">1491</span><br><span class="line">1492</span><br><span class="line">1493</span><br><span class="line">1494</span><br><span class="line">1495</span><br><span class="line">1496</span><br><span class="line">1497</span><br><span class="line">1498</span><br><span class="line">1499</span><br><span class="line">1500</span><br><span class="line">1501</span><br><span class="line">1502</span><br><span class="line">1503</span><br><span class="line">1504</span><br><span class="line">1505</span><br><span class="line">1506</span><br><span class="line">1507</span><br><span class="line">1508</span><br><span class="line">1509</span><br><span class="line">1510</span><br><span class="line">1511</span><br><span class="line">1512</span><br><span class="line">1513</span><br><span class="line">1514</span><br><span class="line">1515</span><br><span class="line">1516</span><br><span class="line">1517</span><br><span class="line">1518</span><br><span class="line">1519</span><br><span class="line">1520</span><br><span class="line">1521</span><br><span class="line">1522</span><br><span class="line">1523</span><br><span class="line">1524</span><br><span class="line">1525</span><br><span class="line">1526</span><br><span class="line">1527</span><br><span class="line">1528</span><br><span class="line">1529</span><br><span class="line">1530</span><br><span class="line">1531</span><br><span class="line">1532</span><br><span class="line">1533</span><br><span class="line">1534</span><br><span class="line">1535</span><br><span class="line">1536</span><br><span class="line">1537</span><br><span class="line">1538</span><br><span class="line">1539</span><br><span class="line">1540</span><br><span class="line">1541</span><br><span class="line">1542</span><br><span class="line">1543</span><br><span class="line">1544</span><br><span class="line">1545</span><br><span class="line">1546</span><br><span class="line">1547</span><br><span class="line">1548</span><br><span class="line">1549</span><br><span class="line">1550</span><br><span class="line">1551</span><br><span class="line">1552</span><br><span class="line">1553</span><br><span class="line">1554</span><br><span class="line">1555</span><br><span class="line">1556</span><br><span class="line">1557</span><br><span class="line">1558</span><br><span class="line">1559</span><br><span class="line">1560</span><br><span class="line">1561</span><br><span class="line">1562</span><br><span class="line">1563</span><br><span class="line">1564</span><br><span class="line">1565</span><br><span class="line">1566</span><br><span class="line">1567</span><br><span class="line">1568</span><br><span class="line">1569</span><br><span class="line">1570</span><br><span class="line">1571</span><br><span class="line">1572</span><br><span class="line">1573</span><br><span class="line">1574</span><br><span class="line">1575</span><br><span class="line">1576</span><br><span class="line">1577</span><br><span class="line">1578</span><br><span class="line">1579</span><br><span class="line">1580</span><br><span class="line">1581</span><br><span class="line">1582</span><br><span class="line">1583</span><br><span class="line">1584</span><br><span class="line">1585</span><br><span class="line">1586</span><br><span class="line">1587</span><br><span class="line">1588</span><br><span class="line">1589</span><br><span class="line">1590</span><br><span class="line">1591</span><br><span class="line">1592</span><br><span class="line">1593</span><br><span class="line">1594</span><br><span class="line">1595</span><br><span class="line">1596</span><br><span class="line">1597</span><br><span class="line">1598</span><br><span class="line">1599</span><br><span class="line">1600</span><br><span class="line">1601</span><br><span class="line">1602</span><br><span class="line">1603</span><br><span class="line">1604</span><br><span class="line">1605</span><br><span class="line">1606</span><br><span class="line">1607</span><br><span class="line">1608</span><br><span class="line">1609</span><br><span class="line">1610</span><br><span class="line">1611</span><br><span class="line">1612</span><br><span class="line">1613</span><br><span class="line">1614</span><br><span class="line">1615</span><br><span class="line">1616</span><br><span class="line">1617</span><br><span class="line">1618</span><br><span class="line">1619</span><br><span class="line">1620</span><br><span class="line">1621</span><br><span class="line">1622</span><br><span class="line">1623</span><br><span class="line">1624</span><br><span class="line">1625</span><br><span class="line">1626</span><br><span class="line">1627</span><br><span class="line">1628</span><br><span class="line">1629</span><br><span class="line">1630</span><br><span class="line">1631</span><br><span class="line">1632</span><br><span class="line">1633</span><br><span class="line">1634</span><br><span class="line">1635</span><br><span class="line">1636</span><br><span class="line">1637</span><br><span class="line">1638</span><br><span class="line">1639</span><br><span class="line">1640</span><br><span class="line">1641</span><br><span class="line">1642</span><br><span class="line">1643</span><br><span class="line">1644</span><br><span class="line">1645</span><br><span class="line">1646</span><br><span class="line">1647</span><br><span class="line">1648</span><br><span class="line">1649</span><br><span class="line">1650</span><br><span class="line">1651</span><br><span class="line">1652</span><br><span class="line">1653</span><br><span class="line">1654</span><br><span class="line">1655</span><br><span class="line">1656</span><br><span class="line">1657</span><br><span class="line">1658</span><br><span class="line">1659</span><br><span class="line">1660</span><br><span class="line">1661</span><br><span class="line">1662</span><br><span class="line">1663</span><br><span class="line">1664</span><br><span class="line">1665</span><br><span class="line">1666</span><br><span class="line">1667</span><br><span class="line">1668</span><br><span class="line">1669</span><br><span class="line">1670</span><br><span class="line">1671</span><br><span class="line">1672</span><br><span class="line">1673</span><br><span class="line">1674</span><br><span class="line">1675</span><br><span class="line">1676</span><br><span class="line">1677</span><br><span class="line">1678</span><br><span class="line">1679</span><br><span class="line">1680</span><br><span class="line">1681</span><br><span class="line">1682</span><br><span class="line">1683</span><br><span class="line">1684</span><br><span class="line">1685</span><br><span class="line">1686</span><br><span class="line">1687</span><br><span class="line">1688</span><br><span class="line">1689</span><br><span class="line">1690</span><br><span class="line">1691</span><br><span class="line">1692</span><br><span class="line">1693</span><br><span class="line">1694</span><br><span class="line">1695</span><br><span class="line">1696</span><br><span class="line">1697</span><br><span class="line">1698</span><br><span class="line">1699</span><br><span class="line">1700</span><br><span class="line">1701</span><br><span class="line">1702</span><br><span class="line">1703</span><br><span class="line">1704</span><br><span class="line">1705</span><br><span class="line">1706</span><br><span class="line">1707</span><br><span class="line">1708</span><br><span class="line">1709</span><br><span class="line">1710</span><br><span class="line">1711</span><br><span class="line">1712</span><br><span class="line">1713</span><br><span class="line">1714</span><br><span class="line">1715</span><br><span class="line">1716</span><br><span class="line">1717</span><br><span class="line">1718</span><br><span class="line">1719</span><br><span class="line">1720</span><br><span class="line">1721</span><br><span class="line">1722</span><br><span class="line">1723</span><br><span class="line">1724</span><br><span class="line">1725</span><br><span class="line">1726</span><br><span class="line">1727</span><br><span class="line">1728</span><br><span class="line">1729</span><br><span class="line">1730</span><br><span class="line">1731</span><br><span class="line">1732</span><br><span class="line">1733</span><br><span class="line">1734</span><br><span class="line">1735</span><br><span class="line">1736</span><br><span class="line">1737</span><br><span class="line">1738</span><br><span class="line">1739</span><br><span class="line">1740</span><br><span class="line">1741</span><br><span class="line">1742</span><br><span class="line">1743</span><br><span class="line">1744</span><br><span class="line">1745</span><br><span class="line">1746</span><br><span class="line">1747</span><br><span class="line">1748</span><br><span class="line">1749</span><br><span class="line">1750</span><br><span class="line">1751</span><br><span class="line">1752</span><br><span class="line">1753</span><br><span class="line">1754</span><br><span class="line">1755</span><br><span class="line">1756</span><br><span class="line">1757</span><br><span class="line">1758</span><br><span class="line">1759</span><br><span class="line">1760</span><br><span class="line">1761</span><br><span class="line">1762</span><br><span class="line">1763</span><br><span class="line">1764</span><br><span class="line">1765</span><br><span class="line">1766</span><br><span class="line">1767</span><br><span class="line">1768</span><br><span class="line">1769</span><br><span class="line">1770</span><br><span class="line">1771</span><br></pre></td><td class="code"><pre><span class="line">Flag --admission-control has been deprecated, Use --enable-admission-plugins or --disable-admission-plugins instead. Will be removed in a future version.</span><br><span class="line">Flag --insecure-port has been deprecated, This flag will be removed in a future version.</span><br><span class="line">Flag --insecure-bind-address has been deprecated, This flag will be removed in a future version.</span><br><span class="line">Flag --enable-swagger-ui has been deprecated, swagger 1.2 support has been removed</span><br><span class="line">I0705 22:38:00.836860   18145 flags.go:33] FLAG: --address&#x3D;&quot;127.0.0.1&quot;</span><br><span class="line">I0705 22:38:00.836914   18145 flags.go:33] FLAG: --admission-control&#x3D;&quot;[NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction]&quot;</span><br><span class="line">I0705 22:38:00.836926   18145 flags.go:33] FLAG: --admission-control-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.836932   18145 flags.go:33] FLAG: --advertise-address&#x3D;&quot;&lt;nil&gt;&quot;</span><br><span class="line">I0705 22:38:00.836936   18145 flags.go:33] FLAG: --allow-privileged&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.836942   18145 flags.go:33] FLAG: --alsologtostderr&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836948   18145 flags.go:33] FLAG: --anonymous-auth&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836952   18145 flags.go:33] FLAG: --api-audiences&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.836959   18145 flags.go:33] FLAG: --apiserver-count&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.836965   18145 flags.go:33] FLAG: --audit-dynamic-configuration&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836969   18145 flags.go:33] FLAG: --audit-log-batch-buffer-size&#x3D;&quot;10000&quot;</span><br><span class="line">I0705 22:38:00.836973   18145 flags.go:33] FLAG: --audit-log-batch-max-size&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.836977   18145 flags.go:33] FLAG: --audit-log-batch-max-wait&#x3D;&quot;0s&quot;</span><br><span class="line">I0705 22:38:00.836982   18145 flags.go:33] FLAG: --audit-log-batch-throttle-burst&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.836986   18145 flags.go:33] FLAG: --audit-log-batch-throttle-enable&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.836990   18145 flags.go:33] FLAG: --audit-log-batch-throttle-qps&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.836995   18145 flags.go:33] FLAG: --audit-log-format&#x3D;&quot;json&quot;</span><br><span class="line">I0705 22:38:00.837000   18145 flags.go:33] FLAG: --audit-log-maxage&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837004   18145 flags.go:33] FLAG: --audit-log-maxbackup&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837008   18145 flags.go:33] FLAG: --audit-log-maxsize&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837011   18145 flags.go:33] FLAG: --audit-log-mode&#x3D;&quot;blocking&quot;</span><br><span class="line">I0705 22:38:00.837015   18145 flags.go:33] FLAG: --audit-log-path&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837019   18145 flags.go:33] FLAG: --audit-log-truncate-enabled&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837023   18145 flags.go:33] FLAG: --audit-log-truncate-max-batch-size&#x3D;&quot;10485760&quot;</span><br><span class="line">I0705 22:38:00.837029   18145 flags.go:33] FLAG: --audit-log-truncate-max-event-size&#x3D;&quot;102400&quot;</span><br><span class="line">I0705 22:38:00.837034   18145 flags.go:33] FLAG: --audit-log-version&#x3D;&quot;audit.k8s.io&#x2F;v1&quot;</span><br><span class="line">I0705 22:38:00.837038   18145 flags.go:33] FLAG: --audit-policy-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837042   18145 flags.go:33] FLAG: --audit-webhook-batch-buffer-size&#x3D;&quot;10000&quot;</span><br><span class="line">I0705 22:38:00.837046   18145 flags.go:33] FLAG: --audit-webhook-batch-initial-backoff&#x3D;&quot;10s&quot;</span><br><span class="line">I0705 22:38:00.837051   18145 flags.go:33] FLAG: --audit-webhook-batch-max-size&#x3D;&quot;400&quot;</span><br><span class="line">I0705 22:38:00.837055   18145 flags.go:33] FLAG: --audit-webhook-batch-max-wait&#x3D;&quot;30s&quot;</span><br><span class="line">I0705 22:38:00.837060   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-burst&#x3D;&quot;15&quot;</span><br><span class="line">I0705 22:38:00.837064   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-enable&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837068   18145 flags.go:33] FLAG: --audit-webhook-batch-throttle-qps&#x3D;&quot;10&quot;</span><br><span class="line">I0705 22:38:00.837073   18145 flags.go:33] FLAG: --audit-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837076   18145 flags.go:33] FLAG: --audit-webhook-initial-backoff&#x3D;&quot;10s&quot;</span><br><span class="line">I0705 22:38:00.837080   18145 flags.go:33] FLAG: --audit-webhook-mode&#x3D;&quot;batch&quot;</span><br><span class="line">I0705 22:38:00.837086   18145 flags.go:33] FLAG: --audit-webhook-truncate-enabled&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837090   18145 flags.go:33] FLAG: --audit-webhook-truncate-max-batch-size&#x3D;&quot;10485760&quot;</span><br><span class="line">I0705 22:38:00.837094   18145 flags.go:33] FLAG: --audit-webhook-truncate-max-event-size&#x3D;&quot;102400&quot;</span><br><span class="line">I0705 22:38:00.837098   18145 flags.go:33] FLAG: --audit-webhook-version&#x3D;&quot;audit.k8s.io&#x2F;v1&quot;</span><br><span class="line">I0705 22:38:00.837103   18145 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl&#x3D;&quot;2m0s&quot;</span><br><span class="line">I0705 22:38:00.837107   18145 flags.go:33] FLAG: --authentication-token-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837119   18145 flags.go:33] FLAG: --authorization-mode&#x3D;&quot;[Node,RBAC]&quot;</span><br><span class="line">I0705 22:38:00.837125   18145 flags.go:33] FLAG: --authorization-policy-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837128   18145 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl&#x3D;&quot;5m0s&quot;</span><br><span class="line">I0705 22:38:00.837132   18145 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl&#x3D;&quot;30s&quot;</span><br><span class="line">I0705 22:38:00.837136   18145 flags.go:33] FLAG: --authorization-webhook-config-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837140   18145 flags.go:33] FLAG: --basic-auth-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;basic-auth.csv&quot;</span><br><span class="line">I0705 22:38:00.837145   18145 flags.go:33] FLAG: --bind-address&#x3D;&quot;0.0.0.0&quot;</span><br><span class="line">I0705 22:38:00.837149   18145 flags.go:33] FLAG: --cert-dir&#x3D;&quot;&#x2F;var&#x2F;run&#x2F;kubernetes&quot;</span><br><span class="line">I0705 22:38:00.837153   18145 flags.go:33] FLAG: --client-ca-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837158   18145 flags.go:33] FLAG: --cloud-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837162   18145 flags.go:33] FLAG: --cloud-provider&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837165   18145 flags.go:33] FLAG: --cloud-provider-gce-lb-src-cidrs&#x3D;&quot;130.211.0.0&#x2F;22,209.85.152.0&#x2F;22,209.85.204.0&#x2F;22,35.191.0.0&#x2F;16&quot;</span><br><span class="line">I0705 22:38:00.837172   18145 flags.go:33] FLAG: --contention-profiling&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837176   18145 flags.go:33] FLAG: --cors-allowed-origins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837182   18145 flags.go:33] FLAG: --default-not-ready-toleration-seconds&#x3D;&quot;300&quot;</span><br><span class="line">I0705 22:38:00.837186   18145 flags.go:33] FLAG: --default-unreachable-toleration-seconds&#x3D;&quot;300&quot;</span><br><span class="line">I0705 22:38:00.837190   18145 flags.go:33] FLAG: --default-watch-cache-size&#x3D;&quot;100&quot;</span><br><span class="line">I0705 22:38:00.837194   18145 flags.go:33] FLAG: --delete-collection-workers&#x3D;&quot;1&quot;</span><br><span class="line">I0705 22:38:00.837198   18145 flags.go:33] FLAG: --deserialization-cache-size&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837202   18145 flags.go:33] FLAG: --disable-admission-plugins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837206   18145 flags.go:33] FLAG: --enable-admission-plugins&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837213   18145 flags.go:33] FLAG: --enable-aggregator-routing&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837217   18145 flags.go:33] FLAG: --enable-bootstrap-token-auth&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837221   18145 flags.go:33] FLAG: --enable-garbage-collector&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837225   18145 flags.go:33] FLAG: --enable-inflight-quota-handler&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837228   18145 flags.go:33] FLAG: --enable-logs-handler&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837232   18145 flags.go:33] FLAG: --enable-swagger-ui&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837236   18145 flags.go:33] FLAG: --encryption-provider-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837240   18145 flags.go:33] FLAG: --endpoint-reconciler-type&#x3D;&quot;lease&quot;</span><br><span class="line">I0705 22:38:00.837244   18145 flags.go:33] FLAG: --etcd-cafile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837248   18145 flags.go:33] FLAG: --etcd-certfile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837253   18145 flags.go:33] FLAG: --etcd-compaction-interval&#x3D;&quot;5m0s&quot;</span><br><span class="line">I0705 22:38:00.837257   18145 flags.go:33] FLAG: --etcd-count-metric-poll-period&#x3D;&quot;1m0s&quot;</span><br><span class="line">I0705 22:38:00.837261   18145 flags.go:33] FLAG: --etcd-keyfile&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837265   18145 flags.go:33] FLAG: --etcd-prefix&#x3D;&quot;&#x2F;registry&quot;</span><br><span class="line">I0705 22:38:00.837269   18145 flags.go:33] FLAG: --etcd-servers&#x3D;&quot;[https:&#x2F;&#x2F;10.129.173.92:2379,https:&#x2F;&#x2F;10.129.173.93:2379,https:&#x2F;&#x2F;10.129.173.94:2379]&quot;</span><br><span class="line">I0705 22:38:00.837277   18145 flags.go:33] FLAG: --etcd-servers-overrides&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837283   18145 flags.go:33] FLAG: --event-ttl&#x3D;&quot;1h0m0s&quot;</span><br><span class="line">I0705 22:38:00.837287   18145 flags.go:33] FLAG: --experimental-encryption-provider-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837291   18145 flags.go:33] FLAG: --external-hostname&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837296   18145 flags.go:33] FLAG: --feature-gates&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837303   18145 flags.go:33] FLAG: --help&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837307   18145 flags.go:33] FLAG: --http2-max-streams-per-connection&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837311   18145 flags.go:33] FLAG: --insecure-bind-address&#x3D;&quot;127.0.0.1&quot;</span><br><span class="line">I0705 22:38:00.837315   18145 flags.go:33] FLAG: --insecure-port&#x3D;&quot;8073&quot;</span><br><span class="line">I0705 22:38:00.837319   18145 flags.go:33] FLAG: --kubelet-certificate-authority&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837323   18145 flags.go:33] FLAG: --kubelet-client-certificate&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837331   18145 flags.go:33] FLAG: --kubelet-client-key&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837336   18145 flags.go:33] FLAG: --kubelet-https&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837340   18145 flags.go:33] FLAG: --kubelet-port&#x3D;&quot;10250&quot;</span><br><span class="line">I0705 22:38:00.837346   18145 flags.go:33] FLAG: --kubelet-preferred-address-types&#x3D;&quot;[Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]&quot;</span><br><span class="line">I0705 22:38:00.837352   18145 flags.go:33] FLAG: --kubelet-read-only-port&#x3D;&quot;10255&quot;</span><br><span class="line">I0705 22:38:00.837356   18145 flags.go:33] FLAG: --kubelet-timeout&#x3D;&quot;5s&quot;</span><br><span class="line">I0705 22:38:00.837360   18145 flags.go:33] FLAG: --kubernetes-service-node-port&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837364   18145 flags.go:33] FLAG: --log-backtrace-at&#x3D;&quot;:0&quot;</span><br><span class="line">I0705 22:38:00.837370   18145 flags.go:33] FLAG: --log-dir&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837375   18145 flags.go:33] FLAG: --log-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837379   18145 flags.go:33] FLAG: --log-file-max-size&#x3D;&quot;1800&quot;</span><br><span class="line">I0705 22:38:00.837383   18145 flags.go:33] FLAG: --log-flush-frequency&#x3D;&quot;5s&quot;</span><br><span class="line">I0705 22:38:00.837387   18145 flags.go:33] FLAG: --logtostderr&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837391   18145 flags.go:33] FLAG: --master-service-namespace&#x3D;&quot;default&quot;</span><br><span class="line">I0705 22:38:00.837395   18145 flags.go:33] FLAG: --max-connection-bytes-per-sec&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837399   18145 flags.go:33] FLAG: --max-mutating-requests-inflight&#x3D;&quot;200&quot;</span><br><span class="line">I0705 22:38:00.837403   18145 flags.go:33] FLAG: --max-requests-inflight&#x3D;&quot;400&quot;</span><br><span class="line">I0705 22:38:00.837407   18145 flags.go:33] FLAG: --min-request-timeout&#x3D;&quot;1800&quot;</span><br><span class="line">I0705 22:38:00.837411   18145 flags.go:33] FLAG: --oidc-ca-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837415   18145 flags.go:33] FLAG: --oidc-client-id&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837419   18145 flags.go:33] FLAG: --oidc-groups-claim&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837422   18145 flags.go:33] FLAG: --oidc-groups-prefix&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837426   18145 flags.go:33] FLAG: --oidc-issuer-url&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837430   18145 flags.go:33] FLAG: --oidc-required-claim&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837436   18145 flags.go:33] FLAG: --oidc-signing-algs&#x3D;&quot;[RS256]&quot;</span><br><span class="line">I0705 22:38:00.837443   18145 flags.go:33] FLAG: --oidc-username-claim&#x3D;&quot;sub&quot;</span><br><span class="line">I0705 22:38:00.837446   18145 flags.go:33] FLAG: --oidc-username-prefix&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837450   18145 flags.go:33] FLAG: --port&#x3D;&quot;8073&quot;</span><br><span class="line">I0705 22:38:00.837454   18145 flags.go:33] FLAG: --profiling&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837458   18145 flags.go:33] FLAG: --proxy-client-cert-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;aggregator-proxy.pem&quot;</span><br><span class="line">I0705 22:38:00.837463   18145 flags.go:33] FLAG: --proxy-client-key-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;aggregator-proxy-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837468   18145 flags.go:33] FLAG: --request-timeout&#x3D;&quot;1m0s&quot;</span><br><span class="line">I0705 22:38:00.837472   18145 flags.go:33] FLAG: --requestheader-allowed-names&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837476   18145 flags.go:33] FLAG: --requestheader-client-ca-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;</span><br><span class="line">I0705 22:38:00.837480   18145 flags.go:33] FLAG: --requestheader-extra-headers-prefix&#x3D;&quot;[X-Remote-Extra-]&quot;</span><br><span class="line">I0705 22:38:00.837487   18145 flags.go:33] FLAG: --requestheader-group-headers&#x3D;&quot;[X-Remote-Group]&quot;</span><br><span class="line">I0705 22:38:00.837492   18145 flags.go:33] FLAG: --requestheader-username-headers&#x3D;&quot;[X-Remote-User]&quot;</span><br><span class="line">I0705 22:38:00.837498   18145 flags.go:33] FLAG: --runtime-config&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837505   18145 flags.go:33] FLAG: --secure-port&#x3D;&quot;6443&quot;</span><br><span class="line">I0705 22:38:00.837510   18145 flags.go:33] FLAG: --service-account-api-audiences&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837515   18145 flags.go:33] FLAG: --service-account-issuer&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837518   18145 flags.go:33] FLAG: --service-account-key-file&#x3D;&quot;[&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca-key.pem]&quot;</span><br><span class="line">I0705 22:38:00.837526   18145 flags.go:33] FLAG: --service-account-lookup&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837530   18145 flags.go:33] FLAG: --service-account-max-token-expiration&#x3D;&quot;0s&quot;</span><br><span class="line">I0705 22:38:00.837534   18145 flags.go:33] FLAG: --service-account-signing-key-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837538   18145 flags.go:33] FLAG: --service-cluster-ip-range&#x3D;&quot;172.26.0.0&#x2F;16&quot;</span><br><span class="line">I0705 22:38:00.837544   18145 flags.go:33] FLAG: --service-node-port-range&#x3D;&quot;20000-40000&quot;</span><br><span class="line">I0705 22:38:00.837558   18145 flags.go:33] FLAG: --skip-headers&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837566   18145 flags.go:33] FLAG: --skip-log-headers&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837570   18145 flags.go:33] FLAG: --ssh-keyfile&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837574   18145 flags.go:33] FLAG: --ssh-user&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837578   18145 flags.go:33] FLAG: --stderrthreshold&#x3D;&quot;2&quot;</span><br><span class="line">I0705 22:38:00.837582   18145 flags.go:33] FLAG: --storage-backend&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837585   18145 flags.go:33] FLAG: --storage-media-type&#x3D;&quot;application&#x2F;vnd.kubernetes.protobuf&quot;</span><br><span class="line">I0705 22:38:00.837589   18145 flags.go:33] FLAG: --target-ram-mb&#x3D;&quot;0&quot;</span><br><span class="line">I0705 22:38:00.837607   18145 flags.go:33] FLAG: --tls-cert-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;</span><br><span class="line">I0705 22:38:00.837613   18145 flags.go:33] FLAG: --tls-cipher-suites&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837617   18145 flags.go:33] FLAG: --tls-min-version&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837621   18145 flags.go:33] FLAG: --tls-private-key-file&#x3D;&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;</span><br><span class="line">I0705 22:38:00.837626   18145 flags.go:33] FLAG: --tls-sni-cert-key&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837631   18145 flags.go:33] FLAG: --token-auth-file&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837635   18145 flags.go:33] FLAG: --v&#x3D;&quot;5&quot;</span><br><span class="line">I0705 22:38:00.837639   18145 flags.go:33] FLAG: --version&#x3D;&quot;false&quot;</span><br><span class="line">I0705 22:38:00.837645   18145 flags.go:33] FLAG: --vmodule&#x3D;&quot;&quot;</span><br><span class="line">I0705 22:38:00.837652   18145 flags.go:33] FLAG: --watch-cache&#x3D;&quot;true&quot;</span><br><span class="line">I0705 22:38:00.837656   18145 flags.go:33] FLAG: --watch-cache-sizes&#x3D;&quot;[]&quot;</span><br><span class="line">I0705 22:38:00.837838   18145 interface.go:384] Looking for default routes with IPv4 addresses</span><br><span class="line">I0705 22:38:00.837845   18145 interface.go:389] Default route transits interface &quot;eth0&quot;</span><br><span class="line">I0705 22:38:00.838072   18145 interface.go:196] Interface eth0 is up</span><br><span class="line">I0705 22:38:00.838128   18145 interface.go:244] Interface &quot;eth0&quot; has 1 addresses :[10.129.173.93&#x2F;22].</span><br><span class="line">I0705 22:38:00.838147   18145 interface.go:211] Checking addr  10.129.173.93&#x2F;22.</span><br><span class="line">I0705 22:38:00.838154   18145 interface.go:218] IP found 10.129.173.93</span><br><span class="line">I0705 22:38:00.838161   18145 interface.go:250] Found valid IPv4 address 10.129.173.93 for interface &quot;eth0&quot;.</span><br><span class="line">I0705 22:38:00.838167   18145 interface.go:395] Found active IP 10.129.173.93 </span><br><span class="line">I0705 22:38:00.838183   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">I0705 22:38:00.838191   18145 server.go:560] external host was not specified, using 10.129.173.93</span><br><span class="line">I0705 22:38:00.838200   18145 server.go:603] Initializing cache sizes based on 0MB limit</span><br><span class="line">I0705 22:38:00.838462   18145 server.go:147] Version: v1.15.5</span><br><span class="line">I0705 22:38:01.255999   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:01.256040   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:01.256057   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">E0705 22:38:01.256668   18145 prometheus.go:55] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256708   18145 prometheus.go:68] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256750   18145 prometheus.go:82] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256782   18145 prometheus.go:96] failed to register workDuration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256822   18145 prometheus.go:112] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256858   18145 prometheus.go:126] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256876   18145 prometheus.go:152] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256902   18145 prometheus.go:164] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.256976   18145 prometheus.go:176] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257022   18145 prometheus.go:188] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257057   18145 prometheus.go:203] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:01.257086   18145 prometheus.go:216] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">I0705 22:38:01.257111   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:01.257119   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:01.260013   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.260032   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.260104   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.260268   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.266868   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.266952   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.267066   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.267074   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: &quot;10.129.173.94:2379&quot; is up but not pinned (already pinned &quot;10.129.173.92:2379&quot;)</span><br><span class="line">W0705 22:38:01.267176   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.267294   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.267308   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.267337   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.267425   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.273677   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.273740   18145 storage_factory.go:50] Storage caching is enabled for *apiextensions.CustomResourceDefinition with capacity 100</span><br><span class="line">I0705 22:38:01.273759   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.273837   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.273893   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.274465   18145 store.go:1343] Monitoring customresourcedefinitions.apiextensions.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions</span><br><span class="line">I0705 22:38:01.274588   18145 reflector.go:160] Listing and watching *apiextensions.CustomResourceDefinition from storage&#x2F;cacher.go:&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions</span><br><span class="line">I0705 22:38:01.277287   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.297217   18145 services.go:45] Setting service IP to &quot;172.26.0.1&quot; (read-write).</span><br><span class="line">I0705 22:38:01.297246   18145 master.go:233] Using reconciler: lease</span><br><span class="line">I0705 22:38:01.297293   18145 storage_factory.go:285] storing apiServerIPInfo in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.297799   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.297812   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.297844   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.297883   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.304364   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.304410   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.304499   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.304538   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.305969   18145 storage_factory.go:285] storing podtemplates in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.306443   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.306455   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.306484   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.306515   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.312747   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.312802   18145 storage_factory.go:50] Storage caching is enabled for *core.PodTemplate with capacity 100</span><br><span class="line">I0705 22:38:01.312802   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.312893   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.312929   18145 store.go:1343] Monitoring podtemplates count at &lt;storage-prefix&gt;&#x2F;&#x2F;podtemplates</span><br><span class="line">W0705 22:38:01.312949   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.312969   18145 reflector.go:160] Listing and watching *core.PodTemplate from storage&#x2F;cacher.go:&#x2F;podtemplates</span><br><span class="line">I0705 22:38:01.312962   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.313529   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.313541   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.313570   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.313640   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.316212   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.319641   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.319682   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.319684   18145 storage_factory.go:46] Storage caching is disabled for *core.Event</span><br><span class="line">W0705 22:38:01.319767   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.319773   18145 store.go:1343] Monitoring events count at &lt;storage-prefix&gt;&#x2F;&#x2F;events</span><br><span class="line">W0705 22:38:01.319806   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.319815   18145 storage_factory.go:285] storing limitranges in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.320331   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.320344   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.320372   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.320409   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.326178   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.326213   18145 storage_factory.go:50] Storage caching is enabled for *core.LimitRange with capacity 100</span><br><span class="line">I0705 22:38:01.326227   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.326298   18145 store.go:1343] Monitoring limitranges count at &lt;storage-prefix&gt;&#x2F;&#x2F;limitranges</span><br><span class="line">I0705 22:38:01.326328   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.326344   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.326352   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.326365   18145 reflector.go:160] Listing and watching *core.LimitRange from storage&#x2F;cacher.go:&#x2F;limitranges</span><br><span class="line">I0705 22:38:01.326789   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.326801   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.326827   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.327266   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.329416   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.333949   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.334002   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.334049   18145 storage_factory.go:50] Storage caching is enabled for *core.ResourceQuota with capacity 100</span><br><span class="line">W0705 22:38:01.334117   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.334183   18145 store.go:1343] Monitoring resourcequotas count at &lt;storage-prefix&gt;&#x2F;&#x2F;resourcequotas</span><br><span class="line">I0705 22:38:01.334225   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.334240   18145 reflector.go:160] Listing and watching *core.ResourceQuota from storage&#x2F;cacher.go:&#x2F;resourcequotas</span><br><span class="line">W0705 22:38:01.334251   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.334409   18145 storage_factory.go:285] storing secrets in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.334997   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.335011   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.335060   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.335107   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.336791   18145 watch_cache.go:405] Replace watchCache (rev: 75099442) </span><br><span class="line">I0705 22:38:01.341685   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.341737   18145 storage_factory.go:50] Storage caching is enabled for *core.Secret with capacity 100</span><br><span class="line">I0705 22:38:01.341753   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.341819   18145 store.go:1343] Monitoring secrets count at &lt;storage-prefix&gt;&#x2F;&#x2F;secrets</span><br><span class="line">W0705 22:38:01.341843   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.341854   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.341855   18145 reflector.go:160] Listing and watching *core.Secret from storage&#x2F;cacher.go:&#x2F;secrets</span><br><span class="line">I0705 22:38:01.341953   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.342468   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.342480   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.342535   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.342579   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.345126   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.348069   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.348110   18145 storage_factory.go:50] Storage caching is enabled for *core.PersistentVolume with capacity 100</span><br><span class="line">I0705 22:38:01.348133   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.348190   18145 store.go:1343] Monitoring persistentvolumes count at &lt;storage-prefix&gt;&#x2F;&#x2F;persistentvolumes</span><br><span class="line">W0705 22:38:01.348228   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.348239   18145 reflector.go:160] Listing and watching *core.PersistentVolume from storage&#x2F;cacher.go:&#x2F;persistentvolumes</span><br><span class="line">W0705 22:38:01.348248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.348373   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.348916   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.348930   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.348986   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.349049   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.350015   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.355158   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.355202   18145 storage_factory.go:50] Storage caching is enabled for *core.PersistentVolumeClaim with capacity 100</span><br><span class="line">I0705 22:38:01.355203   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.355297   18145 store.go:1343] Monitoring persistentvolumeclaims count at &lt;storage-prefix&gt;&#x2F;&#x2F;persistentvolumeclaims</span><br><span class="line">I0705 22:38:01.355343   18145 reflector.go:160] Listing and watching *core.PersistentVolumeClaim from storage&#x2F;cacher.go:&#x2F;persistentvolumeclaims</span><br><span class="line">W0705 22:38:01.355357   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.355377   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.355467   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.355442   18145 storage_factory.go:285] storing configmaps in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.355993   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.356010   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.356046   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.356101   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.360455   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.361771   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.361808   18145 storage_factory.go:50] Storage caching is enabled for *core.ConfigMap with capacity 100</span><br><span class="line">I0705 22:38:01.361847   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.361872   18145 store.go:1343] Monitoring configmaps count at &lt;storage-prefix&gt;&#x2F;&#x2F;configmaps</span><br><span class="line">W0705 22:38:01.361918   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.361923   18145 reflector.go:160] Listing and watching *core.ConfigMap from storage&#x2F;cacher.go:&#x2F;configmaps</span><br><span class="line">W0705 22:38:01.361936   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.361979   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.362436   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.362448   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.362475   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.362509   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.368369   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.368402   18145 storage_factory.go:50] Storage caching is enabled for *core.Namespace with capacity 100</span><br><span class="line">I0705 22:38:01.368406   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.368475   18145 store.go:1343] Monitoring namespaces count at &lt;storage-prefix&gt;&#x2F;&#x2F;namespaces</span><br><span class="line">W0705 22:38:01.368480   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.368527   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.368523   18145 reflector.go:160] Listing and watching *core.Namespace from storage&#x2F;cacher.go:&#x2F;namespaces</span><br><span class="line">I0705 22:38:01.368640   18145 storage_factory.go:285] storing endpoints in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.369074   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.369088   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.369118   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.369152   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.376059   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.376127   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.376137   18145 storage_factory.go:50] Storage caching is enabled for *core.Endpoints with capacity 1000</span><br><span class="line">W0705 22:38:01.376216   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.376219   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.376246   18145 store.go:1343] Monitoring endpoints count at &lt;storage-prefix&gt;&#x2F;&#x2F;services&#x2F;endpoints</span><br><span class="line">I0705 22:38:01.376288   18145 reflector.go:160] Listing and watching *core.Endpoints from storage&#x2F;cacher.go:&#x2F;services&#x2F;endpoints</span><br><span class="line">I0705 22:38:01.376450   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.376450   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.377222   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.377246   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.377295   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.377372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.379460   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.382925   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.382961   18145 storage_factory.go:50] Storage caching is enabled for *core.Node with capacity 1000</span><br><span class="line">I0705 22:38:01.382977   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.383040   18145 store.go:1343] Monitoring nodes count at &lt;storage-prefix&gt;&#x2F;&#x2F;minions</span><br><span class="line">W0705 22:38:01.383054   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.383063   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.383102   18145 reflector.go:160] Listing and watching *core.Node from storage&#x2F;cacher.go:&#x2F;minions</span><br><span class="line">I0705 22:38:01.383436   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.383884   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.383896   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.383921   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.383955   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.386054   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.389904   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.389951   18145 storage_factory.go:50] Storage caching is enabled for *core.Pod with capacity 1000</span><br><span class="line">I0705 22:38:01.389990   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.390047   18145 store.go:1343] Monitoring pods count at &lt;storage-prefix&gt;&#x2F;&#x2F;pods</span><br><span class="line">W0705 22:38:01.390100   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.390107   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.390140   18145 reflector.go:160] Listing and watching *core.Pod from storage&#x2F;cacher.go:&#x2F;pods</span><br><span class="line">I0705 22:38:01.390180   18145 storage_factory.go:285] storing serviceaccounts in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.391245   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.391263   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.391310   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.391381   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.398242   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.398285   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.398297   18145 storage_factory.go:50] Storage caching is enabled for *core.ServiceAccount with capacity 100</span><br><span class="line">W0705 22:38:01.398378   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.398395   18145 store.go:1343] Monitoring serviceaccounts count at &lt;storage-prefix&gt;&#x2F;&#x2F;serviceaccounts</span><br><span class="line">W0705 22:38:01.398413   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.398524   18145 reflector.go:160] Listing and watching *core.ServiceAccount from storage&#x2F;cacher.go:&#x2F;serviceaccounts</span><br><span class="line">I0705 22:38:01.398559   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.399760   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.399789   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.399832   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.399894   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.402304   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.406325   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.406360   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.406413   18145 storage_factory.go:50] Storage caching is enabled for *core.Service with capacity 1000</span><br><span class="line">I0705 22:38:01.406421   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.406516   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.406546   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.406580   18145 store.go:1343] Monitoring services count at &lt;storage-prefix&gt;&#x2F;&#x2F;services&#x2F;specs</span><br><span class="line">I0705 22:38:01.406645   18145 reflector.go:160] Listing and watching *core.Service from storage&#x2F;cacher.go:&#x2F;services&#x2F;specs</span><br><span class="line">I0705 22:38:01.406642   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.407151   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.407163   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.407208   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.407260   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.412982   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.413048   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.413128   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.413155   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.413252   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.413526   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.413539   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.413567   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.413619   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.414368   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.419479   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.419514   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.419579   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.419647   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.419647   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.420108   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.420119   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.420144   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.420203   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.426157   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.426208   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.426217   18145 storage_factory.go:50] Storage caching is enabled for *core.ReplicationController with capacity 100</span><br><span class="line">W0705 22:38:01.426293   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.426306   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.426312   18145 store.go:1343] Monitoring replicationcontrollers count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllers</span><br><span class="line">I0705 22:38:01.426333   18145 reflector.go:160] Listing and watching *core.ReplicationController from storage&#x2F;cacher.go:&#x2F;controllers</span><br><span class="line">I0705 22:38:01.428279   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.480260   18145 storage_factory.go:285] storing bindings in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.480424   18145 storage_factory.go:285] storing componentstatuses in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.480904   18145 storage_factory.go:285] storing configmaps in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.481312   18145 storage_factory.go:285] storing endpoints in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.481698   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482104   18145 storage_factory.go:285] storing limitranges in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482346   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482426   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482543   18145 storage_factory.go:285] storing namespaces in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.482825   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483147   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483255   18145 storage_factory.go:285] storing nodes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483733   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.483904   18145 storage_factory.go:285] storing persistentvolumeclaims in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484203   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484330   18145 storage_factory.go:285] storing persistentvolumes in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484703   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484828   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484905   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.484984   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485119   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485212   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485326   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485764   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.485937   18145 storage_factory.go:285] storing pods in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486385   18145 storage_factory.go:285] storing podtemplates in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486832   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.486982   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487134   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487533   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.487697   18145 storage_factory.go:285] storing resourcequotas in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488101   18145 storage_factory.go:285] storing secrets in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488494   18145 storage_factory.go:285] storing serviceaccounts in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.488886   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489313   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489466   18145 storage_factory.go:285] storing services in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.489537   18145 master.go:417] Skipping disabled API group &quot;auditregistration.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489550   18145 master.go:425] Enabling API group &quot;authentication.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489562   18145 master.go:425] Enabling API group &quot;authorization.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.489679   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.490153   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.490164   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.490200   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.490237   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.496374   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.496423   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.496450   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.496501   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">W0705 22:38:01.496531   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.496536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.496589   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.496664   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.497098   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.497109   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.497135   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.497176   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.500515   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.502989   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.503023   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.503047   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.503101   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">W0705 22:38:01.503114   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.503161   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.503155   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.503245   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.503665   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.503676   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.503702   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.503743   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.506520   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.509091   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.509124   18145 storage_factory.go:50] Storage caching is enabled for *autoscaling.HorizontalPodAutoscaler with capacity 100</span><br><span class="line">I0705 22:38:01.509161   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.509195   18145 store.go:1343] Monitoring horizontalpodautoscalers.autoscaling count at &lt;storage-prefix&gt;&#x2F;&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.509211   18145 master.go:425] Enabling API group &quot;autoscaling&quot;.</span><br><span class="line">W0705 22:38:01.509223   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.509248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.509265   18145 reflector.go:160] Listing and watching *autoscaling.HorizontalPodAutoscaler from storage&#x2F;cacher.go:&#x2F;horizontalpodautoscalers</span><br><span class="line">I0705 22:38:01.509331   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.509807   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.509821   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.509848   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.509885   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.515303   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.515327   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.515362   18145 storage_factory.go:50] Storage caching is enabled for *batch.Job with capacity 100</span><br><span class="line">W0705 22:38:01.515407   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.515414   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.515436   18145 store.go:1343] Monitoring jobs.batch count at &lt;storage-prefix&gt;&#x2F;&#x2F;jobs</span><br><span class="line">I0705 22:38:01.515472   18145 reflector.go:160] Listing and watching *batch.Job from storage&#x2F;cacher.go:&#x2F;jobs</span><br><span class="line">I0705 22:38:01.515529   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.516011   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.516023   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.516047   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.516080   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.516374   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.517439   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.522214   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.522244   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.522277   18145 storage_factory.go:50] Storage caching is enabled for *batch.CronJob with capacity 100</span><br><span class="line">W0705 22:38:01.522297   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.522325   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.522347   18145 store.go:1343] Monitoring cronjobs.batch count at &lt;storage-prefix&gt;&#x2F;&#x2F;cronjobs</span><br><span class="line">I0705 22:38:01.522361   18145 master.go:425] Enabling API group &quot;batch&quot;.</span><br><span class="line">I0705 22:38:01.522409   18145 reflector.go:160] Listing and watching *batch.CronJob from storage&#x2F;cacher.go:&#x2F;cronjobs</span><br><span class="line">I0705 22:38:01.522456   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.522889   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.522901   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.522926   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.522959   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.525358   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.529067   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.529096   18145 storage_factory.go:50] Storage caching is enabled for *certificates.CertificateSigningRequest with capacity 100</span><br><span class="line">I0705 22:38:01.529108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.529164   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.529169   18145 store.go:1343] Monitoring certificatesigningrequests.certificates.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;certificatesigningrequests</span><br><span class="line">W0705 22:38:01.529172   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.529187   18145 master.go:425] Enabling API group &quot;certificates.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.529202   18145 reflector.go:160] Listing and watching *certificates.CertificateSigningRequest from storage&#x2F;cacher.go:&#x2F;certificatesigningrequests</span><br><span class="line">I0705 22:38:01.529330   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.529757   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.529769   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.529799   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.529831   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.535795   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.535826   18145 storage_factory.go:50] Storage caching is enabled for *coordination.Lease with capacity 100</span><br><span class="line">I0705 22:38:01.535851   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.535884   18145 store.go:1343] Monitoring leases.coordination.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;leases</span><br><span class="line">W0705 22:38:01.535922   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.535951   18145 reflector.go:160] Listing and watching *coordination.Lease from storage&#x2F;cacher.go:&#x2F;leases</span><br><span class="line">W0705 22:38:01.535986   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.536022   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.536035   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.536453   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.536464   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.536487   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.536530   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.538229   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.543527   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.543566   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: &quot;10.129.173.92:2379&quot; is up but not pinned (already pinned &quot;10.129.173.94:2379&quot;)</span><br><span class="line">I0705 22:38:01.543625   18145 storage_factory.go:50] Storage caching is enabled for *coordination.Lease with capacity 100</span><br><span class="line">I0705 22:38:01.543664   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.543732   18145 store.go:1343] Monitoring leases.coordination.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;leases</span><br><span class="line">I0705 22:38:01.544459   18145 reflector.go:160] Listing and watching *coordination.Lease from storage&#x2F;cacher.go:&#x2F;leases</span><br><span class="line">I0705 22:38:01.544471   18145 master.go:425] Enabling API group &quot;coordination.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.544507   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.544785   18145 storage_factory.go:285] storing replicationcontrollers in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.544955   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">W0705 22:38:01.544970   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.546012   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.546032   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.546074   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.546142   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.546857   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.552338   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.552372   18145 storage_factory.go:50] Storage caching is enabled for *core.ReplicationController with capacity 100</span><br><span class="line">I0705 22:38:01.552402   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.552431   18145 store.go:1343] Monitoring replicationcontrollers count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllers</span><br><span class="line">W0705 22:38:01.552478   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.552513   18145 reflector.go:160] Listing and watching *core.ReplicationController from storage&#x2F;cacher.go:&#x2F;controllers</span><br><span class="line">W0705 22:38:01.552563   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.552569   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.553038   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.553048   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.553075   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.553108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.554206   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.559089   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.559133   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.559160   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.559227   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">W0705 22:38:01.559248   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.559277   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.559295   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.559365   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.559903   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.559919   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.559950   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.560049   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.562229   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.565407   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.565440   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.565451   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">W0705 22:38:01.565511   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.565538   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.565544   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.565609   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.565689   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.566128   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.566140   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.566173   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.566208   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.572128   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.572160   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.572184   18145 storage_factory.go:50] Storage caching is enabled for *networking.Ingress with capacity 100</span><br><span class="line">W0705 22:38:01.572230   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.572257   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.572275   18145 store.go:1343] Monitoring ingresses.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;ingress</span><br><span class="line">I0705 22:38:01.572328   18145 reflector.go:160] Listing and watching *networking.Ingress from storage&#x2F;cacher.go:&#x2F;ingress</span><br><span class="line">I0705 22:38:01.572433   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.573010   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.573029   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.573060   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.573099   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.573357   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.578443   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.578472   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodSecurityPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.578477   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.578536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.578548   18145 store.go:1343] Monitoring podsecuritypolicies.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;podsecuritypolicy</span><br><span class="line">I0705 22:38:01.578574   18145 reflector.go:160] Listing and watching *policy.PodSecurityPolicy from storage&#x2F;cacher.go:&#x2F;podsecuritypolicy</span><br><span class="line">W0705 22:38:01.578581   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.578667   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.579104   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.579114   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.579140   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.579171   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.579353   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.580264   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.584838   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.584871   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.584886   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.584942   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.584975   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.584943   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.584987   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.585104   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.585519   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.585529   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.585553   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.585612   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.590996   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.591666   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.591699   18145 storage_factory.go:50] Storage caching is enabled for *networking.NetworkPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.591744   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.591767   18145 store.go:1343] Monitoring networkpolicies.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;networkpolicies</span><br><span class="line">I0705 22:38:01.591780   18145 master.go:425] Enabling API group &quot;extensions&quot;.</span><br><span class="line">W0705 22:38:01.591812   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.591816   18145 reflector.go:160] Listing and watching *networking.NetworkPolicy from storage&#x2F;cacher.go:&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.591814   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.591919   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.592337   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.592347   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.592382   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.592420   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.593678   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.598473   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.598508   18145 storage_factory.go:50] Storage caching is enabled for *networking.NetworkPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.598534   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.598556   18145 store.go:1343] Monitoring networkpolicies.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.598620   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.598621   18145 reflector.go:160] Listing and watching *networking.NetworkPolicy from storage&#x2F;cacher.go:&#x2F;networkpolicies</span><br><span class="line">W0705 22:38:01.598670   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.598678   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.599263   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.599279   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.599307   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.599340   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.605672   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.605867   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.605898   18145 storage_factory.go:50] Storage caching is enabled for *networking.Ingress with capacity 100</span><br><span class="line">I0705 22:38:01.605918   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.605966   18145 store.go:1343] Monitoring ingresses.networking.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;ingress</span><br><span class="line">I0705 22:38:01.605981   18145 master.go:425] Enabling API group &quot;networking.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.605983   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.606016   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.606017   18145 storage_factory.go:285] storing runtimeclasses.node.k8s.io in node.k8s.io&#x2F;v1beta1, reading as node.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.606036   18145 reflector.go:160] Listing and watching *networking.Ingress from storage&#x2F;cacher.go:&#x2F;ingress</span><br><span class="line">I0705 22:38:01.606558   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.606571   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.606613   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.606667   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.612169   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.612196   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.612210   18145 storage_factory.go:50] Storage caching is enabled for *node.RuntimeClass with capacity 100</span><br><span class="line">W0705 22:38:01.612254   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.612274   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.612300   18145 store.go:1343] Monitoring runtimeclasses.node.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;runtimeclasses</span><br><span class="line">I0705 22:38:01.612325   18145 master.go:425] Enabling API group &quot;node.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.612343   18145 reflector.go:160] Listing and watching *node.RuntimeClass from storage&#x2F;cacher.go:&#x2F;runtimeclasses</span><br><span class="line">I0705 22:38:01.612348   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.612449   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.612946   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.612958   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.612989   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.613026   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.618576   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.618621   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.618644   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodDisruptionBudget with capacity 100</span><br><span class="line">W0705 22:38:01.618680   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.618713   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.618732   18145 store.go:1343] Monitoring poddisruptionbudgets.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;poddisruptionbudgets</span><br><span class="line">I0705 22:38:01.618756   18145 reflector.go:160] Listing and watching *policy.PodDisruptionBudget from storage&#x2F;cacher.go:&#x2F;poddisruptionbudgets</span><br><span class="line">I0705 22:38:01.618878   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.619304   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.619315   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.619340   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.619372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.620634   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.621640   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.625143   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.625177   18145 storage_factory.go:50] Storage caching is enabled for *policy.PodSecurityPolicy with capacity 100</span><br><span class="line">I0705 22:38:01.625238   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.625266   18145 reflector.go:160] Listing and watching *policy.PodSecurityPolicy from storage&#x2F;cacher.go:&#x2F;podsecuritypolicy</span><br><span class="line">I0705 22:38:01.625246   18145 store.go:1343] Monitoring podsecuritypolicies.policy count at &lt;storage-prefix&gt;&#x2F;&#x2F;podsecuritypolicy</span><br><span class="line">W0705 22:38:01.625317   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.625324   18145 master.go:425] Enabling API group &quot;policy&quot;.</span><br><span class="line">W0705 22:38:01.625351   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.625367   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.625915   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.625934   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.625979   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.626040   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.627123   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.632111   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.632147   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.632160   18145 storage_factory.go:50] Storage caching is enabled for *rbac.Role with capacity 100</span><br><span class="line">W0705 22:38:01.632224   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.632295   18145 store.go:1343] Monitoring roles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;roles</span><br><span class="line">W0705 22:38:01.632296   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.632328   18145 reflector.go:160] Listing and watching *rbac.Role from storage&#x2F;cacher.go:&#x2F;roles</span><br><span class="line">I0705 22:38:01.632406   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.632889   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.632904   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.632930   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.632988   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.634959   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.639061   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.639098   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.639170   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.639101   18145 storage_factory.go:50] Storage caching is enabled for *rbac.RoleBinding with capacity 100</span><br><span class="line">W0705 22:38:01.639217   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.639258   18145 store.go:1343] Monitoring rolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.639292   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.639326   18145 reflector.go:160] Listing and watching *rbac.RoleBinding from storage&#x2F;cacher.go:&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.639724   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.639736   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.639761   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.639792   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.641705   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.645513   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.645546   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRole with capacity 100</span><br><span class="line">I0705 22:38:01.645589   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.645616   18145 store.go:1343] Monitoring clusterroles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterroles</span><br><span class="line">I0705 22:38:01.645686   18145 reflector.go:160] Listing and watching *rbac.ClusterRole from storage&#x2F;cacher.go:&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.645696   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.645689   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.645776   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.646238   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.646250   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.646277   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.646312   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.652231   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.652258   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.652280   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRoleBinding with capacity 100</span><br><span class="line">W0705 22:38:01.652325   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.652374   18145 store.go:1343] Monitoring clusterrolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterrolebindings</span><br><span class="line">W0705 22:38:01.652385   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.652417   18145 reflector.go:160] Listing and watching *rbac.ClusterRoleBinding from storage&#x2F;cacher.go:&#x2F;clusterrolebindings</span><br><span class="line">I0705 22:38:01.652484   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.652951   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.652963   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.652990   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.653027   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.653114   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.655550   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.659098   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.659135   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.659162   18145 storage_factory.go:50] Storage caching is enabled for *rbac.Role with capacity 100</span><br><span class="line">W0705 22:38:01.659210   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.659216   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.659231   18145 store.go:1343] Monitoring roles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;roles</span><br><span class="line">I0705 22:38:01.659290   18145 reflector.go:160] Listing and watching *rbac.Role from storage&#x2F;cacher.go:&#x2F;roles</span><br><span class="line">I0705 22:38:01.659382   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.659882   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.659894   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.659921   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.659976   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.661045   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.666391   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.666434   18145 storage_factory.go:50] Storage caching is enabled for *rbac.RoleBinding with capacity 100</span><br><span class="line">I0705 22:38:01.666455   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.666501   18145 store.go:1343] Monitoring rolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;rolebindings</span><br><span class="line">I0705 22:38:01.666563   18145 reflector.go:160] Listing and watching *rbac.RoleBinding from storage&#x2F;cacher.go:&#x2F;rolebindings</span><br><span class="line">W0705 22:38:01.666586   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.666570   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.667163   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.667615   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.667628   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.667659   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.667696   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.669460   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.673937   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.673974   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRole with capacity 100</span><br><span class="line">I0705 22:38:01.673998   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.674038   18145 store.go:1343] Monitoring clusterroles.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.674107   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.674139   18145 reflector.go:160] Listing and watching *rbac.ClusterRole from storage&#x2F;cacher.go:&#x2F;clusterroles</span><br><span class="line">W0705 22:38:01.674241   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.674246   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.674250   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.674768   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.674781   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.674809   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.674843   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.679062   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.680225   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.680261   18145 storage_factory.go:50] Storage caching is enabled for *rbac.ClusterRoleBinding with capacity 100</span><br><span class="line">I0705 22:38:01.680286   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.680333   18145 store.go:1343] Monitoring clusterrolebindings.rbac.authorization.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;clusterrolebindings</span><br><span class="line">I0705 22:38:01.680363   18145 master.go:425] Enabling API group &quot;rbac.authorization.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.680382   18145 reflector.go:160] Listing and watching *rbac.ClusterRoleBinding from storage&#x2F;cacher.go:&#x2F;clusterrolebindings</span><br><span class="line">W0705 22:38:01.680417   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.680389   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.682487   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.682847   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.682943   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.682958   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.682986   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.683031   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.688965   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.689018   18145 storage_factory.go:50] Storage caching is enabled for *scheduling.PriorityClass with capacity 100</span><br><span class="line">I0705 22:38:01.689064   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.689105   18145 store.go:1343] Monitoring priorityclasses.scheduling.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;priorityclasses</span><br><span class="line">W0705 22:38:01.689143   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.689144   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.689160   18145 reflector.go:160] Listing and watching *scheduling.PriorityClass from storage&#x2F;cacher.go:&#x2F;priorityclasses</span><br><span class="line">I0705 22:38:01.689271   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.689806   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.689819   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.689858   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.689924   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.691729   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.696232   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.696268   18145 storage_factory.go:50] Storage caching is enabled for *scheduling.PriorityClass with capacity 100</span><br><span class="line">I0705 22:38:01.696271   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.696334   18145 store.go:1343] Monitoring priorityclasses.scheduling.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;priorityclasses</span><br><span class="line">I0705 22:38:01.696348   18145 master.go:425] Enabling API group &quot;scheduling.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.696351   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.696382   18145 reflector.go:160] Listing and watching *scheduling.PriorityClass from storage&#x2F;cacher.go:&#x2F;priorityclasses</span><br><span class="line">W0705 22:38:01.696400   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.696458   18145 master.go:417] Skipping disabled API group &quot;settings.k8s.io&quot;.</span><br><span class="line">I0705 22:38:01.696568   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.697044   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.697056   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.697082   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.697116   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.697826   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.703204   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.703245   18145 storage_factory.go:50] Storage caching is enabled for *storage.StorageClass with capacity 100</span><br><span class="line">I0705 22:38:01.703295   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.703320   18145 store.go:1343] Monitoring storageclasses.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;storageclasses</span><br><span class="line">W0705 22:38:01.703359   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.703394   18145 reflector.go:160] Listing and watching *storage.StorageClass from storage&#x2F;cacher.go:&#x2F;storageclasses</span><br><span class="line">I0705 22:38:01.703456   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.703398   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.703972   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.703988   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.704034   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.704108   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.705744   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.710344   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.710372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.710376   18145 storage_factory.go:50] Storage caching is enabled for *storage.VolumeAttachment with capacity 100</span><br><span class="line">W0705 22:38:01.710430   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.710445   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.710450   18145 store.go:1343] Monitoring volumeattachments.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.710464   18145 reflector.go:160] Listing and watching *storage.VolumeAttachment from storage&#x2F;cacher.go:&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.710483   18145 storage_factory.go:285] storing csinodes.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.710942   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.710954   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.710990   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.711054   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.713079   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.716838   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.716867   18145 storage_factory.go:50] Storage caching is enabled for *storage.CSINode with capacity 100</span><br><span class="line">I0705 22:38:01.716906   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.716965   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.717340   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.717348   18145 store.go:1343] Monitoring csinodes.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;csinodes</span><br><span class="line">I0705 22:38:01.717384   18145 storage_factory.go:285] storing csidrivers.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.717555   18145 reflector.go:160] Listing and watching *storage.CSINode from storage&#x2F;cacher.go:&#x2F;csinodes</span><br><span class="line">I0705 22:38:01.719158   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.719180   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.719235   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.719314   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.725650   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:01.725704   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.725750   18145 storage_factory.go:50] Storage caching is enabled for *storage.CSIDriver with capacity 100</span><br><span class="line">W0705 22:38:01.725830   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.725833   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.725881   18145 store.go:1343] Monitoring csidrivers.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;csidrivers</span><br><span class="line">I0705 22:38:01.725922   18145 reflector.go:160] Listing and watching *storage.CSIDriver from storage&#x2F;cacher.go:&#x2F;csidrivers</span><br><span class="line">I0705 22:38:01.726070   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.726147   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.726622   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.726641   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.726682   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.726742   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.727907   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.733148   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.733192   18145 storage_factory.go:50] Storage caching is enabled for *storage.StorageClass with capacity 100</span><br><span class="line">I0705 22:38:01.733195   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.733264   18145 store.go:1343] Monitoring storageclasses.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;storageclasses</span><br><span class="line">W0705 22:38:01.733277   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.733282   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.733318   18145 reflector.go:160] Listing and watching *storage.StorageClass from storage&#x2F;cacher.go:&#x2F;storageclasses</span><br><span class="line">I0705 22:38:01.733393   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.733942   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.733956   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.733983   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.734017   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.739573   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.739653   18145 storage_factory.go:50] Storage caching is enabled for *storage.VolumeAttachment with capacity 100</span><br><span class="line">I0705 22:38:01.739670   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.739743   18145 store.go:1343] Monitoring volumeattachments.storage.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;volumeattachments</span><br><span class="line">I0705 22:38:01.739770   18145 master.go:425] Enabling API group &quot;storage.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.739785   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.739794   18145 reflector.go:160] Listing and watching *storage.VolumeAttachment from storage&#x2F;cacher.go:&#x2F;volumeattachments</span><br><span class="line">W0705 22:38:01.739820   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.739830   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.739938   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.740394   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.740405   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.740433   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.740471   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.745885   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.746023   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.746067   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">I0705 22:38:01.746068   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.746150   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.746192   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">W0705 22:38:01.746199   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.746195   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.746278   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.746862   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.746876   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.746903   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.746941   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.752900   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.752930   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.752989   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.752995   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.753021   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.753114   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">I0705 22:38:01.753145   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">I0705 22:38:01.753245   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.753664   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.753785   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.753799   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.753835   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.753881   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.759938   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.759972   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">I0705 22:38:01.759996   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.760029   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">W0705 22:38:01.760072   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.760088   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.760094   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.760179   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.760648   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.760660   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.760693   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.760752   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.761629   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.762737   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.766537   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.766570   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">I0705 22:38:01.766574   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.766639   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">W0705 22:38:01.766666   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.766667   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.766671   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.766798   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.767271   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.767283   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.767309   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.767408   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.773951   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.773995   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.773998   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.774074   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.774125   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.774172   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.774188   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.774230   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.774946   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.774963   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.774996   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.775036   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.776461   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.779672   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.780664   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.780705   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.780734   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.780809   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.780836   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">W0705 22:38:01.780811   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.780867   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.780956   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.781418   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.781428   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.781460   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.781499   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.782842   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.787419   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.787444   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.787502   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.787530   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.787553   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.787656   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.787742   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">I0705 22:38:01.787796   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.788235   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.788246   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.788273   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.788315   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.794135   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.794631   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.92:2379&quot;</span><br><span class="line">I0705 22:38:01.794681   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.794701   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">W0705 22:38:01.794771   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.794819   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">W0705 22:38:01.794829   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.794861   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.794960   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.795409   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.795421   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.795450   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.795502   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.797129   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.801393   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.801432   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.801471   18145 storage_factory.go:50] Storage caching is enabled for *apps.Deployment with capacity 100</span><br><span class="line">W0705 22:38:01.801508   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.801536   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.801545   18145 store.go:1343] Monitoring deployments.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;deployments</span><br><span class="line">I0705 22:38:01.801562   18145 reflector.go:160] Listing and watching *apps.Deployment from storage&#x2F;cacher.go:&#x2F;deployments</span><br><span class="line">I0705 22:38:01.801702   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.802137   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.802147   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.802184   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.802251   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.807741   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.807776   18145 storage_factory.go:50] Storage caching is enabled for *apps.StatefulSet with capacity 100</span><br><span class="line">I0705 22:38:01.807792   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.807849   18145 store.go:1343] Monitoring statefulsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.807897   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.807922   18145 reflector.go:160] Listing and watching *apps.StatefulSet from storage&#x2F;cacher.go:&#x2F;statefulsets</span><br><span class="line">W0705 22:38:01.807964   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.807960   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.808424   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.808435   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.809098   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.809163   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.809901   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.814015   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.814893   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.814938   18145 storage_factory.go:50] Storage caching is enabled for *apps.DaemonSet with capacity 100</span><br><span class="line">I0705 22:38:01.814943   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.815016   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.815024   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.815026   18145 store.go:1343] Monitoring daemonsets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.815048   18145 reflector.go:160] Listing and watching *apps.DaemonSet from storage&#x2F;cacher.go:&#x2F;daemonsets</span><br><span class="line">I0705 22:38:01.815185   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.815699   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.815712   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.815762   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.815819   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.821338   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.821378   18145 storage_factory.go:50] Storage caching is enabled for *apps.ReplicaSet with capacity 100</span><br><span class="line">I0705 22:38:01.821343   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.821406   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.821445   18145 store.go:1343] Monitoring replicasets.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.821497   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.821514   18145 reflector.go:160] Listing and watching *apps.ReplicaSet from storage&#x2F;cacher.go:&#x2F;replicasets</span><br><span class="line">W0705 22:38:01.821554   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.821562   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.822050   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.822063   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.822109   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.822173   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.828748   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.828791   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.828823   18145 storage_factory.go:50] Storage caching is enabled for *apps.ControllerRevision with capacity 100</span><br><span class="line">W0705 22:38:01.828858   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.828865   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.828923   18145 store.go:1343] Monitoring controllerrevisions.apps count at &lt;storage-prefix&gt;&#x2F;&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.828944   18145 master.go:425] Enabling API group &quot;apps&quot;.</span><br><span class="line">I0705 22:38:01.828977   18145 storage_factory.go:285] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.828998   18145 reflector.go:160] Listing and watching *apps.ControllerRevision from storage&#x2F;cacher.go:&#x2F;controllerrevisions</span><br><span class="line">I0705 22:38:01.829405   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.829417   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.829444   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.829477   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.832275   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.833018   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.835374   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.835425   18145 storage_factory.go:50] Storage caching is enabled for *admissionregistration.ValidatingWebhookConfiguration with capacity 100</span><br><span class="line">I0705 22:38:01.835447   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:01.835516   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:01.835523   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.835521   18145 store.go:1343] Monitoring validatingwebhookconfigurations.admissionregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;validatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.835546   18145 reflector.go:160] Listing and watching *admissionregistration.ValidatingWebhookConfiguration from storage&#x2F;cacher.go:&#x2F;validatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.835570   18145 storage_factory.go:285] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.836092   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.836103   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.836130   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.836163   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.837500   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.843429   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.843486   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.843504   18145 storage_factory.go:50] Storage caching is enabled for *admissionregistration.MutatingWebhookConfiguration with capacity 100</span><br><span class="line">W0705 22:38:01.843562   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.843696   18145 store.go:1343] Monitoring mutatingwebhookconfigurations.admissionregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;mutatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.843729   18145 master.go:425] Enabling API group &quot;admissionregistration.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.843815   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.843787   18145 storage_factory.go:285] storing events in v1, reading as __internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.844020   18145 reflector.go:160] Listing and watching *admissionregistration.MutatingWebhookConfiguration from storage&#x2F;cacher.go:&#x2F;mutatingwebhookconfigurations</span><br><span class="line">I0705 22:38:01.844236   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">I0705 22:38:01.844544   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:01.844556   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:01.844583   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.844651   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.850614   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:01.850627   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:01.850643   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:01.850668   18145 storage_factory.go:46] Storage caching is disabled for *core.Event</span><br><span class="line">I0705 22:38:01.850771   18145 store.go:1343] Monitoring events count at &lt;storage-prefix&gt;&#x2F;&#x2F;events</span><br><span class="line">I0705 22:38:01.850791   18145 master.go:425] Enabling API group &quot;events.k8s.io&quot;.</span><br><span class="line">W0705 22:38:01.850817   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.850841   18145 controlbuf.go:382] transport: loopyWriter.run returning. connection error: desc &#x3D; &quot;transport is closing&quot;</span><br><span class="line">W0705 22:38:01.850860   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:01.954244   18145 storage_factory.go:285] storing tokenreviews.authentication.k8s.io in authentication.k8s.io&#x2F;v1, reading as authentication.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954407   18145 storage_factory.go:285] storing tokenreviews.authentication.k8s.io in authentication.k8s.io&#x2F;v1, reading as authentication.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954618   18145 storage_factory.go:285] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954700   18145 storage_factory.go:285] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954785   18145 storage_factory.go:285] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.954852   18145 storage_factory.go:285] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955039   18145 storage_factory.go:285] storing localsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955186   18145 storage_factory.go:285] storing selfsubjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955253   18145 storage_factory.go:285] storing selfsubjectrulesreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955326   18145 storage_factory.go:285] storing subjectaccessreviews.authorization.k8s.io in authorization.k8s.io&#x2F;v1, reading as authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.955939   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956125   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956633   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.956819   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957276   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957422   18145 storage_factory.go:285] storing horizontalpodautoscalers.autoscaling in autoscaling&#x2F;v1, reading as autoscaling&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.957939   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.958119   18145 storage_factory.go:285] storing jobs.batch in batch&#x2F;v1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.958615   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.959458   18145 storage_factory.go:285] storing cronjobs.batch in batch&#x2F;v1beta1, reading as batch&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.959508   18145 genericapiserver.go:351] Skipping API batch&#x2F;v2alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.959965   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960065   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960202   18145 storage_factory.go:285] storing certificatesigningrequests.certificates.k8s.io in certificates.k8s.io&#x2F;v1beta1, reading as certificates.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.960724   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961138   18145 storage_factory.go:285] storing leases.coordination.k8s.io in coordination.k8s.io&#x2F;v1beta1, reading as coordination.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961675   18145 storage_factory.go:285] storing daemonsets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.961861   18145 storage_factory.go:285] storing daemonsets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962283   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962374   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962535   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.962709   18145 storage_factory.go:285] storing deployments.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963138   18145 storage_factory.go:285] storing ingresses.extensions in networking.k8s.io&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963278   18145 storage_factory.go:285] storing ingresses.extensions in networking.k8s.io&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.963727   18145 storage_factory.go:285] storing networkpolicies.extensions in networking.k8s.io&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964049   18145 storage_factory.go:285] storing podsecuritypolicies.extensions in policy&#x2F;v1beta1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964505   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.964702   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965393   18145 storage_factory.go:285] storing replicasets.extensions in apps&#x2F;v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965440   18145 storage_factory.go:285] storing replicationcontrollers.extensions in v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.965587   18145 storage_factory.go:285] storing replicationcontrollers.extensions in v1, reading as extensions&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966078   18145 storage_factory.go:285] storing networkpolicies.networking.k8s.io in networking.k8s.io&#x2F;v1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966475   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.966634   18145 storage_factory.go:285] storing ingresses.networking.k8s.io in networking.k8s.io&#x2F;v1beta1, reading as networking.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.967053   18145 storage_factory.go:285] storing runtimeclasses.node.k8s.io in node.k8s.io&#x2F;v1beta1, reading as node.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.967095   18145 genericapiserver.go:351] Skipping API node.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.967569   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.967746   18145 storage_factory.go:285] storing poddisruptionbudgets.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968069   18145 storage_factory.go:285] storing podsecuritypolicies.policy in policy&#x2F;v1beta1, reading as policy&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968450   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.968782   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969174   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969538   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.969891   18145 storage_factory.go:285] storing clusterrolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.970204   18145 storage_factory.go:285] storing clusterroles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.971161   18145 storage_factory.go:285] storing rolebindings.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.971562   18145 storage_factory.go:285] storing roles.rbac.authorization.k8s.io in rbac.authorization.k8s.io&#x2F;v1, reading as rbac.authorization.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.971614   18145 genericapiserver.go:351] Skipping API rbac.authorization.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.971959   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.972304   18145 storage_factory.go:285] storing priorityclasses.scheduling.k8s.io in scheduling.k8s.io&#x2F;v1, reading as scheduling.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.972342   18145 genericapiserver.go:351] Skipping API scheduling.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.972684   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973037   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973178   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973545   18145 storage_factory.go:285] storing csidrivers.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.973841   18145 storage_factory.go:285] storing csinodes.storage.k8s.io in storage.k8s.io&#x2F;v1beta1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.974138   18145 storage_factory.go:285] storing storageclasses.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.974435   18145 storage_factory.go:285] storing volumeattachments.storage.k8s.io in storage.k8s.io&#x2F;v1, reading as storage.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">W0705 22:38:01.974482   18145 genericapiserver.go:351] Skipping API storage.k8s.io&#x2F;v1alpha1 because it has no resources.</span><br><span class="line">I0705 22:38:01.974974   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.975387   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.975552   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976543   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976761   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.976906   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977277   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977404   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977530   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.977953   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978104   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978247   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.978707   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979080   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979237   18145 storage_factory.go:285] storing daemonsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979612   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979751   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.979874   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980238   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980373   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980500   18145 storage_factory.go:285] storing replicasets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.980918   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.981586   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.981758   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982238   18145 storage_factory.go:285] storing controllerrevisions.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982585   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982685   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982814   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.982947   18145 storage_factory.go:285] storing deployments.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983330   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983478   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.983624   18145 storage_factory.go:285] storing statefulsets.apps in apps&#x2F;v1, reading as apps&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984096   18145 storage_factory.go:285] storing mutatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984466   18145 storage_factory.go:285] storing validatingwebhookconfigurations.admissionregistration.k8s.io in admissionregistration.k8s.io&#x2F;v1beta1, reading as admissionregistration.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:01.984926   18145 storage_factory.go:285] storing events.events.k8s.io in v1, reading as events.k8s.io&#x2F;__internal from storagebackend.Config&#123;Type:&quot;&quot;, Prefix:&quot;&#x2F;registry&quot;, Transport:storagebackend.TransportConfig&#123;ServerList:[]string&#123;&quot;https:&#x2F;&#x2F;10.129.173.92:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.93:2379&quot;, &quot;https:&#x2F;&#x2F;10.129.173.94:2379&quot;&#125;, KeyFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes-key.pem&quot;, CertFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;kubernetes.pem&quot;, CAFile:&quot;&#x2F;etc&#x2F;kubernetes&#x2F;cluster1&#x2F;ssl&#x2F;ca.pem&quot;&#125;, Paging:true, Codec:runtime.Codec(nil), EncodeVersioner:runtime.GroupVersioner(nil), Transformer:value.Transformer(nil), CompactionInterval:300000000000, CountMetricPollPeriod:60000000000&#125;</span><br><span class="line">I0705 22:38:02.254177   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.254203   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.254244   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.254372   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.261077   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.94:2379&quot;</span><br><span class="line">I0705 22:38:02.261128   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:02.261209   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.261287   18145 asm_amd64.s:1337] Failed to dial 10.129.173.93:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.603860   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;,&quot;poststarthook&#x2F;bootstrap-controller&quot;,&quot;poststarthook&#x2F;rbac&#x2F;bootstrap-roles&quot;,&quot;poststarthook&#x2F;scheduling&#x2F;bootstrap-system-priority-classes&quot;,&quot;poststarthook&#x2F;ca-registration&quot;,&quot;poststarthook&#x2F;start-kube-apiserver-admission-initializer&quot;</span><br><span class="line">I0705 22:38:02.646973   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;</span><br><span class="line">E0705 22:38:02.648065   18145 prometheus.go:55] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648116   18145 prometheus.go:68] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648155   18145 prometheus.go:82] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648181   18145 prometheus.go:96] failed to register workDuration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648208   18145 prometheus.go:112] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648232   18145 prometheus.go:126] failed to register unfinished metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648256   18145 prometheus.go:152] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648279   18145 prometheus.go:164] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648323   18145 prometheus.go:176] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648401   18145 prometheus.go:188] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648426   18145 prometheus.go:203] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">E0705 22:38:02.648449   18145 prometheus.go:216] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted</span><br><span class="line">I0705 22:38:02.648476   18145 plugins.go:158] Loaded 5 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,DefaultStorageClass.</span><br><span class="line">I0705 22:38:02.648483   18145 plugins.go:161] Loaded 3 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,ResourceQuota.</span><br><span class="line">I0705 22:38:02.650334   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.650349   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.650387   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.650508   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.656909   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:02.656960   18145 storage_factory.go:50] Storage caching is enabled for *apiregistration.APIService with capacity 1000</span><br><span class="line">I0705 22:38:02.657001   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">W0705 22:38:02.657086   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.657094   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.657250   18145 store.go:1343] Monitoring apiservices.apiregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.657324   18145 reflector.go:160] Listing and watching *apiregistration.APIService from storage&#x2F;cacher.go:&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.657709   18145 client.go:354] parsed scheme: &quot;&quot;</span><br><span class="line">I0705 22:38:02.657729   18145 client.go:354] scheme &quot;&quot; not registered, fallback to default scheme</span><br><span class="line">I0705 22:38:02.657761   18145 asm_amd64.s:1337] ccResolverWrapper: sending new addresses to cc: [&#123;10.129.173.92:2379 0  &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.657796   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.92:2379 &lt;nil&gt;&#125; &#123;10.129.173.93:2379 &lt;nil&gt;&#125; &#123;10.129.173.94:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.663447   18145 balancer_conn_wrappers.go:131] clientv3&#x2F;balancer: pin &quot;10.129.173.93:2379&quot;</span><br><span class="line">I0705 22:38:02.663513   18145 asm_amd64.s:1337] balancerWrapper: got update addr from Notify: [&#123;10.129.173.93:2379 &lt;nil&gt;&#125;]</span><br><span class="line">I0705 22:38:02.663518   18145 storage_factory.go:50] Storage caching is enabled for *apiregistration.APIService with capacity 1000</span><br><span class="line">W0705 22:38:02.663656   18145 asm_amd64.s:1337] Failed to dial 10.129.173.92:2379: grpc: the connection is closing; please retry.</span><br><span class="line">W0705 22:38:02.663610   18145 asm_amd64.s:1337] Failed to dial 10.129.173.94:2379: grpc: the connection is closing; please retry.</span><br><span class="line">I0705 22:38:02.663728   18145 store.go:1343] Monitoring apiservices.apiregistration.k8s.io count at &lt;storage-prefix&gt;&#x2F;&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.663773   18145 reflector.go:160] Listing and watching *apiregistration.APIService from storage&#x2F;cacher.go:&#x2F;apiregistration.k8s.io&#x2F;apiservices</span><br><span class="line">I0705 22:38:02.665260   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:02.666477   18145 watch_cache.go:405] Replace watchCache (rev: 75099443) </span><br><span class="line">I0705 22:38:02.674592   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:02.674627   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:03.616355   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:03.616397   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:04.268801   18145 deprecated_insecure_serving.go:53] Serving insecurely on 127.0.0.1:8073</span><br><span class="line">I0705 22:38:04.268882   18145 healthz.go:107] Installing healthz checkers:&quot;ping&quot;,&quot;log&quot;,&quot;etcd&quot;,&quot;poststarthook&#x2F;generic-apiserver-start-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-informers&quot;,&quot;poststarthook&#x2F;start-apiextensions-controllers&quot;,&quot;poststarthook&#x2F;crd-informer-synced&quot;,&quot;poststarthook&#x2F;bootstrap-controller&quot;,&quot;poststarthook&#x2F;rbac&#x2F;bootstrap-roles&quot;,&quot;poststarthook&#x2F;scheduling&#x2F;bootstrap-system-priority-classes&quot;,&quot;poststarthook&#x2F;ca-registration&quot;,&quot;poststarthook&#x2F;start-kube-apiserver-admission-initializer&quot;,&quot;poststarthook&#x2F;start-kube-aggregator-informers&quot;,&quot;poststarthook&#x2F;apiservice-registration-controller&quot;,&quot;poststarthook&#x2F;apiservice-status-available-controller&quot;,&quot;poststarthook&#x2F;apiservice-openapi-controller&quot;,&quot;poststarthook&#x2F;kube-apiserver-autoregistration&quot;,&quot;autoregister-completion&quot;</span><br><span class="line">I0705 22:38:04.269477   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269524   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269546   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269551   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269572   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269582   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269487   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269624   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269643   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269667   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269626   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269685   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;policy&#x2F;v1beta1</span><br><span class="line">I0705 22:38:04.269694   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269711   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269754   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269755   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.269648   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269805   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269807   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269843   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269863   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.269740   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.269905   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.269926   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1</span><br><span class="line">I0705 22:38:04.269501   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270002   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270029   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.269615   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270116   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270137   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270197   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;persistentvolumes?limit&#x3D;500&amp;resourceVersion&#x3D;0: (896.994µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47384]</span><br><span class="line">I0705 22:38:04.270317   18145 wrap.go:47] GET &#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (810.468µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47382]</span><br><span class="line">I0705 22:38:04.269636   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270387   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270408   18145 secure_serving.go:116] Serving securely on [::]:6443</span><br><span class="line">I0705 22:38:04.269501   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.270438   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.270446   18145 apiservice_controller.go:94] Starting APIServiceRegistrationController</span><br><span class="line">I0705 22:38:04.270455   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270468   18145 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller</span><br><span class="line">I0705 22:38:04.270497   18145 wrap.go:47] GET &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses?limit&#x3D;500&amp;resourceVersion&#x3D;0: (944.078µs) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47386]</span><br><span class="line">I0705 22:38:04.270522   18145 crdregistration_controller.go:112] Starting crd-autoregister controller</span><br><span class="line">I0705 22:38:04.270535   18145 controller_utils.go:1029] Waiting for caches to sync for crd-autoregister controller</span><br><span class="line">I0705 22:38:04.270538   18145 available_controller.go:376] Starting AvailableConditionController</span><br><span class="line">I0705 22:38:04.270559   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;replicationcontrollers?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.21442ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47388]</span><br><span class="line">I0705 22:38:04.270572   18145 naming_controller.go:288] Starting NamingConditionController</span><br><span class="line">I0705 22:38:04.270575   18145 establishing_controller.go:73] Starting EstablishingController</span><br><span class="line">I0705 22:38:04.270652   18145 nonstructuralschema_controller.go:191] Starting NonStructuralSchemaConditionController</span><br><span class="line">I0705 22:38:04.270550   18145 crd_finalizer.go:255] Starting CRDFinalizer</span><br><span class="line">I0705 22:38:04.270509   18145 autoregister_controller.go:140] Starting autoregister controller</span><br><span class="line">I0705 22:38:04.270683   18145 cache.go:32] Waiting for caches to sync for autoregister controller</span><br><span class="line">I0705 22:38:04.270523   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:04.270694   18145 wrap.go:47] GET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.172166ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47392]</span><br><span class="line">I0705 22:38:04.270730   18145 controller.go:81] Starting OpenAPI AggregationController</span><br><span class="line">I0705 22:38:04.270745   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:04.270567   18145 customresource_discovery_controller.go:208] Starting DiscoveryController</span><br><span class="line">I0705 22:38:04.270566   18145 cache.go:32] Waiting for caches to sync for AvailableConditionController controller</span><br><span class="line">I0705 22:38:04.270560   18145 controller.go:83] Starting OpenAPI controller</span><br><span class="line">I0705 22:38:04.270815   18145 reflector.go:122] Starting reflector *v1.Node (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.269778   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.270831   18145 reflector.go:160] Listing and watching *v1.Node from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271226   18145 reflector.go:122] Starting reflector *v1.VolumeAttachment (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271245   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;nodes?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.966293ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47378]</span><br><span class="line">I0705 22:38:04.271268   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.271289   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.271294   18145 reflector.go:122] Starting reflector *v1.ClusterRoleBinding (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271313   18145 reflector.go:160] Listing and watching *v1.ClusterRoleBinding from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271226   18145 reflector.go:122] Starting reflector *v1.Role (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271330   18145 reflector.go:160] Listing and watching *v1.Role from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271335   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;services?limit&#x3D;500&amp;resourceVersion&#x3D;0: (2.065372ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47380]</span><br><span class="line">I0705 22:38:04.271277   18145 reflector.go:122] Starting reflector *v1.Service (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271421   18145 reflector.go:160] Listing and watching *v1.Service from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271299   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1</span><br><span class="line">I0705 22:38:04.271246   18145 reflector.go:160] Listing and watching *v1.VolumeAttachment from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.270926   18145 reflector.go:122] Starting reflector *v1.ClusterRole (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271521   18145 reflector.go:160] Listing and watching *v1.ClusterRole from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271547   18145 reflector.go:122] Starting reflector *v1.Namespace (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271560   18145 reflector.go:160] Listing and watching *v1.Namespace from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271568   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m1s</span><br><span class="line">I0705 22:38:04.271011   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.271591   18145 reflector.go:122] Starting reflector *v1.StorageClass (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271628   18145 reflector.go:160] Listing and watching *v1.StorageClass from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.270877   18145 reflector.go:122] Starting reflector *apiextensions.CustomResourceDefinition (5m0s) from k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271679   18145 reflector.go:160] Listing and watching *apiextensions.CustomResourceDefinition from k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271690   18145 reflector.go:122] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271728   18145 reflector.go:160] Listing and watching *v1.ServiceAccount from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271618   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.271772   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.271831   18145 reflector.go:122] Starting reflector *v1.LimitRange (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271845   18145 reflector.go:122] Starting reflector *v1.RoleBinding (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271870   18145 reflector.go:160] Listing and watching *v1.RoleBinding from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271856   18145 reflector.go:160] Listing and watching *v1.LimitRange from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271105   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims?limit&#x3D;500&amp;resourceVersion&#x3D;0: (1.59494ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47390]</span><br><span class="line">I0705 22:38:04.272020   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;persistentvolumes, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m27s</span><br><span class="line">I0705 22:38:04.271112   18145 reflector.go:122] Starting reflector *v1.PersistentVolume (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271077   18145 reflector.go:122] Starting reflector *v1.Secret (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272079   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272104   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272086   18145 reflector.go:160] Listing and watching *v1.Secret from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272125   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.271432   18145 reflector.go:122] Starting reflector *v1.Endpoints (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272173   18145 reflector.go:160] Listing and watching *v1.Endpoints from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.271027   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272219   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272236   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;policy&#x2F;v1beta1</span><br><span class="line">E0705 22:38:04.272283   18145 controller.go:148] Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: &#x2F;registry&#x2F;masterleases&#x2F;10.129.173.93, ResourceVersion: 0, AdditionalErrorMsg: </span><br><span class="line">I0705 22:38:04.271034   18145 reflector.go:122] Starting reflector *v1.ResourceQuota (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272332   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;statefulsets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m58s</span><br><span class="line">I0705 22:38:04.272351   18145 reflector.go:160] Listing and watching *v1.ResourceQuota from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272413   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.904677ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:04.272069   18145 reflector.go:160] Listing and watching *v1.PersistentVolume from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272410   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;policy&#x2F;v1beta1&#x2F;poddisruptionbudgets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m16s</span><br><span class="line">I0705 22:38:04.271906   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.535252ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:04.270860   18145 reflector.go:122] Starting reflector *apiregistration.APIService (30s) from k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.271255   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272581   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272618   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;replicationcontrollers&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272540   18145 reflector.go:160] Listing and watching *apiregistration.APIService from k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117</span><br><span class="line">I0705 22:38:04.272651   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272689   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272550   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272736   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272749   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;nodes&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272763   18145 reflector.go:122] Starting reflector *v1.Pod (10m0s) from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272781   18145 reflector.go:160] Listing and watching *v1.Pod from k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133</span><br><span class="line">I0705 22:38:04.272707   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.272807   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.272709   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;services&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272823   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.272852   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;replicationcontrollers, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;9m32s</span><br><span class="line">I0705 22:38:04.272924   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;nodes, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;5m3s</span><br><span class="line">I0705 22:38:04.273029   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;persistentvolumeclaims, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;9m31s</span><br><span class="line">I0705 22:38:04.273029   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;services, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m33s</span><br><span class="line">I0705 22:38:04.274175   18145 wrap.go:47] GET &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets?limit&#x3D;500&amp;resourceVersion&#x3D;0: (4.688192ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47394]</span><br><span class="line">I0705 22:38:04.276744   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.276763   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.276773   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets&quot; satisfied by gorestful with webservice &#x2F;apis&#x2F;apps&#x2F;v1</span><br><span class="line">I0705 22:38:04.276915   18145 get.go:250] Starting watch for &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;replicasets, rv&#x3D;75099443 labels&#x3D; fields&#x3D; timeout&#x3D;6m28s</span><br><span class="line">I0705 22:38:04.278832   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;pods?fieldSelector&#x3D;status.phase%21%3DFailed%2Cstatus.phase%21%3DSucceeded&amp;limit&#x3D;500&amp;resourceVersion&#x3D;0: (9.407581ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;scheduler 127.0.0.1:47396]</span><br><span class="line">I0705 22:38:04.289292   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:04.289312   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:04.289321   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;pods&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:04.289495   18145 get.go:250] Starting watch for &#x2F;api&#x2F;v1&#x2F;pods, rv&#x3D;75099443 labels&#x3D; fields&#x3D;status.phase!&#x3D;Failed,status.phase!&#x3D;Succeeded timeout&#x3D;7m21s</span><br><span class="line">I0705 22:38:05.268153   18145 controller.go:107] OpenAPI AggregationController: Processing item </span><br><span class="line">I0705 22:38:05.268190   18145 controller.go:130] OpenAPI AggregationController: action for item : Nothing (removed from the queue).</span><br><span class="line">I0705 22:38:05.268201   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000000</span><br><span class="line">I0705 22:38:05.268208   18145 controller.go:130] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).</span><br><span class="line">I0705 22:38:05.268215   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:05.268278   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:05.268310   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:05.268358   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:05.268389   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:05.268397   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:06.268539   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:06.268663   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:06.268687   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:06.268752   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:06.268776   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:06.268785   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.268914   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:07.269013   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.269030   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.269080   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:07.269105   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.269113   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:07.554461   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:07.554491   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:07.554501   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:07.556815   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.426905ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:08.269219   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:08.269327   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.269343   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:08.269394   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:08.269425   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.269434   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:08.419039   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:08.419077   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:08.419089   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:08.421916   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.948807ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:09.269557   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:09.269664   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:09.269680   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:09.269747   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:09.269778   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:09.269786   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:10.269881   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:10.269989   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:10.270004   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:10.270056   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:10.270083   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:10.270092   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.270200   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:11.270306   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.270321   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.270373   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:11.270398   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.270408   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:11.483133   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.483166   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:11.483178   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:11.485580   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.529963ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:11.598249   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:11.598284   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:11.598294   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:11.601107   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.928152ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:12.270550   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:12.270681   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:12.270698   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:12.270752   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:12.270801   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:12.270809   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.270887   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:13.270984   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.271000   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.271050   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:13.271074   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.271083   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:13.693554   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:13.693619   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:13.693637   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:13.696146   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.666525ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:14.271182   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:14.271290   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.271305   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:14.271356   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:14.271380   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.271398   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:14.380002   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:14.380035   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:14.380045   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:14.382250   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.312715ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:15.271510   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:15.271590   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:15.271616   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:15.271664   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:15.271688   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:15.271696   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:16.271827   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:16.271937   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:16.271980   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:16.272042   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:16.272065   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:16.272073   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.272204   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:17.272313   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.272329   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.272380   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:17.272405   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.272414   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:17.283099   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.283123   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:17.283133   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:17.285196   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.159257ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:17.870495   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:17.870526   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:17.870543   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:17.873004   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.577282ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:18.272560   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:18.272685   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:18.272701   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:18.272760   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:18.272786   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:18.272794   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.272924   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:19.273020   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.273035   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.273084   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:19.273106   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.273116   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:19.659566   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:19.659613   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:19.659624   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:19.662463   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.967004ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:20.273261   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:20.273367   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:20.273382   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:20.273429   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:20.273451   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:20.273459   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:21.273614   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:21.273740   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:21.273759   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:21.273840   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:21.273866   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:21.273878   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.126431   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.126472   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:22.126482   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:22.129022   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.675659ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:22.273990   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:22.274084   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.274097   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.274146   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:22.274168   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.274176   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:22.957210   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:22.957246   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:22.957255   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:22.959473   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.335823ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:23.274299   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:23.274407   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:23.274422   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:23.274471   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:23.274492   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:23.274500   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.274621   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:24.274734   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.274749   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.274801   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:24.274824   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.274832   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:24.618447   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:24.618488   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:24.618498   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:24.621739   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (3.351212ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:25.274963   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:25.275074   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:25.275092   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:25.275145   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:25.275181   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:25.275190   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.275293   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:26.275395   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.275410   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.275465   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:26.275488   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.275497   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:26.925578   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:26.925641   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:26.925652   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:26.927894   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.404426ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:27.275645   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:27.275777   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:27.275793   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:27.275846   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:27.275868   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:27.275877   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.276002   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:28.276107   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.276122   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.276172   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:28.276194   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.276203   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:28.664065   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:28.664097   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:28.664107   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:28.666395   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.399236ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:29.276334   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:29.276422   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:29.276436   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:29.276489   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:29.276522   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:29.276531   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.276644   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:30.276774   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.276790   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.276843   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:30.276868   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.276877   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:30.616203   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:30.616236   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:30.616246   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:30.618555   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-controller-manager?timeout&#x3D;10s: (2.432457ms) 200 [kube-controller-manager&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47734]</span><br><span class="line">I0705 22:38:31.277014   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:31.277123   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:31.277138   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:31.277186   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:31.277207   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:31.277216   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.277331   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:32.277437   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.277452   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.277515   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:32.277542   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.277551   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:32.511091   18145 handler.go:153] kube-aggregator: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:32.511128   18145 pathrecorder.go:253] kube-aggregator: &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by NotFoundHandler</span><br><span class="line">I0705 22:38:32.511138   18145 handler.go:143] kube-apiserver: GET &quot;&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler&quot; satisfied by gorestful with webservice &#x2F;api&#x2F;v1</span><br><span class="line">I0705 22:38:32.513369   18145 wrap.go:47] GET &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;endpoints&#x2F;kube-scheduler?timeout&#x3D;10s: (2.359242ms) 200 [kube-scheduler&#x2F;v1.15.5 (linux&#x2F;amd64) kubernetes&#x2F;20c265f&#x2F;leader-election 127.0.0.1:47866]</span><br><span class="line">I0705 22:38:33.277687   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:33.277802   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:33.277817   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:33.277883   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:33.277906   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:33.277915   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:34.270818   18145 discovery.go:214] Invalidating discovery information</span><br><span class="line">I0705 22:38:34.271625   18145 trace.go:81] Trace[916838159]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271124387 +0800 CST m&#x3D;+3.485745147) (total time: 30.000454926s):</span><br><span class="line">Trace[916838159]: [30.000454926s] [30.000454926s] END</span><br><span class="line">E0705 22:38:34.271664   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Node: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;nodes?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271749   18145 trace.go:81] Trace[1068818673]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271318857 +0800 CST m&#x3D;+3.485939616) (total time: 30.000409266s):</span><br><span class="line">Trace[1068818673]: [30.000409266s] [30.000409266s] END</span><br><span class="line">I0705 22:38:34.271762   18145 trace.go:81] Trace[1453832293]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271338371 +0800 CST m&#x3D;+3.485959213) (total time: 30.00039864s):</span><br><span class="line">Trace[1453832293]: [30.00039864s] [30.00039864s] END</span><br><span class="line">E0705 22:38:34.271784   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Role: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;roles?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271764   18145 trace.go:81] Trace[1415476892]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271426707 +0800 CST m&#x3D;+3.486047513) (total time: 30.000302054s):</span><br><span class="line">Trace[1415476892]: [30.000302054s] [30.000302054s] END</span><br><span class="line">E0705 22:38:34.271817   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Service: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;services?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">E0705 22:38:34.271764   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ClusterRoleBinding: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterrolebindings?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271863   18145 trace.go:81] Trace[1943398002]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27156591 +0800 CST m&#x3D;+3.486186674) (total time: 30.000282292s):</span><br><span class="line">Trace[1943398002]: [30.000282292s] [30.000282292s] END</span><br><span class="line">E0705 22:38:34.271873   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Namespace: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;namespaces?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271862   18145 trace.go:81] Trace[2040039466]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271529276 +0800 CST m&#x3D;+3.486150039) (total time: 30.000321445s):</span><br><span class="line">Trace[2040039466]: [30.000321445s] [30.000321445s] END</span><br><span class="line">E0705 22:38:34.271884   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ClusterRole: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;clusterroles?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271906   18145 trace.go:81] Trace[1482887971]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271448982 +0800 CST m&#x3D;+3.486069767) (total time: 30.000436887s):</span><br><span class="line">Trace[1482887971]: [30.000436887s] [30.000436887s] END</span><br><span class="line">E0705 22:38:34.271923   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.VolumeAttachment: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;volumeattachments?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.271952   18145 trace.go:81] Trace[1817353754]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271633058 +0800 CST m&#x3D;+3.486253814) (total time: 30.000305162s):</span><br><span class="line">Trace[1817353754]: [30.000305162s] [30.000305162s] END</span><br><span class="line">E0705 22:38:34.271961   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.StorageClass: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;storage.k8s.io&#x2F;v1&#x2F;storageclasses?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272034   18145 trace.go:81] Trace[1163666912]: &quot;Reflector k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27168685 +0800 CST m&#x3D;+3.486307613) (total time: 30.000322616s):</span><br><span class="line">Trace[1163666912]: [30.000322616s] [30.000322616s] END</span><br><span class="line">E0705 22:38:34.272081   18145 reflector.go:125] k8s.io&#x2F;apiextensions-apiserver&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117: Failed to list *apiextensions.CustomResourceDefinition: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;apiextensions.k8s.io&#x2F;v1beta1&#x2F;customresourcedefinitions?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272069   18145 trace.go:81] Trace[821616895]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271737097 +0800 CST m&#x3D;+3.486357868) (total time: 30.0003199s):</span><br><span class="line">Trace[821616895]: [30.0003199s] [30.0003199s] END</span><br><span class="line">E0705 22:38:34.272096   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ServiceAccount: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;serviceaccounts?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272148   18145 trace.go:81] Trace[1495176524]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271923158 +0800 CST m&#x3D;+3.486543922) (total time: 30.000215143s):</span><br><span class="line">Trace[1495176524]: [30.000215143s] [30.000215143s] END</span><br><span class="line">E0705 22:38:34.272162   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.LimitRange: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;limitranges?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272297   18145 trace.go:81] Trace[1304373005]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.271876523 +0800 CST m&#x3D;+3.486497285) (total time: 30.000397052s):</span><br><span class="line">Trace[1304373005]: [30.000397052s] [30.000397052s] END</span><br><span class="line">E0705 22:38:34.272308   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.RoleBinding: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;rbac.authorization.k8s.io&#x2F;v1&#x2F;rolebindings?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272401   18145 trace.go:81] Trace[2092646535]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27211704 +0800 CST m&#x3D;+3.486737797) (total time: 30.000268938s):</span><br><span class="line">Trace[2092646535]: [30.000268938s] [30.000268938s] END</span><br><span class="line">E0705 22:38:34.272410   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Secret: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;secrets?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272483   18145 trace.go:81] Trace[475362393]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272182677 +0800 CST m&#x3D;+3.486803437) (total time: 30.000287299s):</span><br><span class="line">Trace[475362393]: [30.000287299s] [30.000287299s] END</span><br><span class="line">E0705 22:38:34.272490   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Endpoints: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;endpoints?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272688   18145 trace.go:81] Trace[990205759]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272462183 +0800 CST m&#x3D;+3.487082944) (total time: 30.000213936s):</span><br><span class="line">Trace[990205759]: [30.000213936s] [30.000213936s] END</span><br><span class="line">E0705 22:38:34.272697   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.PersistentVolume: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;persistentvolumes?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272692   18145 trace.go:81] Trace[24141869]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.27236239 +0800 CST m&#x3D;+3.486983158) (total time: 30.000315126s):</span><br><span class="line">Trace[24141869]: [30.000315126s] [30.000315126s] END</span><br><span class="line">E0705 22:38:34.272707   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.ResourceQuota: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;resourcequotas?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.272982   18145 trace.go:81] Trace[868760541]: &quot;Reflector k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272649159 +0800 CST m&#x3D;+3.487269923) (total time: 30.000318724s):</span><br><span class="line">Trace[868760541]: [30.000318724s] [30.000318724s] END</span><br><span class="line">E0705 22:38:34.272991   18145 reflector.go:125] k8s.io&#x2F;kube-aggregator&#x2F;pkg&#x2F;client&#x2F;informers&#x2F;internalversion&#x2F;factory.go:117: Failed to list *apiregistration.APIService: Get https:&#x2F;&#x2F;localhost:6443&#x2F;apis&#x2F;apiregistration.k8s.io&#x2F;v1&#x2F;apiservices?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.273086   18145 trace.go:81] Trace[1379481655]: &quot;Reflector k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133 ListAndWatch&quot; (started: 2021-07-05 22:38:04.272785699 +0800 CST m&#x3D;+3.487406462) (total time: 30.000282688s):</span><br><span class="line">Trace[1379481655]: [30.000282688s] [30.000282688s] END</span><br><span class="line">E0705 22:38:34.273100   18145 reflector.go:125] k8s.io&#x2F;client-go&#x2F;informers&#x2F;factory.go:133: Failed to list *v1.Pod: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;pods?limit&#x3D;500&amp;resourceVersion&#x3D;0: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">F0705 22:38:34.275033   18145 controller.go:157] Unable to perform initial IP allocation check: unable to refresh the service IP block: Get https:&#x2F;&#x2F;localhost:6443&#x2F;api&#x2F;v1&#x2F;services: dial tcp 127.0.0.1:6443: i&#x2F;o timeout</span><br><span class="line">I0705 22:38:34.303248   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000001</span><br><span class="line">I0705 22:38:34.341834   18145 handler.go:153] kube-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:34.341849   18145 pathrecorder.go:240] kube-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br><span class="line">I0705 22:38:34.341904   18145 controller.go:105] OpenAPI AggregationController: Processing item k8s_internal_local_delegation_chain_0000000002</span><br><span class="line">I0705 22:38:34.341926   18145 handler.go:153] apiextensions-apiserver: GET &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by nonGoRestful</span><br><span class="line">I0705 22:38:34.341934   18145 pathrecorder.go:240] apiextensions-apiserver: &quot;&#x2F;openapi&#x2F;v2&quot; satisfied by exact match</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/kubernetes/kubernetes/issues/82067">https://github.com/kubernetes/kubernetes/issues/82067</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;p&gt;三个 master （etcd 也在 master 上，master上也有 kubelet）和 n 个 node。maste</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="systemd" scheme="http://zhangguanzhang.github.io/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>Job for docker.service canceled</title>
    <link href="http://zhangguanzhang.github.io/2021/07/05/systemctl-start-docker-canceled/"/>
    <id>http://zhangguanzhang.github.io/2021/07/05/systemctl-start-docker-canceled/</id>
    <published>2021-07-05T17:08:06.000Z</published>
    <updated>2021-07-05T17:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><p>内部安装 docker 的脚本报错 docker 安装失败。然后启动发现下面奇怪的问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line">$ systemctl start docker</span><br><span class="line">Job for docker.service canceled.</span><br></pre></td></tr></table></figure><p>但是用 <code>service docker start</code> 能启动，这就很迷，尝试前台启动也无啥错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ service docker start</span><br><span class="line">$ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Mon 2021-07-05 14:41:30 CST; 15s ago</span><br><span class="line">     Docs: http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line">$ systemctl cat docker</span><br><span class="line"># &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Docker Application Container Engine</span><br><span class="line">Documentation&#x3D;http:&#x2F;&#x2F;docs.docker.io</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Environment&#x3D;&quot;PATH&#x3D;&#x2F;data&#x2F;kube&#x2F;bin:&#x2F;bin:&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;sbin&quot;</span><br><span class="line">ExecStart&#x3D;&#x2F;data&#x2F;kube&#x2F;bin&#x2F;dockerd </span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;sh -c &#39;&#x2F;sbin&#x2F;iptables --wait -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;sh -c &#39;&#x2F;sbin&#x2F;iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecReload&#x3D;&#x2F;bin&#x2F;kill -s HUP $MAINPID</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">RestartSec&#x3D;5</span><br><span class="line">LimitNOFILE&#x3D;infinity</span><br><span class="line">LimitNPROC&#x3D;infinity</span><br><span class="line">LimitCORE&#x3D;infinity</span><br><span class="line">Delegate&#x3D;yes</span><br><span class="line">KillMode&#x3D;process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>从上面<code>systemctl cat docker</code>看是没有依赖服务的，如果官方<code>rpm</code> 包安装的会依赖<code>containerd</code>。不过先看下失败的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl --failed</span><br><span class="line">  UNIT       LOAD   ACTIVE SUB    DESCRIPTION                                                                                                                                                                                              </span><br><span class="line">● data.mount loaded failed failed &#x2F;data                                                                                                                                                                                                    </span><br><span class="line"></span><br><span class="line">LOAD   &#x3D; Reflects whether the unit definition was properly loaded.</span><br><span class="line">ACTIVE &#x3D; The high-level unit activation state, i.e. generalization of SUB.</span><br><span class="line">SUB    &#x3D; The low-level unit activation state, values depend on unit type.</span><br><span class="line"></span><br><span class="line">1 loaded units listed. Pass --all to see loaded but inactive units, too.</span><br><span class="line">To show all installed unit files use &#39;systemctl list-unit-files&#39;.</span><br></pre></td></tr></table></figure><p>信息被冲没了，后面拿其他机器信息复制下，<code>systemctl start</code>会连 dbus 之类的，而 service 不会，结合前面的 data 挂载失败，系统应该是 <code>emergency</code> 半启动导致的，看了下果然</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status emergency </span><br><span class="line">● emergency.service - Emergency Shell</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;emergency.service; static; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Mon 2021-07-05 17:18:28 CST; 5min ago</span><br><span class="line">     Docs: man:sulogin(8)</span><br><span class="line">  Process: 674 ExecStartPre&#x3D;&#x2F;bin&#x2F;plymouth --wait quit (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 675 (systemd-sulogin)</span><br><span class="line">    Tasks: 5 (limit: 4915)</span><br><span class="line">   Memory: 32.9M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;emergency.service</span><br><span class="line">           ├─647 &#x2F;sbin&#x2F;sulogin</span><br><span class="line">           ├─675 &#x2F;lib&#x2F;systemd&#x2F;systemd-sulogin-shell emergency</span><br><span class="line">           ├─676 &#x2F;sbin&#x2F;sulogin</span><br><span class="line">           ├─677 bash</span><br><span class="line">           └─678 &#x2F;sbin&#x2F;sulogin</span><br><span class="line"></span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: Started Emergency Shell.</span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: emergency.service: Found left-over process 647 (sulogin) in control group while starting unit. Ignoring.</span><br><span class="line">Jul 05 17:18:28 host100 systemd[1]: This usually indicates unclean termination of a previous run, or service implementation deficiencies.</span><br></pre></td></tr></table></figure><p>然后看了下<code>/etc/fstab</code>，是把<code>defaults</code>写成了<code>default</code>导致的无法挂载。然后解决重启后好了。</p><p>询问了测试人员，她说她改了<code>/etc/fstab</code>后看启动<code>emergency mode</code>的输入root密码提示，然后输入root密码进去。然后启动 sshd 失败，然后用 <code>service sshd start</code>。然后 ssh 上去部署docker，然后我们这边 ssh 上来看，之前接触的centos 版本在 emergency 模式貌似不会有网。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;故障现象&quot;&gt;&lt;a href=&quot;#故障现象&quot; class=&quot;headerlink&quot; title=&quot;故障现象&quot;&gt;&lt;/a&gt;故障现象&lt;/h2&gt;&lt;p&gt;内部安装 docker 的脚本报错 docker 安装失败。然后启动发现下面奇怪的问题:&lt;/p&gt;
&lt;figure class</summary>
      
    
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
    <category term="systemd" scheme="http://zhangguanzhang.github.io/tags/systemd/"/>
    
  </entry>
  
  <entry>
    <title>openshift 4.5.9 etcd损坏+脑裂修复过程</title>
    <link href="http://zhangguanzhang.github.io/2021/06/08/ocp4.5.9-restore-etcd/"/>
    <id>http://zhangguanzhang.github.io/2021/06/08/ocp4.5.9-restore-etcd/</id>
    <published>2021-06-08T18:36:06.000Z</published>
    <updated>2021-06-08T18:36:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h2><p>内部机器和环境都是在 vcenter 里，之前的 ocp 集群是 3 master + 1 worker，也就是之前的<a href="./ocp-4.5-install.md">openshift 4.5.9 离线安装</a>后的环境，后面有几台宿主机负载太高，同事看我机器负载最高，关了几台，这几天需要用下 <code>openshift</code> 环境。登录到 <code>bastion</code> 上 get 超时，看了下 haproxy 的 stat web，全部红了。。然后把所有机器开机后发现还是起不来。</p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>openshift 的 master 节点和 kubeadm 很像，几个组件都是 <code>staticPod</code> 形式起的。客户端也不是 <code>docker</code>，使用 <code>crictl</code> 就行了</p><h3 id="查看-kube-apiserver"><a href="#查看-kube-apiserver" class="headerlink" title="查看 kube-apiserver"></a>查看 kube-apiserver</h3><p>ssh 到 master1 上，查看日志发现是 etcd 无法起来</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh -i ~/.ssh/new_rsa core@10.x.45.251</span><br><span class="line">crictl ps -a | grep kube-apiserver</span><br><span class="line">crictl logs xxx</span><br></pre></td></tr></table></figure><p>etcd 只有一台正常，一台日志报错 snap 文件损坏，一台报错 revision 太低，先进入正常的那台上面 etcd 容器（下文所有 etcdctl 都是在 etcd 容器里执行的，进容器就是下面命令）:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | grep etcd</span><br><span class="line">crictl <span class="built_in">exec</span> -ti xxx bash</span><br><span class="line"></span><br><span class="line">$ env | grep ETCDCTL</span><br><span class="line">ETCDCTL_CERT=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master2.openshift4.example.com.crt</span><br><span class="line">ETCDCTL_ENDPOINTS=https://10.x.45.251:2379,https://10.x.45.252:2379,https://10.x.45.222:2379</span><br><span class="line">ETCDCTL_CACERT=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span><br><span class="line">ETCDCTL_API=3</span><br><span class="line">ETCDCTL_KEY=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master2.openshift4.example.com.key</span><br><span class="line"></span><br><span class="line">$ etcd --version</span><br><span class="line">etcd Version: 3.4.9</span><br><span class="line">Git SHA: 4657b9e</span><br><span class="line">Go Version: go1.13.4</span><br><span class="line">Go OS/Arch: linux/amd64</span><br></pre></td></tr></table></figure><h3 id="故障的开始"><a href="#故障的开始" class="headerlink" title="故障的开始"></a>故障的开始</h3><p>这里有个知识点就是 etcd 和 etcdctl 的一些命令行选项都可以被环境变量替代，例如上面的这些。忘了从哪个版本开始了。 <code>etcdctl snapshot save </code> 时候 <code>endpoints</code> 只能指定一个节点，尝试备份，结果卡住：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl snapshot save 0608-etcd.db</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623124958.5931418,<span class="string">&quot;caller&quot;</span>,<span class="string">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;create temporary db file&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;0608-etcd.db.part&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h3 id="一点进展"><a href="#一点进展" class="headerlink" title="一点进展"></a>一点进展</h3><p>这套集群当初以为就用一下，没考虑备份。然后在机器上乱逛，发现了高版本集群是自带了备份的（至少我这个版本是自带了）。在目录 <code>/etc/kubernetes/rollbackcopy</code> 下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/kubernetes/</span><br><span class="line">$ ll rollbackcopy/*</span><br><span class="line">rollbackcopy/currentVersion.latest:</span><br><span class="line">total 707152</span><br><span class="line">-rw-r--r--. 1 root root        58 Jun  8 09:17 backupenv.json</span><br><span class="line">-rw-------. 1 root root 724054048 Jun  8 09:17 snapshot_2021-06-08_091655.db</span><br><span class="line">-rw-------. 1 root root     59426 Jun  8 09:17 static_kuberesources_2021-06-08_091655.tar.gz</span><br><span class="line"></span><br><span class="line">rollbackcopy/currentVersion.prev:</span><br><span class="line">total 707152</span><br><span class="line">-rw-r--r--. 1 root root        58 Jun  8 08:11 backupenv.json</span><br><span class="line">-rw-------. 1 root root 724054048 Jun  8 08:11 snapshot_2021-06-08_081148.db</span><br><span class="line">-rw-------. 1 root root     59426 Jun  8 08:11 static_kuberesources_2021-06-08_081148.tar.gz</span><br></pre></td></tr></table></figure><p>然后像用 etcdctl 恢复备份，从容器里拷贝出来，结果 crictl 没 cp 命令，查看了下 etcdctl 的挂载，想进 etcd 容器里把 etcdctl 复制到挂载的路径上，这样宿主机上就有了。结果搞出来之后，习惯性的把二进制文件移到<code>/usr/local/bin/</code>下，结果 tab 按键补全看到有下面几个脚本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">cluster-backup.sh  cluster-restore.sh  recover-kubeconfig.sh</span><br></pre></td></tr></table></figure><h4 id="自带的备份恢复"><a href="#自带的备份恢复" class="headerlink" title="自带的备份恢复"></a>自带的备份恢复</h4><p>查看了下 <code>cluster-restore.sh</code> 脚本，脚本第一个参数是指定备份目录，也就是上面发现的目录，<strong>尝试在不正常的两个节点上</strong> 运行下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /etc/kubernetes/rollbackcopy/currentVersion.latest</span><br><span class="line">$ cluster-restore.sh .</span><br><span class="line">...stopping kube-apiserver-pod.yml</span><br><span class="line">...stopping kube-controller-manager-pod.yml</span><br><span class="line">...stopping kube-scheduler-pod.yml</span><br><span class="line">...stopping etcd-pod.yml</span><br><span class="line">Waiting <span class="keyword">for</span> container etcd to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container etcdctl to stop</span><br><span class="line">...................................complete</span><br><span class="line">Waiting <span class="keyword">for</span> container etcd-metrics to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-controller-manager to stop</span><br><span class="line">complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-apiserver to stop</span><br><span class="line">.........................complete</span><br><span class="line">Waiting <span class="keyword">for</span> container kube-scheduler to stop</span><br><span class="line">complete</span><br><span class="line">starting restore-etcd static pod</span><br><span class="line">starting kube-apiserver-pod.yml</span><br><span class="line">static-pod-resource/kube-apiserver-pod-50/kube-apiserver-pod.yaml</span><br><span class="line">starting kube-controller-manager-pod.yml</span><br><span class="line">static-pod-resource/kube-controller-manager-pod-7/kube-controller-manager-pod.yml</span><br><span class="line">starting kube-scheduler-pod.yml</span><br><span class="line">static-pod-resource/kube-scheduler-pod-7/kube-scheduler-pod.yml</span><br></pre></td></tr></table></figure><p>这个脚本运行期间的等待容器停止要根据实际情况可能需要自己去手动stop，可以使用下面的去 stop 相关容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">STATIC_POD_CONTAINERS=(<span class="string">&quot;etcd&quot;</span> <span class="string">&quot;etcdctl&quot;</span> <span class="string">&quot;etcd-metrics&quot;</span> <span class="string">&quot;kube-controller-manager&quot;</span> <span class="string">&quot;kube-apiserver&quot;</span> <span class="string">&quot;kube-scheduler&quot;</span>)</span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">wait_for_containers_to_stop</span></span>()&#123;</span><br><span class="line">  <span class="built_in">local</span> CONTAINERS=(<span class="string">&quot;<span class="variable">$@</span>&quot;</span>) ctrID</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> NAME <span class="keyword">in</span> <span class="string">&quot;<span class="variable">$&#123;CONTAINERS[@]&#125;</span>&quot;</span>; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Waiting for container <span class="variable">$&#123;NAME&#125;</span> to stop&quot;</span></span><br><span class="line">    ctrID=<span class="string">&quot;<span class="subst">$(crictl ps --label io.kubernetes.container.name=$&#123;NAME&#125; -q)</span>&quot;</span></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$ctrID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">      crictl stop <span class="variable">$ctrID</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">wait_for_containers_to_stop <span class="variable">$&#123;STATIC_POD_CONTAINERS[*]&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>执行完后的状态:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ crictl ps -a</span><br><span class="line">CONTAINER           IMAGE                                                              CREATED             STATE               NAME                                          ATTEMPT             POD ID</span><br><span class="line">44f09a621803e       5e0c1e21da05b4b6455632cb70d9e29c76a5710e0bfa129ebef00d1cc1d5ee85   39 seconds ago      Running             etcd                                          0                   ad5b9a73adb3a</span><br><span class="line">b728530b11ea7       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-scheduler                                1                   af1e63ab536fd</span><br><span class="line">ec9b583321b64       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-apiserver                                26                  f68ce6fa73ad6</span><br><span class="line">05431a2d159b9       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   40 seconds ago      Running             kube-controller-manager                       1                   cc1a03361154d</span><br><span class="line">4a1d734b14faf       b7838c3ae6383695ca8c6b3e900e9b9ce221d843bf16a7c61fe1a5e13f58f4a6   36 minutes ago      Exited              kube-apiserver                                25                  f68ce6fa73ad6</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><code>kube-apiserver</code>起来了，然后能使用 oc 了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ oc get node</span><br><span class="line">NAME                             STATUS   ROLES           AGE    VERSION</span><br><span class="line">master1.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">master2.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">master3.openshift4.example.com   Ready    master,worker   262d   v1.18.3+6c42de8</span><br><span class="line">worker1.openshift4.example.com   Ready    worker          259d   v1.18.3+6c42de8</span><br></pre></td></tr></table></figure><h3 id="etcd-的脑裂"><a href="#etcd-的脑裂" class="headerlink" title="etcd 的脑裂"></a>etcd 的脑裂</h3><p>然后我的开发 namespaces 下有个 pod pending，<code>kubectl</code> 删了下报错，大致是 etcd 删不掉啥的。然后看了下 etcd 的状态。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl endpoint status --write-out=table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 7ad933dac58f4549 |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   30260441 |           30260441 |        |</span><br><span class="line">| https://10.x.45.252:2379 | f4351098cae1d407 |   3.4.9 |  726 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   40195520 |           40195520 |        |</span><br><span class="line">| https://10.x.45.222:2379 | 2399ef0cea33ebf3 |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |         2 |   50408739 |           50408739 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>没错，脑裂了。三个都找不到其他的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">7ad933dac58f4549, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl member list</span><br><span class="line">f7f6c198cb519536, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl member list</span><br><span class="line">ac62bec820f40228, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br></pre></td></tr></table></figure><h4 id="处理脑裂"><a href="#处理脑裂" class="headerlink" title="处理脑裂"></a>处理脑裂</h4><h5 id="前置准备"><a href="#前置准备" class="headerlink" title="前置准备"></a>前置准备</h5><p>尝试下 <code>move-leader</code> 命令看看能否操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl move-leader 7ad933dac58f4549</span><br><span class="line">2021-06-08 02:59:35.782766 C | pkg/flags: conflicting environment variable <span class="string">&quot;ETCDCTL_ENDPOINTS&quot;</span> is shadowed by corresponding command-line flag (either <span class="built_in">unset</span> environment variable or <span class="built_in">disable</span> flag)</span><br></pre></td></tr></table></figure><p>说环境变量和命令行同时设置了 endpoints ，unset 下它后尝试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ unset ETCDCTL_ENDPOINTS</span><br><span class="line">$ export ETCDCTL_ENDPOINTS&#x3D;https:&#x2F;&#x2F;10.x.45.222:2379</span><br><span class="line">$ etcdctl move-leader 7ad933dac58f4549</span><br><span class="line">2021-06-08 03:00:11.019150 C | pkg&#x2F;flags: conflicting environment variable &quot;ETCDCTL_CERT&quot; is shadowed by corresponding command-line flag (either unset environment variable or disable flag)</span><br></pre></td></tr></table></figure><p>然后另一个变量报错，搜了下这个是 <code>etcdctl move-leader</code> 的 bug，见 <a href="https://github.com/etcd-io/etcd/pull/12757">pr</a> ，手动 unset 相关 <code>ETCDCTL_xxx</code> 变量后执行下报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl --endpoints https://10.x.45.222:2379 \</span><br><span class="line">  --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \</span><br><span class="line">  --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master3.openshift4.example.com.crt \</span><br><span class="line">  --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master3.openshift4.example.com.key \</span><br><span class="line">  move-leader 7ad933dac58f4549</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T03:07:08.767Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;endpoint://client-27e0f779-7d90-4be3-9491-f0e915374f3c/10.xxx.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = FailedPrecondition desc = etcdserver: bad leader transferee&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>好吧，<code>move-leader</code>对这种场景用不了。不过现在三个都起来了，应该是能备份了。准备在其他节点上用备份恢复下，先看了下 etcd staticPod 的 yaml <code>/etc/kubernetes/manifests/etcd-pod.yaml</code> 的内容启动参数是否需要调整：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">|</span></span><br><span class="line">        <span class="comment">#!/bin/sh</span></span><br><span class="line">        <span class="string">set</span> <span class="string">-euo</span> <span class="string">pipefail</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="string">if</span> [ <span class="string">!</span> <span class="string">-z</span> <span class="string">$(ls</span> <span class="string">-A</span> <span class="string">&quot;/var/lib/etcd&quot;</span><span class="string">)</span> ]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;please delete the contents of data directory before restoring, running the restore script will do this for you&quot;</span></span><br><span class="line">          <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if we have backup file to be restored</span></span><br><span class="line">        <span class="comment"># if the file exist, check if it has not changed size in last 5 seconds</span></span><br><span class="line">        <span class="string">if</span> [ <span class="string">!</span> <span class="string">-f</span> <span class="string">/var/lib/etcd-backup/snapshot.db</span> ]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">&quot;please make a copy of the snapshot db file, then move that copy to /var/lib/etcd-backup/snapshot.db&quot;</span></span><br><span class="line">          <span class="string">exit</span> <span class="number">1</span></span><br><span class="line">        <span class="string">else</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>从逻辑看是启动的时候如果数据目录必须不为空，然后<code>/var/lib/etcd-backup/</code>目录得存在备份的 db 文件，看了下挂载目录，宿主机上也是这个目录。打算先在第一个 master 节点的 etcd 容器里备份。然后用备份文件在其他节点恢复备份。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl  snapshot save /var/lib/etcd-backup/snapshot.db</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143517.6130972,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:119&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;created temporary db file&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;/var/lib/etcd-backup/snapshot.db.part&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:11:57.621Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/maintenance.go:200&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;opened snapshot stream; downloading&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143517.6213868,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:127&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;fetching snapshot&quot;</span>,<span class="string">&quot;endpoint&quot;</span>:<span class="string">&quot;https://10.x.45.251:2379&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:12:03.315Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/maintenance.go:208&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;completed snapshot read; closing&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143524.4544568,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:142&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;fetched snapshot&quot;</span>,<span class="string">&quot;endpoint&quot;</span>:<span class="string">&quot;https://10.x.45.251:2379&quot;</span>,<span class="string">&quot;size&quot;</span>:<span class="string">&quot;724 MB&quot;</span>,<span class="string">&quot;took&quot;</span>:6.841299825&#125;</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;info&quot;</span>,<span class="string">&quot;ts&quot;</span>:1623143524.4545853,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;snapshot/v3_snapshot.go:152&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;saved&quot;</span>,<span class="string">&quot;path&quot;</span>:<span class="string">&quot;/var/lib/etcd-backup/snapshot.db&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>然后停掉后面的两台 master 节点的 etcd。改名 <code>/var/lib/etcd</code> 目录（不要一上来就删除目录，改名是永远最稳妥的手段）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/kubernetes/</span><br><span class="line">cp manifests/etcd-pod.yaml .</span><br><span class="line">mv manifests/etcd-pod.yaml /tmp/</span><br><span class="line"></span><br><span class="line">etcdID=<span class="string">&quot;<span class="subst">$(crictl ps --label io.kubernetes.container.name=etcd -q)</span>&quot;</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$etcdID</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">    crictl stop <span class="variable">$etcdID</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">mv /var/lib/etcd /var/lib/etcd-bak</span><br></pre></td></tr></table></figure><p>然后用密钥 ssh 到其他机器上把 ssh 的 root 和密码登录开了来让我们可以使用 scp 过去，然后备份文件<code>/var/lib/etcd-backup/snapshot.db</code> scp 过去到其余 master 上的同样路径。</p><h5 id="处理脑裂-1"><a href="#处理脑裂-1" class="headerlink" title="处理脑裂"></a>处理脑裂</h5><p>细心观察看前面的每个 endpoint 下 member list 是看到的自己的。所以是在 master1 上逐渐添加其他 member。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member add master2.openshift4.example.com --peer-urls=https://10.x.45.252:2380</span><br><span class="line">Member 3e27197aa4521ea0 added to cluster 1c2134e7d41c45b1</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;master2.openshift4.example.com&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;master2.openshift4.example.com=https://10.x.45.252:2380,master1.openshift4.example.com=https://10.x.45.251:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">&quot;https://10.x.45.252:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure><p>然后在第二个 master 上改下 etcd 的 staticPod yaml <code>/tmp/etcd-pod.yaml</code>。根据自身的实际更改，删掉备份和恢复相关的逻辑。主要是更改 <code>ETCD_INITIAL_CLUSTER</code> 为所有集群，格式为 <code>$&#123;name1&#125;=https://$&#123;ip1&#125;:2380,$&#123;name2&#125;=https://$&#123;ip2&#125;:2380...</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">        <span class="comment">#export ETCD_INITIAL_CLUSTER=xxx</span></span><br><span class="line">        <span class="string">NAME_ETCD_ARRAY=()</span></span><br><span class="line">        <span class="string">for</span> <span class="string">i</span> <span class="string">in</span> <span class="string">$(env</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-Po</span> <span class="string">&#x27;(?&lt;=NODE_).+(?=_ETCD_NAME)&#x27;</span> <span class="string">|</span> <span class="string">sort</span> <span class="string">);do</span></span><br><span class="line">            <span class="string">etcd_name=NODE_$&#123;i&#125;_ETCD_NAME</span></span><br><span class="line">            <span class="string">url_host_var=NODE_$&#123;i&#125;_ETCD_URL_HOST</span></span><br><span class="line">            <span class="string">NAME_ETCD_ARRAY+=(&quot;$&#123;!etcd_name&#125;=https://$&#123;!url_host_var&#125;:2380&quot;)</span></span><br><span class="line">        <span class="string">done</span></span><br><span class="line">        <span class="string">export</span> <span class="string">ETCD_INITIAL_CLUSTER=$(</span> <span class="string">echo</span> <span class="string">$&#123;NAME_ETCD_ARRAY[*]&#125;</span> <span class="string">|</span> <span class="string">tr</span> <span class="string">&#x27; &#x27;</span> <span class="string">&#x27;,&#x27;</span> <span class="string">)</span></span><br></pre></td></tr></table></figure><p>改好后在 master2 上启动 etcd ：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /tmp/etcd-pod.yaml /etc/kubernetes/manifests/</span><br></pre></td></tr></table></figure><p>在 master1 上查看:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, unstarted, , https://10.x.45.252:2380, , <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, unstarted, , https://10.x.45.252:2380, , <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:26:16.669Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;passthrough:///https://10.x.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = DeadlineExceeded desc = latest balancer error: connection error: desc = \&quot;transport: Error while dialing dial tcp 10.x.45.222:2379: connect: connection refused\&quot;&quot;</span>&#125;</span><br><span class="line">Failed to get the status of endpoint https://10.x.45.222:2379 (context deadline exceeded)</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       1041 |               1041 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       1041 |               1041 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>好消息，然后恢复第三个，添加第三个member：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl member add master3.openshift4.example.com --peer-urls=https://10.x.45.222:2380</span><br><span class="line">Member d319fa1cbb0e28fe added to cluster 1c2134e7d41c45b1</span><br><span class="line"></span><br><span class="line">ETCD_NAME=<span class="string">&quot;master3.openshift4.example.com&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">&quot;master2.openshift4.example.com=https://10.x.45.252:2380,master1.openshift4.example.com=https://10.x.45.251:2380,master3.openshift4.example.com=https://10.x.45.222:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">&quot;https://10.x.45.222:2380&quot;</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">&quot;existing&quot;</span></span><br></pre></td></tr></table></figure><p>第三个 etcd yaml 也像之前一样更改。启动后在持续观察：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">&#123;<span class="string">&quot;level&quot;</span>:<span class="string">&quot;warn&quot;</span>,<span class="string">&quot;ts&quot;</span>:<span class="string">&quot;2021-06-08T09:32:55.197Z&quot;</span>,<span class="string">&quot;caller&quot;</span>:<span class="string">&quot;clientv3/retry_interceptor.go:62&quot;</span>,<span class="string">&quot;msg&quot;</span>:<span class="string">&quot;retrying of unary invoker failed&quot;</span>,<span class="string">&quot;target&quot;</span>:<span class="string">&quot;passthrough:///https://10.x.45.222:2379&quot;</span>,<span class="string">&quot;attempt&quot;</span>:0,<span class="string">&quot;error&quot;</span>:<span class="string">&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;</span>&#125;</span><br><span class="line">Failed to get the status of endpoint https://10.x.45.222:2379 (context deadline exceeded)</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2388 |               2388 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2388 |               2388 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2593 |               2593 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2593 |               2593 |        |</span><br><span class="line">| https://10.x.45.222:2379 | d319fa1cbb0e28fe |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2596 |               2596 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">[root@master1 /]$ etcdctl endpoint status -w table</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br><span class="line">| https://10.x.45.251:2379 | 831fd1ef9bc83a2b |   3.4.9 |  724 MB |      <span class="literal">true</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">| https://10.x.45.252:2379 | 3e27197aa4521ea0 |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">| https://10.x.45.222:2379 | d319fa1cbb0e28fe |   3.4.9 |  724 MB |     <span class="literal">false</span> |      <span class="literal">false</span> |       632 |       2939 |               2939 |        |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</span><br></pre></td></tr></table></figure><p>单独看每个 endpoint 的 member list 正常否：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.251:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.252:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br><span class="line">[root@master1 /]$ ETCDCTL_ENDPOINTS=https://10.x.45.222:2379 etcdctl  member list</span><br><span class="line">3e27197aa4521ea0, started, master2.openshift4.example.com, https://10.x.45.252:2380, https://10.x.45.252:2379, <span class="literal">false</span></span><br><span class="line">831fd1ef9bc83a2b, started, master1.openshift4.example.com, https://10.x.45.251:2380, https://10.x.45.251:2379, <span class="literal">false</span></span><br><span class="line">d319fa1cbb0e28fe, started, master3.openshift4.example.com, https://10.x.45.222:2380, https://10.x.45.222:2379, <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>然后看了下 node not ready了，approve 了所有 csr后就好了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc get csr</span><br><span class="line">oc adm certificate approve xxx</span><br></pre></td></tr></table></figure><p>然后测试了下，kubectl 能删掉 pod 了。后续等稳定后再手动备份下</p><h5 id="无法调度和-logs-报错-remote-error-tls-internal-error"><a href="#无法调度和-logs-报错-remote-error-tls-internal-error" class="headerlink" title="无法调度和 logs 报错 remote error: tls: internal error"></a>无法调度和 logs 报错 remote error: tls: internal error</h5><p>同时也无法调度，master 上去看 <code>kube-apiserver</code>日志刷：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">authentication.go:53] Unable to authenticate the request due to an error: x509: certificate signed by unkown authority</span><br></pre></td></tr></table></figure><p>搜了下都没解决办法，最后自己在 master 上屏直觉找到解决办法了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> :;<span class="keyword">do</span></span><br><span class="line">  sleep 2</span><br><span class="line">  oc get csr -o name | xargs -r oc adm certificate approve</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>另一个窗口 ssh 到 master上停掉 <code>cert-syncer</code> 相关容器:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | awk <span class="string">&#x27;/Running/&amp;&amp;/-cert-syncer/&#123;print $1&#125;&#x27;</span> | xargs -r crictl stop</span><br></pre></td></tr></table></figure><h4 id="一些疑惑"><a href="#一些疑惑" class="headerlink" title="一些疑惑"></a>一些疑惑</h4><p>后面尝试自带的 yaml 文件+那个备份脚本恢复的就是脑裂集群，询问了个 4.7.13 集群的，看了下 etcd 的 yaml 文件是下面的。没有启动前恢复备份啥的了。可能我这个版本才存在这种问题吧。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">                    <span class="comment">#!/bin/sh</span></span><br><span class="line">                    <span class="string">set</span> <span class="string">-euo</span> <span class="string">pipefail</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">etcdctl</span> <span class="string">member</span> <span class="string">list</span> <span class="string">||</span> <span class="literal">true</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># this has a non-zero return code if the command is non-zero.  If you use an export first, it doesn&#x27;t and you</span></span><br><span class="line">                    <span class="comment"># will succeed when you should fail.</span></span><br><span class="line">                    <span class="string">ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--endpoints=$&#123;ALL_ETCD_ENDPOINTS&#125;</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--data-dir=/var/lib/etcd</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--target-peer-url-host=$&#123;NODE_master11_cluster_lonlife_dev_ETCD_URL_HOST&#125;</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--target-name=master11.cluster.lonlife.dev)</span></span><br><span class="line">                     <span class="string">export</span> <span class="string">ETCD_INITIAL_CLUSTER</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># we cannot use the \&quot;normal\&quot; port conflict initcontainer because when we upgrade, the existing static pod will never yield,</span></span><br><span class="line">                    <span class="comment"># so we do the detection in etcd container itsefl.</span></span><br><span class="line">                    <span class="string">echo</span> <span class="string">-n</span> <span class="string">\&quot;Waiting</span> <span class="string">for</span> <span class="string">ports</span> <span class="number">2379</span><span class="string">,</span> <span class="number">2380 </span><span class="string">and</span> <span class="number">9978 </span><span class="string">to</span> <span class="string">be</span> <span class="string">released.\&quot;</span></span><br><span class="line">                    <span class="string">while</span> [ <span class="string">-n</span> <span class="string">\&quot;$(ss</span> <span class="string">-Htan</span> <span class="string">&#x27;( sport = 2379 or sport = 2380 or sport = 9978 )&#x27;</span><span class="string">)\&quot;</span> ]<span class="string">;</span> <span class="string">do</span></span><br><span class="line">                      <span class="string">echo</span> <span class="string">-n</span> <span class="string">\&quot;.\&quot;</span></span><br><span class="line">                      <span class="string">sleep</span> <span class="number">1</span></span><br><span class="line">                    <span class="string">done</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">export</span> <span class="string">ETCD_NAME=$&#123;NODE_master11_cluster_lonlife_dev_ETCD_NAME&#125;</span></span><br><span class="line">                    <span class="string">env</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">ETCD</span> <span class="string">|</span> <span class="string">grep</span> <span class="string">-v</span> <span class="string">NODE</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="string">set</span> <span class="string">-x</span></span><br><span class="line">                    <span class="comment"># See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice</span></span><br><span class="line">                    <span class="string">exec</span> <span class="string">ionice</span> <span class="string">-c2</span> <span class="string">-n0</span> <span class="string">etcd</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--log-level=info</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--initial-advertise-peer-urls=https://$&#123;NODE_master11_cluster_lonlife_dev_IP&#125;:2380</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--client-cert-auth=true</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-master11.cluster.lonlife.dev.key</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--peer-client-cert-auth=true</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--advertise-client-urls=https://$&#123;NODE_master11_cluster_lonlife_dev_IP&#125;:2379</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-client-urls=https://0.0.0.0:2379</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-peer-urls=https://0.0.0.0:2380</span> <span class="string">\\</span></span><br><span class="line">                      <span class="string">--listen-metrics-urls=https://0.0.0.0:9978</span> <span class="string">||</span>  <span class="string">mv</span> <span class="string">/etc/kubernetes/etcd-backup-dir/etcd-member.yaml</span> <span class="string">/etc/kubernetes/manifests</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://etcd.io/docs/v3.4/op-guide/runtime-configuration/">etcd op guide</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/backup_and_restore/backing-up-etcd.html">backing-up-etcd</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言介绍&quot;&gt;&lt;a href=&quot;#前言介绍&quot; class=&quot;headerlink&quot; title=&quot;前言介绍&quot;&gt;&lt;/a&gt;前言介绍&lt;/h2&gt;&lt;p&gt;内部机器和环境都是在 vcenter 里，之前的 ocp 集群是 3 master + 1 worker，也就是之前的&lt;a </summary>
      
    
    
    
    
    <category term="openshift" scheme="http://zhangguanzhang.github.io/tags/openshift/"/>
    
    <category term="ocp" scheme="http://zhangguanzhang.github.io/tags/ocp/"/>
    
  </entry>
  
  <entry>
    <title>docker-ce 18.09.3 启动panic: invalid freelist page: 56, page type is leaf的解决处理</title>
    <link href="http://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/"/>
    <id>http://zhangguanzhang.github.io/2021/05/26/docker-panic-invalid-freelist-page/</id>
    <published>2021-05-26T19:52:37.000Z</published>
    <updated>2021-05-26T19:52:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>这个问题和之前的<a href="https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/">docker-18.06.3-ce启动panic: invalid page type: 0: 0的解决处理</a>差不多，不过 db 文件不同。客户停止 docker 后起不来了，查看日志：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -xe -u docker</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17+08:00&quot; level&#x3D;warning msg&#x3D;&quot;The \&quot;graph\&quot; config file option is deprecated. Please use \&quot;data-root\&quot; instead.&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.841082978+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not change group &#x2F;var&#x2F;run&#x2F;docker.sock to docker: group docker not found&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.858839019+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.btrfs&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.859857612+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.aufs&quot; error&#x3D;&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860062002+08:00&quot; level&#x3D;warning msg&#x3D;&quot;failed to load plugin io.containerd.snapshotter.v1.zfs&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860084186+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter zfs in metadata plugin&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860090453+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter btrfs in metadata plugin&quot; error&#x3D;&quot;path &#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">5月 26 18:42:17 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:17.860095653+08:00&quot; level&#x3D;warning msg&#x3D;&quot;could not use snapshotter aufs in metadata plugin&quot; error&#x3D;&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.866980   19061 flags.go:33] FLAG: --container-runtime&#x3D;&quot;docker&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.866984   19061 flags.go:33] FLAG: --container-runtime-endpoint&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;dockershim.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867014   19061 flags.go:33] FLAG: --docker&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867018   19061 flags.go:33] FLAG: --docker-endpoint&#x3D;&quot;unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867022   19061 flags.go:33] FLAG: --docker-env-metadata-whitelist&#x3D;&quot;&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867025   19061 flags.go:33] FLAG: --docker-only&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867028   19061 flags.go:33] FLAG: --docker-root&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;docker&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867032   19061 flags.go:33] FLAG: --docker-tls&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867036   19061 flags.go:33] FLAG: --docker-tls-ca&#x3D;&quot;ca.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867039   19061 flags.go:33] FLAG: --docker-tls-cert&#x3D;&quot;cert.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867043   19061 flags.go:33] FLAG: --docker-tls-key&#x3D;&quot;key.pem&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867138   19061 flags.go:33] FLAG: --experimental-dockershim&#x3D;&quot;false&quot;</span><br><span class="line">5月 26 18:42:17 xxxx kubelet[19061]: I0526 18:42:17.867143   19061 flags.go:33] FLAG: --experimental-dockershim-root-directory&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;dockershim&quot;</span><br><span class="line">5月 26 18:42:18 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:18.208058217+08:00&quot; level&#x3D;error msg&#x3D;&quot;Failed to load container 90e5da1293b04c3eab30e9f7a2d5714aba563b2ab20d15c67aee1d1d79d3154c: json: cannot unmarshal number into Go value of type container.Container&quot;</span><br><span class="line">5月 26 18:42:18 xxxx dockerd[19053]: time&#x3D;&quot;2021-05-26T18:42:18.214213879+08:00&quot; level&#x3D;error msg&#x3D;&quot;Failed to load container 99abceba7154b7dba08cc03bc1651fd18f38b0d485dcd2c195f479fc375a8216: invalid character &#39;e&#39; looking for beginning of value&quot;</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: panic: invalid freelist page: 56, page type is leaf</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: goroutine 1 [running]:</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*freelist).read(0xc424b121b0, 0x7fc46c7b1000)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;freelist.go:237 +0x2ff</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).loadFreelist.func1()</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:292 +0x12d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: sync.(*Once).Do(0xc420b70328, 0xc420fb1f58)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;sync&#x2F;once.go:44 +0xc0</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).loadFreelist(0xc420b701e0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:285 +0x50</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.Open(0xc422bddad0, 0x2b, 0x1a4, 0xc420fb2070, 0x20c754c, 0x23816c0, 0x25ceee8)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:262 +0x316</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb.(*BoltDB).getDBhandle(0xc424b10960, 0x32ef098, 0xc424b109a4, 0x7fc47c368e38)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb&#x2F;boltdb.go:113 +0x93</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb.(*BoltDB).List(0xc424b10960, 0xc422b99fe0, 0x1b, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libkv&#x2F;store&#x2F;boltdb&#x2F;boltdb.go:274 +0xbb</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*cache).kmap(0xc424a60fe0, 0x26b6dc0, 0xc422a0e4d0, 0x4467ea, 0xc422b99f9a, 0x1a668ce)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;cache.go:43 +0x18a</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*cache).list(0xc424a60fe0, 0x26b6dc0, 0xc422a0e4d0, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;cache.go:164 +0x7b</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore.(*datastore).List(0xc42352b180, 0xc422b99f80, 0x1b, 0x26b6dc0, 0xc422a0e4d0, 0x0, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;datastore&#x2F;datastore.go:517 +0x179</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).populateNetworks(0xc42203d5c0, 0x5, 0x1a6a443)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge_store.go:50 +0xe0</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).initStore(0xc42203d5c0, 0xc424b316b0, 0x0, 0xc423529bc0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge_store.go:35 +0x207</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.(*driver).configure(0xc42203d5c0, 0xc424b316b0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge.go:378 +0x1b4</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge.Init(0x268abc0, 0xc424119740, 0xc424b316b0, 0x0, 0xffffffffffffffff)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drivers&#x2F;bridge&#x2F;bridge.go:161 +0xa2</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drvregistry.(*DrvRegistry).AddDriver(0xc424119740, 0x1a6a539, 0x6, 0x266fe78, 0xc424b316b0, 0xc42264df80, 0x8)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;drvregistry&#x2F;drvregistry.go:72 +0x48</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork.New(0xc42264df80, 0x9, 0x10, 0xc4209e03f0, 0xc420c7f620, 0xc42264df80, 0x9)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;docker&#x2F;libnetwork&#x2F;controller.go:220 +0x4a5</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).initNetworkController(0xc420a281e0, 0xc42087d900, 0xc420c7f620, 0xc420a281e0, 0xc42239c740, 0xc420c7f620, 0xc420c7f5f0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon_unix.go:807 +0xa9</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).restore(0xc420a281e0, 0xc4209b8300, 0xc4208e4790)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:419 +0xd6f</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;daemon.NewDaemon(0x26a6240, 0xc4209b8300, 0xc42087d900, 0xc4209e03f0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:987 +0x2c4b</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.(*DaemonCli).start(0xc42024b890, 0xc42018f320, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;daemon.go:180 +0x74f</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.runDaemon(0xc42018f320, 0xc420876200, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker_unix.go:7 +0x47</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.newDaemonCommand.func1(0xc42087ac80, 0x32ef098, 0x0, 0x0, 0x0, 0x0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:29 +0x5d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).execute(0xc42087ac80, 0xc42003a1e0, 0x0, 0x0, 0xc42087ac80, 0xc42003a1e0)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:762 +0x46a</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).ExecuteC(0xc42087ac80, 0x267bb40, 0x2254580, 0x267bb50)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:852 +0x30c</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).Execute(0xc42087ac80, 0xc42000e020, 0x4d067f)</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:800 +0x2d</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: main.main()</span><br><span class="line">5月 26 18:42:25 xxxx dockerd[19053]: &#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:70 +0xa2</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: docker.service: main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENT</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: Unit docker.service entered failed state.</span><br><span class="line">5月 26 18:42:25 xxxx systemd[1]: docker.service failed.</span><br></pre></td></tr></table></figure><p>首先得根据这个 panic 的堆栈，调用关系是 <code>main.main</code> -&gt; <code>cobra</code> -&gt; <code>docker daemon</code> -&gt; <code>daemon.(*Daemon).restore</code> -&gt; <code>initNetworkController</code> -&gt; <code>libnetwork/datastore/cache</code> -&gt; <code>boltdb</code><br>docker 使用了 boltdb 存储了网络信息成 <code>db</code> 文件，但是这个 db 文件损坏了，导致读取字节序列化错误类型，去 docker 的目录 find 下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ find &#x2F;data&#x2F;kube&#x2F;docker -type f -size -5M -name &#39;*.db&#39; | grep -v overlay2</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;containerd&#x2F;daemon&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;volumes&#x2F;metadata.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;network&#x2F;files&#x2F;local-kv.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;builder&#x2F;fscache.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;snapshots.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;metadata.db</span><br><span class="line">&#x2F;data&#x2F;kube&#x2F;docker&#x2F;buildkit&#x2F;cache.db</span><br></pre></td></tr></table></figure><p>改名 db 文件重启 docker 解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv /data/kube/docker/network/files/local-kv.db&#123;,.bak&#125;</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这个问题和之前的&lt;a href=&quot;https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/&quot;&gt;docker-18.06.3-ce启动panic: invalid page type</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/categories/kubernetes/docker/"/>
    
    <category term="panic" scheme="http://zhangguanzhang.github.io/categories/kubernetes/docker/panic/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>一次单节点单个pod网络问题排查过程</title>
    <link href="http://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/"/>
    <id>http://zhangguanzhang.github.io/2021/04/30/kubernetes-sec-agent-node-network-error/</id>
    <published>2021-04-30T11:28:30.000Z</published>
    <updated>2021-04-30T11:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="about"><a href="#about" class="headerlink" title="about"></a>about</h2><p>现场反馈客户环境上业务不正常，根据调用链去看某个业务A日志，发现无法请求另一个业务B，把业务 A 的探针取消了，加上</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tty: true</span><br><span class="line">command: [&quot;bash&quot;]</span><br></pre></td></tr></table></figure><p>起来后进去 curl 了下 B 对应的 svcIP 接口是能通的。然后手动起业务进程，再开个窗口 exec 进去 curl 发现就不通了，k8s node数量是只有一个，并且只有这一个 pod 有问题。后面排查到是用户的安全软件导致的。软件名是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep agent</span><br><span class="line">root       6349  0.3  0.1 21046316 116820 ?     Sl   11:08   0:02 /CloudResetPwdUpdateAgent/depend/jre1.8.0_232/bin/java -Dorg.tanukisoftware.wrapper.WrapperSimpleApp.maxStartMainWait=40 -Djava.library.path=../lib -classpath ../lib/resetpwdupdateagent.jar:../lib/wrapper.jar:../lib/json-20160810.jar:../lib/log4j-api-2.8.2.jar:../lib/log4j-core-2.8.2.jar -Dwrapper.key=osxWGEBk6yYtP6sr -Dwrapper.backend=pipe -Dwrapper.disable_console_input=TRUE -Dwrapper.pid=6019 -Dwrapper.version=3.5.26 -Dwrapper.native_library=wrapper -Dwrapper.arch=x86 -Dwrapper.service=TRUE -Dwrapper.cpu.timeout=10 -Dwrapper.jvmid=1 org.tanukisoftware.wrapper.WrapperSimpleApp CloudResetPwdUpdateAgent</span><br><span class="line">root      13860 76.1  0.3 796288 253072 ?       Sl   11:08   8:27 /usr/local/dbappsecurity/edr/agent_service runservice</span><br><span class="line">root      14188  0.0  0.0  46004  6000 ?        S    11:08   0:00 /usr/local/dbappsecurity/edr/agent_daemon</span><br><span class="line">root      17399  0.0  0.0 112712   976 pts/0    S+   11:19   0:00 grep --color=auto agent</span><br><span class="line">root      22206  0.0  0.0  22496  1448 ?        S    11:08   0:00 vm-agent</span><br><span class="line">root      22215  0.1  0.0 628744  4104 ?        Sl   11:08   0:01 vm-agent</span><br></pre></td></tr></table></figure><p>杀掉 <code>dbappsecurity</code> 两个进程后重建业务 A 的 pod 后就正常了。</p><p>之前也遇到过安全软件导致 pod 网络通信异常 eof 的，列举一些国产遇到过的软件软件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ds_agent  # 查下 agent 关键字</span><br><span class="line">qaxsafed  # 奇安信，查下 qax 看看有没有其他的</span><br><span class="line">secdog    # 也查下 dog 和 sec</span><br><span class="line">sangfor_watchdog # 这个不影响，但是有它基本是深信服的虚拟化环境，会和flannel的8472端口冲突</span><br><span class="line">YDservice</span><br><span class="line">Symantec</span><br><span class="line">start360su_safed   # 推荐 ps aux | grep safe 先查下</span><br><span class="line">gov_defence_service</span><br><span class="line">gov_defence_guard      #  ps aux | grep defence </span><br><span class="line">wsssr_defence_daemon   # 奇安信服务器安全加固系统，和下面是一起的。目前遇到过影响 socat 运行和容器进程访问另一个机器上的mysql端口</span><br><span class="line">wsssr_defence_service</span><br><span class="line">wsssr_defence_agent   # 影响pod网络</span><br><span class="line">ics_agent</span><br><span class="line"></span><br><span class="line">/opt/nubosh/vmsec-host/intedrity/bin/icsintedrity # docker -p 的都无法访问</span><br><span class="line">/opt/nubosh/vmsec-host/file/bin/icsfilesec</span><br><span class="line"></span><br><span class="line">edr_sec_plan   # ps aux | grep edr ,深信服的 edr，这个会下发 iptables 规则，配置错了会影响 node 之间，以及 pod 和 pod 之间通信</span><br></pre></td></tr></table></figure><h2 id="一些卸载笔记"><a href="#一些卸载笔记" class="headerlink" title="一些卸载笔记"></a>一些卸载笔记</h2><p>wsssr_defence_daemon</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/wsssr_defence_agent</span><br><span class="line">./uninstall</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;about&quot;&gt;&lt;a href=&quot;#about&quot; class=&quot;headerlink&quot; title=&quot;about&quot;&gt;&lt;/a&gt;about&lt;/h2&gt;&lt;p&gt;现场反馈客户环境上业务不正常，根据调用链去看某个业务A日志，发现无法请求另一个业务B，把业务 A 的探针取消了，加上</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubelet 和 runc 编译关闭 kmem</title>
    <link href="http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/"/>
    <id>http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/</id>
    <published>2021-04-08T17:28:30.000Z</published>
    <updated>2021-04-08T17:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提详情"><a href="#前提详情" class="headerlink" title="前提详情"></a>前提详情</h2><p>在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修复。</p><p>网上有言论说升级 Linux 内核至 <code>kernel-3.10.0-1075.el7</code> 及以上就可以修复这个问题，详细可见 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1507149#c101">slab leak causing a crash when using kmem control group</a>。但是我测试了下面的都不行：</p><ul><li>CentOS7.4</li><li>CentOS7.6</li><li>CentOS7.7的 3.10.0-1062.el7.x86_64 </li><li>CentOS Linux release 7.8.2003 (Core) - 3.10.0-1127.el7.x86_64</li></ul><p>Linux其余发行版内核如果大于等于 4.4 应该没问题。<br>这里我们编译 kubelet 关闭 kmem。</p><h2 id="准备条件"><a href="#准备条件" class="headerlink" title="准备条件"></a>准备条件</h2><p>这里我们使用的编译参数会使用容器编译的，不需要宿主机上安装 golang，安装个 docker 就行了。</p><ol><li><p><code>1c 4g</code> 的机器，这里我是使用 <code> CentOS 7.8.2003 (Core)</code><br>机器配置 2g 内存的时候编译提示 oom，升级到 4g 内存才编译成功的。</p></li><li><p>最好安装最新版本的docker</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable --now firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -ri &#x27;/^[^#]*SELINUX=/s#=.+$#=disabled#&#x27; /etc/selinux/config</span><br><span class="line"></span><br><span class="line">cat&gt;/etc/security/limits.d/custom.conf&lt;&lt;EOF</span><br><span class="line">*       soft    nproc   131072</span><br><span class="line">*       hard    nproc   131072</span><br><span class="line">*       soft    nofile  131072</span><br><span class="line">*       hard    nofile  131072</span><br><span class="line">root    soft    nproc   131072</span><br><span class="line">root    hard    nproc   131072</span><br><span class="line">root    soft    nofile  131072</span><br><span class="line">root    hard    nofile  131072</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat&lt;&lt;EOF &gt; /etc/sysctl.d/docker.conf</span><br><span class="line"># 要求iptables对bridge的数据进行处理</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-arptables = 1</span><br><span class="line"># 开启转发</span><br><span class="line">net.ipv4.ip_forward = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line">curl -fsSL &quot;https://get.docker.com/&quot; | \</span><br><span class="line">  sed -r &#x27;/add-repo \$yum_repo/a sed -i &quot;s#https://download.docker.com#http://mirrors.aliyun.com/docker-ce#&quot; /etc/yum.repos.d/docker-*.repo &#x27; | \</span><br><span class="line">    bash -s -- --mirror Aliyun</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/docker/</span><br><span class="line">cat&gt;/etc/docker/daemon.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;bip&quot;: &quot;172.17.0.1/16&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https://fz5yth0r.mirror.aliyuncs.com&quot;,</span><br><span class="line">    &quot;https://dockerhub.mirrors.nwafu.edu.cn&quot;,</span><br><span class="line">    &quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">    &quot;https://reg-mirror.qiniu.com&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/systemd/system/docker.service.d/</span><br><span class="line">cat&gt;/etc/systemd/system/docker.service.d/10-docker.conf&lt;&lt;EOF</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPost=/sbin/iptables --wait -I FORWARD -s 0.0.0.0/0 -j ACCEPT</span><br><span class="line">ExecStopPost=/bin/bash -c &#x27;/sbin/iptables --wait -D FORWARD -s 0.0.0.0/0 -j ACCEPT &amp;&gt; /dev/null || :&#x27;</span><br><span class="line">ExecStartPost=/sbin/iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost=/bin/bash -c &#x27;/sbin/iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; /dev/null || :&#x27;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y epel-release bash-completion </span><br><span class="line">cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/</span><br><span class="line"></span><br><span class="line">systemctl enable --now docker</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><h3 id="确定版本"><a href="#确定版本" class="headerlink" title="确定版本"></a>确定版本</h3><p>查看下我们目前使用的 kubelet 版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"> kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>安装编译的基础依赖</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y rsync make</span><br></pre></td></tr></table></figure><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><p>这里我们使用容器编译，所以下载到啥地方都行，也不需要安装 go。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">cd kubernetes</span><br><span class="line">git checkout v1.15.5</span><br></pre></td></tr></table></figure><h3 id="前提操作"><a href="#前提操作" class="headerlink" title="前提操作"></a>前提操作</h3><p>查看 cross 镜像的版本号</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat build/build-image/cross/VERSION</span><br><span class="line">v1.12.10-1</span><br></pre></td></tr></table></figure><p>拉国内的镜像，然后改名</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull registry.aliyuncs.com/k8sxio/kube-cross:v1.12.10-1</span><br><span class="line">$ docker tag registry.aliyuncs.com/k8sxio/kube-cross:v1.12.10-1 k8s.gcr.io/kube-cross:v1.12.10-1</span><br></pre></td></tr></table></figure><p>编译，这个参数测试在 v1.15.5 里可用，网上的 <code>make BUILDTAGS=&quot;nokmem&quot; WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=&quot;-N -l&quot;</code> 会无法编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在v1.15.5似乎无用</span><br><span class="line">./build/run.sh make kubelet KUBE_BUILD_PLATFORMS=linux/amd64 BUILDTAGS=&quot;nokmem&quot;</span><br><span class="line"># 用下面的</span><br><span class="line">./build/run.sh make kubelet GOFLAGS=&quot;-v -tags=nokmem&quot; KUBE_BUILD_PLATFORMS=linux/amd64</span><br></pre></td></tr></table></figure><p>查看编译完成的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7 kubernetes]# ls -l _output/dockerized/bin/linux/amd64/</span><br><span class="line">total 202880</span><br><span class="line">-rwxr-xr-x 1 root root   9203530 Apr  7 22:02 conversion-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9207908 Apr  7 22:02 deepcopy-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9156147 Apr  7 22:02 defaulter-gen</span><br><span class="line">-rwxr-xr-x 1 root root   4709220 Apr  7 22:02 go2make</span><br><span class="line">-rwxr-xr-x 1 root root   2894872 Apr  7 22:03 go-bindata</span><br><span class="line">-rwxr-xr-x 1 root root 157545104 Apr  7 22:13 kubelet</span><br><span class="line">-rwxr-xr-x 1 root root  15018430 Apr  7 22:03 openapi-gen</span><br></pre></td></tr></table></figure><h2 id="runc-关闭-kmem"><a href="#runc-关闭-kmem" class="headerlink" title="runc 关闭 kmem"></a>runc 关闭 kmem</h2><p>v1.0.0-rc94起 kmem 设置就被忽略了</p><p>1.0.0 后的 <code>arm64</code> 的 runc 可以直接下面这样编译</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make shell</span><br><span class="line">make localcross</span><br></pre></td></tr></table></figure><h3 id="1-0-0-rc10-版本"><a href="#1-0-0-rc10-版本" class="headerlink" title="1.0.0-rc10 版本"></a>1.0.0-rc10 版本</h3><p><a href="https://cloud.tencent.com/developer/article/1743789">https://cloud.tencent.com/developer/article/1743789</a> 这个文章里说了 runc 也需要关闭</p><p>如果下面命令能成功执行则说明 runc 没关闭 kmem</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --name test --kernel-memory 100M nginx:alpine</span><br></pre></td></tr></table></figure><p><code>19.03.14</code> 测试发现可以运行，说明没有关闭，查看它的 <code>runc</code> 版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><p>根据 commit 跳转</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/opencontainers/runc/commit/dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br></pre></td></tr></table></figure><p>根据这个 commit，找到了是 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10">https://github.com/opencontainers/runc/tree/v1.0.0-rc10</a> 这个 tag，</p><p>编译支持的 tag 见 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags">https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags</a><br>编译参数从 <a href="https://github.com/opencontainers/runc/blob/v1.0.0-rc10/script/release.sh#L30">release 脚本</a> 找到是下面的参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make BUILDTAGS=&quot;seccomp selinux apparmor&quot; static</span><br><span class="line"><span class="meta">#</span><span class="bash"> 后面的新版本貌似默认的 tags 是 seccomp 了</span></span><br></pre></td></tr></table></figure><p>下载源码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/opencontainers/runc.git</span><br><span class="line">cd runc</span><br><span class="line">git checkout v1.0.0-rc10</span><br></pre></td></tr></table></figure><p>直接上面的编译参数是无法编译成功的，因为很多依赖都是 <code>ubuntu</code>下面的。看了下 <code>Makefile</code> 里面提供了一个起 ubuntu 的容器，我们可以进去编译。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make shell</span><br></pre></td></tr></table></figure><p>直接 <code>make shell</code> 的话，它第一步是 <code>make runcimage</code> 会先构建镜像，然后用这个镜像起一个容器，构建的最后一步会失败，因为下面的 <code>busybox</code> 的 <code>rootfs</code> 下载地址变动了，我们得 <code>hack</code> 下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">get_busybox()&#123;</span><br><span class="line">case $(go env GOARCH) in</span><br><span class="line">arm64)</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-arm64v8/glibc/busybox.tar.xz&#x27;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-amd64/glibc/busybox.tar.xz&#x27;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>hack 之前我们先测试下，获取下输出的镜像名是<code>runc_dev:HEAD</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make shell</span></span><br><span class="line">docker build  -t runc_dev:HEAD .</span><br><span class="line">^Cmake: *** [runcimage] Interruptaemon  557.1kB</span><br></pre></td></tr></table></figure><p>因为过程会取 git 的一些信息，为了不影响，我们先拷贝文件 <code>tests/integration/multi-arch.bash</code> 和 <code>Dockerfile</code>。我们先手动构建出镜像，再删除掉这俩文件保持 git status。</p><p>先修改 <code>Dockerfile</code> 让它使用 <code>tests/integration/multi-arch.bash.new</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp Dockerfile Dockerfile.new</span><br><span class="line">vi Dockerfile.new </span><br><span class="line">tail -n2 Dockerfile.new</span><br><span class="line">RUN . tests/integration/multi-arch.bash.new \</span><br><span class="line">    &amp;&amp; curl -o- -sSL `get_busybox` | tar xfJC - $&#123;ROOTFS&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>新文件下载地址可以在 <a href="https://github.com/opencontainers/runc/blob/bb28c44f12bf24ea64590edfb4f23a4b4d2eaae8/tests/integration/get-images.sh#L59">master 分支最新的脚本里</a> 找到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get &quot;$BUSYBOX_IMAGE&quot; \</span><br><span class="line">&quot;https://github.com/docker-library/busybox/raw/dist-$&#123;arch&#125;/stable/glibc/busybox.tar.xz&quot;</span><br></pre></td></tr></table></figure><p>按照下面修改好</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp  tests/integration/multi-arch.bash tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> grep busybox tests/integration/multi-arch.bash.new</span></span><br><span class="line">get_busybox()&#123;</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-arm64v8/glibc/busybox.tar.xz&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">&#x27;https://github.com/docker-library/busybox/raw/dist-amd64/glibc/busybox.tar.xz&#x27;</span></span></span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-amd64/stable/glibc/busybox.tar.xz&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>手动编译镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build  -t runc_dev:HEAD -f Dockerfile.new .</span><br></pre></td></tr></table></figure><p>移走文件，保持 <code>git status</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv Dockerfile.new tests/integration/multi-arch.bash.new /tmp</span><br></pre></td></tr></table></figure><p>进入容器里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -ti --privileged --rm -v $PWD:/go/src/github.com/opencontainers/runc runc_dev:HEAD bash</span><br></pre></td></tr></table></figure><p>编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make BUILDTAGS=<span class="string">&quot;seccomp selinux apparmor nokmem&quot;</span> static</span></span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o runc .</span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o contrib/cmd/recvtty/recvtty ./contrib/cmd/recvtty</span><br></pre></td></tr></table></figure><p>查看信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod u+x runc</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ldd runc</span></span><br><span class="line">not a dynamic executable</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><h2 id="查看-kmem-开启"><a href="#查看-kmem-开启" class="headerlink" title="查看 kmem 开启"></a>查看 kmem 开启</h2><p>环境信息:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux 82.174-zh 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">$ cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core)</span><br></pre></td></tr></table></figure><p>判断 cgroup kernel memory 是否激活的方式。查看对应 POD container 下的 <code>memory.kmem.slabinfo</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /sys/fs/cgroup/memory/kubepods</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有内容，说明kubelet开了 kmem</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat memory.kmem.slabinfo</span></span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br></pre></td></tr></table></figure><p>pod 目录下面的容器目录或者<code>/sys/fs/cgroup/memory/docker/&lt;uuid&gt;</code>如果有 <code>memory.kmem.slabinfo</code> 则说明 <code>runc</code> 没关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -l | grep pod</span></span><br><span class="line">drwxr-xr-x  5 root root 0 4月   8 11:27 pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28</span><br><span class="line">drwxr-xr-x  4 root root 0 4月   8 11:26 pod64def35b-b44e-410d-9782-745bd47834ca</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/ | grep -E <span class="string">&#x27;^d&#x27;</span></span></span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:28 1b37f9f78f93546e3e4407f03aa84c92e95c99655467e62814ae17e0a0e68686</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 a578004f702d7b20d4b08d49c08cbb6c3ef2b3d08a62f087f5c7be0d022d9d9d</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326/memory.kmem.slabinfo</span> </span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br><span class="line">taskstats              0      0    328   24    2 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">shmem_inode_cache    216    216    680   24    4 : tunables    0    0    0 : slabdata      9      9      0</span><br><span class="line">inode_cache          162    162    592   27    4 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">Acpi-ParseExt        112    112     72   56    1 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">selinux_inode_security      0      0     40  102    1 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">RAWv6                  0      0   1216   26    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">UDP                    0      0   1088   30    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-8192           0      0   8192    4    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">net_namespace          0      0   5184    6    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">pid_namespace          0      0   2200   14    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">mqueue_inode_cache      0      0    896   36    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-2048         112    112   2048   16    8 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-32           768    768     32  128    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">kmalloc-512          128    128    512   32    4 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">kmalloc-128          256    256    128   32    1 : tunables    0    0    0 : slabdata      8      8      0</span><br><span class="line">kmalloc-8           6144   6144      8  512    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">anon_vma             306    306     80   51    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">idr_layer_cache      165    165   2112   15    8 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">vm_area_struct       259    259    216   37    2 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">mnt_cache            231    231    384   21    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">mm_struct             20     20   1600   20    8 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">signal_cache           0      0   1152   28    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">sighand_cache          0      0   2112   15    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">files_cache           50     50    640   25    4 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">kernfs_node_cache    108    108    112   36    1 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-192          273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">task_xstate          156    156    832   39    8 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">task_struct           24     24   4048    8    8 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-1024         160    160   1024   32    8 : tunables    0    0    0 : slabdata      5      5      0</span><br><span class="line">kmalloc-64           768    768     64   64    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">sock_inode_cache      75     75    640   25    4 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">proc_inode_cache     264    264    656   24    4 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">dentry               273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">kmalloc-16          2816   2816     16  256    1 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">kmalloc-96           294    294     96   42    1 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-256          352    352    256   32    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">shared_policy_node     85     85     48   85    1 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">kmalloc-4096         104    104   4096    8    8 : tunables    0    0    0 : slabdata     13     13      0</span><br></pre></td></tr></table></figure><p><code>memory.kmem.slabinfo</code>里有内容说明是开启的</p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>只有在 pod 配置了 memory limit 的时候才打开 memory accounting，即 kmem。我们下面利用 flannel pod测试下，先手动创建 cgroup</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ grep memory /proc/cgroups </span><br><span class="line">memory8871</span><br><span class="line"></span><br><span class="line">$ mkdir /sys/fs/cgroup/memory/test</span><br><span class="line">$ for i in `seq 1 65535`;do mkdir /sys/fs/cgroup/memory/test/test-$&#123;i&#125;; done</span><br><span class="line">$ grep memory /proc/cgroups </span><br><span class="line">memory8655131</span><br></pre></td></tr></table></figure><p>释放出三个，删除当前节点的 flannel，可以创建出来，然后再删除新的，无法创建出来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rmdir /sys/fs/cgroup/memory/test/test-&#123;1..3&#125;</span><br><span class="line">$ kubectl -n kube-system delete pod kube-flannel-ds-z2cgq</span><br><span class="line">....</span><br><span class="line">  Warning  FailedCreatePodContainer  2s (x4 over 35s)  kubelet, 10.13.82.174  unable to ensure pod container exists: failed to create container for [kubepods burstable pod5a41f53f-5ce8-4123-8199-1a865219f297] : mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod5a41f53f-5ce8-4123-8199-1a865219f297: no space left on device</span><br></pre></td></tr></table></figure><p>替换编译好的后，先关闭 <code>kubelet</code> 和 <code>docker</code>， 关闭自启动 <code>systemctl disable docker kubelet</code>。reboot 后，查看目录 <code>/sys/fs/cgroup/memory/</code> 下的 <code>kubepods</code> 是不是不存在。然后启动 docker 和 kubelet。等带内存 limit 的 flannel 调度上来后下面命令查看。输出是 <code>Input/output error</code> 说明已经关闭了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find /sys/fs/cgroup/memory/docker/ -name memory.kmem.slabinfo -exec cat &#123;&#125;  \;</span><br><span class="line">find /sys/fs/cgroup/memory/kubepods/ -name memory.kmem.slabinfo -exec cat &#123;&#125;  \;</span><br></pre></td></tr></table></figure><p>还有种方法是看 slab 的个数，删除 limit 的 pod后等重建看看数量增长否：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /sys/kernel/slab  | wc -l</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://eddycjy.com/posts/why-container-memory-exceed2/">https://eddycjy.com/posts/why-container-memory-exceed2/</a></li><li><a href="https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub:+unable+to+allocate+memory+on+node+-1">https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub%3A+unable+to+allocate+memory+on+node+-1</a></li><li><a href="https://github.com/kubernetes/kubernetes/blob/v1.20.6/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem_disabled.go">https://github.com/kubernetes/kubernetes/blob/v1.20.6/vendor/github.com/opencontainers/runc/libcontainer/cgroups/fs/kmem_disabled.go</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前提详情&quot;&gt;&lt;a href=&quot;#前提详情&quot; class=&quot;headerlink&quot; title=&quot;前提详情&quot;&gt;&lt;/a&gt;前提详情&lt;/h2&gt;&lt;p&gt;在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="kmem" scheme="http://zhangguanzhang.github.io/categories/k8s/kmem/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>iptables --wait -t nat -A DOCKER...: iptables NO chain/target/match by that name</title>
    <link href="http://zhangguanzhang.github.io/2021/03/23/iptables-docker-no-chain/"/>
    <id>http://zhangguanzhang.github.io/2021/03/23/iptables-docker-no-chain/</id>
    <published>2021-03-23T17:42:08.000Z</published>
    <updated>2021-03-23T17:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>我们内部有套部署的工具， 我们部署的流程是先在部署机器（部署机器可能也是node1 ）上用脚本安装好 docker，然后进容器里去起我们部署平台，有个很久的 bug 就是，部署机器上端口映射起容器会有如下报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables --wait -t nat -A DOCKER -p tcp -d 0&#x2F;8 --dport 8089 -j DNAT --to-destination 172.25.0.2:80 ! -i docker0: iptables NO chain&#x2F;target&#x2F;match by that name</span><br></pre></td></tr></table></figure><p>排查也很简单，缺少链，添加上即可:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables --wait -N DOCKER &amp;&gt;/dev/null || true</span><br><span class="line">sudo iptables --wait -t filter -N DOCKER &amp;&gt;/dev/null || true</span><br><span class="line">sudo iptables --wait -t nat  -N DOCKER &amp;&gt;/dev/null || true</span><br></pre></td></tr></table></figure><p>或者重启 docker daemon 会在启动的时候加上链。脚本安装 docker 的时候，我们执行了下面的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables --wait -P INPUT ACCEPT &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t nat &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t nat &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t raw &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t raw &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t mangle &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t mangle</span><br></pre></td></tr></table></figure><p>所以一开始是在安装 docker 前加的三个链，后面发现还是会出现。今天测试内部环境测试的时候百分之百复现了。</p><h2 id="引起原因"><a href="#引起原因" class="headerlink" title="引起原因"></a>引起原因</h2><p>和同事排查了下，确认是部署脚本里没有停止 firewalld ，后面执行 ansible 剧本的时候去停止的。整个流程是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">部署机器 ----安装docker----&gt; ansible 剧本 -----停止firewalld-----&gt; 检测到已经安装 docker 不操作</span><br><span class="line">没运行部署平台的机器    ----&gt; ansible 剧本 -----停止firewalld-----&gt; 安装 docker 并启动 docker daemon </span><br></pre></td></tr></table></figure><p>部署机器是先启动 docker daemon，然后后面剧本停止了 firewalld，而非部署机器是先停的 firewalld，再启动的 docker daemon。</p><p>手动测试了下发现停止 firewalld 会把 iptables 清空。于是部署脚本里提前把所有系统的 case 逻辑里把 firewalld 给停掉。</p><p>另外我们防止有些用户安装了 iptables 的 daemon，是这样停止的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld python-firewall firewalld-filesystem iptables &amp;&gt;&#x2F;dev&#x2F;null</span><br><span class="line">systemctl disable firewalld python-firewall firewalld-filesystem iptables &amp;&gt;&#x2F;dev&#x2F;null</span><br></pre></td></tr></table></figure><p>发现这样执行后 firewalld 还是 enabled 的状态，如果有不存在的 service （例如机器存在firewalld，后面几个都不存在）则整个 disable 会失效（全部没有 disable 成功）。改成了下面的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function Systemctl_Stop_Disable()&#123;</span><br><span class="line">    for target in $@;</span><br><span class="line">    do</span><br><span class="line">        sudo systemctl stop $target &amp;&gt;&#x2F;dev&#x2F;null || true;</span><br><span class="line">        sudo systemctl disable $target &amp;&gt;&#x2F;dev&#x2F;null || true;</span><br><span class="line">    done</span><br><span class="line">    true</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Systemctl_Stop_Disable firewalld python-firewall firewalld-filesystem iptables</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;我们内部有套部署的工具， 我们部署的流程是先在部署机器（部署机器可能也是node1 ）上用脚本安装好 docker，然后进容器里去起我们部署</summary>
      
    
    
    
    <category term="iptables" scheme="http://zhangguanzhang.github.io/categories/iptables/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Internal error occurred: jsonpatch add operation does not apply: doc is missing path: xxx</title>
    <link href="http://zhangguanzhang.github.io/2021/03/22/jsonpatch-doc-is-missing-path/"/>
    <id>http://zhangguanzhang.github.io/2021/03/22/jsonpatch-doc-is-missing-path/</id>
    <published>2021-03-22T20:42:08.000Z</published>
    <updated>2021-03-22T20:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>今天在折腾 admission webhook 注入一些属性的时候遇到了 <code>Error from server (InternalError): error when creating &quot;xxx.yml&quot;: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: &quot;/spec/template/spec/dnsConfig/options&quot;</code>。折腾半天才发现在代码里使用 jsonPatch 的话不能直接绕过结构体实例去 patch。 </p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>WebHook 接收和响应都是一个 AdmissionReview 对象，请求是里面的 AdmissionRequest，响应是里面的 AdmissionResponse。<br>比如说我们创建了个 <code>MutatingWebhookConfiguration</code> 指定让 <code>apps/v1</code> 的 <code>deploy</code> 传到我们的 webhook， 我们要修改 deploy 的一些属性，我个人是增加 dns 的 <code>single-request-reopen</code>的属性的，得在 <code>AdmissionResponse</code> 里传一个 jsonPatch的切片。<br>代码里这块是：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> patchOperation <span class="keyword">struct</span> &#123;</span><br><span class="line">Op    <span class="keyword">string</span>      <span class="string">`json:&quot;op&quot;`</span></span><br><span class="line">Path  <span class="keyword">string</span>      <span class="string">`json:&quot;path&quot;`</span></span><br><span class="line">Value <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:&quot;value,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">patch := []patchOperation&#123;</span><br><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/dnsConfig/options&quot;</span>,</span><br><span class="line">Value: []<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>&#123;</span><br><span class="line">&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;single-request-reopen&quot;</span>&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>后面发现创建 deploy 就报开头的错误。然后日志里打印了下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apiVersion:apps&#x2F;v1 resource: default&#x2F;nginx-deployment, AdmissionResponse: patch&#x3D;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;&#x2F;spec&#x2F;template&#x2F;spec&#x2F;dnsConfig&#x2F;options&quot;,&quot;value&quot;:[&#123;&quot;name&quot;:&quot;single-request-reopen&quot;&#125;]&#125;]</span><br></pre></td></tr></table></figure><p>然后把这个 patch 在 kubectl 上测试了下是可以的:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system patch deployments tiller-deploy --<span class="built_in">type</span>=json -p=<span class="string">&#x27;[&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/template/spec/dnsConfig/options&quot;,&quot;value&quot;:[&#123;&quot;name&quot;:&quot;single-request-reopen&quot;&#125;]&#125;]&#x27;</span></span></span><br><span class="line"></span><br><span class="line">deployment.extensions/tiller-deploy patched</span><br></pre></td></tr></table></figure><p>刚开始把 kube-apiserver 开 -v=8 发现 panic的信息，然后升了下版本还是没用。然后在源码里找了下这个报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> find -<span class="built_in">type</span> f -name <span class="string">&#x27;*.go&#x27;</span> -<span class="built_in">exec</span> grep -l <span class="string">&#x27;replace operation does not apply: doc is missing path&#x27;</span> &#123;&#125; \;</span></span><br><span class="line">./vendor/github.com/evanphx/json-patch/patch.go</span><br></pre></td></tr></table></figure><p>发现这个错误是引入的库抛出来的，和 k8s 无关，想了下后换个 key 试试看。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/securityContext/runAsNonRoot&quot;</span>,</span><br><span class="line">Value: <span class="literal">true</span>,</span><br><span class="line">&#125;,</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> deployment.Spec.Template.Spec.Containers &#123;</span><br><span class="line">patch = <span class="built_in">append</span>(patch, patchOperation&#123;</span><br><span class="line">Op:    <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path:  fmt.Sprintf(<span class="string">&quot;/spec/template/spec/containers/%d/lifecycle/postStart/exec/command&quot;</span>, i),</span><br><span class="line">Value: []<span class="keyword">string</span>&#123;</span><br><span class="line"><span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line"><span class="string">&quot;-c&quot;</span>,</span><br><span class="line"><span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现后面 <code>command</code> 这个也不行，explain 看了下，试试上层的看看：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> deployment.Spec.Template.Spec.Containers &#123;</span><br><span class="line">patch = <span class="built_in">append</span>(patch, patchOperation&#123;</span><br><span class="line">Op:    <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path:  fmt.Sprintf(<span class="string">&quot;/spec/template/spec/containers/%d/lifecycle&quot;</span>, i),</span><br><span class="line">Value: corev1.Lifecycle&#123;</span><br><span class="line">PostStart: &amp;corev1.Handler&#123;</span><br><span class="line">Exec: &amp;corev1.ExecAction&#123;</span><br><span class="line">Command: []<span class="keyword">string</span>&#123;</span><br><span class="line"><span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line"><span class="string">&quot;-c&quot;</span>,</span><br><span class="line"><span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现可以，应该是得给一个嵌套的结构体实例，而不是直接绕过这个结构体，去 <code>patch</code> 里面属性的 <code>value</code>，换下前面的 dnsConfig 试试:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/dnsConfig&quot;</span>,</span><br><span class="line">Value: corev1.PodDNSConfig&#123;</span><br><span class="line">Options:     []corev1.PodDNSConfigOption&#123;</span><br><span class="line">&#123;</span><br><span class="line">Name:  <span class="string">&quot;single-request-reopen&quot;</span>,</span><br><span class="line">Value: <span class="literal">nil</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>发现也可以了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook">https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;今天在折腾 admission webhook 注入一些属性的时候遇到了 &lt;code&gt;Error from server (Internal</summary>
      
    
    
    
    <category term="admission" scheme="http://zhangguanzhang.github.io/categories/admission/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>使用github action 配合 docker buildx 编译 arm64 docker-compose</title>
    <link href="http://zhangguanzhang.github.io/2021/03/12/build-arm64-docker-compose-action/"/>
    <id>http://zhangguanzhang.github.io/2021/03/12/build-arm64-docker-compose-action/</id>
    <published>2021-03-12T19:28:30.000Z</published>
    <updated>2021-03-12T19:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>git 上搜索了很多 docker-compose 的 arm64 的编译基本都是使用 <code>qemu-user-static</code> 之类的设置下后编译的，也看到过用特权容器启动 qemu-user-static 或者 <code>binfmt</code> 之类的，但是我自己机器上试了无效，貌似是因为我操作系统是低版本内核的 centos ，github 上搜了下，其他很多人的编译感觉太啰嗦了。就在 action 上整了下，测试是可用的，而且非常简单。</p><p><code>docker-practice/actions-setup-docker@master</code> 将会在在 action 的 runner 里安装 docker，创建 buildx 和 运行 <code>docker run --rm --privileged ghcr.io/dpsigs/tonistiigi-binfmt:latest --install all</code> 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">build</span> <span class="string">for</span> <span class="string">docker-compose</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span> [ <span class="string">master</span> ]</span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># - name: Check out docker/compose</span></span><br><span class="line">      <span class="comment">#   uses: actions/checkout@v2</span></span><br><span class="line">      <span class="comment">#   with:</span></span><br><span class="line">      <span class="comment">#     repository: docker/compose</span></span><br><span class="line">      <span class="comment">#     path: compose</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">install</span> <span class="string">docker</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">docker-practice/actions-setup-docker@master</span></span><br><span class="line">        <span class="comment"># this will run and buildx</span></span><br><span class="line">        <span class="comment"># docker run --rm --privileged ghcr.io/dpsigs/tonistiigi-binfmt:latest --install all</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build</span> <span class="string">docker-compose</span> </span><br><span class="line">        <span class="attr">id:</span> <span class="string">build</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">          <span class="string">ls</span> <span class="string">-l;</span> <span class="string">docker</span> <span class="string">run</span> <span class="string">--rm</span> <span class="string">arm64v8/python:3.7.10-stretch</span> <span class="string">sh</span> <span class="string">-c</span> <span class="string">&#x27;python3 -V; uname -m&#x27;</span></span><br><span class="line">          <span class="comment"># https://github.com/docker/compose/blob/master/script/build/linux</span></span><br><span class="line">          <span class="string">git</span> <span class="string">clone</span> <span class="string">https://github.com/docker/compose.git</span></span><br><span class="line">          <span class="string">cd</span> <span class="string">compose;</span></span><br><span class="line">          <span class="string">./script/clean;</span></span><br><span class="line">          <span class="string">git</span> <span class="string">checkout</span> <span class="number">1.28</span><span class="number">.5</span></span><br><span class="line">          <span class="string">DOCKER_COMPOSE_GITSHA=&quot;$(script/build/write-git-sha)&quot;;</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">----</span> <span class="string">$&#123;DOCKER_COMPOSE_GITSHA&#125;</span></span><br><span class="line">          <span class="string">docker</span> <span class="string">buildx</span> <span class="string">build</span> <span class="string">--platform</span> <span class="string">linux/arm64</span> <span class="string">.</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--target</span> <span class="string">bin</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--build-arg</span> <span class="string">DISTRO=debian</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--build-arg</span> <span class="string">GIT_COMMIT=&quot;$&#123;DOCKER_COMPOSE_GITSHA&#125;&quot;</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--output</span> <span class="string">dist/</span> <span class="string">||</span> <span class="string">:</span> <span class="string">;</span></span><br><span class="line">          <span class="string">ls</span> <span class="string">-l</span> <span class="string">dist;</span></span><br><span class="line">          <span class="string">docker</span> <span class="string">run</span> <span class="string">--platform</span> <span class="string">linux/arm64</span> <span class="string">\</span></span><br><span class="line">            <span class="string">--rm</span> <span class="string">-v</span> <span class="string">$PWD/dist:/root/</span> <span class="string">\</span></span><br><span class="line">            <span class="string">arm64v8/python:3.7.10-stretch</span> <span class="string">/root/docker-compose-linux-arm64</span> <span class="string">version;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Upload</span> <span class="string">bin</span> <span class="string">directory</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/upload-artifact@main</span></span><br><span class="line">        <span class="attr">if:</span> <span class="string">steps.build.outcome</span> <span class="string">==</span> <span class="string">&#x27;success&#x27;</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">docker-compose-linux</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">compose/dist/</span></span><br></pre></td></tr></table></figure><p>编译过程看 compose 仓库的 makefile，是运行的 <a href="https://github.com/docker/compose/blob/master/script/build/linux">https://github.com/docker/compose/blob/master/script/build/linux</a> 这个脚本。所以克隆 compose 仓库后进目录里，然后 checkout 指定 tag。官方的编译过程都是在 docker build 产生的容器里去编译的。最后有个 build –output就是直接把文件给整出来。我这里是用的 buildx 去替代 build 编译。理论上也可以编译其他架构的，我仓库已经自动化处理这个过程了。 <a href="https://github.com/zhangguanzhang/docker-compose-aarch64/releases%E3%80%82">https://github.com/zhangguanzhang/docker-compose-aarch64/releases。</a></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>银河麒麟 v10 系统，架构 arm64</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ arch</span><br><span class="line">aarch64</span><br><span class="line">$ cat &#x2F;etc&#x2F;os-release </span><br><span class="line">NAME&#x3D;&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION&#x3D;&quot;V10 (Tercel)&quot;</span><br><span class="line">ID&#x3D;&quot;kylin&quot;</span><br><span class="line">VERSION_ID&#x3D;&quot;V10&quot;</span><br><span class="line">PRETTY_NAME&#x3D;&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR&#x3D;&quot;0;31&quot;</span><br></pre></td></tr></table></figure><p>docker 版本信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">$ docker info</span><br><span class="line">Containers: 63</span><br><span class="line"> Running: 44</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 19</span><br><span class="line">Images: 24</span><br><span class="line">Server Version: 18.09.9</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line">runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.19.90-17.ky10.aarch64</span><br><span class="line">Operating System: Kylin Linux Advanced Server V10 (Tercel)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: aarch64</span><br><span class="line">CPUs: 64</span><br><span class="line">Total Memory: 62.76GiB</span><br><span class="line">Name: reg.xxx.lan</span><br><span class="line">ID: RI24:C6CM:WELZ:MQEJ:N5OY:IR74:OQPG:XV72:SFRI:NUSK:DS44:OQNQ</span><br><span class="line">Docker Root Dir: &#x2F;data&#x2F;kube&#x2F;docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> 127.0.0.0&#x2F;8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https:&#x2F;&#x2F;registry.docker-cn.com&#x2F;</span><br><span class="line"> https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&#x2F;</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><h3 id="测试运行"><a href="#测试运行" class="headerlink" title="测试运行"></a>测试运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">$ ldd .&#x2F;docker-compose-linux-arm64 </span><br><span class="line">linux-vdso.so.1 (0x0000fffd72210000)</span><br><span class="line">libdl.so.2 &#x3D;&gt; &#x2F;lib64&#x2F;libdl.so.2 (0x0000fffd721a0000)</span><br><span class="line">libz.so.1 &#x3D;&gt; &#x2F;lib64&#x2F;libz.so.1 (0x0000fffd72160000)</span><br><span class="line">libc.so.6 &#x3D;&gt; &#x2F;lib64&#x2F;libc.so.6 (0x0000fffd71fd0000)</span><br><span class="line">&#x2F;lib&#x2F;ld-linux-aarch64.so.1 (0x0000fffd72220000)</span><br><span class="line">$ ll</span><br><span class="line">总用量 10504</span><br><span class="line">drwxr-xr-x 2 root root       26  3月 13 11:11 conf.d</span><br><span class="line">-rwxr-xr-x 1 root root 10750256  3月 12 13:15 docker-compose-linux-arm64</span><br><span class="line">-rw-r--r-- 1 root root      389  3月 13 11:11 docker-compose.yml</span><br><span class="line">drwxr-xr-x 2 root root        6  3月 13 11:11 down</span><br><span class="line">$ cat conf.d&#x2F;default.conf </span><br><span class="line">server &#123;</span><br><span class="line">    listen       81;</span><br><span class="line">    server_name  localhost;</span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">        autoindex    on;</span><br><span class="line">    &#125;</span><br><span class="line">    error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">    location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">$ cat docker-compose.yml </span><br><span class="line">version: &#39;3.4&#39;</span><br><span class="line">services:</span><br><span class="line">  nginx:</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    container_name: install-nginx</span><br><span class="line">    hostname: install-nginx</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai:&#x2F;etc&#x2F;localtime:ro</span><br><span class="line">      - .&#x2F;down:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</span><br><span class="line">      - .&#x2F;conf.d&#x2F;:&#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;</span><br><span class="line">    network_mode: &quot;host&quot;</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#39;3&#39;</span><br><span class="line">        max-size: 100m</span><br><span class="line">$ mkdir conf.d</span><br><span class="line"></span><br><span class="line">$ .&#x2F;docker-compose-linux-arm64 up -d</span><br><span class="line">Pulling nginx (nginx:alpine)...</span><br><span class="line">alpine: Pulling from library&#x2F;nginx</span><br><span class="line">Digest: sha256:c2ce58e024275728b00a554ac25628af25c54782865b3487b11c21cafb7fabda</span><br><span class="line">Status: Downloaded newer image for nginx:alpine</span><br><span class="line">Creating install-nginx ... done</span><br><span class="line">$.&#x2F;docker-compose-linux-arm64 ps -a</span><br><span class="line">    Name                   Command               State   Ports</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">install-nginx   &#x2F;docker-entrypoint.sh ngin ...   Up           </span><br><span class="line">$ netstat -nlptu | grep -E &#39;:81\s&#39;</span><br><span class="line">tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      4093364&#x2F;nginx: mast </span><br></pre></td></tr></table></figure><p>页面访问了下正常，清理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;docker-compose-linux-arm64 down</span><br><span class="line">Stopping install-nginx ... done</span><br><span class="line">Removing install-nginx ... done</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/ubiquiti/docker-compose-aarch64">https://github.com/ubiquiti/docker-compose-aarch64</a></li><li><a href="https://github.com/RogerLaw/docker-compose-aarch64">https://github.com/RogerLaw/docker-compose-aarch64</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;p&gt;git 上搜索了很多 docker-compose 的 arm64 的编译基本都是使用 &lt;code&gt;qemu-user-static&lt;/co</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>集群节点关机导致dns在eviction pod之前几率不可用</title>
    <link href="http://zhangguanzhang.github.io/2021/02/02/node-shutdown-dns-unavailable/"/>
    <id>http://zhangguanzhang.github.io/2021/02/02/node-shutdown-dns-unavailable/</id>
    <published>2021-02-02T15:42:08.000Z</published>
    <updated>2021-02-02T15:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这几天我们内部在做新项目的容灾测试，业务都是在 K8S 上的。容灾里就是随便选节点 <code>shutdown -h now</code>。关机后同事便发现了（页面有错误，最终问题是）集群内 DNS 解析会有几率无法解析（导致的）。</p><p>根据 <code>SVC</code> 的流程，node 关机后，由于 kubelet 没有 update 自己。node 和 pod 在 apiserver get 的时候显示还是正常的。在 <code>kube-controller-manager</code> 的 <code>--node-monitor-grace-period</code> 时间后再过 <code>--pod-eviction-timeout</code> 时间开始 <code>eviction pod</code>，大概流程是这样。</p><p>在 <code>pod</code> 被 <code>eviction</code> 之前，默认是大概 <code>5m</code> 的时间。这段时间内，<code>node</code> 上 的所有 <code>POD</code> 的 <code>IP</code> 还在 <code>SVC</code> 的 <code>endpoint</code> 里。而同事关机的 <code>node</code> 上恰好有 <code>coredns</code> 。所以在 5m 内一直会有 coredns 副本数之一的几率解析失败。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>其实和 K8S 版本没关系，因为 <code>SVC</code> 和 <code>eviction</code> 的行为都是这样的。实际我调整了 <code>node</code> 更新自身状态的所有 <a href="https://github.com/zhangguanzhang/Kubernetes-ansible/wiki/nodeStatusUpdate">相关参数</a>，调整到在 20s 内就会 <code>eviction pod</code>，但是 20s 内还是存在几率无法解析(后续这个相关的更新时间调很快，结果出现了 svc 选中的pod还在running，但是kubelet实际更新自己 status 失败了，导致kube-controller-mananger把 pod的status patch成了非true，也就是svc的 endpoint消失了，回退更新时间后没这个bug了)。当然也问了下群友和社区群里，发现似乎大家从来没关机测试过这方面，应该是现在大伙都在用公有云了。。。。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">  kubectl  version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><h3 id="loca-dns-真的可以吗"><a href="#loca-dns-真的可以吗" class="headerlink" title="loca-dns 真的可以吗"></a>loca-dns 真的可以吗</h3><p>当然首选是 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns">local-dns 的方案</a> 了。方案搜下，很多人介绍了。简单讲下就是在每个 node 上起 <code>hostNetwork</code> 的 <code>node-cache</code> 进程做代理 ，然后利用 <code>dummy</code> 接口和 nat 来拦截发向 kube-dns <code>SVC</code> IP 的 dns 请求做缓存。</p><p><a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml">官方提供的 yaml 文件</a> 里的 <code>__PILLAR__LOCAL__DNS__,__PILLAR__DNS__SERVER__</code>需要换成<code>dummy</code>接口 IP 和 kube-dns <code>SVC</code> 的 IP，还有 <code>__PILLAR__DNS__DOMAIN__</code> 自行根据文档更换。其余几个变量会在启动的时候替换，可以启动后看日志。</p><p>然后实际测试了下还是有问题。然后捋了下流程，yaml 文件里有这个 <code>SVC</code> 和 node-cache 的启动参数</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns-upstream</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">53</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">53</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line"><span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"> <span class="attr">args:</span> [ <span class="string">...</span>, <span class="string">&quot;-upstreamsvc&quot;</span>, <span class="string">&quot;kube-dns-upstream&quot;</span> ]</span><br></pre></td></tr></table></figure><p>启动的日志里可以看到配置文件被渲染了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">cluster1.local:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    reload</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">        force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    health 169.254.20.10:8080</span><br><span class="line">&#125;</span><br><span class="line">in-addr.arpa:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    &#125;</span><br><span class="line">ip6.arpa:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    &#125;</span><br><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">    prometheus :9253</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为要 nat 去 hook 请求 kube-dns <code>SVC</code> IP（172.26.0.2）的请求，但是它自己也需要访问 kube-dns，所以 yaml 文件里创建了一个和 kube-dns 一样的属性的 svc，启动参数写了这个 SVC 名字，可以看到它代理的是走 <code>SVC</code> 的 ip 的。因为 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/connect-applications-service/#%E8%AE%BF%E9%97%AE-service">enableServiceLinks</a> 的默认开启，pod 会有如下环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> dfa env | grep KUBE_DNS_UPSTREAM_SERVICE_HOST</span></span><br><span class="line">KUBE_DNS_UPSTREAM_SERVICE_HOST=172.26.189.136</span><br></pre></td></tr></table></figure><p><a href="https://github.com/kubernetes/dns/blob/2b7b66c7824a7dd68d38568d913228e8d3d4c8c2/cmd/node-cache/app/cache_app.go#L306">代码里</a> 可以看到就是把参数的 <code>-</code> 转换成 <code>_</code> 取值然后渲染配置文件，这样就能取到 SVC 的 IP 了。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">toSvcEnv</span><span class="params">(svcName <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">envName := strings.Replace(svcName, <span class="string">&quot;-&quot;</span>, <span class="string">&quot;_&quot;</span>, <span class="number">-1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;$&quot;</span> + strings.ToUpper(envName) + <span class="string">&quot;_SERVICE_HOST&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>cluster1.local:53</code> 这个 zone 在默认配置下还是代理到 <code>SVC</code> 上，所以还是有问题。</p><p>所以只有绕过 <code>SVC</code> 才能从根本上解决这个问题。然后就把 <code>coredns</code> 改成 port 153 + <code>hostNetwork: true</code> 加 <code>nodeSelector</code> 到三个 master 上固定了。然后配置文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cluster1.local:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    reload</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">        force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    health 169.254.20.10:8080</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>然后测试还是有几率无法访问。之前看到过 <a href="https://fuckcloudnative.io/">米开朗基杨</a> 分享过 <code>coredns</code> 的一个带故障转移的插件 <a href="https://github.com/leiless/dnsredir">dnsredir</a>，尝试加这个插件去编译。</p><p>查阅文档编译后最后运行起来无法识别配置文件，因为官方不是直接基于 <code>coredns</code> 引入自己的插件开发的，而是自己的代码上来引入 <code>coredns</code> 的内置插件。</p><p>大概过程详情 issue 见链接 <a href="https://github.com/kubernetes/dns/issues/436">include coredns plugin at node-cache don’t work expect</a></p><p>官方的这个 node-cace 里的 bind 插件就是 <code>dummy</code>接口和 iptables 的 nat 部分了，这个特性蛮吸引我的，决定继续尝试下这个看看能不能配置成功。</p><h3 id="意外收获"><a href="#意外收获" class="headerlink" title="意外收获"></a>意外收获</h3><p>在测试加入插件 dnsredir 的时候米开朗基杨叫我试下最小配置段看看有干扰没，尝试了下面的配置段来回切换测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  Corefile: |</span><br><span class="line">    cluster1.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        reload</span><br><span class="line">        dnsredir . &#123;</span><br><span class="line">            to 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153</span><br><span class="line">            max_fails 1</span><br><span class="line">            health_check 1s</span><br><span class="line">            spray</span><br><span class="line">        &#125;</span><br><span class="line">        #forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">        #    max_fails 1</span><br><span class="line">        #    policy round_robin</span><br><span class="line">        #    health_check 0.4s</span><br><span class="line">        #&#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">    &#125;</span><br><span class="line">#----------</span><br><span class="line">  Corefile: |</span><br><span class="line">    cluster1.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        reload</span><br><span class="line">        #dnsredir . &#123;</span><br><span class="line">        #    to 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153</span><br><span class="line">        #    max_fails 1</span><br><span class="line">        #    health_check 1s</span><br><span class="line">        #    spray</span><br><span class="line">        #&#125;</span><br><span class="line">        forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">            max_fails 1</span><br><span class="line">            policy round_robin</span><br><span class="line">            health_check 0.4s</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>然后发现请求居然不会发生解析失败了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">function</span> <span class="function"><span class="title">d</span></span>()&#123; <span class="keyword">while</span> :;<span class="keyword">do</span> sleep 0.2; date;dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short; <span class="keyword">done</span>; &#125;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> d</span></span><br><span class="line">2021年 02月 02日 星期二 12:54:43 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST  &lt;---这个时间点关机了一个 master </span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:45 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:47 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:51 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:51 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:52 CST</span><br><span class="line">172.26.158.130</span><br></pre></td></tr></table></figure><p>然后就不打算继续折腾 dnsredir 插件了，去叫同事测试了下没问题，叫我在另一个环境上应用下修改他再测下，发现还是会发生：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @172.26.0.2 account-gateway +short</span><br><span class="line">; (1 server found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; connection timed out; no servers could be reached</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br></pre></td></tr></table></figure><p>然后我多次测试最小配置 zone，对比排查到是反向解析的问题，反向解析关闭了就不存在任何问题了，注释掉下面的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#in-addr.arpa:53 &#123;</span><br><span class="line">#    errors</span><br><span class="line">#    cache 30</span><br><span class="line">#    reload</span><br><span class="line">#    loop</span><br><span class="line">#    bind 169.254.20.10 172.26.0.2</span><br><span class="line">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">#            force_tcp</span><br><span class="line">#    &#125;</span><br><span class="line">#    prometheus :9253</span><br><span class="line">#    &#125;</span><br><span class="line">#ip6.arpa:53 &#123;</span><br><span class="line">#    errors</span><br><span class="line">#    cache 30</span><br><span class="line">#    reload</span><br><span class="line">#    loop</span><br><span class="line">#    bind 169.254.20.10 172.26.0.2</span><br><span class="line">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">#            force_tcp</span><br><span class="line">#    &#125;</span><br><span class="line">#    prometheus :9253</span><br><span class="line">#    &#125;</span><br></pre></td></tr></table></figure><p>测试解析的过程中去关机任何一台 coredns 所在 node 也没问题了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br></pre></td></tr></table></figure><h3 id="大致的yaml文件"><a href="#大致的yaml文件" class="headerlink" title="大致的yaml文件"></a>大致的yaml文件</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns-upstream</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;KubeDNSUpstream&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.3</span> <span class="comment"># &lt;---- 给他固定了得了，可以直接这个ip不走node-cache作为测试</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">cluster1.local:53</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">cache</span> &#123;</span><br><span class="line">            <span class="string">success</span> <span class="number">9984 </span><span class="number">30</span></span><br><span class="line">            <span class="string">denial</span> <span class="number">9984 </span><span class="number">5</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">bind</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.107</span><span class="string">:153</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.108</span><span class="string">:153</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.109</span><span class="string">:153</span> &#123;</span><br><span class="line">            <span class="string">force_tcp</span></span><br><span class="line">            <span class="string">max_fails</span> <span class="number">1</span></span><br><span class="line">            <span class="string">policy</span> <span class="string">round_robin</span></span><br><span class="line">            <span class="string">health_check</span> <span class="number">0.</span><span class="string">5s</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9253</span></span><br><span class="line">        <span class="string">health</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span><span class="string">:8070</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#in-addr.arpa:53 &#123;</span></span><br><span class="line">    <span class="comment">#    errors</span></span><br><span class="line">    <span class="comment">#    cache 30</span></span><br><span class="line">    <span class="comment">#    reload</span></span><br><span class="line">    <span class="comment">#    loop</span></span><br><span class="line">    <span class="comment">#    bind 169.254.20.10 172.26.0.2</span></span><br><span class="line">    <span class="comment">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span></span><br><span class="line">    <span class="comment">#            force_tcp</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#    prometheus :9253</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#ip6.arpa:53 &#123;</span></span><br><span class="line">    <span class="comment">#    errors</span></span><br><span class="line">    <span class="comment">#    cache 30</span></span><br><span class="line">    <span class="comment">#    reload</span></span><br><span class="line">    <span class="comment">#    loop</span></span><br><span class="line">    <span class="comment">#    bind 169.254.20.10 172.26.0.2</span></span><br><span class="line">    <span class="comment">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span></span><br><span class="line">    <span class="comment">#            force_tcp</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#    prometheus :9253</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="string">.:53</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">bind</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="string">__PILLAR__UPSTREAM__SERVERS__</span></span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9253</span></span><br><span class="line">      &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9253&quot;</span></span><br><span class="line">        <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span>  <span class="comment"># Don&#x27;t use cluster DNS.</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">node-cache</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">xxx.lan:5000/k8s-dns-node-cache:1.16.0</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">args:</span> [ <span class="string">&quot;-localip&quot;</span>, <span class="string">&quot;169.254.20.10,172.26.0.2&quot;</span>, <span class="string">&quot;-conf&quot;</span>, <span class="string">&quot;/etc/Corefile&quot;</span>, <span class="string">&quot;-upstreamsvc&quot;</span>, <span class="string">&quot;kube-dns-upstream&quot;</span>, <span class="string">&quot;-health-port&quot;</span>,<span class="string">&quot;8070&quot;</span> ]</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">53</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">53</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9253</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8070</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">40</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/coredns</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-dns-config</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/kube-dns</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">FileOrCreate</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-dns-config</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-dns</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">Corefile</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">Corefile.base</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods.</span></span><br><span class="line"><span class="comment"># We use this to expose metrics to Prometheus.</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9253&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">9253</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9253</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/bootstrapping:</span> <span class="string">rbac-defaults</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">services</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">namespaces</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/bootstrapping:</span> <span class="string">rbac-defaults</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">.:153</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">health</span> <span class="string">:8180</span></span><br><span class="line">        <span class="string">kubernetes</span> <span class="string">cluster1.local.</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span> &#123;</span><br><span class="line">            <span class="string">pods</span> <span class="string">insecure</span></span><br><span class="line">            <span class="string">fallthrough</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9153</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="string">/etc/resolv.conf</span></span><br><span class="line">        <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;CoreDNS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">seccomp.security.alpha.kubernetes.io/pod:</span> <span class="string">&#x27;docker/default&#x27;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">100</span></span><br><span class="line">              <span class="attr">podAffinityTerm:</span></span><br><span class="line">                <span class="attr">labelSelector:</span></span><br><span class="line">                  <span class="attr">matchExpressions:</span></span><br><span class="line">                    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">k8s-app</span></span><br><span class="line">                      <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                      <span class="attr">values:</span></span><br><span class="line">                        <span class="bullet">-</span> <span class="string">kube-dns</span></span><br><span class="line">                <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-cluster-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">coredns</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">node-role.kubernetes.io/master:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">          <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">xxxx.lan:5000/coredns:1.7.1</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">270Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">150Mi</span></span><br><span class="line">        <span class="attr">args:</span> [ <span class="string">&quot;-conf&quot;</span>, <span class="string">&quot;/etc/coredns/Corefile&quot;</span> ]</span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/coredns</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8180</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">          <span class="attr">capabilities:</span></span><br><span class="line">            <span class="attr">add:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">NET_BIND_SERVICE</span></span><br><span class="line">            <span class="attr">drop:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">all</span></span><br><span class="line">          <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">            <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">Corefile</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">Corefile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9153&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;CoreDNS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br></pre></td></tr></table></figure><h3 id="自己的方案"><a href="#自己的方案" class="headerlink" title="自己的方案"></a>自己的方案</h3><p>但是后面发现 cpu 太高了，决定自己整个方案，中途尝试了很多，最后决定自己把里面的 dummy 接口部分源码抠出来写成一个工具（这样就不用改 svc ip 了），然后高可用用其他手段。主要是替换掉 nodelocaldns 的部分</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span>  <span class="comment"># Don&#x27;t use cluster DNS.</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dummy-tool</span></span><br><span class="line">        <span class="comment">#image: registry.aliyuncs.com/zhangguanzhang/dummy-tool:v0.1</span></span><br><span class="line">        <span class="attr">image:</span> &#123;&#123; <span class="string">docker_repo_url</span> &#125;&#125;<span class="string">/dummy-tool:v0.1</span></span><br><span class="line">        <span class="attr">args:</span> </span><br><span class="line">        <span class="bullet">-</span> <span class="string">-local-ip=169.254.20.10,172.26.0.2</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-health-port=8070</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-interface-name=nodelocaldns</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8070</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">40</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dnsmasq</span></span><br><span class="line">        <span class="comment">#image: registry.aliyuncs.com/zhangguanzhang/dnsmasq:2.83</span></span><br><span class="line">        <span class="attr">image:</span> &#123;&#123; <span class="string">docker_repo_url</span> &#125;&#125;<span class="string">/dnsmasq:2.83</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">dnsmasq</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-d</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--conf-file=/etc/dnsmasq/dnsmasq.conf</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">host-localtime</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/dnsmasq</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">host-localtime</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">dnsmasq.conf:</span> <span class="string">|</span></span><br><span class="line">    <span class="literal">no</span><span class="string">-resolv</span></span><br><span class="line">    <span class="string">all-servers</span></span><br><span class="line">    <span class="string">server=10.11.86.107#153</span></span><br><span class="line">    <span class="string">server=10.11.86.108#153</span></span><br><span class="line">    <span class="string">server=10.11.86.109#153</span></span><br><span class="line">    <span class="comment">#log-queries</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://lework.github.io/2020/11/09/node-local-dns/">在 Kubernetes 集群中使用 NodeLocal DNSCache</a></li><li><a href="https://mritd.com/2019/11/05/writing-plugin-for-coredns/">writing-plugin-for-coredns</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这几天我们内部在做新项目的容灾测试，业务都是在 K8S 上的。容灾里就是随便选节点 &lt;code&gt;shutdown -h now&lt;/code&gt;</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    <category term="coredns" scheme="http://zhangguanzhang.github.io/categories/kubernetes/coredns/"/>
    
    <category term="node-cache" scheme="http://zhangguanzhang.github.io/categories/kubernetes/coredns/node-cache/"/>
    
    
    <category term="coredns" scheme="http://zhangguanzhang.github.io/tags/coredns/"/>
    
  </entry>
  
  <entry>
    <title>docker18.03 hang at &#39;restoring container&#39;</title>
    <link href="http://zhangguanzhang.github.io/2020/12/04/docker1803-hang-container-restore/"/>
    <id>http://zhangguanzhang.github.io/2020/12/04/docker1803-hang-container-restore/</id>
    <published>2020-12-04T19:42:08.000Z</published>
    <updated>2020-12-11T10:15:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>起初是 k8s 有几个 node not ready，上去看了下 kubelet 日志刷 container runtime down，重启了下 docker 后还是没用，docker ps 命令都卡住。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/redhat-release </span></span><br><span class="line">Linux xxx-disk0 3.10.0-1127.13.1.el7.x86_64 #1 SMP Tue Jun 23 15:46:38 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info</span></span><br><span class="line">Containers: 91</span><br><span class="line"> Running: 63</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 28</span><br><span class="line">Images: 539</span><br><span class="line">Server Version: 18.03.0-ce</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: cfd04396dc68220d1cecbe686a6cc3aa5ce3667c</span><br><span class="line">runc version: 4fc53a81fb7c994640722ac585fa9ca548971871</span><br><span class="line">init version: 949e6fa</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 3.10.0-1127.13.1.el7.x86_64</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 8</span><br><span class="line">Total Memory: 15.51GiB</span><br><span class="line">Name: xxx-disk0</span><br><span class="line">ID: UZRM:KRSL:TYWM:VAQY:KWCX:AVFD:NP53:TC35:YHOC:TLLO:YGXO:RMYS</span><br><span class="line">Docker Root Dir: /app/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br></pre></td></tr></table></figure><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>先停掉docker，然后前台启动加 debug 参数启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pgrep dockerd</span></span><br><span class="line">4659</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 4659 &amp;&amp; &gt; /var/run/docker.pid </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep dockerd</span></span><br><span class="line">root      5628  0.0  0.0 112708   980 pts/0    S+   22:33   0:00 grep --color=auto dockerd</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./dockerd -D</span></span><br><span class="line">WARN[0000] The "graph" config file option is deprecated. Please use "data-root" instead. </span><br><span class="line">WARN[2020-12-04T22:33:50.432804342+08:00] could not change group /var/run/docker.sock to docker: group docker not found </span><br><span class="line">DEBU[2020-12-04T22:33:50.432936283+08:00] Listener created for HTTP on unix (/var/run/docker.sock) </span><br><span class="line">INFO[2020-12-04T22:33:50.433612435+08:00] libcontainerd: started new docker-containerd process  pid=5646</span><br><span class="line">INFO[0000] starting containerd                           module=containerd revision=cfd04396dc68220d1cecbe686a6cc3aa5ce3667c version=v1.0.2</span><br><span class="line">DEBU[0000] changing OOM score to -500                    module=containerd</span><br><span class="line">INFO[0000] loading plugin "io.containerd.content.v1.content"...  module=containerd type=io.containerd.content.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.snapshotter.v1.btrfs"...  module=containerd type=io.containerd.snapshotter.v1</span><br><span class="line">WARN[0000] failed to load plugin io.containerd.snapshotter.v1.btrfs  error="path /app/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter" module=containerd</span><br><span class="line">INFO[0000] loading plugin "io.containerd.snapshotter.v1.overlayfs"...  module=containerd type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.metadata.v1.bolt"...  module=containerd type=io.containerd.metadata.v1</span><br><span class="line">WARN[0000] could not use snapshotter btrfs in metadata plugin  error="path /app/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter" module="containerd/io.containerd.metadata.v1.bolt"</span><br><span class="line">INFO[0000] loading plugin "io.containerd.differ.v1.walking"...  module=containerd type=io.containerd.differ.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.gc.v1.scheduler"...  module=containerd type=io.containerd.gc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.containers"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.content"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.diff"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.events"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.healthcheck"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.images"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.leases"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.namespaces"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.snapshots"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.monitor.v1.cgroups"...  module=containerd type=io.containerd.monitor.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.runtime.v1.linux"...  module=containerd type=io.containerd.runtime.v1</span><br><span class="line">DEBU[0000] loading tasks in namespace                    module="containerd/io.containerd.runtime.v1.linux" namespace=moby</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.tasks"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.version"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.introspection"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] serving...                                    address="/var/run/docker/containerd/docker-containerd-debug.sock" module="containerd/debug"</span><br><span class="line">INFO[0000] serving...                                    address="/var/run/docker/containerd/docker-containerd.sock" module="containerd/grpc"</span><br><span class="line">INFO[0000] containerd successfully booted in 0.009604s   module=containerd</span><br><span class="line">DEBU[2020-12-04T22:33:50.456534148+08:00] Golang's threads limit set to 113940         </span><br><span class="line">DEBU[2020-12-04T22:33:50.457345643+08:00] Using default logging driver json-file       </span><br><span class="line">DEBU[2020-12-04T22:33:50.457466912+08:00] [graphdriver] priority list: [btrfs zfs overlay2 aufs overlay devicemapper vfs] </span><br><span class="line">DEBU[2020-12-04T22:33:50.457623030+08:00] processing event stream                       module=libcontainerd namespace=plugins.moby</span><br><span class="line">DEBU[2020-12-04T22:33:50.479691287+08:00] backingFs=xfs,  projectQuotaSupported=false  </span><br><span class="line">INFO[2020-12-04T22:33:50.479712832+08:00] [graphdriver] using prior storage driver: overlay2 </span><br><span class="line">DEBU[2020-12-04T22:33:50.479724151+08:00] Initialized graph driver overlay2            </span><br><span class="line">DEBU[2020-12-04T22:33:50.510882767+08:00] Max Concurrent Downloads: 10                 </span><br><span class="line">DEBU[2020-12-04T22:33:50.510930407+08:00] Max Concurrent Uploads: 5                    </span><br><span class="line">DEBU[0000] garbage collected                             d=24.493383ms module="containerd/io.containerd.gc.v1.scheduler"</span><br><span class="line">INFO[2020-12-04T22:33:50.608483121+08:00] Graph migration to content-addressability took 0.00 seconds </span><br><span class="line">INFO[2020-12-04T22:33:50.610430840+08:00] Loading containers: start.                   </span><br><span class="line">DEBU[2020-12-04T22:33:50.610704281+08:00] processing event stream                       module=libcontainerd namespace=moby</span><br><span class="line">DEBU[2020-12-04T22:33:50.611446797+08:00] Loaded container 027a389c8c1e93629cc5f68af8d023b2ecfe350d7771ba6b87598ff705f6c19f, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.611803503+08:00] Loaded container 24735e5aea2bd91b5fa5d729ca021a09532c2ea9b8b06f5171d0da23fc3bf4cc, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612174253+08:00] Loaded container 487a8c2f30986796c3948d1469d506e1d3ab394e17533040ef7a5444a32be0fc, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612494092+08:00] Loaded container 52d32b0e03c957b6cb9b4d793c47900e689a29d9ae0d63703ea29073a352fbe5, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612816495+08:00] Loaded container 5b7b0b52c71a14164f269853679211b3823e9eecc2d3829bf2db10c9b720217d, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.613447082+08:00] Loaded container 62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be, isRunning: true </span><br><span class="line">DEBU[2020-12-04T22:33:50.613769649+08:00] Loaded container 68ba211ec7328bebd3b241631a703639447c05056ffe07ed633b72d0bc210938, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.614756585+08:00] Loaded container 73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212, isRunning: true </span><br><span class="line">DEBU[2020-12-04T22:33:50.615381990+08:00] Loaded container 7a2a75c8a0c8dac8c5773ad92fa45ac7e1d33c9be85ecb65eb147929955dca50, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616222796+08:00] Loaded container 81095c01c4b99c7d2cc9e6bee8726c11f16d27204523727e7d067d980c26ac64, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616569394+08:00] Loaded container 94098167eb466dbf1a454f5491a162488d1fdb1eebe804c5c1f403f7fce62dc4, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616981038+08:00] Loaded container 9d4cbcce43b0262d972e73b2770f26ca762e2fa86f0de88a7909b8e59c0b805a, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.617460452+08:00] Loaded container aecde8eb18924d8548d79d5e0383baa7ac3ab1cfc4c55e1f32c4089dfc153071, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.617908975+08:00] Loaded container aed0618a325b4b84363357c1830515048d23af6afd79606cbb0ad64bf5f226a2, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618252961+08:00] Loaded container b43e4995720f235c40ffd60bde1fb54e87ece3598f8bd625996042f637896687, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618557604+08:00] Loaded container c1e6a1de9b9c2fd420e718c405c114e726ec5531561a4caf662b757a3724711e, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618942417+08:00] Loaded container c5eb3c941e562153e0cf0af738f1cb43f34591f0b48ad5458ab2002f5be9e0a8, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.619380785+08:00] Loaded container e211ffccacb8f7982899097fbd0f9ce1d95f8f31f290fb10baf40d00f4980bc9, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.619831551+08:00] Loaded container ef547d238cd01ff7ec048de3442fe9293aa1d5d932ea66c5aed34bfff014182b, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.620192032+08:00] Loaded container f3bb916ec5d7847c3be4341975c47f4e2fe587fc726ca7d76e3dca15cb8dd21d, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.620438678+08:00] Loaded container fa6de6f4aa8894c18a9737bac462f57c69893eca5e4b58bc3bd793a76b252951, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:51.379861237+08:00] restoring container                           container=ef547d238cd01ff7ec048de3442fe9293aa1d5d932ea66c5aed34bfff014182b paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.379910464+08:00] restoring container                           container=e211ffccacb8f7982899097fbd0f9ce1d95f8f31f290fb10baf40d00f4980bc9 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.379994141+08:00] restoring container                           container=7a2a75c8a0c8dac8c5773ad92fa45ac7e1d33c9be85ecb65eb147929955dca50 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380029802+08:00] restoring container                           container=c1e6a1de9b9c2fd420e718c405c114e726ec5531561a4caf662b757a3724711e paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380084763+08:00] restoring container                           container=5b7b0b52c71a14164f269853679211b3823e9eecc2d3829bf2db10c9b720217d paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380127006+08:00] restoring container                           container=9d4cbcce43b0262d972e73b2770f26ca762e2fa86f0de88a7909b8e59c0b805a paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380121758+08:00] restoring container                           container=fa6de6f4aa8894c18a9737bac462f57c69893eca5e4b58bc3bd793a76b252951 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380163318+08:00] restoring container                           container=52d32b0e03c957b6cb9b4d793c47900e689a29d9ae0d63703ea29073a352fbe5 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380310029+08:00] restoring container                           container=027a389c8c1e93629cc5f68af8d023b2ecfe350d7771ba6b87598ff705f6c19f paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380382722+08:00] restoring container                           container=68ba211ec7328bebd3b241631a703639447c05056ffe07ed633b72d0bc210938 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380419320+08:00] restoring container                           container=487a8c2f30986796c3948d1469d506e1d3ab394e17533040ef7a5444a32be0fc paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380433522+08:00] restoring container                           container=73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 paused=false running=true</span><br><span class="line">DEBU[2020-12-04T22:33:51.380459224+08:00] restoring container                           container=c5eb3c941e562153e0cf0af738f1cb43f34591f0b48ad5458ab2002f5be9e0a8 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380525276+08:00] restoring container                           container=b43e4995720f235c40ffd60bde1fb54e87ece3598f8bd625996042f637896687 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380563957+08:00] restoring container                           container=81095c01c4b99c7d2cc9e6bee8726c11f16d27204523727e7d067d980c26ac64 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380586567+08:00] restoring container                           container=62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be paused=false running=true</span><br><span class="line">DEBU[2020-12-04T22:33:51.380599061+08:00] restoring container                           container=94098167eb466dbf1a454f5491a162488d1fdb1eebe804c5c1f403f7fce62dc4 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380616220+08:00] restoring container                           container=aecde8eb18924d8548d79d5e0383baa7ac3ab1cfc4c55e1f32c4089dfc153071 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380641090+08:00] restoring container                           container=f3bb916ec5d7847c3be4341975c47f4e2fe587fc726ca7d76e3dca15cb8dd21d paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380825356+08:00] restoring container                           container=24735e5aea2bd91b5fa5d729ca021a09532c2ea9b8b06f5171d0da23fc3bf4cc paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380953092+08:00] restoring container                           container=aed0618a325b4b84363357c1830515048d23af6afd79606cbb0ad64bf5f226a2 paused=false running=false</span><br></pre></td></tr></table></figure><p>然后发现卡在这，正常是会像 gin 那样启动输出支持的 http api 路由信息的。开一个窗口，发送 SIGUSR1 信号打印 goroutine 堆栈信息看看卡在哪儿：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pgrep dockerd</span></span><br><span class="line">3085</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -USR1 3085</span></span><br></pre></td></tr></table></figure><p>docker 的日志和系统日志都会有下面的类似输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dec 04 22:33:52 xxxx dockerd[3085]: time&#x3D;&quot;2020-12-33T58:15:52.906433650+08:00&quot; level&#x3D;info msg&#x3D;&quot;goroutine stacks written to &#x2F;var&#x2F;run&#x2F;docker&#x2F;goroutine-stacks-2020-12-04T223358+0800.log&quot;</span><br></pre></td></tr></table></figure><p>查看了下下面这段比较可疑，<code>daemon/daemon.go:364</code> 附近</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">goroutine 1 [semacquire, 5 minutes]:</span><br><span class="line">sync.runtime_Semacquire(0xc4204de73c)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;sema.go:56 +0x3b</span><br><span class="line">sync.(*WaitGroup).Wait(0xc4204de730)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;sync&#x2F;waitgroup.go:131 +0x74</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).restore(0xc42009a480, 0x190c3a6, 0x4)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:364 +0xfeb</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;daemon.NewDaemon(0xc42018d200, 0x2ec75c0, 0xc4201be410, 0x2ea89e0, 0xc420087d40, 0xc4201323c0, 0x0, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:894 +0x258d</span><br><span class="line">main.(*DaemonCli).start(0xc42051da40, 0xc4201c5d50, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;daemon.go:223 +0x1320</span><br><span class="line">main.runDaemon(0xc4201c5d50, 0xc42044b3b0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:78 +0x78</span><br><span class="line">main.newDaemonCommand.func1(0xc420176000, 0xc4201359e0, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:29 +0x5d</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).execute(0xc420176000, 0xc42000c090, 0x1, 0x1, 0xc420176000, 0xc42000c090)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:646 +0x44f</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).ExecuteC(0xc420176000, 0x2194e40, 0x2419c01, 0xc420135980)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:742 +0x310</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).Execute(0xc420176000, 0xc420135980, 0x190fa00)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:695 +0x2d</span><br><span class="line">main.main()</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:105 +0xe3</span><br></pre></td></tr></table></figure><p>按照docker info 的信息去找了下对应的 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364" target="_blank" rel="noopener">分支代码</a>。364 行一个<code>wg.Wait()</code>，得看前面的 goroutine 是卡在哪儿，根据前面的堆栈信息，应该是卡在 <code>github.com/docker/docker/daemon.(*Daemon).restore</code>，也就是 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L238" target="_blank" rel="noopener">238 行的 daemon.containerd.Restore 方法</a>，卡在<code>wg.Wait()</code>说明有协程没释放锁，这里<a href="https://github.com/moby/moby/blob/v18.03.0-ce/libcontainerd/client_daemon.go#L134" target="_blank" rel="noopener">containerd.Restore方法</a>的第一行就是锁，里面有个方法<code>c.remote.LoadContainer</code>，实际上是和docker-containerd通信的。</p><p>查看下 docker-containerd 进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep containerd</span><br><span class="line">root      5646  0.2  0.0 606436 14048 ?        Ssl  22:33   0:00 docker-containerd --config &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;containerd.toml</span><br><span class="line">appuser   6261  0.0  0.0 112708   984 pts&#x2F;1    S+   22:36   0:00 grep --color&#x3D;auto containerd</span><br><span class="line">root      8355  0.1  0.0   9052  4308 ?        Sl   Dec03   2:56 docker-containerd-shim -namespace moby -workdir &#x2F;app&#x2F;kube&#x2F;dockercontainerd&#x2F;daemon&#x2F;io.containerd.runtime.v1.linux&#x2F;moby&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be -address &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;docker-containerd.sock -containerd-binary &#x2F;app&#x2F;kube&#x2F;bin&#x2F;docker-containerd -runtime-root &#x2F;var&#x2F;run&#x2F;docker&#x2F;runtime-runc</span><br><span class="line">root     11171  0.0  0.0   9052  4052 ?        Sl   Dec03   0:18 docker-containerd-shim -namespace moby -workdir &#x2F;app&#x2F;kube&#x2F;dockercontainerd&#x2F;daemon&#x2F;io.containerd.runtime.v1.linux&#x2F;moby&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 -address &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;docker-containerd.sock -containerd-binary &#x2F;app&#x2F;kube&#x2F;bin&#x2F;docker-containerd -runtime-root &#x2F;var&#x2F;run&#x2F;docker&#x2F;runtime-runc</span><br></pre></td></tr></table></figure><p>有残留的，杀掉一个试试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 11171</span></span><br></pre></td></tr></table></figure><p>然后原窗口有输出了一些日志，实际上是执行了 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L244" target="_blank" rel="noopener">244行的 daemon.containerd.DeleteTask方法</a>，说明思路是对的，进程通信有问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0172] connecting to shim                            error&#x3D;&lt;nil&gt; id&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; namespace&#x3D;moby</span><br><span class="line">DEBU[2020-12-04T22:36:42.690975930+08:00] restored container                            alive&#x3D;false container&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 module&#x3D;libcontainerd namespace&#x3D;moby pid&#x3D;0</span><br><span class="line">DEBU[2020-12-04T22:36:42.701154551+08:00] Trying to unmount &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212&#x2F;mounts </span><br><span class="line">DEBU[2020-12-04T22:36:42.707909556+08:00] Unmounted &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212&#x2F;mounts </span><br><span class="line">DEBU[0172] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;exit&quot; type&#x3D;containerd.events.TaskExit</span><br><span class="line">DEBU[0172] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;delete&quot; type&#x3D;containerd.events.TaskDelete</span><br><span class="line">DEBU[2020-12-04T22:36:42.947670205+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;exit</span><br></pre></td></tr></table></figure><p>接着处理另一个：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep containerd</span></span><br><span class="line">root      5646  0.2  0.0 606692 14048 ?        Ssl  22:33   0:00 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root      6461  0.0  0.0 112708   984 pts/1    S+   22:37   0:00 grep --color=auto containerd</span><br><span class="line">root      8355  0.1  0.0   9052  4260 ?        Sl   Dec03   2:56 docker-containerd-shim -namespace moby -workdir /app/kube/dockercontainerd/daemon/io.containerd.runtime.v1.linux/moby/62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /app/kube/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 8355</span></span><br></pre></td></tr></table></figure><p>然后前台 debug 的日志没有卡住，正常启动了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0249] connecting to shim                            error&#x3D;&lt;nil&gt; id&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; namespace&#x3D;moby</span><br><span class="line">DEBU[2020-12-04T22:37:59.709825146+08:00] restored container                            alive&#x3D;false container&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be module&#x3D;libcontainerd namespace&#x3D;moby pid&#x3D;0</span><br><span class="line">DEBU[2020-12-04T22:37:59.710064357+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete</span><br><span class="line">INFO[2020-12-04T22:37:59.710093459+08:00] ignoring event                                module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">WARN[2020-12-04T22:37:59.710215638+08:00] Ignoring Exit Event, no such exec command found  container&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 exec-id&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 exec-pid&#x3D;11197</span><br><span class="line">DEBU[2020-12-04T22:37:59.719102521+08:00] Trying to unmount &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be&#x2F;mounts </span><br><span class="line">DEBU[2020-12-04T22:37:59.722934436+08:00] Unmounted &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be&#x2F;mounts </span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;containers&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;containers&#x2F;delete&quot; type&#x3D;containerd.events.ContainerDelete</span><br><span class="line">DEBU[2020-12-04T22:37:59.978450001+08:00] container mounted via layerStore: &amp;&#123;&#x2F;app&#x2F;kube&#x2F;docker&#x2F;overlay2&#x2F;97a09a97cf8c3ae835fb0ca6526c0282b26379942dfb49081189a39ce0400596&#x2F;merged 0x2f42600 0x2f42600&#125; </span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;exit&quot; type&#x3D;containerd.events.TaskExit</span><br><span class="line">DEBU[2020-12-04T22:38:00.169804015+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;exit</span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;delete&quot; type&#x3D;containerd.events.TaskDelete</span><br><span class="line">WARN[2020-12-04T22:38:00.169915440+08:00] Ignoring Exit Event, no such exec command found  container&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be exec-id&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be exec-pid&#x3D;8396</span><br><span class="line">DEBU[2020-12-04T22:38:00.170037297+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete</span><br><span class="line">INFO[2020-12-04T22:38:00.170061445+08:00] ignoring event                                module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;containers&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;containers&#x2F;delete&quot; type&#x3D;containerd.events.ContainerDelete</span><br><span class="line">DEBU[2020-12-04T22:38:00.199820461+08:00] container mounted via layerStore: &amp;&#123;&#x2F;app&#x2F;kube&#x2F;docker&#x2F;overlay2&#x2F;2abb109b107ef7f0e5c31b1a100b446234118ae38afe43977c8c718f115cdfd6&#x2F;merged 0x2f42600 0x2f42600&#125; </span><br><span class="line">DEBU[2020-12-04T22:38:00.208519823+08:00] Option Experimental: false                   </span><br><span class="line">DEBU[2020-12-04T22:38:00.208542167+08:00] Option DefaultDriver: bridge                 </span><br><span class="line">DEBU[2020-12-04T22:38:00.208549815+08:00] Option DefaultNetwork: bridge                </span><br><span class="line">DEBU[2020-12-04T22:38:00.208557480+08:00] Network Control Plane MTU: 1500              </span><br><span class="line">DEBU[2020-12-04T22:38:00.245647071+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.247719844+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0&#x2F;8 -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.249828613+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.251439314+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D PREROUTING]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>服务器上有安全狗，可能和安全狗有关系。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364" target="_blank" rel="noopener">https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;起初是 k8s 有几个 node not ready，上去看了下 kubelet 日志刷 container runtime down，重启</summary>
      
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/categories/docker/"/>
    
    
    <category term="hang" scheme="http://zhangguanzhang.github.io/tags/hang/"/>
    
  </entry>
  
  <entry>
    <title>ansible hang in docker container</title>
    <link href="http://zhangguanzhang.github.io/2020/11/23/ansible-hang-in-docker/"/>
    <id>http://zhangguanzhang.github.io/2020/11/23/ansible-hang-in-docker/</id>
    <published>2020-11-23T19:42:08.000Z</published>
    <updated>2020-11-23T19:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这几天同事发现在 docker 容器里运行 ansible 命令很卡，发来了个命令叫我试试 <code>ansible localhost -m setup -a &#39;filter=ansible_default_ipv4&#39; 2&gt;/dev/null |grep &#39;\&quot;address\&quot;&#39; |awk -F&#39;\&quot;&#39; &#39;&#123;print $4&#125;&#39;</code></p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span> </span><br><span class="line">NAME=&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION=&quot;V10 (Tercel)&quot;</span><br><span class="line">ID=&quot;kylin&quot;</span><br><span class="line">VERSION_ID=&quot;V10&quot;</span><br><span class="line">PRETTY_NAME=&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxxxx 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info</span></span><br><span class="line">Containers: 1</span><br><span class="line"> Running: 1</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 1</span><br><span class="line">Server Version: 18.09.9</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line">runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.19.90-17.ky10.aarch64</span><br><span class="line">Operating System: Kylin Linux Advanced Server V10 (Tercel)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: aarch64</span><br><span class="line">CPUs: 64</span><br><span class="line">Total Memory: 62.76GiB</span><br><span class="line">Name: xxx</span><br><span class="line">ID: 3ZQD:MZWN:FNR4:5HEE:F57N:3BLD:EP3T:LJT7:NWEJ:3TZ3:IEBD:KSHZ</span><br><span class="line">Docker Root Dir: /data/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><p>容器里的 ansible 和 python 信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ansible --version</span></span><br><span class="line">ansible 2.8.6</span><br><span class="line">  config file = None</span><br><span class="line">  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]</span><br><span class="line">  ansible python module location = /usr/local/lib/python2.7/dist-packages/ansible</span><br><span class="line">  executable location = /usr/local/bin/ansible</span><br><span class="line">  python version = 2.7.12 (default, Apr 15 2020, 17:07:12) [GCC 5.4.0 20160609]</span><br><span class="line"><span class="meta">$</span><span class="bash"> python --version</span></span><br><span class="line">Python 2.7.12</span><br></pre></td></tr></table></figure><p>他说如果用麒麟的 rpm 包安装 docker 就没问题，用我们的拷贝二进制文件安装的 docker 起的容器里就不行。一开始是怀疑 setup 模块在收集某些信息的时候阻塞了，后面我试了下这样也会卡住 <code>ansible localhost -m shell -a date</code>。</p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>带上了 <code>-vvvv</code> 发现卡在下面的输出这：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;127.0.0.1&gt; PUT &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-local-466216tkG5t&#x2F;tmpML7hBj TO &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py</span><br><span class="line">&lt;127.0.0.1&gt; EXEC &#x2F;bin&#x2F;sh -c &#39;chmod u+x &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F; &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py &amp;&amp; sleep 0&#39;</span><br><span class="line">&lt;127.0.0.1&gt; EXEC &#x2F;bin&#x2F;sh -c &#39;&#x2F;usr&#x2F;bin&#x2F;python &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py &amp;&amp; sleep 0&#39;</span><br></pre></td></tr></table></figure><p>搜了下可以 <code>export ANSIBLE_DEBUG=True</code> 打印更详细的日志，打印了下面的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">23918 1606128393.94311: ANSIBALLZ: Done creating module</span><br><span class="line">23918 1606128393.94465: _low_level_execute_command(): starting</span><br><span class="line">23918 1606128393.94493: _low_level_execute_command(): executing: &#x2F;bin&#x2F;sh -c &#39;&#x2F;usr&#x2F;bin&#x2F;python &amp;&amp; sleep 0&#39;</span><br><span class="line">23918 1606128393.94515: in local.exec_command()</span><br><span class="line">23918 1606128393.94528: opening command with Popen()</span><br><span class="line">23918 1606128393.94979: done running command with Popen()</span><br><span class="line">23918 1606128393.95005: getting output with communicate()</span><br></pre></td></tr></table></figure><p>看样子是子进程卡住了，主进程等子进程。因为容器的进程实际上也是在宿主机上的，宿主机上安装了 strace，查看下进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep ansible</span></span><br><span class="line">root     3020177  2.7  0.0 129408 48960 pts/0    Sl+  19:12   0:26 /usr/bin/python /usr/local/bin/ansible localhost -vvvvv -m shell -a ls</span><br><span class="line">root     3020189  0.0  0.0 134912 50368 pts/0    S+   19:12   0:00 /usr/bin/python /usr/local/bin/ansible localhost -vvvvv -m shell -a ls</span><br><span class="line">root     3020216  0.0  0.0   2368   768 pts/0    S+   19:12   0:00 /bin/sh -c /bin/sh -c &#x27;/usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py &amp;&amp; sleep 0&#x27;</span><br><span class="line">root     3020217  0.0  0.0   2368   768 pts/0    S+   19:12   0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py &amp;&amp; sleep 0</span><br><span class="line">root     3020218  0.0  0.0  19776 14336 pts/0    S+   19:12   0:00 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py</span><br><span class="line">root     3020219  9.3  0.0  18688 10816 pts/0    t+   19:12   1:27 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py</span><br><span class="line">root     3050341  0.3  0.0   8832  4544 ?        Ss   19:28   0:00 ssh: /root/.ansible/cp/5a0929d121 [mux]</span><br><span class="line">root     3052724  0.0  0.0 214080  1536 pts/7    S+   19:28   0:00 grep ansible</span><br></pre></td></tr></table></figure><p>strace 下 3020219 ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> strace -p 3020219</span></span><br><span class="line">close(76267956)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267957)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267958)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267959)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267960)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267961)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267962)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267963)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267964)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267965)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267966)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267967)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267968)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267969)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267970)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267971)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267972^C)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">strace: Process 3020219 detached</span><br></pre></td></tr></table></figure><p>终端一直刷上面的，看样子是文件描述符泄露，<code>错误的文件描述符</code> 英文就是 <code>Bad file descriptor</code>， 谷歌搜了下 <code>in &quot;docker&quot;  (Bad file descriptor) strace</code>，找到了 <a href="https://github.com/docker/for-linux/issues/502">Spawning PTY processes is many times slower on Docker 18.09</a> 里几位大佬排查到是某些 os 下，很多地方都有设置 limit（就是那种我以为你做了，我就没做。结果你以为我做了，你就没做，结果大家都没做的感觉了），导致容器的 nofile 会不固定，经常性的太高。而很多语言的模块会遍历所有描述符，导致会卡。如果启动容器 nofile 设置低则没问题，下面还有个大佬给了个链接在 python 层面修复这个问题 <a href="https://github.com/python/cpython/pull/11584">python/cpython#11584</a> ，还有下面的解决办法，直接设置默认的 limit：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> https://docs.docker.com/engine/reference/commandline/dockerd/<span class="comment">#daemon-configuration-file</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/docker/daemon.json-ulimits</span></span><br><span class="line">&#123;</span><br><span class="line">&quot;default-ulimits&quot;: &#123;</span><br><span class="line">&quot;nofile&quot;: &#123;</span><br><span class="line">&quot;Name&quot;: &quot;nofile&quot;,</span><br><span class="line">&quot;Hard&quot;: 1024,</span><br><span class="line">&quot;Soft&quot;: 1024</span><br><span class="line">&#125;,</span><br><span class="line">&quot;nproc&quot;: &#123;</span><br><span class="line">&quot;Name&quot;: &quot;nproc&quot;,</span><br><span class="line">&quot;Soft&quot;: 65536,</span><br><span class="line">&quot;Hard&quot;: 65536</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>改配置怕影响其他容器，就决定从 python 层面改，看了下提交的 pr <a href="https://github.com/python/cpython/commit/5626fff54ebe5863e64454c081ec585f85cf141c">python/cpython@5626fff</a> 是改的 <code>subprocess.py</code> ，在容器里查找下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">find / -type f -name subprocess.py </span><br><span class="line">/usr/lib/python2.7/subprocess.py</span><br><span class="line">/usr/lib/python3.5/asyncio/subprocess.py</span><br><span class="line">/usr/lib/python3.5/subprocess.py</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/future/moves/subprocess.py</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/gevent/subprocess.py</span><br></pre></td></tr></table></figure><p>对比了下函数名 <code>def _close_fds(self, but):</code> ，确定是 <code>/usr/lib/python2.7/subprocess.py</code>，把 pr 的内容加进去后再执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> time ansible localhost -m shell -a date</span></span><br><span class="line">[WARNING]: No inventory was parsed, only implicit localhost is available</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">localhost | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">2020年 11月 23日 星期一 19:38:14 CST</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real0m3.088s</span><br><span class="line">user0m2.860s</span><br><span class="line">sys  0m0.255s</span><br></pre></td></tr></table></figure><p>最终还是选择了起容器的时候限制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--ulimit nofile&#x3D;65536</span><br></pre></td></tr></table></figure><p>上面的几位大佬给出了其他的解决方案，也可以在 containerd 配置文件里配置把 nofile 固定住，或者 docker daemon，或者应用的软件层面修复。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/pexpect/ptyprocess/issues/50">https://github.com/pexpect/ptyprocess/issues/50</a></li><li><a href="https://github.com/docker/for-linux/issues/502">https://github.com/docker/for-linux/issues/502</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这几天同事发现在 docker 容器里运行 ansible 命令很卡，发来了个命令叫我试试 &lt;code&gt;ansible localhost </summary>
      
    
    
    
    <category term="ansible" scheme="http://zhangguanzhang.github.io/categories/ansible/"/>
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>永久关闭swap的正确姿势</title>
    <link href="http://zhangguanzhang.github.io/2020/11/20/disable-swap/"/>
    <id>http://zhangguanzhang.github.io/2020/11/20/disable-swap/</id>
    <published>2020-11-20T10:29:08.000Z</published>
    <updated>2020-11-20T10:29:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天遇到了 kylin 系统上无法关闭 swap 的情况。记录下和方便别人搜到这个知识点。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> at /etc/issue</span></span><br><span class="line">Kylin 4.0.2 \n \l</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux H-192-168-63-132 4.15.0- 58-generic #64kord1k1&#x27;SMP Thu Aug 1S15:51:97 csT 2919 aarch64 ......</span><br></pre></td></tr></table></figure><h2 id="尝试的步骤"><a href="#尝试的步骤" class="headerlink" title="尝试的步骤"></a>尝试的步骤</h2><p>fstab 里没有 swap 的挂载，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a &amp;&amp; sysctl -w vm.swappiness=0</span><br></pre></td></tr></table></figure><p>重启后，内核参数是关闭的，但是实际没有关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sysctl -a |&amp; grep vm.swappiness</span></span><br><span class="line">vm.swappiness = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           127G         48G         70G        169M        7.8G         64G</span><br><span class="line">Swap:          7.6G          0B        7.8G</span><br></pre></td></tr></table></figure><p>应该有其他的挂载，据我所知， systemd 也会负责挂载的，查找下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl list-units | grep swap</span></span><br><span class="line">dev-sda3.swap         loaded active active    Swap Partition</span><br><span class="line">swap.target           loaded active active    Swap</span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl cat dev-sda3.swap</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /run/systemd/generator.late/dev-sda3.swap</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Automatically generated by systemd-gpt-auto-generator</span></span><br><span class="line"></span><br><span class="line">[unit]</span><br><span class="line">Description=Swap Partition</span><br><span class="line">Documentation=man:systemd-gpt-auto-generator(8)</span><br><span class="line"></span><br><span class="line">[Swap]</span><br><span class="line">What=/dev/sda3</span><br></pre></td></tr></table></figure><p>发现这个无法 disable ，会报错 no such file。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>查找了下文档，<code>systemd-gpt-auto-generator</code> 是一个 GPT 分区 自动发现 与 挂载。会自动生成 mount 和 swap 的 systemd unit 文件。找到了英文文档里有下面的话:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemd-gpt-auto-generator understands the following kernel command line parameters:</span><br><span class="line"></span><br><span class="line">systemd.gpt_auto, rd.systemd.gpt_auto</span><br><span class="line">Those options take an optional boolean argument, and default to yes. The generator is enabled by default, and a negative value may be used to disable it.</span><br></pre></td></tr></table></figure><p>我们可以通过添加 kernel boot cmdline 来关闭 <code>systemd-gpt-auto-generator</code> 。</p><p>先查找到 <code>grub.cfg</code> ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> find /boot  -<span class="built_in">type</span> f -name <span class="string">&#x27;grub*cfg&#x27;</span> -<span class="built_in">exec</span> grep -l <span class="string">&#x27;/vmlinuz&#x27;</span> &#123;&#125; \;</span></span><br><span class="line">/boot/grub/grub.cfg</span><br></pre></td></tr></table></figure><p>进去备份下文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /boot/grub/</span><br><span class="line">cp grub.cfg grub.cfg-20201120</span><br></pre></td></tr></table></figure><p>找到类似下面的行:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux /boot/vmlinuz-xxx</span><br></pre></td></tr></table></figure><p>后面加上 <code>systemd.gpt_auto=false</code> ，文档是写布尔值的，不过我看到有人 <code>systemd.gpt_auto=0</code> 也行。然后重启。</p><p><code>2021/03/23</code> 测试了下面的不行。。。<br>理论上改文件 <code>/etc/default/grub</code> 里 <code>GRUB_CMDLINE_LINUX</code> 后面添加也行。</p><p>Azure Linux 的话有个 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl list-units | grep temp-disk-swapfile</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www.jinbuguo.com/systemd/systemd-gpt-auto-generator.html">systemd-gpt-auto-generator 中文手册</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/systemd-gpt-auto-generator.html">systemd-gpt-auto-generator 英文文档</a></li><li><a href="http://www.jinbuguo.com/systemd/systemd.generator.html#">systemd.generator 中文手册</a></li><li><a href="https://groups.google.com/g/shlug/c/BH11BbecodM">更改分区类型关闭 swap</a></li><li><a href="https://manpages.ubuntu.com/manpages/disco/man7/kernel-command-line.7.html">kernel-command-line man page 7</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天遇到了 kylin 系统上无法关闭 swap 的情况。记录下和方便别人搜到这个知识点。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;figure clas</summary>
      
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/categories/linux/"/>
    
    
    <category term="swap" scheme="http://zhangguanzhang.github.io/tags/swap/"/>
    
  </entry>
  
  <entry>
    <title>银河麒麟arm64系统克隆机器上k8s vxlan跨节点不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2020/11/06/kylin-arm-clone-vxlan-error/"/>
    <id>http://zhangguanzhang.github.io/2020/11/06/kylin-arm-clone-vxlan-error/</id>
    <published>2020-11-06T19:31:01.000Z</published>
    <updated>2020-11-06T19:31:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>和前一篇文章不一样，从来没遇到过这样的问题，这里记录下。实施在客户那边部署业务后，业务在浏览器上无法访问，我远程上去查看日志发现 pod 内部无法 DNS 无法解析，nginx 连不上 upsteam 报错而启动失败，实际上也是跨节点不通。实际排查过程也有往错误的方向浪费了一些时间和尝试，就不写进来了，以正确的角度写下排查过程。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>OS 是 arm64 的银河麒麟系统：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">NAME=&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION=&quot;V10 (Tercel)&quot;</span><br><span class="line">ID=&quot;kylin&quot;</span><br><span class="line">VERSION_ID=&quot;V10&quot;</span><br><span class="line">PRETTY_NAME=&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux localhost.localdomain 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>集群信息（和集群版本没关系）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:17:59Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:09:48Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>node 信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o wide</span></span><br><span class="line">NAME            STATUS   ROLES         AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                   KERNEL-VERSION            CONTAINER-RUNTIME</span><br><span class="line">172.18.27.252   Ready    master,node   32h   v1.15.12   172.18.27.252   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br><span class="line">172.18.27.253   Ready    master,node   32h   v1.15.12   172.18.27.253   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br><span class="line">172.18.27.254   Ready    master,node   32h   v1.15.12   172.18.27.254   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br></pre></td></tr></table></figure><p>coredns 信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span></span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-677d9c57f-pqvfv   1/1     Running   1          21h     10.187.0.5      172.18.27.253   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-677d9c57f-zjf86   1/1     Running   1          4h45m   10.187.1.12     172.18.27.252   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>命令都是在 <code>172.18.27.252</code> 上执行的，用 <code>dig @coredns_svc_ip +short kubernetes.default.svc.cluster1.local</code> 测发现时而能解析，时而不能解析，然后发现是跨节点的问题。</p><p>在 <code>172.18.27.252</code> 上去请求 <code>172.18.27.253</code> 上的 coredns 的 metrics 接口：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -I 10.187.0.5:9153/metrics</span><br></pre></td></tr></table></figure><p>然后在 <code>172.18.27.253</code> 上抓包:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.0.5 and port 9153</span></span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">^C</span><br><span class="line">0 packets captured</span><br><span class="line">0 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure><p>没有包，在 <code>172.18.27.253</code> 上抓了下 <code>8472</code> 端口是正常能收包的。像之前的那个文章 <a href="https://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/">银河麒麟 arm64 系统上 k8s 集群跨节点不通的一次排查</a> 看了下，253 机器上查看路由也没问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.1.0</span></span><br><span class="line">10.187.1.0 via 10.187.1.0 dev flannel.1 src 10.187.0.0 uid 0</span><br><span class="line">    cache</span><br></pre></td></tr></table></figure><p>当时各种手段看了个遍，结果有点眉目了（我应该像上篇文章一样先看下 vxlan 的 vtep 信息的。。。）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;fe:22:77:eb:2f:a4&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.252</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;fe:22:77:eb:2f:a4&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.253</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;fe:22:77:eb:2f:a4&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.254</span><br></pre></td></tr></table></figure><p>vtep 的 mac 地址都一样，查看下，发现三台机器都是一样的，看下网卡和 flannel.1 的信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip -4 a s enp1s0</span></span><br><span class="line">2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether dc:2d:cb:17:3e:a1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.27.252/25 brd 172.18.27.255 scope global noprefixroute enp1s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">394: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 1 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.252 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">#</span><span class="bash"> 另一台机器</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -4 a s enp1s0</span></span><br><span class="line">2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether dc:2d:cb:17:3e:86 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.27.254/25 brd 172.18.27.255 scope global noprefixroute enp1s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">66: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.254 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>可以看到网卡 enp1s0 的 mac 是不一样的，查看了下几个机器的网卡配置文件的 UUID 和 HWADDR 都不一样。上面命令可以看出只有 flannel.1 的 MAC 是一样，尝试删除然后重启 flanneld 看看重建咋样：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip link <span class="built_in">set</span> flannel.1 down</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link <span class="built_in">set</span> flannel.1 up</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a |grep -m1 flanneld</span></span><br><span class="line">0aa5998260ba        122cdb7aa710                                &quot;/opt/bin/flanneld -…&quot;    5 hours ago          Up 3 hours                                          k8s_kube-flannel_kube-flannel-ds-2zqr9_kube-system_1ed6eba1-fa30-405e-83d0-314160c25313_1</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart 0aa</span></span><br><span class="line">0aa</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">22: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>还是一摸一样，好奇这个 mac 地址如何来的，就去看了下 <a href="https://github.com/coreos/flannel/blob/v0.11.0/backend/vxlan/vxlan.go#L104-L137">flannel 添加 flannel.1 的源码部分</a>：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">devAttrs := vxlanDeviceAttrs&#123;</span><br><span class="line">vni:       <span class="keyword">uint32</span>(cfg.VNI),</span><br><span class="line">name:      fmt.Sprintf(<span class="string">&quot;flannel.%v&quot;</span>, cfg.VNI),</span><br><span class="line">vtepIndex: be.extIface.Iface.Index,</span><br><span class="line">vtepAddr:  be.extIface.IfaceAddr,</span><br><span class="line">vtepPort:  cfg.Port,</span><br><span class="line">gbp:       cfg.GBP,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dev, err := newVXLANDevice(&amp;devAttrs)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">dev.directRouting = cfg.DirectRouting</span><br><span class="line"></span><br><span class="line">subnetAttrs, err := newSubnetAttrs(be.extIface.ExtAddr, dev.MACAddr())</span><br></pre></td></tr></table></figure><p><code>dev.MACAddr()</code> 是直接 return 的 <code>dev.link.HardwareAddr</code>，goland 里 find usage 下压根没找到赋值的地方。（可以加代码打印下到底 mac 地址从哪里获取的，但是环境是远程的，得一次一次编译发过去太麻烦了我就没弄了）毫无头绪乱排查了一段时间。然后突发奇想手动按照 vxlan 创建的步骤测试添加下看看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip link add <span class="built_in">test</span> <span class="built_in">type</span> vxlan id 2 dev enp1s0 <span class="built_in">local</span> 10.186.0.0 dstport 8473 nolearning</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show <span class="built_in">test</span></span></span><br><span class="line">696: test: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ca:32:f1:0d:c6:dc brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 2 local 10.186.0.0 dev enp1s0 srcport 0 0 dstport 8473 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete <span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link add <span class="built_in">test</span> <span class="built_in">type</span> vxlan id 2 dev enp1s0 <span class="built_in">local</span> 10.186.0.0 dstport 8473 nolearning</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show <span class="built_in">test</span></span></span><br><span class="line">697: test: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ca:32:f1:0d:c6:dc brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 2 local 10.186.0.0 dev enp1s0 srcport 0 0 dstport 8473 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>发现手动创建的网络设备 mac 居然也是一样的（实际上 ide 里找上面那个 <code>HardwareAddr</code> 的赋值是在引入的一个 <code>netlink</code> 包里赋值的，也就是说 flannel 创建接口的时候和手动添加类似，没有直接设置 mac 地址，而是系统返回的），然后同样步骤在我机器上测试是不一样的，看了下客户是啥服务器，发现居然是虚机。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /sys/class/dmi/id/product_name</span></span><br><span class="line">KVM Virtual Machine</span><br></pre></td></tr></table></figure><p>三台机器上添加接口的 mac 地址都是一样的，机器是不是克隆的？询问了下同事，同事说是的。其实这就是引起故障的根源，应该内部 mac 地址默认是不随机的策略。</p><p>查看下网卡策略是咋样的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> networkctl status enp1s0</span></span><br></pre></td></tr></table></figure><p>居然都为空，ubuntu 的机器上是有个优先级低的 link 文件的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status enp10s0</span></span><br><span class="line">● 2: enp10s0</span><br><span class="line">       Link File: /lib/systemd/network/99-default.link</span><br><span class="line">    Network File: n/a</span><br><span class="line">            Type: ether</span><br><span class="line">           State: n/a (unmanaged)</span><br><span class="line">            Path: platform-80040000000.pcie-controller-pci-0000:0a:00.0</span><br><span class="line">          Driver: igb</span><br><span class="line">          Vendor: Intel Corporation</span><br><span class="line">           Model: I210 Gigabit Network Connection</span><br><span class="line">      HW Address: 00:09:06:xx:xx:xx (Esteem Networks)</span><br><span class="line">         Address: 10.2xx.x5.23</span><br><span class="line">                  fe80::209:xxx:xxxx:xxxx</span><br><span class="line">         Gateway: 10.2xx.x5.254</span><br></pre></td></tr></table></figure><p>一些云主机我看也没这个文件，但是上面添加的接口每次 mac 不一样，应该其他 OS 改了默认的行为。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>我们可以用 systemd 写一个 link 配置文件改变策略：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat&lt;&lt;&#x27;EOF&#x27;&gt;/etc/systemd/network/10-flannel.1.link</span><br><span class="line">[Match]</span><br><span class="line">OriginalName=flannel.1</span><br><span class="line"></span><br><span class="line">[Link]</span><br><span class="line">MACAddressPolicy=none</span><br><span class="line">EOF</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一开始只添加上面的，结果重启后文件没了，下面的文件也追加了下，然后重启还是失效。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后上下两个步骤都整就行了。如果你也遇到了，先单独上面的文件试试，不行再下面的也加上试试。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat&lt;&lt;<span class="string">&#x27;EOF&#x27;</span>&gt;&gt;/etc/systemd/networkd.conf</span></span><br><span class="line">[Match]</span><br><span class="line">OriginalName=flannel*</span><br><span class="line"></span><br><span class="line">[Link]</span><br><span class="line">MACAddressPolicy=none</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>确认生效文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status flannel.1</span></span><br><span class="line">● 36: flannel.1                                              </span><br><span class="line">             Link File: /etc/systemd/network/10-flannel.1.link</span><br><span class="line">          Network File: n/a                                 </span><br><span class="line">                  Type: vxlan                               </span><br><span class="line">                 State: routable (unmanaged)   </span><br><span class="line">                Driver: vxlan                               </span><br><span class="line">            HW Address: fe:22:77:eb:2f:a4                   </span><br><span class="line">                   MTU: 1450 (min: 68, max: 65535)          </span><br><span class="line">                   VNI: 1                                   </span><br><span class="line">                 Local: 172.18.27.252                       </span><br><span class="line">      Destination Port: 8472                                </span><br><span class="line">     Underlying Device: enp1s0                              </span><br><span class="line">  Queue Length (Tx/Rx): 1/1                                 </span><br><span class="line">               Address: 10.187.1.0 </span><br></pre></td></tr></table></figure><p>再测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a | grep -m1 flanneld</span></span><br><span class="line">f8759c103131        122cdb7aa710                                   &quot;/opt/bin/flanneld -…&quot;    27 minutes ago      Up 27 minutes                                   k8s_kube-flannel_kube-flannel-ds-85kps_kube-system_127265f3-f3ea-4f89-87e1-aa6c0e0d356f_0</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart f87</span></span><br><span class="line">f87</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">36: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether 1a:e0:cc:e3:a7:04 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>果然变了，如果没变尝试把 <code>none</code> 改成 <code>random</code> 试试。然后每个节点这样操作后，查看下了下 vtep 信息正常了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;9a:1e:00:9d:0c:60&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.252</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;1a:e0:cc:e3:a7:04&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.253</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;fe:22:77:eb:2f:a4&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.254</span><br></pre></td></tr></table></figure><p>测试下跨节点通信：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.187.0.5:9153/metrics</span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Length: 16905</span><br><span class="line">Content-Type: text/plain; version=0.0.4; charset=utf-8</span><br><span class="line">Date: Fri, 06 Nov 2020 01:47:31 GMT</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>应该一开始就看下 vtep 信息的，其次应该是 os 的问题，缺少 link 文件。还有这个和 flannel 没关系，只要用到 Linux 自带的 vxlan 接口在这种场景上都会出现，例如 calico 新版本也有 vxlan 模式。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.jinbuguo.com/systemd/systemd.link.html">systemd link</a></li><li><a href="https://wiki.archlinux.org/index.php/Systemd-networkd_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)">arch linux</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;和前一篇文章不一样，从来没遇到过这样的问题，这里记录下。实施在客户那边部署业务后，业务在浏览器上无法访问，我远程上去查看日志发现 pod 内</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/categories/k8s/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/tags/Kylin/"/>
    
  </entry>
  
</feed>
