<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhangguanzhang</title>
  
  <subtitle>站在巨人的肩膀上</subtitle>
  <link href="http://zhangguanzhang.github.io/atom.xml" rel="self"/>
  
  <link href="http://zhangguanzhang.github.io/"/>
  <updated>2021-04-08T17:28:30.000Z</updated>
  <id>http://zhangguanzhang.github.io/</id>
  
  <author>
    <name>Zhangguanzhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>kubelet 和 runc 编译关闭 kmem</title>
    <link href="http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/"/>
    <id>http://zhangguanzhang.github.io/2021/04/08/kubelet-runc-disable-kmem/</id>
    <published>2021-04-08T17:28:30.000Z</published>
    <updated>2021-04-08T17:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提详情"><a href="#前提详情" class="headerlink" title="前提详情"></a>前提详情</h2><p>在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修复。</p><p>网上有言论说升级 Linux 内核至 <code>kernel-3.10.0-1075.el7</code> 及以上就可以修复这个问题，详细可见 <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1507149#c101">slab leak causing a crash when using kmem control group</a>。但是我测试了下面的都不行：</p><ul><li>CentOS7.4</li><li>CentOS7.6</li><li>CentOS7.7的 3.10.0-1062.el7.x86_64 </li><li>CentOS Linux release 7.8.2003 (Core) - 3.10.0-1127.el7.x86_64</li></ul><p>Linux其余发行版内核如果大于等于 4.4 应该没问题。<br>这里我们编译 kubelet 关闭 kmem。</p><h2 id="准备条件"><a href="#准备条件" class="headerlink" title="准备条件"></a>准备条件</h2><p>这里我们使用的编译参数会使用容器编译的，不需要宿主机上安装 golang，安装个 docker 就行了。</p><ol><li><p><code>1c 4g</code> 的机器，这里我是使用 <code> CentOS 7.8.2003 (Core)</code><br>机器配置 2g 内存的时候编译提示 oom，升级到 4g 内存才编译成功的。</p></li><li><p>最好安装最新版本的docker</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">systemctl disable --now firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -ri &#39;&#x2F;^[^#]*SELINUX&#x3D;&#x2F;s#&#x3D;.+$#&#x3D;disabled#&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"></span><br><span class="line">cat&gt;&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;custom.conf&lt;&lt;EOF</span><br><span class="line">*       soft    nproc   131072</span><br><span class="line">*       hard    nproc   131072</span><br><span class="line">*       soft    nofile  131072</span><br><span class="line">*       hard    nofile  131072</span><br><span class="line">root    soft    nproc   131072</span><br><span class="line">root    hard    nproc   131072</span><br><span class="line">root    soft    nofile  131072</span><br><span class="line">root    hard    nofile  131072</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat&lt;&lt;EOF &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;docker.conf</span><br><span class="line"># 要求iptables对bridge的数据进行处理</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-arptables &#x3D; 1</span><br><span class="line"># 开启转发</span><br><span class="line">net.ipv4.ip_forward &#x3D; 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"></span><br><span class="line">curl -fsSL &quot;https:&#x2F;&#x2F;get.docker.com&#x2F;&quot; | \</span><br><span class="line">  sed -r &#39;&#x2F;add-repo \$yum_repo&#x2F;a sed -i &quot;s#https:&#x2F;&#x2F;download.docker.com#http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce#&quot; &#x2F;etc&#x2F;yum.repos.d&#x2F;docker-*.repo &#39; | \</span><br><span class="line">    bash -s -- --mirror Aliyun</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker&#x2F;</span><br><span class="line">cat&gt;&#x2F;etc&#x2F;docker&#x2F;daemon.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;bip&quot;: &quot;172.17.0.1&#x2F;16&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https:&#x2F;&#x2F;fz5yth0r.mirror.aliyuncs.com&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;dockerhub.mirrors.nwafu.edu.cn&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;reg-mirror.qiniu.com&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">    &quot;max-file&quot;: &quot;3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;</span><br><span class="line">cat&gt;&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d&#x2F;10-docker.conf&lt;&lt;EOF</span><br><span class="line">[Service]</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#39;&#x2F;sbin&#x2F;iptables --wait -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables --wait -I INPUT -i cni0 -j ACCEPT</span><br><span class="line">ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#39;&#x2F;sbin&#x2F;iptables --wait -D INPUT -i cni0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || :&#39;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y epel-release bash-completion </span><br><span class="line">cp &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;completions&#x2F;docker &#x2F;etc&#x2F;bash_completion.d&#x2F;</span><br><span class="line"></span><br><span class="line">systemctl enable --now docker</span><br></pre></td></tr></table></figure><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><h3 id="确定版本"><a href="#确定版本" class="headerlink" title="确定版本"></a>确定版本</h3><p>查看下我们目前使用的 kubelet 版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"> kubectl version -o json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux&#x2F;amd64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>安装编译的基础依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y rsync make</span><br></pre></td></tr></table></figure><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><p>这里我们使用容器编译，所以下载到啥地方都行，也不需要安装 go。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/kubernetes/kubernetes.git</span><br><span class="line">cd kubernetes</span><br><span class="line">git checkout v1.15.5</span><br></pre></td></tr></table></figure><h3 id="前提操作"><a href="#前提操作" class="headerlink" title="前提操作"></a>前提操作</h3><p>查看 cross 镜像的版本号</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat build&#x2F;build-image&#x2F;cross&#x2F;VERSION</span><br><span class="line">v1.12.10-1</span><br></pre></td></tr></table></figure><p>拉国内的镜像，然后改名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull registry.aliyuncs.com&#x2F;k8sxio&#x2F;kube-cross:v1.12.10-1</span><br><span class="line">$ docker tag registry.aliyuncs.com&#x2F;k8sxio&#x2F;kube-cross:v1.12.10-1 k8s.gcr.io&#x2F;kube-cross:v1.12.10-1</span><br></pre></td></tr></table></figure><p>编译，这个参数测试在 v1.15.5 里可用，网上的 <code>make BUILDTAGS=&quot;nokmem&quot; WHAT=cmd/kubelet GOFLAGS=-v GOGCFLAGS=&quot;-N -l&quot;</code> 会无法编译</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在v1.15.5似乎无用</span><br><span class="line">.&#x2F;build&#x2F;run.sh make kubelet KUBE_BUILD_PLATFORMS&#x3D;linux&#x2F;amd64 BUILDTAGS&#x3D;&quot;nokmem&quot;</span><br><span class="line"># 用下面的</span><br><span class="line">.&#x2F;build&#x2F;run.sh make kubelet GOFLAGS&#x3D;&quot;-v -tags&#x3D;nokmem&quot; KUBE_BUILD_PLATFORMS&#x3D;linux&#x2F;amd64</span><br></pre></td></tr></table></figure><p>查看编译完成的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7 kubernetes]# ls -l _output&#x2F;dockerized&#x2F;bin&#x2F;linux&#x2F;amd64&#x2F;</span><br><span class="line">total 202880</span><br><span class="line">-rwxr-xr-x 1 root root   9203530 Apr  7 22:02 conversion-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9207908 Apr  7 22:02 deepcopy-gen</span><br><span class="line">-rwxr-xr-x 1 root root   9156147 Apr  7 22:02 defaulter-gen</span><br><span class="line">-rwxr-xr-x 1 root root   4709220 Apr  7 22:02 go2make</span><br><span class="line">-rwxr-xr-x 1 root root   2894872 Apr  7 22:03 go-bindata</span><br><span class="line">-rwxr-xr-x 1 root root 157545104 Apr  7 22:13 kubelet</span><br><span class="line">-rwxr-xr-x 1 root root  15018430 Apr  7 22:03 openapi-gen</span><br></pre></td></tr></table></figure><h2 id="runc-关闭-kmem"><a href="#runc-关闭-kmem" class="headerlink" title="runc 关闭 kmem"></a>runc 关闭 kmem</h2><p><a href="https://cloud.tencent.com/developer/article/1743789">https://cloud.tencent.com/developer/article/1743789</a> 这个文章里说了 runc 也需要关闭</p><p>如果下面命令能成功执行则说明 runc 没关闭 kmem</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm --name test --kernel-memory 100M nginx:1.14.2</span><br></pre></td></tr></table></figure><p><code>19.03.14</code> 测试发现可以运行，说明没有关闭，查看它的 <code>runc</code> 版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><p>根据 commit 跳转</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;opencontainers&#x2F;runc&#x2F;commit&#x2F;dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br></pre></td></tr></table></figure><p>根据这个 commit，找到了是 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10">https://github.com/opencontainers/runc/tree/v1.0.0-rc10</a> 这个 tag，</p><p>编译支持的 tag 见 <a href="https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags">https://github.com/opencontainers/runc/tree/v1.0.0-rc10#build-tags</a><br>编译参数从 <a href="https://github.com/opencontainers/runc/blob/v1.0.0-rc10/script/release.sh#L30">release 脚本</a> 找到是下面的参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make BUILDTAGS=&quot;seccomp selinux apparmor&quot; static</span><br></pre></td></tr></table></figure><p>下载源码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/opencontainers/runc.git</span><br><span class="line">cd runc</span><br><span class="line">git checkout v1.0.0-rc10</span><br></pre></td></tr></table></figure><p>直接上面的编译参数是无法编译成功的，因为很多依赖都是 <code>ubuntu</code>下面的。看了下 <code>Makefile</code> 里面提供了一个起 ubuntu 的容器，我们可以进去编译。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make shell</span><br></pre></td></tr></table></figure><p>直接 <code>make shell</code> 的话，它第一步是 <code>make runcimage</code> 会先构建镜像，然后用这个镜像起一个容器，构建的最后一步会失败，因为下面的 <code>busybox</code> 的 <code>rootfs</code> 下载地址变动了，我们得 <code>hack</code> 下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">get_busybox()&#123;</span><br><span class="line">case $(go env GOARCH) in</span><br><span class="line">arm64)</span><br><span class="line">echo &#39;https:&#x2F;&#x2F;github.com&#x2F;docker-library&#x2F;busybox&#x2F;raw&#x2F;dist-arm64v8&#x2F;glibc&#x2F;busybox.tar.xz&#39;</span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">echo &#39;https:&#x2F;&#x2F;github.com&#x2F;docker-library&#x2F;busybox&#x2F;raw&#x2F;dist-amd64&#x2F;glibc&#x2F;busybox.tar.xz&#39;</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>hack 之前我们先测试下，获取下输出的镜像名是<code>runc_dev:HEAD</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make shell</span></span><br><span class="line">docker build  -t runc_dev:HEAD .</span><br><span class="line">^Cmake: *** [runcimage] Interruptaemon  557.1kB</span><br></pre></td></tr></table></figure><p>因为过程会取 git 的一些信息，为了不影响，我们先拷贝文件 <code>tests/integration/multi-arch.bash</code> 和 <code>Dockerfile</code>。我们先手动构建出镜像，再删除掉这俩文件保持 git status。</p><p>先修改 <code>Dockerfile</code> 让它使用 <code>tests/integration/multi-arch.bash.new</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cp Dockerfile Dockerfile.new</span><br><span class="line">vi Dockerfile.new </span><br><span class="line">tail -n2 Dockerfile.new</span><br><span class="line">RUN . tests&#x2F;integration&#x2F;multi-arch.bash.new \</span><br><span class="line">    &amp;&amp; curl -o- -sSL &#96;get_busybox&#96; | tar xfJC - $&#123;ROOTFS&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>新文件下载地址可以在 <a href="https://github.com/opencontainers/runc/blob/bb28c44f12bf24ea64590edfb4f23a4b4d2eaae8/tests/integration/get-images.sh#L59">master 分支最新的脚本里</a> 找到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get &quot;$BUSYBOX_IMAGE&quot; \</span><br><span class="line">&quot;https://github.com/docker-library/busybox/raw/dist-$&#123;arch&#125;/stable/glibc/busybox.tar.xz&quot;</span><br></pre></td></tr></table></figure><p>按照下面修改好</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp  tests/integration/multi-arch.bash tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vi tests/integration/multi-arch.bash.new</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> grep busybox tests/integration/multi-arch.bash.new</span></span><br><span class="line">get_busybox()&#123;</span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-arm64v8/glibc/busybox.tar.xz&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">&#x27;https://github.com/docker-library/busybox/raw/dist-amd64/glibc/busybox.tar.xz&#x27;</span></span></span><br><span class="line">echo &#x27;https://github.com/docker-library/busybox/raw/dist-amd64/stable/glibc/busybox.tar.xz&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>手动编译镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build  -t runc_dev:HEAD -f Dockerfile.new .</span><br></pre></td></tr></table></figure><p>移走文件，保持 <code>git status</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv Dockerfile.new tests/integration/multi-arch.bash.new /tmp</span><br></pre></td></tr></table></figure><p>进入容器里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -ti --privileged --rm -v $PWD:/go/src/github.com/opencontainers/runc runc_dev:HEAD bash</span><br></pre></td></tr></table></figure><p>编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make BUILDTAGS=<span class="string">&quot;seccomp selinux apparmor nokmem&quot;</span> static</span></span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o runc .</span><br><span class="line">CGO_ENABLED=1 go build  -tags &quot;seccomp selinux apparmor nokmem netgo osusergo&quot; -installsuffix netgo -ldflags &quot;-w -extldflags -static -X main.gitCommit=&quot;dc9208a3303feef5b3839f4323d9beb36df0a9dd&quot; -X main.version=1.0.0-rc10 &quot; -o contrib/cmd/recvtty/recvtty ./contrib/cmd/recvtty</span><br></pre></td></tr></table></figure><p>查看信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chmod u+x runc</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ldd runc</span></span><br><span class="line">not a dynamic executable</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./runc --version</span></span><br><span class="line">runc version 1.0.0-rc10</span><br><span class="line">commit: dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line">spec: 1.0.1-dev</span><br></pre></td></tr></table></figure><h2 id="查看-kmem-开启"><a href="#查看-kmem-开启" class="headerlink" title="查看 kmem 开启"></a>查看 kmem 开启</h2><p>环境信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux 82.174-zh 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br><span class="line">$ cat &#x2F;etc&#x2F;redhat-release </span><br><span class="line">CentOS Linux release 7.4.1708 (Core)</span><br></pre></td></tr></table></figure><p>判断 cgroup kernel memory 是否激活的方式。查看对应 POD container 下的 <code>memory.kmem.slabinfo</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /sys/fs/cgroup/memory/kubepods</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 有内容，说明kubelet开了 kmem</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat memory.kmem.slabinfo</span></span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br></pre></td></tr></table></figure><p>pod 目录下面的容器目录或者<code>/sys/fs/cgroup/memory/docker/&lt;uuid&gt;</code>如果有 <code>memory.kmem.slabinfo</code> 则说明 <code>runc</code> 没关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -l | grep pod</span></span><br><span class="line">drwxr-xr-x  5 root root 0 4月   8 11:27 pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28</span><br><span class="line">drwxr-xr-x  4 root root 0 4月   8 11:26 pod64def35b-b44e-410d-9782-745bd47834ca</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/ | grep -E <span class="string">&#x27;^d&#x27;</span></span></span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:28 1b37f9f78f93546e3e4407f03aa84c92e95c99655467e62814ae17e0a0e68686</span><br><span class="line">drwxr-xr-x 2 root root 0 4月   8 11:27 a578004f702d7b20d4b08d49c08cbb6c3ef2b3d08a62f087f5c7be0d022d9d9d</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat pod1f1bdb40-defe-44ad-9138-14f2dbcf3b28/0c97468753e9933793457e90e9964e9ef6493daae048eb0841bae634e6d5d326/memory.kmem.slabinfo</span> </span><br><span class="line">slabinfo - version: 2.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;<span class="built_in">limit</span>&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span></span><br><span class="line">taskstats              0      0    328   24    2 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">shmem_inode_cache    216    216    680   24    4 : tunables    0    0    0 : slabdata      9      9      0</span><br><span class="line">inode_cache          162    162    592   27    4 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">Acpi-ParseExt        112    112     72   56    1 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">selinux_inode_security      0      0     40  102    1 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">RAWv6                  0      0   1216   26    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">UDP                    0      0   1088   30    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-8192           0      0   8192    4    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">net_namespace          0      0   5184    6    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">pid_namespace          0      0   2200   14    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">mqueue_inode_cache      0      0    896   36    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">kmalloc-2048         112    112   2048   16    8 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-32           768    768     32  128    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">kmalloc-512          128    128    512   32    4 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">kmalloc-128          256    256    128   32    1 : tunables    0    0    0 : slabdata      8      8      0</span><br><span class="line">kmalloc-8           6144   6144      8  512    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">anon_vma             306    306     80   51    1 : tunables    0    0    0 : slabdata      6      6      0</span><br><span class="line">idr_layer_cache      165    165   2112   15    8 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">vm_area_struct       259    259    216   37    2 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">mnt_cache            231    231    384   21    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">mm_struct             20     20   1600   20    8 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">signal_cache           0      0   1152   28    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">sighand_cache          0      0   2112   15    8 : tunables    0    0    0 : slabdata      0      0      0</span><br><span class="line">files_cache           50     50    640   25    4 : tunables    0    0    0 : slabdata      2      2      0</span><br><span class="line">kernfs_node_cache    108    108    112   36    1 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-192          273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">task_xstate          156    156    832   39    8 : tunables    0    0    0 : slabdata      4      4      0</span><br><span class="line">task_struct           24     24   4048    8    8 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">kmalloc-1024         160    160   1024   32    8 : tunables    0    0    0 : slabdata      5      5      0</span><br><span class="line">kmalloc-64           768    768     64   64    1 : tunables    0    0    0 : slabdata     12     12      0</span><br><span class="line">sock_inode_cache      75     75    640   25    4 : tunables    0    0    0 : slabdata      3      3      0</span><br><span class="line">proc_inode_cache     264    264    656   24    4 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">dentry               273    273    192   21    1 : tunables    0    0    0 : slabdata     13     13      0</span><br><span class="line">kmalloc-16          2816   2816     16  256    1 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">kmalloc-96           294    294     96   42    1 : tunables    0    0    0 : slabdata      7      7      0</span><br><span class="line">kmalloc-256          352    352    256   32    2 : tunables    0    0    0 : slabdata     11     11      0</span><br><span class="line">shared_policy_node     85     85     48   85    1 : tunables    0    0    0 : slabdata      1      1      0</span><br><span class="line">kmalloc-4096         104    104   4096    8    8 : tunables    0    0    0 : slabdata     13     13      0</span><br></pre></td></tr></table></figure><p><code>memory.kmem.slabinfo</code>里有内容说明是开启的</p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>只有在 pod 配置了 memory limit 的时候才打开 memory accounting，即 kmem。我们下面利用 flannel pod测试下，先手动创建 cgroup</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ grep memory &#x2F;proc&#x2F;cgroups </span><br><span class="line">memory8871</span><br><span class="line"></span><br><span class="line">$ mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;test</span><br><span class="line">$ for i in &#96;seq 1 65535&#96;;do mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;test&#x2F;test-$&#123;i&#125;; done</span><br><span class="line">$ grep memory &#x2F;proc&#x2F;cgroups </span><br><span class="line">memory8655131</span><br></pre></td></tr></table></figure><p>释放出三个，删除当前节点的 flannel，可以创建出来，然后再删除新的，无法创建出来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rmdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;test&#x2F;test-&#123;1..3&#125;</span><br><span class="line">$ kubectl -n kube-system delete pod kube-flannel-ds-z2cgq</span><br><span class="line">....</span><br><span class="line">  Warning  FailedCreatePodContainer  2s (x4 over 35s)  kubelet, 10.13.82.174  unable to ensure pod container exists: failed to create container for [kubepods burstable pod5a41f53f-5ce8-4123-8199-1a865219f297] : mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;kubepods&#x2F;burstable&#x2F;pod5a41f53f-5ce8-4123-8199-1a865219f297: no space left on device</span><br></pre></td></tr></table></figure><p>替换编译好的后，先关闭 <code>kubelet</code> 和 <code>docker</code>， 关闭自启动 <code>systemctl disable docker kubelet</code>。reboot 后，查看目录 <code>/sys/fs/cgroup/memory/</code> 下的 <code>kubepods</code> 是不是不存在。然后启动 docker 和 kubelet。等带内存 limit 的 flannel 调度上来后下面命令查看。输出是 <code>Input/output error</code> 说明已经关闭了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;kubepods&#x2F; -name memory.kmem.slabinfo -exec cat &#123;&#125;  \;</span><br></pre></td></tr></table></figure><p>还有种方法是看 slab 的个数，删除 limit 的 pod后等重建看看数量增长否：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls &#x2F;sys&#x2F;kernel&#x2F;slab  | wc -l</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://eddycjy.com/posts/why-container-memory-exceed2/">https://eddycjy.com/posts/why-container-memory-exceed2/</a></li><li><a href="https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub:+unable+to+allocate+memory+on+node+-1">https://cloud.tencent.com/developer/article/1739289?from=information.detail.slub%3A+unable+to+allocate+memory+on+node+-1</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前提详情&quot;&gt;&lt;a href=&quot;#前提详情&quot; class=&quot;headerlink&quot; title=&quot;前提详情&quot;&gt;&lt;/a&gt;前提详情&lt;/h2&gt;&lt;p&gt;在 3.x 的内核上，cgroup 的 kmem account 特性有内存泄露问题。kubelet 和 runc 都需要修</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="kmem" scheme="http://zhangguanzhang.github.io/categories/k8s/kmem/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>iptables --wait -t nat -A DOCKER...: iptables NO chain/target/match by that name</title>
    <link href="http://zhangguanzhang.github.io/2021/03/23/iptables-docker-no-chain/"/>
    <id>http://zhangguanzhang.github.io/2021/03/23/iptables-docker-no-chain/</id>
    <published>2021-03-23T17:42:08.000Z</published>
    <updated>2021-03-23T17:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>我们内部有套部署的工具， 我们部署的流程是先在部署机器（部署机器可能也是node1 ）上用脚本安装好 docker，然后进容器里去起我们部署平台，有个很久的 bug 就是，部署机器上端口映射起容器会有如下报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables --wait -t nat -A DOCKER -p tcp -d 0&#x2F;8 --dport 8089 -j DNAT --to-destination 172.25.0.2:80 ! -i docker0: iptables NO chain&#x2F;target&#x2F;match by that name</span><br></pre></td></tr></table></figure><p>排查也很简单，缺少链，添加上即可:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables --wait -N DOCKER &amp;&gt;/dev/null || true</span><br><span class="line">sudo iptables --wait -t filter -N DOCKER &amp;&gt;/dev/null || true</span><br><span class="line">sudo iptables --wait -t nat  -N DOCKER &amp;&gt;/dev/null || true</span><br></pre></td></tr></table></figure><p>或者重启 docker daemon 会在启动的时候加上链。脚本安装 docker 的时候，我们执行了下面的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables --wait -P INPUT ACCEPT &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t nat &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t nat &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t raw &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t raw &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -F -t mangle &amp;&amp; \</span><br><span class="line">    sudo iptables --wait -X -t mangle</span><br></pre></td></tr></table></figure><p>所以一开始是在安装 docker 前加的三个链，后面发现还是会出现。今天测试内部环境测试的时候百分之百复现了。</p><h2 id="引起原因"><a href="#引起原因" class="headerlink" title="引起原因"></a>引起原因</h2><p>和同事排查了下，确认是部署脚本里没有停止 firewalld ，后面执行 ansible 剧本的时候去停止的。整个流程是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">部署机器 ----安装docker----&gt; ansible 剧本 -----停止firewalld-----&gt; 检测到已经安装 docker 不操作</span><br><span class="line">没运行部署平台的机器    ----&gt; ansible 剧本 -----停止firewalld-----&gt; 安装 docker 并启动 docker daemon </span><br></pre></td></tr></table></figure><p>部署机器是先启动 docker daemon，然后后面剧本停止了 firewalld，而非部署机器是先停的 firewalld，再启动的 docker daemon。</p><p>手动测试了下发现停止 firewalld 会把 iptables 清空。于是部署脚本里提前把所有系统的 case 逻辑里把 firewalld 给停掉。</p><p>另外我们防止有些用户安装了 iptables 的 daemon，是这样停止的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld python-firewall firewalld-filesystem iptables &amp;&gt;&#x2F;dev&#x2F;null</span><br><span class="line">systemctl disable firewalld python-firewall firewalld-filesystem iptables &amp;&gt;&#x2F;dev&#x2F;null</span><br></pre></td></tr></table></figure><p>发现这样执行后 firewalld 还是 enabled 的状态，如果有不存在的 service （例如机器存在firewalld，后面几个都不存在）则整个 disable 会失效（全部没有 disable 成功）。改成了下面的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function Systemctl_Stop_Disable()&#123;</span><br><span class="line">    for target in $@;</span><br><span class="line">    do</span><br><span class="line">        sudo systemctl stop $target &amp;&gt;&#x2F;dev&#x2F;null || true;</span><br><span class="line">        sudo systemctl disable $target &amp;&gt;&#x2F;dev&#x2F;null || true;</span><br><span class="line">    done</span><br><span class="line">    true</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Systemctl_Stop_Disable firewalld python-firewall firewalld-filesystem iptables</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;我们内部有套部署的工具， 我们部署的流程是先在部署机器（部署机器可能也是node1 ）上用脚本安装好 docker，然后进容器里去起我们部署</summary>
      
    
    
    
    <category term="iptables" scheme="http://zhangguanzhang.github.io/categories/iptables/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Internal error occurred: jsonpatch add operation does not apply: doc is missing path: xxx</title>
    <link href="http://zhangguanzhang.github.io/2021/03/22/jsonpatch-doc-is-missing-path/"/>
    <id>http://zhangguanzhang.github.io/2021/03/22/jsonpatch-doc-is-missing-path/</id>
    <published>2021-03-22T20:42:08.000Z</published>
    <updated>2021-03-22T20:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>今天在折腾 admission webhook 注入一些属性的时候遇到了 <code>Error from server (InternalError): error when creating &quot;xxx.yml&quot;: Internal error occurred: jsonpatch add operation does not apply: doc is missing path: &quot;/spec/template/spec/dnsConfig/options&quot;</code>。折腾半天才发现在代码里使用 jsonPatch 的话不能直接绕过结构体实例去 patch。 </p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>WebHook 接收和响应都是一个 AdmissionReview 对象，请求是里面的 AdmissionRequest，响应是里面的 AdmissionResponse。<br>比如说我们创建了个 <code>MutatingWebhookConfiguration</code> 指定让 <code>apps/v1</code> 的 <code>deploy</code> 传到我们的 webhook， 我们要修改 deploy 的一些属性，我个人是增加 dns 的 <code>single-request-reopen</code>的属性的，得在 <code>AdmissionResponse</code> 里传一个 jsonPatch的切片。<br>代码里这块是：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> patchOperation <span class="keyword">struct</span> &#123;</span><br><span class="line">Op    <span class="keyword">string</span>      <span class="string">`json:&quot;op&quot;`</span></span><br><span class="line">Path  <span class="keyword">string</span>      <span class="string">`json:&quot;path&quot;`</span></span><br><span class="line">Value <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:&quot;value,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">patch := []patchOperation&#123;</span><br><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/dnsConfig/options&quot;</span>,</span><br><span class="line">Value: []<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span>&#123;</span><br><span class="line">&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;single-request-reopen&quot;</span>&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>后面发现创建 deploy 就报开头的错误。然后日志里打印了下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apiVersion:apps&#x2F;v1 resource: default&#x2F;nginx-deployment, AdmissionResponse: patch&#x3D;[&#123;&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;&#x2F;spec&#x2F;template&#x2F;spec&#x2F;dnsConfig&#x2F;options&quot;,&quot;value&quot;:[&#123;&quot;name&quot;:&quot;single-request-reopen&quot;&#125;]&#125;]</span><br></pre></td></tr></table></figure><p>然后把这个 patch 在 kubectl 上测试了下是可以的:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system patch deployments tiller-deploy --<span class="built_in">type</span>=json -p=<span class="string">&#x27;[&#123;&quot;op&quot;:&quot;add&quot;,&quot;path&quot;:&quot;/spec/template/spec/dnsConfig/options&quot;,&quot;value&quot;:[&#123;&quot;name&quot;:&quot;single-request-reopen&quot;&#125;]&#125;]&#x27;</span></span></span><br><span class="line"></span><br><span class="line">deployment.extensions/tiller-deploy patched</span><br></pre></td></tr></table></figure><p>刚开始把 kube-apiserver 开 -v=8 发现 panic的信息，然后升了下版本还是没用。然后在源码里找了下这个报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> find -<span class="built_in">type</span> f -name <span class="string">&#x27;*.go&#x27;</span> -<span class="built_in">exec</span> grep -l <span class="string">&#x27;replace operation does not apply: doc is missing path&#x27;</span> &#123;&#125; \;</span></span><br><span class="line">./vendor/github.com/evanphx/json-patch/patch.go</span><br></pre></td></tr></table></figure><p>发现这个错误是引入的库抛出来的，和 k8s 无关，想了下后换个 key 试试看。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/securityContext/runAsNonRoot&quot;</span>,</span><br><span class="line">Value: <span class="literal">true</span>,</span><br><span class="line">&#125;,</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> deployment.Spec.Template.Spec.Containers &#123;</span><br><span class="line">patch = <span class="built_in">append</span>(patch, patchOperation&#123;</span><br><span class="line">Op:    <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path:  fmt.Sprintf(<span class="string">&quot;/spec/template/spec/containers/%d/lifecycle/postStart/exec/command&quot;</span>, i),</span><br><span class="line">Value: []<span class="keyword">string</span>&#123;</span><br><span class="line"><span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line"><span class="string">&quot;-c&quot;</span>,</span><br><span class="line"><span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现后面 <code>command</code> 这个也不行，explain 看了下，试试上层的看看：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> deployment.Spec.Template.Spec.Containers &#123;</span><br><span class="line">patch = <span class="built_in">append</span>(patch, patchOperation&#123;</span><br><span class="line">Op:    <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path:  fmt.Sprintf(<span class="string">&quot;/spec/template/spec/containers/%d/lifecycle&quot;</span>, i),</span><br><span class="line">Value: corev1.Lifecycle&#123;</span><br><span class="line">PostStart: &amp;corev1.Handler&#123;</span><br><span class="line">Exec: &amp;corev1.ExecAction&#123;</span><br><span class="line">Command: []<span class="keyword">string</span>&#123;</span><br><span class="line"><span class="string">&quot;/bin/sh&quot;</span>,</span><br><span class="line"><span class="string">&quot;-c&quot;</span>,</span><br><span class="line"><span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现可以，应该是得给一个嵌套的结构体实例，而不是直接绕过这个结构体，去 <code>patch</code> 里面属性的 <code>value</code>，换下前面的 dnsConfig 试试:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">Op:   <span class="string">&quot;add&quot;</span>,</span><br><span class="line">Path: <span class="string">&quot;/spec/template/spec/dnsConfig&quot;</span>,</span><br><span class="line">Value: corev1.PodDNSConfig&#123;</span><br><span class="line">Options:     []corev1.PodDNSConfigOption&#123;</span><br><span class="line">&#123;</span><br><span class="line">Name:  <span class="string">&quot;single-request-reopen&quot;</span>,</span><br><span class="line">Value: <span class="literal">nil</span>,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><p>发现也可以了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook">https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;今天在折腾 admission webhook 注入一些属性的时候遇到了 &lt;code&gt;Error from server (Internal</summary>
      
    
    
    
    <category term="admission" scheme="http://zhangguanzhang.github.io/categories/admission/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>使用github action 配合 docker buildx 编译 arm64 docker-compose</title>
    <link href="http://zhangguanzhang.github.io/2021/03/12/build-arm64-docker-compose-action/"/>
    <id>http://zhangguanzhang.github.io/2021/03/12/build-arm64-docker-compose-action/</id>
    <published>2021-03-12T19:28:30.000Z</published>
    <updated>2021-03-12T19:28:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>git 上搜索了很多 docker-compose 的 arm64 的编译基本都是使用 <code>qemu-user-static</code> 之类的设置下后编译的，也看到过用特权容器启动 qemu-user-static 或者 <code>binfmt</code> 之类的，但是我自己机器上试了无效，貌似是因为我操作系统是低版本内核的 centos ，github 上搜了下，其他很多人的编译感觉太啰嗦了。就在 action 上整了下，测试是可用的，而且非常简单。</p><p><code>docker-practice/actions-setup-docker@master</code> 将会在在 action 的 runner 里安装 docker，创建 buildx 和 运行 <code>docker run --rm --privileged ghcr.io/dpsigs/tonistiigi-binfmt:latest --install all</code> 。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">build</span> <span class="string">for</span> <span class="string">docker-compose</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span> [ <span class="string">master</span> ]</span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">build:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># - name: Check out docker/compose</span></span><br><span class="line">      <span class="comment">#   uses: actions/checkout@v2</span></span><br><span class="line">      <span class="comment">#   with:</span></span><br><span class="line">      <span class="comment">#     repository: docker/compose</span></span><br><span class="line">      <span class="comment">#     path: compose</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">install</span> <span class="string">docker</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">docker-practice/actions-setup-docker@master</span></span><br><span class="line">        <span class="comment"># this will run and buildx</span></span><br><span class="line">        <span class="comment"># docker run --rm --privileged ghcr.io/dpsigs/tonistiigi-binfmt:latest --install all</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build</span> <span class="string">docker-compose</span> </span><br><span class="line">        <span class="attr">id:</span> <span class="string">build</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line">          <span class="string">ls</span> <span class="string">-l;</span> <span class="string">docker</span> <span class="string">run</span> <span class="string">--rm</span> <span class="string">arm64v8/python:3.7.10-stretch</span> <span class="string">sh</span> <span class="string">-c</span> <span class="string">&#x27;python3 -V; uname -m&#x27;</span></span><br><span class="line">          <span class="comment"># https://github.com/docker/compose/blob/master/script/build/linux</span></span><br><span class="line">          <span class="string">git</span> <span class="string">clone</span> <span class="string">https://github.com/docker/compose.git</span></span><br><span class="line">          <span class="string">cd</span> <span class="string">compose;</span></span><br><span class="line">          <span class="string">./script/clean;</span></span><br><span class="line">          <span class="string">git</span> <span class="string">checkout</span> <span class="number">1.28</span><span class="number">.5</span></span><br><span class="line">          <span class="string">DOCKER_COMPOSE_GITSHA=&quot;$(script/build/write-git-sha)&quot;;</span></span><br><span class="line">          <span class="string">echo</span> <span class="string">----</span> <span class="string">$&#123;DOCKER_COMPOSE_GITSHA&#125;</span></span><br><span class="line">          <span class="string">docker</span> <span class="string">buildx</span> <span class="string">build</span> <span class="string">--platform</span> <span class="string">linux/arm64</span> <span class="string">.</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--target</span> <span class="string">bin</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--build-arg</span> <span class="string">DISTRO=debian</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--build-arg</span> <span class="string">GIT_COMMIT=&quot;$&#123;DOCKER_COMPOSE_GITSHA&#125;&quot;</span> <span class="string">\</span></span><br><span class="line">          <span class="string">--output</span> <span class="string">dist/</span> <span class="string">||</span> <span class="string">:</span> <span class="string">;</span></span><br><span class="line">          <span class="string">ls</span> <span class="string">-l</span> <span class="string">dist;</span></span><br><span class="line">          <span class="string">docker</span> <span class="string">run</span> <span class="string">--platform</span> <span class="string">linux/arm64</span> <span class="string">\</span></span><br><span class="line">            <span class="string">--rm</span> <span class="string">-v</span> <span class="string">$PWD/dist:/root/</span> <span class="string">\</span></span><br><span class="line">            <span class="string">arm64v8/python:3.7.10-stretch</span> <span class="string">/root/docker-compose-linux-arm64</span> <span class="string">version;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Upload</span> <span class="string">bin</span> <span class="string">directory</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/upload-artifact@main</span></span><br><span class="line">        <span class="attr">if:</span> <span class="string">steps.build.outcome</span> <span class="string">==</span> <span class="string">&#x27;success&#x27;</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">docker-compose-linux</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">compose/dist/</span></span><br></pre></td></tr></table></figure><p>编译过程看 compose 仓库的 makefile，是运行的 <a href="https://github.com/docker/compose/blob/master/script/build/linux">https://github.com/docker/compose/blob/master/script/build/linux</a> 这个脚本。所以克隆 compose 仓库后进目录里，然后 checkout 指定 tag。官方的编译过程都是在 docker build 产生的容器里去编译的。最后有个 build –output就是直接把文件给整出来。我这里是用的 buildx 去替代 build 编译。理论上也可以编译其他架构的，我仓库已经自动化处理这个过程了。 <a href="https://github.com/zhangguanzhang/docker-compose-aarch64/releases%E3%80%82">https://github.com/zhangguanzhang/docker-compose-aarch64/releases。</a></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>银河麒麟 v10 系统，架构 arm64</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ arch</span><br><span class="line">aarch64</span><br><span class="line">$ cat &#x2F;etc&#x2F;os-release </span><br><span class="line">NAME&#x3D;&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION&#x3D;&quot;V10 (Tercel)&quot;</span><br><span class="line">ID&#x3D;&quot;kylin&quot;</span><br><span class="line">VERSION_ID&#x3D;&quot;V10&quot;</span><br><span class="line">PRETTY_NAME&#x3D;&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR&#x3D;&quot;0;31&quot;</span><br></pre></td></tr></table></figure><p>docker 版本信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">$ docker info</span><br><span class="line">Containers: 63</span><br><span class="line"> Running: 44</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 19</span><br><span class="line">Images: 24</span><br><span class="line">Server Version: 18.09.9</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line">runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.19.90-17.ky10.aarch64</span><br><span class="line">Operating System: Kylin Linux Advanced Server V10 (Tercel)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: aarch64</span><br><span class="line">CPUs: 64</span><br><span class="line">Total Memory: 62.76GiB</span><br><span class="line">Name: reg.xxx.lan</span><br><span class="line">ID: RI24:C6CM:WELZ:MQEJ:N5OY:IR74:OQPG:XV72:SFRI:NUSK:DS44:OQNQ</span><br><span class="line">Docker Root Dir: &#x2F;data&#x2F;kube&#x2F;docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> 127.0.0.0&#x2F;8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https:&#x2F;&#x2F;registry.docker-cn.com&#x2F;</span><br><span class="line"> https:&#x2F;&#x2F;docker.mirrors.ustc.edu.cn&#x2F;</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><h3 id="测试运行"><a href="#测试运行" class="headerlink" title="测试运行"></a>测试运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">$ ldd .&#x2F;docker-compose-linux-arm64 </span><br><span class="line">linux-vdso.so.1 (0x0000fffd72210000)</span><br><span class="line">libdl.so.2 &#x3D;&gt; &#x2F;lib64&#x2F;libdl.so.2 (0x0000fffd721a0000)</span><br><span class="line">libz.so.1 &#x3D;&gt; &#x2F;lib64&#x2F;libz.so.1 (0x0000fffd72160000)</span><br><span class="line">libc.so.6 &#x3D;&gt; &#x2F;lib64&#x2F;libc.so.6 (0x0000fffd71fd0000)</span><br><span class="line">&#x2F;lib&#x2F;ld-linux-aarch64.so.1 (0x0000fffd72220000)</span><br><span class="line">$ ll</span><br><span class="line">总用量 10504</span><br><span class="line">drwxr-xr-x 2 root root       26  3月 13 11:11 conf.d</span><br><span class="line">-rwxr-xr-x 1 root root 10750256  3月 12 13:15 docker-compose-linux-arm64</span><br><span class="line">-rw-r--r-- 1 root root      389  3月 13 11:11 docker-compose.yml</span><br><span class="line">drwxr-xr-x 2 root root        6  3月 13 11:11 down</span><br><span class="line">$ cat conf.d&#x2F;default.conf </span><br><span class="line">server &#123;</span><br><span class="line">    listen       81;</span><br><span class="line">    server_name  localhost;</span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">        autoindex    on;</span><br><span class="line">    &#125;</span><br><span class="line">    error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">    location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">$ cat docker-compose.yml </span><br><span class="line">version: &#39;3.4&#39;</span><br><span class="line">services:</span><br><span class="line">  nginx:</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    container_name: install-nginx</span><br><span class="line">    hostname: install-nginx</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai:&#x2F;etc&#x2F;localtime:ro</span><br><span class="line">      - .&#x2F;down:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</span><br><span class="line">      - .&#x2F;conf.d&#x2F;:&#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;</span><br><span class="line">    network_mode: &quot;host&quot;</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#39;3&#39;</span><br><span class="line">        max-size: 100m</span><br><span class="line">$ mkdir conf.d</span><br><span class="line"></span><br><span class="line">$ .&#x2F;docker-compose-linux-arm64 up -d</span><br><span class="line">Pulling nginx (nginx:alpine)...</span><br><span class="line">alpine: Pulling from library&#x2F;nginx</span><br><span class="line">Digest: sha256:c2ce58e024275728b00a554ac25628af25c54782865b3487b11c21cafb7fabda</span><br><span class="line">Status: Downloaded newer image for nginx:alpine</span><br><span class="line">Creating install-nginx ... done</span><br><span class="line">$.&#x2F;docker-compose-linux-arm64 ps -a</span><br><span class="line">    Name                   Command               State   Ports</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">install-nginx   &#x2F;docker-entrypoint.sh ngin ...   Up           </span><br><span class="line">$ netstat -nlptu | grep -E &#39;:81\s&#39;</span><br><span class="line">tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      4093364&#x2F;nginx: mast </span><br></pre></td></tr></table></figure><p>页面访问了下正常，清理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;docker-compose-linux-arm64 down</span><br><span class="line">Stopping install-nginx ... done</span><br><span class="line">Removing install-nginx ... done</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://github.com/ubiquiti/docker-compose-aarch64">https://github.com/ubiquiti/docker-compose-aarch64</a></li><li><a href="https://github.com/RogerLaw/docker-compose-aarch64">https://github.com/RogerLaw/docker-compose-aarch64</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;p&gt;git 上搜索了很多 docker-compose 的 arm64 的编译基本都是使用 &lt;code&gt;qemu-user-static&lt;/co</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>集群节点关机导致dns在eviction pod之前几率不可用</title>
    <link href="http://zhangguanzhang.github.io/2021/02/02/node-shutdown-dns-unavailable/"/>
    <id>http://zhangguanzhang.github.io/2021/02/02/node-shutdown-dns-unavailable/</id>
    <published>2021-02-02T15:42:08.000Z</published>
    <updated>2021-02-02T15:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这几天我们内部在做新项目的容灾测试，业务都是在 K8S 上的。容灾里就是随便选节点 <code>shutdown -h now</code>。关机后同事便发现了（页面有错误，最终问题是）集群内 DNS 解析会有几率无法解析（导致的）。</p><p>根据 <code>SVC</code> 的流程，node 关机后，由于 kubelet 没有 update 自己。node 和 pod 在 apiserver get 的时候显示还是正常的。在 <code>kube-controller-manager</code> 的 <code>--node-monitor-grace-period</code> 时间后再过 <code>--pod-eviction-timeout</code> 时间开始 <code>eviction pod</code>，大概流程是这样。</p><p>在 <code>pod</code> 被 <code>eviction</code> 之前，默认是大概 <code>5m</code> 的时间。这段时间内，<code>node</code> 上 的所有 <code>POD</code> 的 <code>IP</code> 还在 <code>SVC</code> 的 <code>endpoint</code> 里。而同事关机的 <code>node</code> 上恰好有 <code>coredns</code> 。所以在 5m 内一直会有 coredns 副本数之一的几率解析失败。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>其实和 K8S 版本没关系，因为 <code>SVC</code> 和 <code>eviction</code> 的行为都是这样的。实际我调整了 <code>node</code> 更新自身状态的所有 <a href="https://github.com/zhangguanzhang/Kubernetes-ansible/wiki/nodeStatusUpdate">相关参数</a>，调整到在 20s 内就会 <code>eviction pod</code>，但是 20s 内还是存在几率无法解析。当然也问了下群友和社区群里，发现似乎大家从来没关机测试过这方面，应该是现在大伙都在用公有云了。。。。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">  kubectl  version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:16:51Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.5&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;20c265fef0741dd71a66480e35bd69f18351daea&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2019-10-15T19:07:57Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.10&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/amd64&quot;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><h3 id="loca-dns-真的可以吗"><a href="#loca-dns-真的可以吗" class="headerlink" title="loca-dns 真的可以吗"></a>loca-dns 真的可以吗</h3><p>当然首选是 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns">local-dns 的方案</a> 了。方案搜下，很多人介绍了。简单讲下就是在每个 node 上起 <code>hostNetwork</code> 的 <code>node-cache</code> 进程做代理 ，然后利用 <code>dummy</code> 接口和 nat 来拦截发向 kube-dns <code>SVC</code> IP 的 dns 请求做缓存。</p><p><a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml">官方提供的 yaml 文件</a> 里的 <code>__PILLAR__LOCAL__DNS__,__PILLAR__DNS__SERVER__</code>需要换成<code>dummy</code>接口 IP 和 kube-dns <code>SVC</code> 的 IP，还有 <code>__PILLAR__DNS__DOMAIN__</code> 自行根据文档更换。其余几个变量会在启动的时候替换，可以启动后看日志。</p><p>然后实际测试了下还是有问题。然后捋了下流程，yaml 文件里有这个 <code>SVC</code> 和 node-cache 的启动参数</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns-upstream</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">53</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">53</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line"><span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"> <span class="attr">args:</span> [ <span class="string">...</span>, <span class="string">&quot;-upstreamsvc&quot;</span>, <span class="string">&quot;kube-dns-upstream&quot;</span> ]</span><br></pre></td></tr></table></figure><p>启动的日志里可以看到配置文件被渲染了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">cluster1.local:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    reload</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">        force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    health 169.254.20.10:8080</span><br><span class="line">&#125;</span><br><span class="line">in-addr.arpa:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    &#125;</span><br><span class="line">ip6.arpa:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 172.26.189.136 &#123;</span><br><span class="line">            force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    &#125;</span><br><span class="line">.:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">    prometheus :9253</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>因为要 nat 去 hook 请求 kube-dns <code>SVC</code> IP（172.26.0.2）的请求，但是它自己也需要访问 kube-dns，所以 yaml 文件里创建了一个和 kube-dns 一样的属性的 svc，启动参数写了这个 SVC 名字，可以看到它代理的是走 <code>SVC</code> 的 ip 的。因为 <a href="https://kubernetes.io/zh/docs/concepts/services-networking/connect-applications-service/#%E8%AE%BF%E9%97%AE-service">enableServiceLinks</a> 的默认开启，pod 会有如下环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> dfa env | grep KUBE_DNS_UPSTREAM_SERVICE_HOST</span></span><br><span class="line">KUBE_DNS_UPSTREAM_SERVICE_HOST=172.26.189.136</span><br></pre></td></tr></table></figure><p><a href="https://github.com/kubernetes/dns/blob/2b7b66c7824a7dd68d38568d913228e8d3d4c8c2/cmd/node-cache/app/cache_app.go#L306">代码里</a> 可以看到就是把参数的 <code>-</code> 转换成 <code>_</code> 取值然后渲染配置文件，这样就能取到 SVC 的 IP 了。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">toSvcEnv</span><span class="params">(svcName <span class="keyword">string</span>)</span> <span class="title">string</span></span> &#123;</span><br><span class="line">envName := strings.Replace(svcName, <span class="string">&quot;-&quot;</span>, <span class="string">&quot;_&quot;</span>, <span class="number">-1</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="string">&quot;$&quot;</span> + strings.ToUpper(envName) + <span class="string">&quot;_SERVICE_HOST&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>cluster1.local:53</code> 这个 zone 在默认配置下还是代理到 <code>SVC</code> 上，所以还是有问题。</p><p>所以只有绕过 <code>SVC</code> 才能从根本上解决这个问题。然后就把 <code>coredns</code> 改成 port 153 + <code>hostNetwork: true</code> 加 <code>nodeSelector</code> 到三个 master 上固定了。然后配置文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cluster1.local:53 &#123;</span><br><span class="line">    errors</span><br><span class="line">    reload</span><br><span class="line">    bind 169.254.20.10 172.26.0.2</span><br><span class="line">    forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">        force_tcp</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus :9253</span><br><span class="line">    health 169.254.20.10:8080</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>然后测试还是有几率无法访问。之前看到过 <a href="https://fuckcloudnative.io/">米开朗基杨</a> 分享过 <code>coredns</code> 的一个带故障转移的插件 <a href="https://github.com/leiless/dnsredir">dnsredir</a>，尝试加这个插件去编译。</p><p>查阅文档编译后最后运行起来无法识别配置文件，因为官方不是直接基于 <code>coredns</code> 引入自己的插件开发的，而是自己的代码上来引入 <code>coredns</code> 的内置插件。</p><p>大概过程详情 issue 见链接 <a href="https://github.com/kubernetes/dns/issues/436">include coredns plugin at node-cache don’t work expect</a></p><p>官方的这个 node-cace 里的 bind 插件就是 <code>dummy</code>接口和 iptables 的 nat 部分了，这个特性蛮吸引我的，决定继续尝试下这个看看能不能配置成功。</p><h3 id="意外收获"><a href="#意外收获" class="headerlink" title="意外收获"></a>意外收获</h3><p>在测试加入插件 dnsredir 的时候米开朗基杨叫我试下最小配置段看看有干扰没，尝试了下面的配置段来回切换测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  Corefile: |</span><br><span class="line">    cluster1.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        reload</span><br><span class="line">        dnsredir . &#123;</span><br><span class="line">            to 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153</span><br><span class="line">            max_fails 1</span><br><span class="line">            health_check 1s</span><br><span class="line">            spray</span><br><span class="line">        &#125;</span><br><span class="line">        #forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">        #    max_fails 1</span><br><span class="line">        #    policy round_robin</span><br><span class="line">        #    health_check 0.4s</span><br><span class="line">        #&#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">    &#125;</span><br><span class="line">#----------</span><br><span class="line">  Corefile: |</span><br><span class="line">    cluster1.local:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        reload</span><br><span class="line">        #dnsredir . &#123;</span><br><span class="line">        #    to 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153</span><br><span class="line">        #    max_fails 1</span><br><span class="line">        #    health_check 1s</span><br><span class="line">        #    spray</span><br><span class="line">        #&#125;</span><br><span class="line">        forward . 10.11.86.107:153 10.11.86.108:153 10.11.86.109:153 &#123;</span><br><span class="line">            max_fails 1</span><br><span class="line">            policy round_robin</span><br><span class="line">            health_check 0.4s</span><br><span class="line">        &#125;</span><br><span class="line">        prometheus :9253</span><br><span class="line">        health 169.254.20.10:8080</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>然后发现请求居然不会发生解析失败了：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">function</span> <span class="function"><span class="title">d</span></span>()&#123; <span class="keyword">while</span> :;<span class="keyword">do</span> sleep 0.2; date;dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short; <span class="keyword">done</span>; &#125;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> d</span></span><br><span class="line">2021年 02月 02日 星期二 12:54:43 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:44 CST  &lt;---这个时间点关机了一个 master </span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:45 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:47 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:48 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:51 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:51 CST</span><br><span class="line">172.26.158.130</span><br><span class="line">2021年 02月 02日 星期二 12:54:52 CST</span><br><span class="line">172.26.158.130</span><br></pre></td></tr></table></figure><p>然后就不打算继续折腾 dnsredir 插件了，去叫同事测试了下没问题，叫我在另一个环境上应用下修改他再测下，发现还是会发生：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @172.26.0.2 account-gateway +short</span><br><span class="line">; (1 server found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; connection timed out; no servers could be reached</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br></pre></td></tr></table></figure><p>然后我多次测试最小配置 zone，对比排查到是反向解析的问题，反向解析关闭了就不存在任何问题了，注释掉下面的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#in-addr.arpa:53 &#123;</span><br><span class="line">#    errors</span><br><span class="line">#    cache 30</span><br><span class="line">#    reload</span><br><span class="line">#    loop</span><br><span class="line">#    bind 169.254.20.10 172.26.0.2</span><br><span class="line">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">#            force_tcp</span><br><span class="line">#    &#125;</span><br><span class="line">#    prometheus :9253</span><br><span class="line">#    &#125;</span><br><span class="line">#ip6.arpa:53 &#123;</span><br><span class="line">#    errors</span><br><span class="line">#    cache 30</span><br><span class="line">#    reload</span><br><span class="line">#    loop</span><br><span class="line">#    bind 169.254.20.10 172.26.0.2</span><br><span class="line">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span><br><span class="line">#            force_tcp</span><br><span class="line">#    &#125;</span><br><span class="line">#    prometheus :9253</span><br><span class="line">#    &#125;</span><br></pre></td></tr></table></figure><p>测试解析的过程中去关机任何一台 coredns 所在 node 也没问题了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br><span class="line"><span class="meta">$</span><span class="bash"> dig @172.26.0.2 account-gateway.default.svc.cluster1.local +short</span></span><br><span class="line">172.26.158.124</span><br></pre></td></tr></table></figure><h3 id="大致的yaml文件"><a href="#大致的yaml文件" class="headerlink" title="大致的yaml文件"></a>大致的yaml文件</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns-upstream</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;KubeDNSUpstream&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.3</span> <span class="comment"># &lt;---- 给他固定了得了，可以直接这个ip不走node-cache作为测试</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">cluster1.local:53</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">cache</span> &#123;</span><br><span class="line">            <span class="string">success</span> <span class="number">9984 </span><span class="number">30</span></span><br><span class="line">            <span class="string">denial</span> <span class="number">9984 </span><span class="number">5</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">bind</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.107</span><span class="string">:153</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.108</span><span class="string">:153</span> <span class="number">10.11</span><span class="number">.86</span><span class="number">.109</span><span class="string">:153</span> &#123;</span><br><span class="line">            <span class="string">force_tcp</span></span><br><span class="line">            <span class="string">max_fails</span> <span class="number">1</span></span><br><span class="line">            <span class="string">policy</span> <span class="string">round_robin</span></span><br><span class="line">            <span class="string">health_check</span> <span class="number">0.</span><span class="string">5s</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9253</span></span><br><span class="line">        <span class="string">health</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span><span class="string">:8070</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#in-addr.arpa:53 &#123;</span></span><br><span class="line">    <span class="comment">#    errors</span></span><br><span class="line">    <span class="comment">#    cache 30</span></span><br><span class="line">    <span class="comment">#    reload</span></span><br><span class="line">    <span class="comment">#    loop</span></span><br><span class="line">    <span class="comment">#    bind 169.254.20.10 172.26.0.2</span></span><br><span class="line">    <span class="comment">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span></span><br><span class="line">    <span class="comment">#            force_tcp</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#    prometheus :9253</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#ip6.arpa:53 &#123;</span></span><br><span class="line">    <span class="comment">#    errors</span></span><br><span class="line">    <span class="comment">#    cache 30</span></span><br><span class="line">    <span class="comment">#    reload</span></span><br><span class="line">    <span class="comment">#    loop</span></span><br><span class="line">    <span class="comment">#    bind 169.254.20.10 172.26.0.2</span></span><br><span class="line">    <span class="comment">#    forward . __PILLAR__CLUSTER__DNS__ &#123;</span></span><br><span class="line">    <span class="comment">#            force_tcp</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="comment">#    prometheus :9253</span></span><br><span class="line">    <span class="comment">#    &#125;</span></span><br><span class="line">    <span class="string">.:53</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">bind</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="string">__PILLAR__UPSTREAM__SERVERS__</span></span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9253</span></span><br><span class="line">      &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9253&quot;</span></span><br><span class="line">        <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span>  <span class="comment"># Don&#x27;t use cluster DNS.</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">node-cache</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">xxx.lan:5000/k8s-dns-node-cache:1.16.0</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">args:</span> [ <span class="string">&quot;-localip&quot;</span>, <span class="string">&quot;169.254.20.10,172.26.0.2&quot;</span>, <span class="string">&quot;-conf&quot;</span>, <span class="string">&quot;/etc/Corefile&quot;</span>, <span class="string">&quot;-upstreamsvc&quot;</span>, <span class="string">&quot;kube-dns-upstream&quot;</span>, <span class="string">&quot;-health-port&quot;</span>,<span class="string">&quot;8070&quot;</span> ]</span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">53</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">53</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9253</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8070</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">40</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/coredns</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-dns-config</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/kube-dns</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">xtables-lock</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/run/xtables.lock</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">FileOrCreate</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-dns-config</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-dns</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">          <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">Corefile</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">Corefile.base</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># A headless service is a service with a service IP but instead of load-balancing it will return the IPs of our associated Pods.</span></span><br><span class="line"><span class="comment"># We use this to expose metrics to Prometheus.</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9253&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">9253</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">9253</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/bootstrapping:</span> <span class="string">rbac-defaults</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">services</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">namespaces</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/bootstrapping:</span> <span class="string">rbac-defaults</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:coredns</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">.:153</span> &#123;</span><br><span class="line">        <span class="string">errors</span></span><br><span class="line">        <span class="string">health</span> <span class="string">:8180</span></span><br><span class="line">        <span class="string">kubernetes</span> <span class="string">cluster1.local.</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span> &#123;</span><br><span class="line">            <span class="string">pods</span> <span class="string">insecure</span></span><br><span class="line">            <span class="string">fallthrough</span> <span class="string">in-addr.arpa</span> <span class="string">ip6.arpa</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">prometheus</span> <span class="string">:9153</span></span><br><span class="line">        <span class="string">forward</span> <span class="string">.</span> <span class="string">/etc/resolv.conf</span></span><br><span class="line">        <span class="string">cache</span> <span class="number">30</span></span><br><span class="line">        <span class="string">loop</span></span><br><span class="line">        <span class="string">reload</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;CoreDNS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">seccomp.security.alpha.kubernetes.io/pod:</span> <span class="string">&#x27;docker/default&#x27;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">100</span></span><br><span class="line">              <span class="attr">podAffinityTerm:</span></span><br><span class="line">                <span class="attr">labelSelector:</span></span><br><span class="line">                  <span class="attr">matchExpressions:</span></span><br><span class="line">                    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">k8s-app</span></span><br><span class="line">                      <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                      <span class="attr">values:</span></span><br><span class="line">                        <span class="bullet">-</span> <span class="string">kube-dns</span></span><br><span class="line">                <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-cluster-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">coredns</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">node-role.kubernetes.io/master:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">          <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">xxxx.lan:5000/coredns:1.7.1</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">270Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">150Mi</span></span><br><span class="line">        <span class="attr">args:</span> [ <span class="string">&quot;-conf&quot;</span>, <span class="string">&quot;/etc/coredns/Corefile&quot;</span> ]</span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/coredns</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9153</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">metrics</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8180</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line">          <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">          <span class="attr">failureThreshold:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">          <span class="attr">capabilities:</span></span><br><span class="line">            <span class="attr">add:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">NET_BIND_SERVICE</span></span><br><span class="line">            <span class="attr">drop:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">all</span></span><br><span class="line">          <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">            <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">Corefile</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">Corefile</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">prometheus.io/port:</span> <span class="string">&quot;9153&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line">    <span class="attr">kubernetes.io/name:</span> <span class="string">&quot;CoreDNS&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-dns</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="number">172.26</span><span class="number">.0</span><span class="number">.2</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dns-tcp</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">153</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br></pre></td></tr></table></figure><h3 id="自己的方案"><a href="#自己的方案" class="headerlink" title="自己的方案"></a>自己的方案</h3><p>但是后面发现 cpu 太高了，决定自己整个方案，中途尝试了很多，最后决定自己把里面的 dummy 接口部分源码抠出来写成一个工具（这样就不用改 svc ip 了），然后高可用用其他手段。主要是替换掉 nodelocaldns 的部分</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">updateStrategy:</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">10</span><span class="string">%</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">node-local-dns</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">regcred</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-node-critical</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">dnsPolicy:</span> <span class="string">Default</span>  <span class="comment"># Don&#x27;t use cluster DNS.</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;CriticalAddonsOnly&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dummy-tool</span></span><br><span class="line">        <span class="comment">#image: registry.aliyuncs.com/zhangguanzhang/dummy-tool:v0.1</span></span><br><span class="line">        <span class="attr">image:</span> &#123;&#123; <span class="string">docker_repo_url</span> &#125;&#125;<span class="string">/dummy-tool:v0.1</span></span><br><span class="line">        <span class="attr">args:</span> </span><br><span class="line">        <span class="bullet">-</span> <span class="string">-local-ip=169.254.20.10,172.26.0.2</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-health-port=8070</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-interface-name=nodelocaldns</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">host:</span> <span class="number">169.254</span><span class="number">.20</span><span class="number">.10</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/health</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">8070</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">40</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">3</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dnsmasq</span></span><br><span class="line">        <span class="comment">#image: registry.aliyuncs.com/zhangguanzhang/dnsmasq:2.83</span></span><br><span class="line">        <span class="attr">image:</span> &#123;&#123; <span class="string">docker_repo_url</span> &#125;&#125;<span class="string">/dnsmasq:2.83</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">dnsmasq</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-d</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--conf-file=/etc/dnsmasq/dnsmasq.conf</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">10Mi</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">host-localtime</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/etc/dnsmasq</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">host-localtime</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-local-dns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">EnsureExists</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">dnsmasq.conf:</span> <span class="string">|</span></span><br><span class="line">    <span class="literal">no</span><span class="string">-resolv</span></span><br><span class="line">    <span class="string">all-servers</span></span><br><span class="line">    <span class="string">server=10.11.86.107#153</span></span><br><span class="line">    <span class="string">server=10.11.86.108#153</span></span><br><span class="line">    <span class="string">server=10.11.86.109#153</span></span><br><span class="line">    <span class="comment">#log-queries</span></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://lework.github.io/2020/11/09/node-local-dns/">在 Kubernetes 集群中使用 NodeLocal DNSCache</a></li><li><a href="https://mritd.com/2019/11/05/writing-plugin-for-coredns/">writing-plugin-for-coredns</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这几天我们内部在做新项目的容灾测试，业务都是在 K8S 上的。容灾里就是随便选节点 &lt;code&gt;shutdown -h now&lt;/code&gt;</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    <category term="coredns" scheme="http://zhangguanzhang.github.io/categories/kubernetes/coredns/"/>
    
    <category term="node-cache" scheme="http://zhangguanzhang.github.io/categories/kubernetes/coredns/node-cache/"/>
    
    
    <category term="coredns" scheme="http://zhangguanzhang.github.io/tags/coredns/"/>
    
  </entry>
  
  <entry>
    <title>docker18.03 hang at &#39;restoring container&#39;</title>
    <link href="http://zhangguanzhang.github.io/2020/12/04/docker1803-hang-container-restore/"/>
    <id>http://zhangguanzhang.github.io/2020/12/04/docker1803-hang-container-restore/</id>
    <published>2020-12-04T19:42:08.000Z</published>
    <updated>2020-12-11T10:15:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>起初是 k8s 有几个 node not ready，上去看了下 kubelet 日志刷 container runtime down，重启了下 docker 后还是没用，docker ps 命令都卡住。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/redhat-release </span></span><br><span class="line">Linux xxx-disk0 3.10.0-1127.13.1.el7.x86_64 #1 SMP Tue Jun 23 15:46:38 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line">CentOS Linux release 7.4.1708 (Core) </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info</span></span><br><span class="line">Containers: 91</span><br><span class="line"> Running: 63</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 28</span><br><span class="line">Images: 539</span><br><span class="line">Server Version: 18.03.0-ce</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: cfd04396dc68220d1cecbe686a6cc3aa5ce3667c</span><br><span class="line">runc version: 4fc53a81fb7c994640722ac585fa9ca548971871</span><br><span class="line">init version: 949e6fa</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 3.10.0-1127.13.1.el7.x86_64</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 8</span><br><span class="line">Total Memory: 15.51GiB</span><br><span class="line">Name: xxx-disk0</span><br><span class="line">ID: UZRM:KRSL:TYWM:VAQY:KWCX:AVFD:NP53:TC35:YHOC:TLLO:YGXO:RMYS</span><br><span class="line">Docker Root Dir: /app/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br></pre></td></tr></table></figure><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>先停掉docker，然后前台启动加 debug 参数启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pgrep dockerd</span></span><br><span class="line">4659</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 4659 &amp;&amp; &gt; /var/run/docker.pid </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep dockerd</span></span><br><span class="line">root      5628  0.0  0.0 112708   980 pts/0    S+   22:33   0:00 grep --color=auto dockerd</span><br><span class="line"><span class="meta">$</span><span class="bash"> ./dockerd -D</span></span><br><span class="line">WARN[0000] The "graph" config file option is deprecated. Please use "data-root" instead. </span><br><span class="line">WARN[2020-12-04T22:33:50.432804342+08:00] could not change group /var/run/docker.sock to docker: group docker not found </span><br><span class="line">DEBU[2020-12-04T22:33:50.432936283+08:00] Listener created for HTTP on unix (/var/run/docker.sock) </span><br><span class="line">INFO[2020-12-04T22:33:50.433612435+08:00] libcontainerd: started new docker-containerd process  pid=5646</span><br><span class="line">INFO[0000] starting containerd                           module=containerd revision=cfd04396dc68220d1cecbe686a6cc3aa5ce3667c version=v1.0.2</span><br><span class="line">DEBU[0000] changing OOM score to -500                    module=containerd</span><br><span class="line">INFO[0000] loading plugin "io.containerd.content.v1.content"...  module=containerd type=io.containerd.content.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.snapshotter.v1.btrfs"...  module=containerd type=io.containerd.snapshotter.v1</span><br><span class="line">WARN[0000] failed to load plugin io.containerd.snapshotter.v1.btrfs  error="path /app/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter" module=containerd</span><br><span class="line">INFO[0000] loading plugin "io.containerd.snapshotter.v1.overlayfs"...  module=containerd type=io.containerd.snapshotter.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.metadata.v1.bolt"...  module=containerd type=io.containerd.metadata.v1</span><br><span class="line">WARN[0000] could not use snapshotter btrfs in metadata plugin  error="path /app/kube/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter" module="containerd/io.containerd.metadata.v1.bolt"</span><br><span class="line">INFO[0000] loading plugin "io.containerd.differ.v1.walking"...  module=containerd type=io.containerd.differ.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.gc.v1.scheduler"...  module=containerd type=io.containerd.gc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.containers"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.content"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.diff"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.events"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.healthcheck"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.images"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.leases"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.namespaces"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.snapshots"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.monitor.v1.cgroups"...  module=containerd type=io.containerd.monitor.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.runtime.v1.linux"...  module=containerd type=io.containerd.runtime.v1</span><br><span class="line">DEBU[0000] loading tasks in namespace                    module="containerd/io.containerd.runtime.v1.linux" namespace=moby</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.tasks"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.version"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] loading plugin "io.containerd.grpc.v1.introspection"...  module=containerd type=io.containerd.grpc.v1</span><br><span class="line">INFO[0000] serving...                                    address="/var/run/docker/containerd/docker-containerd-debug.sock" module="containerd/debug"</span><br><span class="line">INFO[0000] serving...                                    address="/var/run/docker/containerd/docker-containerd.sock" module="containerd/grpc"</span><br><span class="line">INFO[0000] containerd successfully booted in 0.009604s   module=containerd</span><br><span class="line">DEBU[2020-12-04T22:33:50.456534148+08:00] Golang's threads limit set to 113940         </span><br><span class="line">DEBU[2020-12-04T22:33:50.457345643+08:00] Using default logging driver json-file       </span><br><span class="line">DEBU[2020-12-04T22:33:50.457466912+08:00] [graphdriver] priority list: [btrfs zfs overlay2 aufs overlay devicemapper vfs] </span><br><span class="line">DEBU[2020-12-04T22:33:50.457623030+08:00] processing event stream                       module=libcontainerd namespace=plugins.moby</span><br><span class="line">DEBU[2020-12-04T22:33:50.479691287+08:00] backingFs=xfs,  projectQuotaSupported=false  </span><br><span class="line">INFO[2020-12-04T22:33:50.479712832+08:00] [graphdriver] using prior storage driver: overlay2 </span><br><span class="line">DEBU[2020-12-04T22:33:50.479724151+08:00] Initialized graph driver overlay2            </span><br><span class="line">DEBU[2020-12-04T22:33:50.510882767+08:00] Max Concurrent Downloads: 10                 </span><br><span class="line">DEBU[2020-12-04T22:33:50.510930407+08:00] Max Concurrent Uploads: 5                    </span><br><span class="line">DEBU[0000] garbage collected                             d=24.493383ms module="containerd/io.containerd.gc.v1.scheduler"</span><br><span class="line">INFO[2020-12-04T22:33:50.608483121+08:00] Graph migration to content-addressability took 0.00 seconds </span><br><span class="line">INFO[2020-12-04T22:33:50.610430840+08:00] Loading containers: start.                   </span><br><span class="line">DEBU[2020-12-04T22:33:50.610704281+08:00] processing event stream                       module=libcontainerd namespace=moby</span><br><span class="line">DEBU[2020-12-04T22:33:50.611446797+08:00] Loaded container 027a389c8c1e93629cc5f68af8d023b2ecfe350d7771ba6b87598ff705f6c19f, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.611803503+08:00] Loaded container 24735e5aea2bd91b5fa5d729ca021a09532c2ea9b8b06f5171d0da23fc3bf4cc, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612174253+08:00] Loaded container 487a8c2f30986796c3948d1469d506e1d3ab394e17533040ef7a5444a32be0fc, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612494092+08:00] Loaded container 52d32b0e03c957b6cb9b4d793c47900e689a29d9ae0d63703ea29073a352fbe5, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.612816495+08:00] Loaded container 5b7b0b52c71a14164f269853679211b3823e9eecc2d3829bf2db10c9b720217d, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.613447082+08:00] Loaded container 62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be, isRunning: true </span><br><span class="line">DEBU[2020-12-04T22:33:50.613769649+08:00] Loaded container 68ba211ec7328bebd3b241631a703639447c05056ffe07ed633b72d0bc210938, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.614756585+08:00] Loaded container 73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212, isRunning: true </span><br><span class="line">DEBU[2020-12-04T22:33:50.615381990+08:00] Loaded container 7a2a75c8a0c8dac8c5773ad92fa45ac7e1d33c9be85ecb65eb147929955dca50, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616222796+08:00] Loaded container 81095c01c4b99c7d2cc9e6bee8726c11f16d27204523727e7d067d980c26ac64, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616569394+08:00] Loaded container 94098167eb466dbf1a454f5491a162488d1fdb1eebe804c5c1f403f7fce62dc4, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.616981038+08:00] Loaded container 9d4cbcce43b0262d972e73b2770f26ca762e2fa86f0de88a7909b8e59c0b805a, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.617460452+08:00] Loaded container aecde8eb18924d8548d79d5e0383baa7ac3ab1cfc4c55e1f32c4089dfc153071, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.617908975+08:00] Loaded container aed0618a325b4b84363357c1830515048d23af6afd79606cbb0ad64bf5f226a2, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618252961+08:00] Loaded container b43e4995720f235c40ffd60bde1fb54e87ece3598f8bd625996042f637896687, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618557604+08:00] Loaded container c1e6a1de9b9c2fd420e718c405c114e726ec5531561a4caf662b757a3724711e, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.618942417+08:00] Loaded container c5eb3c941e562153e0cf0af738f1cb43f34591f0b48ad5458ab2002f5be9e0a8, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.619380785+08:00] Loaded container e211ffccacb8f7982899097fbd0f9ce1d95f8f31f290fb10baf40d00f4980bc9, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.619831551+08:00] Loaded container ef547d238cd01ff7ec048de3442fe9293aa1d5d932ea66c5aed34bfff014182b, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.620192032+08:00] Loaded container f3bb916ec5d7847c3be4341975c47f4e2fe587fc726ca7d76e3dca15cb8dd21d, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:50.620438678+08:00] Loaded container fa6de6f4aa8894c18a9737bac462f57c69893eca5e4b58bc3bd793a76b252951, isRunning: false </span><br><span class="line">DEBU[2020-12-04T22:33:51.379861237+08:00] restoring container                           container=ef547d238cd01ff7ec048de3442fe9293aa1d5d932ea66c5aed34bfff014182b paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.379910464+08:00] restoring container                           container=e211ffccacb8f7982899097fbd0f9ce1d95f8f31f290fb10baf40d00f4980bc9 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.379994141+08:00] restoring container                           container=7a2a75c8a0c8dac8c5773ad92fa45ac7e1d33c9be85ecb65eb147929955dca50 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380029802+08:00] restoring container                           container=c1e6a1de9b9c2fd420e718c405c114e726ec5531561a4caf662b757a3724711e paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380084763+08:00] restoring container                           container=5b7b0b52c71a14164f269853679211b3823e9eecc2d3829bf2db10c9b720217d paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380127006+08:00] restoring container                           container=9d4cbcce43b0262d972e73b2770f26ca762e2fa86f0de88a7909b8e59c0b805a paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380121758+08:00] restoring container                           container=fa6de6f4aa8894c18a9737bac462f57c69893eca5e4b58bc3bd793a76b252951 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380163318+08:00] restoring container                           container=52d32b0e03c957b6cb9b4d793c47900e689a29d9ae0d63703ea29073a352fbe5 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380310029+08:00] restoring container                           container=027a389c8c1e93629cc5f68af8d023b2ecfe350d7771ba6b87598ff705f6c19f paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380382722+08:00] restoring container                           container=68ba211ec7328bebd3b241631a703639447c05056ffe07ed633b72d0bc210938 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380419320+08:00] restoring container                           container=487a8c2f30986796c3948d1469d506e1d3ab394e17533040ef7a5444a32be0fc paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380433522+08:00] restoring container                           container=73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 paused=false running=true</span><br><span class="line">DEBU[2020-12-04T22:33:51.380459224+08:00] restoring container                           container=c5eb3c941e562153e0cf0af738f1cb43f34591f0b48ad5458ab2002f5be9e0a8 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380525276+08:00] restoring container                           container=b43e4995720f235c40ffd60bde1fb54e87ece3598f8bd625996042f637896687 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380563957+08:00] restoring container                           container=81095c01c4b99c7d2cc9e6bee8726c11f16d27204523727e7d067d980c26ac64 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380586567+08:00] restoring container                           container=62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be paused=false running=true</span><br><span class="line">DEBU[2020-12-04T22:33:51.380599061+08:00] restoring container                           container=94098167eb466dbf1a454f5491a162488d1fdb1eebe804c5c1f403f7fce62dc4 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380616220+08:00] restoring container                           container=aecde8eb18924d8548d79d5e0383baa7ac3ab1cfc4c55e1f32c4089dfc153071 paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380641090+08:00] restoring container                           container=f3bb916ec5d7847c3be4341975c47f4e2fe587fc726ca7d76e3dca15cb8dd21d paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380825356+08:00] restoring container                           container=24735e5aea2bd91b5fa5d729ca021a09532c2ea9b8b06f5171d0da23fc3bf4cc paused=false running=false</span><br><span class="line">DEBU[2020-12-04T22:33:51.380953092+08:00] restoring container                           container=aed0618a325b4b84363357c1830515048d23af6afd79606cbb0ad64bf5f226a2 paused=false running=false</span><br></pre></td></tr></table></figure><p>然后发现卡在这，正常是会像 gin 那样启动输出支持的 http api 路由信息的。开一个窗口，发送 SIGUSR1 信号打印 goroutine 堆栈信息看看卡在哪儿：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pgrep dockerd</span></span><br><span class="line">3085</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> -USR1 3085</span></span><br></pre></td></tr></table></figure><p>docker 的日志和系统日志都会有下面的类似输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Dec 04 22:33:52 xxxx dockerd[3085]: time&#x3D;&quot;2020-12-33T58:15:52.906433650+08:00&quot; level&#x3D;info msg&#x3D;&quot;goroutine stacks written to &#x2F;var&#x2F;run&#x2F;docker&#x2F;goroutine-stacks-2020-12-04T223358+0800.log&quot;</span><br></pre></td></tr></table></figure><p>查看了下下面这段比较可疑，<code>daemon/daemon.go:364</code> 附近</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">goroutine 1 [semacquire, 5 minutes]:</span><br><span class="line">sync.runtime_Semacquire(0xc4204de73c)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;sema.go:56 +0x3b</span><br><span class="line">sync.(*WaitGroup).Wait(0xc4204de730)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;sync&#x2F;waitgroup.go:131 +0x74</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;daemon.(*Daemon).restore(0xc42009a480, 0x190c3a6, 0x4)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:364 +0xfeb</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;daemon.NewDaemon(0xc42018d200, 0x2ec75c0, 0xc4201be410, 0x2ea89e0, 0xc420087d40, 0xc4201323c0, 0x0, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;daemon&#x2F;daemon.go:894 +0x258d</span><br><span class="line">main.(*DaemonCli).start(0xc42051da40, 0xc4201c5d50, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;daemon.go:223 +0x1320</span><br><span class="line">main.runDaemon(0xc4201c5d50, 0xc42044b3b0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:78 +0x78</span><br><span class="line">main.newDaemonCommand.func1(0xc420176000, 0xc4201359e0, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:29 +0x5d</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).execute(0xc420176000, 0xc42000c090, 0x1, 0x1, 0xc420176000, 0xc42000c090)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:646 +0x44f</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).ExecuteC(0xc420176000, 0x2194e40, 0x2419c01, 0xc420135980)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:742 +0x310</span><br><span class="line">github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra.(*Command).Execute(0xc420176000, 0xc420135980, 0x190fa00)</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;vendor&#x2F;github.com&#x2F;spf13&#x2F;cobra&#x2F;command.go:695 +0x2d</span><br><span class="line">main.main()</span><br><span class="line">&#x2F;go&#x2F;src&#x2F;github.com&#x2F;docker&#x2F;docker&#x2F;cmd&#x2F;dockerd&#x2F;docker.go:105 +0xe3</span><br></pre></td></tr></table></figure><p>按照docker info 的信息去找了下对应的 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364" target="_blank" rel="noopener">分支代码</a>。364 行一个<code>wg.Wait()</code>，得看前面的 goroutine 是卡在哪儿，根据前面的堆栈信息，应该是卡在 <code>github.com/docker/docker/daemon.(*Daemon).restore</code>，也就是 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L238" target="_blank" rel="noopener">238 行的 daemon.containerd.Restore 方法</a>，卡在<code>wg.Wait()</code>说明有协程没释放锁，这里<a href="https://github.com/moby/moby/blob/v18.03.0-ce/libcontainerd/client_daemon.go#L134" target="_blank" rel="noopener">containerd.Restore方法</a>的第一行就是锁，里面有个方法<code>c.remote.LoadContainer</code>，实际上是和docker-containerd通信的。</p><p>查看下 docker-containerd 进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux | grep containerd</span><br><span class="line">root      5646  0.2  0.0 606436 14048 ?        Ssl  22:33   0:00 docker-containerd --config &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;containerd.toml</span><br><span class="line">appuser   6261  0.0  0.0 112708   984 pts&#x2F;1    S+   22:36   0:00 grep --color&#x3D;auto containerd</span><br><span class="line">root      8355  0.1  0.0   9052  4308 ?        Sl   Dec03   2:56 docker-containerd-shim -namespace moby -workdir &#x2F;app&#x2F;kube&#x2F;dockercontainerd&#x2F;daemon&#x2F;io.containerd.runtime.v1.linux&#x2F;moby&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be -address &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;docker-containerd.sock -containerd-binary &#x2F;app&#x2F;kube&#x2F;bin&#x2F;docker-containerd -runtime-root &#x2F;var&#x2F;run&#x2F;docker&#x2F;runtime-runc</span><br><span class="line">root     11171  0.0  0.0   9052  4052 ?        Sl   Dec03   0:18 docker-containerd-shim -namespace moby -workdir &#x2F;app&#x2F;kube&#x2F;dockercontainerd&#x2F;daemon&#x2F;io.containerd.runtime.v1.linux&#x2F;moby&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 -address &#x2F;var&#x2F;run&#x2F;docker&#x2F;containerd&#x2F;docker-containerd.sock -containerd-binary &#x2F;app&#x2F;kube&#x2F;bin&#x2F;docker-containerd -runtime-root &#x2F;var&#x2F;run&#x2F;docker&#x2F;runtime-runc</span><br></pre></td></tr></table></figure><p>有残留的，杀掉一个试试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 11171</span></span><br></pre></td></tr></table></figure><p>然后原窗口有输出了一些日志，实际上是执行了 <a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L244" target="_blank" rel="noopener">244行的 daemon.containerd.DeleteTask方法</a>，说明思路是对的，进程通信有问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0172] connecting to shim                            error&#x3D;&lt;nil&gt; id&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; namespace&#x3D;moby</span><br><span class="line">DEBU[2020-12-04T22:36:42.690975930+08:00] restored container                            alive&#x3D;false container&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 module&#x3D;libcontainerd namespace&#x3D;moby pid&#x3D;0</span><br><span class="line">DEBU[2020-12-04T22:36:42.701154551+08:00] Trying to unmount &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212&#x2F;mounts </span><br><span class="line">DEBU[2020-12-04T22:36:42.707909556+08:00] Unmounted &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212&#x2F;mounts </span><br><span class="line">DEBU[0172] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;exit&quot; type&#x3D;containerd.events.TaskExit</span><br><span class="line">DEBU[0172] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;delete&quot; type&#x3D;containerd.events.TaskDelete</span><br><span class="line">DEBU[2020-12-04T22:36:42.947670205+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;exit</span><br></pre></td></tr></table></figure><p>接着处理另一个：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep containerd</span></span><br><span class="line">root      5646  0.2  0.0 606692 14048 ?        Ssl  22:33   0:00 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">root      6461  0.0  0.0 112708   984 pts/1    S+   22:37   0:00 grep --color=auto containerd</span><br><span class="line">root      8355  0.1  0.0   9052  4260 ?        Sl   Dec03   2:56 docker-containerd-shim -namespace moby -workdir /app/kube/dockercontainerd/daemon/io.containerd.runtime.v1.linux/moby/62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /app/kube/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">kill</span> 8355</span></span><br></pre></td></tr></table></figure><p>然后前台 debug 的日志没有卡住，正常启动了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ERRO[0249] connecting to shim                            error&#x3D;&lt;nil&gt; id&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; namespace&#x3D;moby</span><br><span class="line">DEBU[2020-12-04T22:37:59.709825146+08:00] restored container                            alive&#x3D;false container&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be module&#x3D;libcontainerd namespace&#x3D;moby pid&#x3D;0</span><br><span class="line">DEBU[2020-12-04T22:37:59.710064357+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete</span><br><span class="line">INFO[2020-12-04T22:37:59.710093459+08:00] ignoring event                                module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">WARN[2020-12-04T22:37:59.710215638+08:00] Ignoring Exit Event, no such exec command found  container&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 exec-id&#x3D;73cfe941e7a948a77783c77f963efc66327323c2603e058e7ab61f85f8e98212 exec-pid&#x3D;11197</span><br><span class="line">DEBU[2020-12-04T22:37:59.719102521+08:00] Trying to unmount &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be&#x2F;mounts </span><br><span class="line">DEBU[2020-12-04T22:37:59.722934436+08:00] Unmounted &#x2F;app&#x2F;kube&#x2F;docker&#x2F;containers&#x2F;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be&#x2F;mounts </span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;containers&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;containers&#x2F;delete&quot; type&#x3D;containerd.events.ContainerDelete</span><br><span class="line">DEBU[2020-12-04T22:37:59.978450001+08:00] container mounted via layerStore: &amp;&#123;&#x2F;app&#x2F;kube&#x2F;docker&#x2F;overlay2&#x2F;97a09a97cf8c3ae835fb0ca6526c0282b26379942dfb49081189a39ce0400596&#x2F;merged 0x2f42600 0x2f42600&#125; </span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;exit&quot; type&#x3D;containerd.events.TaskExit</span><br><span class="line">DEBU[2020-12-04T22:38:00.169804015+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;exit</span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;io.containerd.runtime.v1.linux&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;tasks&#x2F;delete&quot; type&#x3D;containerd.events.TaskDelete</span><br><span class="line">WARN[2020-12-04T22:38:00.169915440+08:00] Ignoring Exit Event, no such exec command found  container&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be exec-id&#x3D;62b049d16d1fe03c193295329e7055b3e675a5e94b9566eee6accc35820530be exec-pid&#x3D;8396</span><br><span class="line">DEBU[2020-12-04T22:38:00.170037297+08:00] event                                         module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete</span><br><span class="line">INFO[2020-12-04T22:38:00.170061445+08:00] ignoring event                                module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">DEBU[0249] event published                               module&#x3D;&quot;containerd&#x2F;containers&quot; ns&#x3D;moby topic&#x3D;&quot;&#x2F;containers&#x2F;delete&quot; type&#x3D;containerd.events.ContainerDelete</span><br><span class="line">DEBU[2020-12-04T22:38:00.199820461+08:00] container mounted via layerStore: &amp;&#123;&#x2F;app&#x2F;kube&#x2F;docker&#x2F;overlay2&#x2F;2abb109b107ef7f0e5c31b1a100b446234118ae38afe43977c8c718f115cdfd6&#x2F;merged 0x2f42600 0x2f42600&#125; </span><br><span class="line">DEBU[2020-12-04T22:38:00.208519823+08:00] Option Experimental: false                   </span><br><span class="line">DEBU[2020-12-04T22:38:00.208542167+08:00] Option DefaultDriver: bridge                 </span><br><span class="line">DEBU[2020-12-04T22:38:00.208549815+08:00] Option DefaultNetwork: bridge                </span><br><span class="line">DEBU[2020-12-04T22:38:00.208557480+08:00] Network Control Plane MTU: 1500              </span><br><span class="line">DEBU[2020-12-04T22:38:00.245647071+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.247719844+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0&#x2F;8 -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.249828613+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER] </span><br><span class="line">DEBU[2020-12-04T22:38:00.251439314+08:00] &#x2F;sbin&#x2F;iptables, [--wait -t nat -D PREROUTING]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>服务器上有安全狗，可能和安全狗有关系。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364" target="_blank" rel="noopener">https://github.com/moby/moby/blob/v18.03.0-ce/daemon/daemon.go#L364</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;起初是 k8s 有几个 node not ready，上去看了下 kubelet 日志刷 container runtime down，重启</summary>
      
    
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/categories/docker/"/>
    
    
    <category term="hang" scheme="http://zhangguanzhang.github.io/tags/hang/"/>
    
  </entry>
  
  <entry>
    <title>ansible hang in docker container</title>
    <link href="http://zhangguanzhang.github.io/2020/11/23/ansible-hang-in-docker/"/>
    <id>http://zhangguanzhang.github.io/2020/11/23/ansible-hang-in-docker/</id>
    <published>2020-11-23T19:42:08.000Z</published>
    <updated>2020-11-23T19:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这几天同事发现在 docker 容器里运行 ansible 命令很卡，发来了个命令叫我试试 <code>ansible localhost -m setup -a &#39;filter=ansible_default_ipv4&#39; 2&gt;/dev/null |grep &#39;\&quot;address\&quot;&#39; |awk -F&#39;\&quot;&#39; &#39;&#123;print $4&#125;&#39;</code></p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span> </span><br><span class="line">NAME=&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION=&quot;V10 (Tercel)&quot;</span><br><span class="line">ID=&quot;kylin&quot;</span><br><span class="line">VERSION_ID=&quot;V10&quot;</span><br><span class="line">PRETTY_NAME=&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxxxx 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker info</span></span><br><span class="line">Containers: 1</span><br><span class="line"> Running: 1</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 1</span><br><span class="line">Server Version: 18.09.9</span><br><span class="line">Storage Driver: overlay2</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Supports d_type: true</span><br><span class="line"> Native Overlay Diff: true</span><br><span class="line">Logging Driver: json-file</span><br><span class="line">Cgroup Driver: cgroupfs</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog</span><br><span class="line">Swarm: inactive</span><br><span class="line">Runtimes: runc</span><br><span class="line">Default Runtime: runc</span><br><span class="line">Init Binary: docker-init</span><br><span class="line">containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line">runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line">init version: fec3683</span><br><span class="line">Security Options:</span><br><span class="line"> seccomp</span><br><span class="line">  Profile: default</span><br><span class="line">Kernel Version: 4.19.90-17.ky10.aarch64</span><br><span class="line">Operating System: Kylin Linux Advanced Server V10 (Tercel)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: aarch64</span><br><span class="line">CPUs: 64</span><br><span class="line">Total Memory: 62.76GiB</span><br><span class="line">Name: xxx</span><br><span class="line">ID: 3ZQD:MZWN:FNR4:5HEE:F57N:3BLD:EP3T:LJT7:NWEJ:3TZ3:IEBD:KSHZ</span><br><span class="line">Docker Root Dir: /data/kube/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Labels:</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> treg.yun.xxx.cn</span><br><span class="line"> reg.xxx.lan:5000</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br><span class="line"> https://docker.mirrors.ustc.edu.cn/</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line">Product License: Community Engine</span><br></pre></td></tr></table></figure><p>容器里的 ansible 和 python 信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ansible --version</span></span><br><span class="line">ansible 2.8.6</span><br><span class="line">  config file = None</span><br><span class="line">  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]</span><br><span class="line">  ansible python module location = /usr/local/lib/python2.7/dist-packages/ansible</span><br><span class="line">  executable location = /usr/local/bin/ansible</span><br><span class="line">  python version = 2.7.12 (default, Apr 15 2020, 17:07:12) [GCC 5.4.0 20160609]</span><br><span class="line"><span class="meta">$</span><span class="bash"> python --version</span></span><br><span class="line">Python 2.7.12</span><br></pre></td></tr></table></figure><p>他说如果用麒麟的 rpm 包安装 docker 就没问题，用我们的拷贝二进制文件安装的 docker 起的容器里就不行。一开始是怀疑 setup 模块在收集某些信息的时候阻塞了，后面我试了下这样也会卡住 <code>ansible localhost -m shell -a date</code>。</p><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>带上了 <code>-vvvv</code> 发现卡在下面的输出这：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;127.0.0.1&gt; PUT &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-local-466216tkG5t&#x2F;tmpML7hBj TO &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py</span><br><span class="line">&lt;127.0.0.1&gt; EXEC &#x2F;bin&#x2F;sh -c &#39;chmod u+x &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F; &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py &amp;&amp; sleep 0&#39;</span><br><span class="line">&lt;127.0.0.1&gt; EXEC &#x2F;bin&#x2F;sh -c &#39;&#x2F;usr&#x2F;bin&#x2F;python &#x2F;root&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-1606180831.93-276734338965603&#x2F;AnsiballZ_command.py &amp;&amp; sleep 0&#39;</span><br></pre></td></tr></table></figure><p>搜了下可以 <code>export ANSIBLE_DEBUG=True</code> 打印更详细的日志，打印了下面的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">23918 1606128393.94311: ANSIBALLZ: Done creating module</span><br><span class="line">23918 1606128393.94465: _low_level_execute_command(): starting</span><br><span class="line">23918 1606128393.94493: _low_level_execute_command(): executing: &#x2F;bin&#x2F;sh -c &#39;&#x2F;usr&#x2F;bin&#x2F;python &amp;&amp; sleep 0&#39;</span><br><span class="line">23918 1606128393.94515: in local.exec_command()</span><br><span class="line">23918 1606128393.94528: opening command with Popen()</span><br><span class="line">23918 1606128393.94979: done running command with Popen()</span><br><span class="line">23918 1606128393.95005: getting output with communicate()</span><br></pre></td></tr></table></figure><p>看样子是子进程卡住了，主进程等子进程。因为容器的进程实际上也是在宿主机上的，宿主机上安装了 strace，查看下进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps aux | grep ansible</span></span><br><span class="line">root     3020177  2.7  0.0 129408 48960 pts/0    Sl+  19:12   0:26 /usr/bin/python /usr/local/bin/ansible localhost -vvvvv -m shell -a ls</span><br><span class="line">root     3020189  0.0  0.0 134912 50368 pts/0    S+   19:12   0:00 /usr/bin/python /usr/local/bin/ansible localhost -vvvvv -m shell -a ls</span><br><span class="line">root     3020216  0.0  0.0   2368   768 pts/0    S+   19:12   0:00 /bin/sh -c /bin/sh -c &#x27;/usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py &amp;&amp; sleep 0&#x27;</span><br><span class="line">root     3020217  0.0  0.0   2368   768 pts/0    S+   19:12   0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py &amp;&amp; sleep 0</span><br><span class="line">root     3020218  0.0  0.0  19776 14336 pts/0    S+   19:12   0:00 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py</span><br><span class="line">root     3020219  9.3  0.0  18688 10816 pts/0    t+   19:12   1:27 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1606129970.28-271461867131881/AnsiballZ_command.py</span><br><span class="line">root     3050341  0.3  0.0   8832  4544 ?        Ss   19:28   0:00 ssh: /root/.ansible/cp/5a0929d121 [mux]</span><br><span class="line">root     3052724  0.0  0.0 214080  1536 pts/7    S+   19:28   0:00 grep ansible</span><br></pre></td></tr></table></figure><p>strace 下 3020219 ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> strace -p 3020219</span></span><br><span class="line">close(76267956)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267957)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267958)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267959)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267960)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267961)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267962)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267963)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267964)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267965)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267966)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267967)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267968)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267969)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267970)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267971)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">close(76267972^C)                         = -1 EBADF (错误的文件描述符)</span><br><span class="line">strace: Process 3020219 detached</span><br></pre></td></tr></table></figure><p>终端一直刷上面的，看样子是文件描述符泄露，<code>错误的文件描述符</code> 英文就是 <code>Bad file descriptor</code>， 谷歌搜了下 <code>in &quot;docker&quot;  (Bad file descriptor) strace</code>，找到了 <a href="https://github.com/docker/for-linux/issues/502">Spawning PTY processes is many times slower on Docker 18.09</a> 里几位大佬排查到是某些 os 下，很多地方都有设置 limit（就是那种我以为你做了，我就没做。结果你以为我做了，你就没做，结果大家都没做的感觉了），导致容器的 nofile 会不固定，经常性的太高。而很多语言的模块会遍历所有描述符，导致会卡。如果启动容器 nofile 设置低则没问题，下面还有个大佬给了个链接在 python 层面修复这个问题 <a href="https://github.com/python/cpython/pull/11584">python/cpython#11584</a> ，还有下面的解决办法，直接设置默认的 limit：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> https://docs.docker.com/engine/reference/commandline/dockerd/<span class="comment">#daemon-configuration-file</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/docker/daemon.json-ulimits</span></span><br><span class="line">&#123;</span><br><span class="line">&quot;default-ulimits&quot;: &#123;</span><br><span class="line">&quot;nofile&quot;: &#123;</span><br><span class="line">&quot;Name&quot;: &quot;nofile&quot;,</span><br><span class="line">&quot;Hard&quot;: 1024,</span><br><span class="line">&quot;Soft&quot;: 1024</span><br><span class="line">&#125;,</span><br><span class="line">&quot;nproc&quot;: &#123;</span><br><span class="line">&quot;Name&quot;: &quot;nproc&quot;,</span><br><span class="line">&quot;Soft&quot;: 65536,</span><br><span class="line">&quot;Hard&quot;: 65536</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>改配置怕影响其他容器，就决定从 python 层面改，看了下提交的 pr <a href="https://github.com/python/cpython/commit/5626fff54ebe5863e64454c081ec585f85cf141c">python/cpython@5626fff</a> 是改的 <code>subprocess.py</code> ，在容器里查找下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">find / -type f -name subprocess.py </span><br><span class="line">/usr/lib/python2.7/subprocess.py</span><br><span class="line">/usr/lib/python3.5/asyncio/subprocess.py</span><br><span class="line">/usr/lib/python3.5/subprocess.py</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/future/moves/subprocess.py</span><br><span class="line">/usr/local/lib/python2.7/dist-packages/gevent/subprocess.py</span><br></pre></td></tr></table></figure><p>对比了下函数名 <code>def _close_fds(self, but):</code> ，确定是 <code>/usr/lib/python2.7/subprocess.py</code>，把 pr 的内容加进去后再执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> time ansible localhost -m shell -a date</span></span><br><span class="line">[WARNING]: No inventory was parsed, only implicit localhost is available</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">localhost | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">2020年 11月 23日 星期一 19:38:14 CST</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real0m3.088s</span><br><span class="line">user0m2.860s</span><br><span class="line">sys  0m0.255s</span><br></pre></td></tr></table></figure><p>最终还是选择了起容器的时候限制</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--ulimit nofile&#x3D;65536</span><br></pre></td></tr></table></figure><p>上面的几位大佬给出了其他的解决方案，也可以在 containerd 配置文件里配置把 nofile 固定住，或者 docker daemon，或者应用的软件层面修复。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/pexpect/ptyprocess/issues/50">https://github.com/pexpect/ptyprocess/issues/50</a></li><li><a href="https://github.com/docker/for-linux/issues/502">https://github.com/docker/for-linux/issues/502</a></li><li><a href="https://github.com/moby/moby/issues/38814">https://github.com/moby/moby/issues/38814</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这几天同事发现在 docker 容器里运行 ansible 命令很卡，发来了个命令叫我试试 &lt;code&gt;ansible localhost </summary>
      
    
    
    
    <category term="ansible" scheme="http://zhangguanzhang.github.io/categories/ansible/"/>
    
    
    <category term="docker" scheme="http://zhangguanzhang.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>永久关闭swap的正确姿势</title>
    <link href="http://zhangguanzhang.github.io/2020/11/20/disable-swap/"/>
    <id>http://zhangguanzhang.github.io/2020/11/20/disable-swap/</id>
    <published>2020-11-20T10:29:08.000Z</published>
    <updated>2020-11-20T10:29:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天遇到了 kylin 系统上无法关闭 swap 的情况。记录下和方便别人搜到这个知识点。</p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> at /etc/issue</span></span><br><span class="line">Kylin 4.0.2 \n \l</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux H-192-168-63-132 4.15.0- 58-generic #64kord1k1&#x27;SMP Thu Aug 1S15:51:97 csT 2919 aarch64 ......</span><br></pre></td></tr></table></figure><h2 id="尝试的步骤"><a href="#尝试的步骤" class="headerlink" title="尝试的步骤"></a>尝试的步骤</h2><p>fstab 里没有 swap 的挂载，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a &amp;&amp; sysctl -w vm.swappiness=0</span><br></pre></td></tr></table></figure><p>重启后，内核参数是关闭的，但是实际没有关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sysctl -a |&amp; grep vm.swappiness</span></span><br><span class="line">vm.swappiness = 0</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> free -h</span></span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           127G         48G         70G        169M        7.8G         64G</span><br><span class="line">Swap:          7.6G          0B        7.8G</span><br></pre></td></tr></table></figure><p>应该有其他的挂载，据我所知， systemd 也会负责挂载的，查找下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl list-units | grep swap</span></span><br><span class="line">dev-sda3.swap         loaded active active    Swap Partition</span><br><span class="line">swap.target           loaded active active    Swap</span><br><span class="line"><span class="meta">$</span><span class="bash"> systemctl cat dev-sda3.swap</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> /run/systemd/generator.late/dev-sda3.swap</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Automatically generated by systemd-gpt-auto-generator</span></span><br><span class="line"></span><br><span class="line">[unit]</span><br><span class="line">Description=Swap Partition</span><br><span class="line">Documentation=man:systemd-gpt-auto-generator(8)</span><br><span class="line"></span><br><span class="line">[Swap]</span><br><span class="line">What=/dev/sda3</span><br></pre></td></tr></table></figure><p>发现这个无法 disable ，会报错 no such file。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>查找了下文档，<code>systemd-gpt-auto-generator</code> 是一个 GPT 分区 自动发现 与 挂载。会自动生成 mount 和 swap 的 systemd unit 文件。找到了英文文档里有下面的话:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemd-gpt-auto-generator understands the following kernel command line parameters:</span><br><span class="line"></span><br><span class="line">systemd.gpt_auto, rd.systemd.gpt_auto</span><br><span class="line">Those options take an optional boolean argument, and default to yes. The generator is enabled by default, and a negative value may be used to disable it.</span><br></pre></td></tr></table></figure><p>我们可以通过添加 kernel boot cmdline 来关闭 <code>systemd-gpt-auto-generator</code> 。</p><p>先查找到 <code>grub.cfg</code> ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> find /boot  -<span class="built_in">type</span> f -name <span class="string">&#x27;grub*cfg&#x27;</span> -<span class="built_in">exec</span> grep -l <span class="string">&#x27;/vmlinuz&#x27;</span> &#123;&#125; \;</span></span><br><span class="line">/boot/grub/grub.cfg</span><br></pre></td></tr></table></figure><p>进去备份下文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /boot/grub/</span><br><span class="line">cp grub.cfg grub.cfg-20201120</span><br></pre></td></tr></table></figure><p>找到类似下面的行:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux /boot/vmlinuz-xxx</span><br></pre></td></tr></table></figure><p>后面加上 <code>systemd.gpt_auto=false</code> ，文档是写布尔值的，不过我看到有人 <code>systemd.gpt_auto=0</code> 也行。然后重启。</p><p><code>2021/03/23</code> 测试了下面的不行。。。<br>理论上改文件 <code>/etc/default/grub</code> 里 <code>GRUB_CMDLINE_LINUX</code> 后面添加也行。</p><p>Azure Linux 的话有个 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl list-units | grep temp-disk-swapfile</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://www.jinbuguo.com/systemd/systemd-gpt-auto-generator.html">systemd-gpt-auto-generator 中文手册</a></li><li><a href="https://www.freedesktop.org/software/systemd/man/systemd-gpt-auto-generator.html">systemd-gpt-auto-generator 英文文档</a></li><li><a href="http://www.jinbuguo.com/systemd/systemd.generator.html#">systemd.generator 中文手册</a></li><li><a href="https://groups.google.com/g/shlug/c/BH11BbecodM">更改分区类型关闭 swap</a></li><li><a href="https://manpages.ubuntu.com/manpages/disco/man7/kernel-command-line.7.html">kernel-command-line man page 7</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天遇到了 kylin 系统上无法关闭 swap 的情况。记录下和方便别人搜到这个知识点。&lt;/p&gt;
&lt;h2 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=&quot;headerlink&quot; title=&quot;环境信息&quot;&gt;&lt;/a&gt;环境信息&lt;/h2&gt;&lt;figure clas</summary>
      
    
    
    
    <category term="linux" scheme="http://zhangguanzhang.github.io/categories/linux/"/>
    
    
    <category term="swap" scheme="http://zhangguanzhang.github.io/tags/swap/"/>
    
  </entry>
  
  <entry>
    <title>银河麒麟arm64系统克隆机器上k8s vxlan跨节点不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2020/11/06/kylin-arm-clone-vxlan-error/"/>
    <id>http://zhangguanzhang.github.io/2020/11/06/kylin-arm-clone-vxlan-error/</id>
    <published>2020-11-06T19:31:01.000Z</published>
    <updated>2020-11-11T02:11:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>和前一篇文章不一样，从来没遇到过这样的问题，这里记录下。实施在客户那边部署业务后，业务在浏览器上无法访问，我远程上去查看日志发现 pod 内部无法 DNS 无法解析，nginx 连不上 upsteam 报错而启动失败，实际上也是跨节点不通。实际排查过程也有往错误的方向浪费了一些时间和尝试，就不写进来了，以正确的角度写下排查过程。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>OS 是 arm64 的银河麒麟系统：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">NAME="Kylin Linux Advanced Server"</span><br><span class="line">VERSION="V10 (Tercel)"</span><br><span class="line">ID="kylin"</span><br><span class="line">VERSION_ID="V10"</span><br><span class="line">PRETTY_NAME="Kylin Linux Advanced Server V10 (Tercel)"</span><br><span class="line">ANSI_COLOR="0;31"</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux localhost.localdomain 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>集群信息（和集群版本没关系）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  "clientVersion": &#123;</span><br><span class="line">    "major": "1",</span><br><span class="line">    "minor": "15",</span><br><span class="line">    "gitVersion": "v1.15.12",</span><br><span class="line">    "gitCommit": "e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725",</span><br><span class="line">    "gitTreeState": "clean",</span><br><span class="line">    "buildDate": "2020-05-06T05:17:59Z",</span><br><span class="line">    "goVersion": "go1.12.17",</span><br><span class="line">    "compiler": "gc",</span><br><span class="line">    "platform": "linux/arm64"</span><br><span class="line">  &#125;,</span><br><span class="line">  "serverVersion": &#123;</span><br><span class="line">    "major": "1",</span><br><span class="line">    "minor": "15",</span><br><span class="line">    "gitVersion": "v1.15.12",</span><br><span class="line">    "gitCommit": "e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725",</span><br><span class="line">    "gitTreeState": "clean",</span><br><span class="line">    "buildDate": "2020-05-06T05:09:48Z",</span><br><span class="line">    "goVersion": "go1.12.17",</span><br><span class="line">    "compiler": "gc",</span><br><span class="line">    "platform": "linux/arm64"</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>node 信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o wide</span></span><br><span class="line">NAME            STATUS   ROLES         AGE   VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                   KERNEL-VERSION            CONTAINER-RUNTIME</span><br><span class="line">172.18.27.252   Ready    master,node   32h   v1.15.12   172.18.27.252   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br><span class="line">172.18.27.253   Ready    master,node   32h   v1.15.12   172.18.27.253   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br><span class="line">172.18.27.254   Ready    master,node   32h   v1.15.12   172.18.27.254   &lt;none&gt;        Kylin Linux Advanced Server V10 (Tercel)   4.19.90-17.ky10.aarch64   docker://18.9.0</span><br></pre></td></tr></table></figure><p>coredns 信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span></span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-677d9c57f-pqvfv   1/1     Running   1          21h     10.187.0.5      172.18.27.253   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-677d9c57f-zjf86   1/1     Running   1          4h45m   10.187.1.12     172.18.27.252   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h3><p>命令都是在 <code>172.18.27.252</code> 上执行的，用 <code>dig @coredns_svc_ip +short kubernetes.default.svc.cluster1.local</code> 测发现时而能解析，时而不能解析，然后发现是跨节点的问题。</p><p>在 <code>172.18.27.252</code> 上去请求 <code>172.18.27.253</code> 上的 coredns 的 metrics 接口：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -I 10.187.0.5:9153/metrics</span><br></pre></td></tr></table></figure><p>然后在 <code>172.18.27.253</code> 上抓包:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.0.5 and port 9153</span></span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">^C</span><br><span class="line">0 packets captured</span><br><span class="line">0 packets received by filter</span><br><span class="line">0 packets dropped by kernel</span><br></pre></td></tr></table></figure><p>没有包，在 <code>172.18.27.253</code> 上抓了下 <code>8472</code> 端口是正常能收包的。像之前的那个文章 <a href="https://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/">银河麒麟 arm64 系统上 k8s 集群跨节点不通的一次排查</a> 看了下，253 机器上查看路由也没问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.1.0</span></span><br><span class="line">10.187.1.0 via 10.187.1.0 dev flannel.1 src 10.187.0.0 uid 0</span><br><span class="line">    cache</span><br></pre></td></tr></table></figure><p>当时各种手段看了个遍，结果有点眉目了（我应该像上篇文章一样先看下 vxlan 的 vtep 信息的。。。）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.252</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.253</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.254</span><br></pre></td></tr></table></figure><p>vtep 的 mac 地址都一样，查看下，发现三台机器都是一样的，看下网卡和 flannel.1 的信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip -4 a s enp1s0</span></span><br><span class="line">2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether dc:2d:cb:17:3e:a1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.27.252/25 brd 172.18.27.255 scope global noprefixroute enp1s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">394: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 1 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.252 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">#</span><span class="bash"> 另一台机器</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -4 a s enp1s0</span></span><br><span class="line">2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether dc:2d:cb:17:3e:86 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.27.254/25 brd 172.18.27.255 scope global noprefixroute enp1s0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">66: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.254 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>可以看到网卡 enp1s0 的 mac 是不一样的，查看了下几个机器的网卡配置文件的 UUID 和 HWADDR 都不一样。上面命令可以看出只有 flannel.1 的 MAC 是一样，尝试删除然后重启 flanneld 看看重建咋样：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip link <span class="built_in">set</span> flannel.1 down</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link <span class="built_in">set</span> flannel.1 up</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode none numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a |grep -m1 flanneld</span></span><br><span class="line">0aa5998260ba        122cdb7aa710                                "/opt/bin/flanneld -…"    5 hours ago          Up 3 hours                                          k8s_kube-flannel_kube-flannel-ds-2zqr9_kube-system_1ed6eba1-fa30-405e-83d0-314160c25313_1</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart 0aa</span></span><br><span class="line">0aa</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show  flannel.1</span></span><br><span class="line">22: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>还是一摸一样，好奇这个 mac 地址如何来的，就去看了下 <a href="https://github.com/coreos/flannel/blob/v0.11.0/backend/vxlan/vxlan.go#L104-L137" target="_blank" rel="noopener">flannel 添加 flannel.1 的源码部分</a>：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">devAttrs := vxlanDeviceAttrs&#123;</span><br><span class="line">vni:       <span class="keyword">uint32</span>(cfg.VNI),</span><br><span class="line">name:      fmt.Sprintf(<span class="string">"flannel.%v"</span>, cfg.VNI),</span><br><span class="line">vtepIndex: be.extIface.Iface.Index,</span><br><span class="line">vtepAddr:  be.extIface.IfaceAddr,</span><br><span class="line">vtepPort:  cfg.Port,</span><br><span class="line">gbp:       cfg.GBP,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dev, err := newVXLANDevice(&amp;devAttrs)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">dev.directRouting = cfg.DirectRouting</span><br><span class="line"></span><br><span class="line">subnetAttrs, err := newSubnetAttrs(be.extIface.ExtAddr, dev.MACAddr())</span><br></pre></td></tr></table></figure><p><code>dev.MACAddr()</code> 是直接 return 的 <code>dev.link.HardwareAddr</code>，goland 里 find usage 下压根没找到赋值的地方。（可以加代码打印下到底 mac 地址从哪里获取的，但是环境是远程的，得一次一次编译发过去太麻烦了我就没弄了）毫无头绪乱排查了一段时间。然后突发奇想手动按照 vxlan 创建的步骤测试添加下看看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip link add <span class="built_in">test</span> <span class="built_in">type</span> vxlan id 2 dev enp1s0 <span class="built_in">local</span> 10.186.0.0 dstport 8473 nolearning</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show <span class="built_in">test</span></span></span><br><span class="line">696: test: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ca:32:f1:0d:c6:dc brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 2 local 10.186.0.0 dev enp1s0 srcport 0 0 dstport 8473 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete <span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link add <span class="built_in">test</span> <span class="built_in">type</span> vxlan id 2 dev enp1s0 <span class="built_in">local</span> 10.186.0.0 dstport 8473 nolearning</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show <span class="built_in">test</span></span></span><br><span class="line">697: test: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether ca:32:f1:0d:c6:dc brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 2 local 10.186.0.0 dev enp1s0 srcport 0 0 dstport 8473 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>发现手动创建的网络设备 mac 居然也是一样的（实际上 ide 里找上面那个 <code>HardwareAddr</code> 的赋值是在引入的一个 <code>netlink</code> 包里赋值的，也就是说 flannel 创建接口的时候和手动添加类似，没有直接设置 mac 地址，而是系统返回的），然后同样步骤在我机器上测试是不一样的，看了下客户是啥服务器，发现居然是虚机。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /sys/class/dmi/id/product_name</span></span><br><span class="line">KVM Virtual Machine</span><br></pre></td></tr></table></figure><p>三台机器上添加接口的 mac 地址都是一样的，机器是不是克隆的？询问了下同事，同事说是的。其实这就是引起故障的根源，应该内部 mac 地址默认是不随机的策略。</p><p>查看下网卡策略是咋样的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> networkctl status enp1s0</span></span><br></pre></td></tr></table></figure><p>居然都为空，ubuntu 的机器上是有个优先级低的 link 文件的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status enp10s0</span></span><br><span class="line">● 2: enp10s0</span><br><span class="line">       Link File: /lib/systemd/network/99-default.link</span><br><span class="line">    Network File: n/a</span><br><span class="line">            Type: ether</span><br><span class="line">           State: n/a (unmanaged)</span><br><span class="line">            Path: platform-80040000000.pcie-controller-pci-0000:0a:00.0</span><br><span class="line">          Driver: igb</span><br><span class="line">          Vendor: Intel Corporation</span><br><span class="line">           Model: I210 Gigabit Network Connection</span><br><span class="line">      HW Address: 00:09:06:xx:xx:xx (Esteem Networks)</span><br><span class="line">         Address: 10.226.45.23</span><br><span class="line">                  fe80::209:xxx:xxxx:xxxx</span><br><span class="line">         Gateway: 10.226.45.254</span><br></pre></td></tr></table></figure><p>一些云主机我看也没这个文件，但是上面添加的接口每次 mac 不一样，应该其他 OS 改了默认的行为。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>我们可以用 systemd 写一个 link 配置文件改变策略：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat&lt;&lt;'EOF'&gt;/etc/systemd/network/10-flannel.link</span><br><span class="line">[Match]</span><br><span class="line">OriginalName=flannel*</span><br><span class="line"></span><br><span class="line">[Link]</span><br><span class="line">MACAddressPolicy=none</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一开始只添加上面的，结果重启后文件没了，下面的文件也追加了下，然后重启还是失效。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后上下两个步骤都整就行了。如果你也遇到了，先单独上面的文件试试，不行再下面的也加上试试。</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat&lt;&lt;<span class="string">'EOF'</span>&gt;&gt;/etc/systemd/networkd.conf</span></span><br><span class="line">[Match]</span><br><span class="line">OriginalName=flannel*</span><br><span class="line"></span><br><span class="line">[Link]</span><br><span class="line">MACAddressPolicy=none</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>确认生效文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> networkctl status flannel.1</span></span><br><span class="line">● 36: flannel.1                                              </span><br><span class="line">             Link File: /etc/systemd/network/10-flannel.link</span><br><span class="line">          Network File: n/a                                 </span><br><span class="line">                  Type: vxlan                               </span><br><span class="line">                 State: routable (unmanaged)   </span><br><span class="line">                Driver: vxlan                               </span><br><span class="line">            HW Address: fe:22:77:eb:2f:a4                   </span><br><span class="line">                   MTU: 1450 (min: 68, max: 65535)          </span><br><span class="line">                   VNI: 1                                   </span><br><span class="line">                 Local: 172.18.27.252                       </span><br><span class="line">      Destination Port: 8472                                </span><br><span class="line">     Underlying Device: enp1s0                              </span><br><span class="line">  Queue Length (Tx/Rx): 1/1                                 </span><br><span class="line">               Address: 10.187.1.0</span><br></pre></td></tr></table></figure><p>再测试下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether fe:22:77:eb:2f:a4 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip link delete flannel.1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a | grep -m1 flanneld</span></span><br><span class="line">f8759c103131        122cdb7aa710                                   "/opt/bin/flanneld -…"    27 minutes ago      Up 27 minutes                                   k8s_kube-flannel_kube-flannel-ds-85kps_kube-system_127265f3-f3ea-4f89-87e1-aa6c0e0d356f_0</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart f87</span></span><br><span class="line">f87</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip -d link show flannel.1</span></span><br><span class="line">36: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether 1a:e0:cc:e3:a7:04 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 172.18.27.253 dev enp1s0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure><p>果然变了，如果没变尝试把 <code>none</code> 改成 <code>random</code> 试试。然后每个节点这样操作后，查看下了下 vtep 信息正常了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"9a:1e:00:9d:0c:60"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.252</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"1a:e0:cc:e3:a7:04"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.253</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: '&#123;"VtepMAC":"fe:22:77:eb:2f:a4"&#125;'</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: "true"</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.18.27.254</span><br></pre></td></tr></table></figure><p>测试下跨节点通信：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.187.0.5:9153/metrics</span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Length: 16905</span><br><span class="line">Content-Type: text/plain; version=0.0.4; charset=utf-8</span><br><span class="line">Date: Fri, 06 Nov 2020 01:47:31 GMT</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>应该一开始就看下 vtep 信息的，其次应该是 os 的问题，缺少 link 文件。还有这个和 flannel 没关系，只要用到 Linux 自带的 vxlan 接口在这种场景上都会出现，例如 calico 新版本也有 vxlan 模式。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.jinbuguo.com/systemd/systemd.link.html" target="_blank" rel="noopener">systemd link</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;和前一篇文章不一样，从来没遇到过这样的问题，这里记录下。实施在客户那边部署业务后，业务在浏览器上无法访问，我远程上去查看日志发现 pod 内</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/categories/k8s/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>统信USO 20 hostPort 无法访问</title>
    <link href="http://zhangguanzhang.github.io/2020/10/30/uos20-nftables/"/>
    <id>http://zhangguanzhang.github.io/2020/10/30/uos20-nftables/</id>
    <published>2020-10-30T18:31:01.000Z</published>
    <updated>2021-01-06T02:03:24.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>这些天陆续发现很多客户是统信的系统，部署我们业务后无法访问</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>我自己环境和客户的环境都遇到了无法访问，我自己测试的机器信息是:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">PRETTY_NAME=&quot;Uniontech OS Server 20 Enterprise&quot;</span><br><span class="line">NAME=&quot;Uniontech OS Server 20 Enterprise&quot;</span><br><span class="line">VERSION_ID=&quot;20&quot;</span><br><span class="line">VERSION=&quot;20&quot;</span><br><span class="line">ID=UOS</span><br><span class="line">HOME_URL=&quot;https://www.chinauos.com/&quot;</span><br><span class="line">BUG_REPORT_URL=&quot;http://bbs.chinauos.com&quot;</span><br><span class="line">VERSION_CODENAME=fou</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxx-PC 4.19.0-arm64-server #1760 SMP Tue Jun 30 19:51:30 CST 2020 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>客户信息是:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">PRETTY_NAME=&quot;uos 20 SP1&quot;</span><br><span class="line">NAME=&quot;uos&quot;</span><br><span class="line">VERSION_ID=&quot;20 SP1&quot;</span><br><span class="line">VERSION=&quot;20 SP1&quot;</span><br><span class="line">ID=uos</span><br><span class="line">HOME_URL=&quot;https://www.chinauos.com/&quot;</span><br><span class="line">BUG_REPORT_URL=&quot;http://bbs.chinauos.com&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux kunpeng-PC 4.19.0-arm64-server #1707 SMP Thu Mar 26 17:43:52 CST 2020 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><p>k8s 版本信息:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:17:59Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:09:48Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>理论上<code>v1.15</code>以后的大版本号都不会有这种问题</p><h3 id="问题现象和解决手段"><a href="#问题现象和解决手段" class="headerlink" title="问题现象和解决手段"></a>问题现象和解决手段</h3><h4 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h4><p>我们业务入口是一个用的<code>hostPort</code>的 nginx ，部署好后无法访问，在宿主机上 curl 也会无法访问，同时<code>iptables</code>的条目会异常（下面的-m mark各个系统的先后可能顺序不同）:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> iptables -S</span></span><br><span class="line">...</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP</span><br><span class="line">.....</span><br><span class="line"><span class="meta">$</span><span class="bash"> iptables -S | grep <span class="string">&#x27;KUBE-FIREWALL&#x27;</span> | wc -l</span></span><br><span class="line">11054</span><br></pre></td></tr></table></figure><p>实际上这个问题就是因为统信系统基于<code>ubuntu</code>改的，很多新发行版系统包括centos8都是开始使用<code>nf_tables</code>作为rule规则管理，默认的<code>iptables</code>是<code>nf_tables</code>，低版本<code>kube-proxy</code>对它的兼容性不好，需要我们切换下<code>iptables</code>到老版本。</p><p>防止相关进程更新iptables，每台机器都得这样操作，先停掉相关进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop docker kubelet kube-proxy</span><br></pre></td></tr></table></figure><p>清空iptables</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">iptables -F</span><br><span class="line">iptables -t nat -F</span><br><span class="line">iptables -t mangle -F</span><br><span class="line">iptables -X</span><br><span class="line">kube-proxy --cleanup</span><br></pre></td></tr></table></figure><p>切换到老的iptables，apt系列的<code>update-alternatives</code>是系统自带的软连接管理，下面是把老的itables做成系统PATH的软连</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">update-alternatives --set iptables /usr/sbin/iptables-legacy</span><br><span class="line">update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy</span><br><span class="line">update-alternatives --set arptables /usr/sbin/arptables-legacy</span><br><span class="line">update-alternatives --set ebtables /usr/sbin/ebtables-legacy</span><br></pre></td></tr></table></figure><p>然后启动相关的，恢复正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker kubelet kube-proxy</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;这些天陆续发现很多客户是统信的系统，部署我们业务后无法访问&lt;/p&gt;
&lt;h3 id=&quot;环境信息&quot;&gt;&lt;a href=&quot;#环境信息&quot; class=</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="uos" scheme="http://zhangguanzhang.github.io/categories/k8s/uos/"/>
    
    
    <category term="uos" scheme="http://zhangguanzhang.github.io/tags/uos/"/>
    
  </entry>
  
  <entry>
    <title>银河麒麟arm64系统上k8s集群跨节点不通的一次排查</title>
    <link href="http://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/"/>
    <id>http://zhangguanzhang.github.io/2020/10/20/kylin-v10-k8s-overlay-error/</id>
    <published>2020-10-20T18:31:01.000Z</published>
    <updated>2021-01-12T03:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>同事在客户那边部署的集群问题频繁，先给他解决了个问题后又反映说业务 POD 由于 DNS 无法解析而启动失败，排查完发现这样的情况从没遇到过，挺有意思的，这里记录下。实际排查过程也有往错误的方向浪费了一些时间和尝试，就不写进来了，以正确的角度写下排查过程。</p><h3 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h3><p>集群信息:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl version -o json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;clientVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:17:59Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;serverVersion&quot;: &#123;</span><br><span class="line">    &quot;major&quot;: &quot;1&quot;,</span><br><span class="line">    &quot;minor&quot;: &quot;15&quot;,</span><br><span class="line">    &quot;gitVersion&quot;: &quot;v1.15.12&quot;,</span><br><span class="line">    &quot;gitCommit&quot;: &quot;e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725&quot;,</span><br><span class="line">    &quot;gitTreeState&quot;: &quot;clean&quot;,</span><br><span class="line">    &quot;buildDate&quot;: &quot;2020-05-06T05:09:48Z&quot;,</span><br><span class="line">    &quot;goVersion&quot;: &quot;go1.12.17&quot;,</span><br><span class="line">    &quot;compiler&quot;: &quot;gc&quot;,</span><br><span class="line">    &quot;platform&quot;: &quot;linux/arm64&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>OS 是 arm64 的银河麒麟系统</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/os-release</span></span><br><span class="line">NAME=&quot;Kylin Linux Advanced Server&quot;</span><br><span class="line">VERSION=&quot;V10 (Tercel)&quot;</span><br><span class="line">ID=&quot;kylin&quot;</span><br><span class="line">VERSION_ID=&quot;V10&quot;</span><br><span class="line">PRETTY_NAME=&quot;Kylin Linux Advanced Server V10 (Tercel)&quot;</span><br><span class="line">ANSI_COLOR=&quot;0;31&quot;</span><br><span class="line"><span class="meta">$</span><span class="bash"> uname -a</span></span><br><span class="line">Linux xxx 4.19.90-17.ky10.aarch64 #1 SMP Sun Jun 28 14:27:40 CST 2020 aarch64 aarch64 aarch64 GNU/Linux</span><br></pre></td></tr></table></figure><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><p>先看下集群 DNS 的 SVC IP。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get svc -l k8s-app=kube-dns</span></span><br><span class="line">NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">kube-dns             ClusterIP   10.186.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP       87m</span><br></pre></td></tr></table></figure><p>手动用 dig 发 DNS 请求看看，刚开始是用的<code>cluster.local</code>，后面感觉不对劲看了下 kubelet 的参数发现<code>cluster.domain</code>是<code>cluster1.local</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig @10.186.0.2 kubernetes.default.svc.cluster1.local +tcp</span></span><br><span class="line">;; Connection to 10.186.0.2#53(10.186.0.2) for kubernetes.default.svc.cluster1.local failed: timed out.</span><br><span class="line">;; Connection to 10.186.0.2#53(10.186.0.2) for kubernetes.default.svc.cluster1.local failed: timed out.</span><br></pre></td></tr></table></figure><p>超时，用 coredns 的 metrics 接口试试:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.186.0.2:9153/metrics</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>还是超时，看下 flannel 的 vtep 都正确</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -o yaml | grep -A3 Vtep</span></span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;ea:77:37:86:ee:bf&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.19</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;f2:d2:28:8e:4c:61&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.20</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;2a:f1:d4:d0:32:24&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.21</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;4a:e7:02:47:20:b8&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.22</span><br><span class="line">--</span><br><span class="line">      flannel.alpha.coreos.com/backend-data: &#x27;&#123;&quot;VtepMAC&quot;:&quot;ce:ce:f3:fc:3f:77&quot;&#125;&#x27;</span><br><span class="line">      flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">      flannel.alpha.coreos.com/kube-subnet-manager: &quot;true&quot;</span><br><span class="line">      flannel.alpha.coreos.com/public-ip: 172.31.159.23</span><br></pre></td></tr></table></figure><p>看下 coredns 的 pod ip，绕过集群 SVC 使用 pod ip 测试下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl -n kube-system get po -o wide -l k8s-app=kube-dns</span></span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-677d9c57f-tdnd4               1/1     Running   0          10m   10.187.1.24     172.31.159.21   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-677d9c57f-x274j               1/1     Running   0          10m   10.187.4.24     172.31.159.22   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -I 10.187.1.24:9153/metrics</span></span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>还是超时，继续上面的 curl ，因为是 curl 的 9153 ，它不是常见的端口，否则下文的 tcpdump 过滤条件太麻烦了。这里去目的主机上抓包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.1.24 and port 9153 -vv</span></span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:39:35.019165 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xe94e (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440201878 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:35.068097 IP (tos 0x0, ttl 64, id 39684, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.45920 &gt; 10.187.1.24.9153: Flags [S], cksum 0x80ed (correct), seq 1103099580, win 64860, options [mss 1410,sackOK,TS val 632716806 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:35.068241 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xe91c (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440201928 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:43.419197 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xc87e (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440210278 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:43.708101 IP (tos 0x0, ttl 64, id 39685, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.45920 &gt; 10.187.1.24.9153: Flags [S], cksum 0x5f2d (correct), seq 1103099580, win 64860, options [mss 1410,sackOK,TS val 632725446 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:43.708233 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0xc75c (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440210568 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:39:54.141929 IP (tos 0x0, ttl 64, id 12300, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0x0a5a (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632735880 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:54.142080 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xeb46 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440221001 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:55.148096 IP (tos 0x0, ttl 64, id 12301, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0x066c (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632736886 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:55.148381 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xe757 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440222008 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:56.219200 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xe329 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440223078 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:57.228103 IP (tos 0x0, ttl 64, id 12302, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0xfe4b (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632738966 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:39:57.228247 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xdf37 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440224088 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:39:59.259269 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xd748 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440226119 ecr 632735880,nop,wscale 7], length 0</span><br><span class="line">16:40:00.059221 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.45920: Flags [S.], cksum 0x877e (correct), seq 1670919335, ack 1103099581, win 64308, options [mss 1410,sackOK,TS val 3440226918 ecr 632709592,nop,wscale 7], length 0</span><br><span class="line">16:40:01.308098 IP (tos 0x0, ttl 64, id 12303, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.46388 &gt; 10.187.1.24.9153: Flags [S], cksum 0xee5b (correct), seq 3149899513, win 64860, options [mss 1410,sackOK,TS val 632743046 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:40:01.308248 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.46388: Flags [S.], cksum 0xcf47 (correct), seq 14530549, ack 3149899514, win 64308, options [mss 1410,sackOK,TS val 3440228168 ecr 632735880,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>可以看到回了包，但是报文的<code>Flags</code>都是<code>[S]</code>和<code>[S.]</code>，说明是 TCP 的 SYN 的报文重传了，回到 curl 的机器上，另开一个窗口抓包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tcpdump -nn -i flannel.1 host 10.187.1.24 and port 9153 -vv</span></span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:46:20.324596 IP (tos 0x0, ttl 64, id 7952, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.53748 &gt; 10.187.1.24.9153: Flags [S], cksum 0x29f7 (correct), seq 1340604575, win 64860, options [mss 1410,sackOK,TS val 633118295 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:46:20.324636 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xdf09 (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440603416 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:21.346975 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xdb0b (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440604438 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:21.395375 IP (tos 0x0, ttl 64, id 7953, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.53748 &gt; 10.187.1.24.9153: Flags [S], cksum 0x25c8 (correct), seq 1340604575, win 64860, options [mss 1410,sackOK,TS val 633119366 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:46:21.395409 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xdada (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440604487 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:23.426969 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.1.24.9153 &gt; 10.187.0.0.53748: Flags [S.], cksum 0xd2eb (correct), seq 1764271535, ack 1340604576, win 64308, options [mss 1410,sackOK,TS val 3440606518 ecr 633118295,nop,wscale 7], length 0</span><br><span class="line">16:46:23.475374 IP (tos 0x0, ttl 64, id 7954, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    10.187.0.0.53748 &gt; 10.187.1.24.9153: Flags [S], cksum 0x1da8 (correct), seq 1340604575, win 64860, options [mss 1410,sackOK,TS val 633121446 ecr 0,nop,wscale 7], length 0</span><br></pre></td></tr></table></figure><p>当时没详细的看上面的报文，这里来仔细分析下上面的报文，收到<code>10.187.1.24.9153</code>回复的报文里<code>seq</code>都是<code>1340604575</code>，从抓包现象看是这个握手包确实回来了，但是从<code>seq</code>的数字看是没有接收者，也是就是目的主机上 pod 一直 tcp 重传。查看了下路由:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat /run/flannel/subnet.env</span></span><br><span class="line">FLANNEL_NETWORK=10.187.0.0/16</span><br><span class="line">FLANNEL_SUBNET=10.187.0.1/24</span><br><span class="line">FLANNEL_MTU=1450</span><br><span class="line">FLANNEL_IPMASQ=true</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip a s flannel.1</span></span><br><span class="line">542: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default </span><br><span class="line">    link/ether b6:9b:ed:b0:37:74 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.187.0.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::b49b:edff:feb0:3774/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.0.0</span></span><br><span class="line">local 10.187.0.0 dev lo src 10.187.0.0 uid 0</span><br><span class="line">    cache &lt;local&gt;</span><br></pre></td></tr></table></figure><p>绝了，居然错了，莫名奇妙的是<code>lo</code>，看了下<code>NetworkManager</code>是开启的，重启了下它。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> systemctl restart NetworkManager</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.0.0</span></span><br><span class="line">broadcast 10.187.0.0 dev cni0 src 10.187.0.1 uid 0</span><br><span class="line">    cache &lt;local,brd&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.31.159.254  0.0.0.0         UG    100    0        0 eno1</span><br><span class="line">10.185.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0</span><br><span class="line">10.187.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0</span><br><span class="line">172.31.159.0    0.0.0.0         255.255.255.0   U     100    0        0 eno1</span><br></pre></td></tr></table></figure><p>路由正确了，但是 flannel 到其他节点的路由消失了，得重启下 flannel。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker ps -a | grep flanneld</span></span><br><span class="line">4b3f04e62b25        122cdb7aa710                                 &quot;/opt/bin/flanneld -…&quot;    2 hours ago         Up 2 hours                                             k8s_kube-flannel_kube-flannel-ds-22bwd_kube-system_6f5ce812-c5ae-4102-9398-c4a6fee4c7ab_0</span><br><span class="line"><span class="meta">$</span><span class="bash"> docker restart 4b3</span></span><br><span class="line">4b3</span><br><span class="line"><span class="meta">$</span><span class="bash"> route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.31.159.254  0.0.0.0         UG    100    0        0 eno1</span><br><span class="line">10.185.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0</span><br><span class="line">10.187.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0</span><br><span class="line">10.187.1.0      10.187.1.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">10.187.2.0      10.187.2.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">10.187.3.0      10.187.3.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">10.187.4.0      10.187.4.0      255.255.255.0   UG    0      0        0 flannel.1</span><br><span class="line">172.31.159.0    0.0.0.0         255.255.255.0   U     100    0        0 eno1</span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route get 10.187.0.0</span></span><br><span class="line">broadcast 10.187.0.0 dev cni0 src 10.187.0.1 uid 0</span><br><span class="line">    cache &lt;local,brd&gt;</span><br></pre></td></tr></table></figure><p>再 curl 下试试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -I 10.187.4.24:9153/metrics</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Content-Length: 19491</span><br><span class="line">Content-Type: text/plain; version=0.0.4; charset=utf-8</span><br><span class="line">Date: Tue, 20 Oct 2020 10:14:42 GMT</span><br></pre></td></tr></table></figure><p>然后每台机器上去操作了下，集群跨节点网络没有任何问题了。我们也有其他开了<code>NetworkManager</code>的 K8S 环境，但是麒麟系统上是头一次遇到这个</p><h3 id="个人对于-NetworkManager-的一些看法"><a href="#个人对于-NetworkManager-的一些看法" class="headerlink" title="个人对于 NetworkManager 的一些看法"></a>个人对于 NetworkManager 的一些看法</h3><p>这个东西我个人角度讲是感觉不成熟，之前有次同事用 nmcli 配置的掩码导致 VIP 失效，配置文件里是 PREFIX ，最后我改回 NETMASK 正常，其他的一些问题也有，这里不多说。它是一个 daemon 进程，但是现在 Linux 上的网络技术层出不穷，它并没有及时的适配好，而且更新发布缓慢。</p><p>2020/01/12 尝试了下面，在部署之前执行下面即可永久解决</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cat&gt;</span><span class="bash"> /etc/NetworkManager/conf.d/k8s.conf &lt;&lt; <span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">[keyfile]</span><br><span class="line">unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:flannel*;interface-name:veth*;interface-name:cni0;interface-name:docker0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>参考 <a href="https://docs.projectcalico.org/maintenance/troubleshoot/troubleshooting#configure-networkmanager">calico文档</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;同事在客户那边部署的集群问题频繁，先给他解决了个问题后又反映说业务 POD 由于 DNS 无法解析而启动失败，排查完发现这样的情况从没遇到过</summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/categories/k8s/Kylin/"/>
    
    
    <category term="Kylin" scheme="http://zhangguanzhang.github.io/tags/Kylin/"/>
    
  </entry>
  
  <entry>
    <title>openshift 4.5.9 离线安装</title>
    <link href="http://zhangguanzhang.github.io/2020/09/18/ocp-4.5-install/"/>
    <id>http://zhangguanzhang.github.io/2020/09/18/ocp-4.5-install/</id>
    <published>2020-09-18T11:14:06.000Z</published>
    <updated>2021-01-15T10:32:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是全部离线安装，也就是 UPI (User Provisioned Infrastructure) 模式安装，假设机器只能配置静态ip不能有网络配置权限和配置 dhcp 和 pxe。机器可以是物理机和虚拟机。</p><h2 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h2><h3 id="ocp介绍"><a href="#ocp介绍" class="headerlink" title="ocp介绍"></a>ocp介绍</h3><p>openshift 分为社区版本 okd 和企业版本 ocp(openshift container platform)，okd 的更新很慢，ocp 个人也是可以安装的，不购买 license 则不会享受 redhat 的官方支持。</p><p>openshift 不像其他的 dashboard 诸如 rancher，k3s 之类的，它自己实现了 cs 的三个组件，也给 k8s 贡献了 rbac 和 ingress 的代码。它的节点系统是使用 Red Hat Enterprise Linux CoreOS (RHCOS)，这是一款面向容器的操作系统，结合了 CoreOS 和 Red Hat Atomic Host 操作系统的一些最佳特性和功能。</p><p>RHCOS 是专门为从 OpenShift Container Platform 运行容器化应用程序而设计的，能够与新工具配合，提供快速安装、基于 Operator 的管理和简化的升级，ocp 里 master节点必须是 rhcos 系统，而 worker 节点除了 rhcos 以外还可以选择 rhel。并且集群里有大量的 operator，通过使用 k8s 的声明式 yaml 减少了对系统和集群底层的关注度，甚至集群升级也是通过 cluster-version-operator 完成。</p><h3 id="ocp安装流程"><a href="#ocp安装流程" class="headerlink" title="ocp安装流程"></a>ocp安装流程</h3><p>先看这个官方的图，从图来讲流程</p><p><img src="https://image-static.segmentfault.com/241/514/241514728-5ee6e815a4963_articlex" alt="ocp_install_pic"></p><p>它的安装是先在一台机器（bastion）上准备 pxe 和相关的安装集群描述信息需要的文件（Ignition）以及 dns，负载均衡，然后引导主机（<code>Bootstrap</code>）通过 dhcp 或者 人为挂载 rhcos 的 iso 在安装界面手写 boot cmdline 从 bastion 指定获取 <code>bootstrap.ign</code>和 os.raw.gz 文件完成安装， 随后 master 节点启动会获取<code>master.ign</code>文件并且从 bootstrap 节点获取 machine-config 信息，随后 node 同理。</p><p>在安装过程中 bootstrap 的 ign文件里证书是24小时过期的，因为官方这种涉及理念是 bootstrap 作用于引导集群的，安装完后会将控制平面移交到 master上，所以我们要配置负载均衡（以及代理后续集群里的 ingress controller）</p><ul><li>先在一台机器（bastion）上准备pxe和相关的安装集群描述信息需要的文件（Ignition）以及 dns，负载均衡，也就是图里的 <code>Cluster Installer</code></li><li><code>bastion</code>上手写一个集群安装的 yaml ，然后使用 oc 命令把 yaml 转换成集群部署清单和 Ignition 文件</li><li>在引导主机启动前准备好 DNS，负载均衡，镜像仓库</li><li>引导主机（<code>Bootstrap</code>）通过 dhcp 或者挂载 rhcos 的 iso 启动后在安装界面手写<code>boot cmdline</code>（包含网络配置，nameserver，install_dev,image_url,ignition_url），安装完系统后重启后，bootstrap 机器会执行 bootkube.sh 脚本，内部是 crio 和 podman 启动容器和 pod 来启动控制平面，并等待 master 加入</li><li><code>Master</code> 节点如果像我没有dhcp就手动配置boot cmdline 安装后会从引导主机远程获取资源并完成引导，会作为 node 注册。</li><li>bootstrap 节点 watch 到 node的注册后，会在集群里部署 operator 执行 一些 install-n-master的 pod，例如引导主机会让 master 节点构建 <code>Etcd</code> 集群。</li><li>引导主机使用新的 <code>Etcd</code> 集群启动临时 <code>Kubernetes</code> 控制平面。</li><li>临时控制平面在 Master 节点启动生成控制平面。</li><li>临时控制平面关闭并将控制权传递给生产控制平面。</li><li>引导主机将 OCP 组件注入生成控制平面。</li><li>整个过程主要是 bootstrap 上的 <code>bootkube.sh</code> 执行，最后执行完后会在集群里添加一个 <code>-n kube-system configmap/bootstrap</code> 作为保存状态。</li></ul><p>引导安装过程完成以后，OCP 集群部署完毕。然后集群开始下载并配置日常操作所需的其余组件，包括创建计算节点、通过 <code>Operator</code> 安装其他服务等。</p><h3 id="服务器规划"><a href="#服务器规划" class="headerlink" title="服务器规划"></a>服务器规划</h3><h4 id="资源规划"><a href="#资源规划" class="headerlink" title="资源规划"></a>资源规划</h4><p>官方文档<a href="https://docs.openshift.com/container-platform/4.5/installing/installing_bare_metal/installing-bare-metal-network-customizations.html#minimum-resource-requirements_installing-bare-metal-network-customizations">最小配置</a>信息为:</p><table><thead><tr><th>Machine</th><th>Operating System</th><th>vCPU</th><th>RAM</th><th>Storage</th></tr></thead><tbody><tr><td>Bootstrap</td><td>RHCOS</td><td>4</td><td>16GB</td><td>120 GB</td></tr><tr><td>Control plane</td><td>RHCOS</td><td>4</td><td>16 GB</td><td>120 GB</td></tr><tr><td>Compute</td><td>RHCOS or RHEL 7.6</td><td>2</td><td>8 GB</td><td>120 GB</td></tr></tbody></table><p>这里机器信息是如下，官方很多镜像都是存在<code>quay.io</code>这个仓库下，因为墙的问题会无法拉取，所以实际部署都会部署一个镜像仓库，这里我镜像仓库是使用 quay ，它会使用 443 端口(暂时没找到更改端口的方法)，负载均衡会负载 ingress controller 的 http 和 https（443端口）节点，所以镜像仓库单独一个机器，如果你用其他的仓库例如 registry 可以放在负载均衡的节点上。节点的配置和 role 如下关系，因为 bootstrap 用完后可以删除，可以后面用作 worker 节点。<strong>单 master 节点参照网上的我部署有问题，后文会说明原因</strong></p><p>服务器规划如下：</p><ul><li>三个控制平面节点，安装 <code>Etcd</code>、控制平面组件。</li><li>一个计算节点，运行实际负载。</li><li>一个引导主机，执行安装任务，集群部署完成后当作 worker 使用。</li><li>一个基础节点，用于准备上离线资源，同时用来部署 DNS 和负载均衡（负载<code>machine-config</code>、ocp 的 <code>kube-apiserver</code> 和 集群里的 <code>ingress controller</code>）。</li><li>一个镜像节点，用来部署私有镜像仓库 <code>Quay</code>，因为官方镜像在 <code>quay.io</code> 上很难拉取。</li><li>翻墙节点不在下面列表里，或者有软路由之类的可以在基础节点直接拉镜像更好</li></ul><table><thead><tr><th>Machine and hostName</th><th>OS</th><th>vCPU</th><th>RAM</th><th>Storage</th><th>IP</th><th>FQND</th><th>describe</th></tr></thead><tbody><tr><td>registry</td><td>Centos7.8</td><td>4</td><td>8GB</td><td>100 GB</td><td>10.226.45.226</td><td>registry.openshit4.example.com</td><td>镜像仓库</td></tr><tr><td>bastion</td><td>Centos7.8</td><td>8</td><td>8GB</td><td>100 GB</td><td>10.226.45.250</td><td>bastion.openshit4.example.com</td><td>dns，负载均衡(LB)，http文件下载</td></tr><tr><td>bootstrap</td><td>RHCOS</td><td>4</td><td>16GB</td><td>120 GB</td><td>10.226.45.223</td><td>bootstrap.openshit4.example.com</td><td>也叫引导节点(Bootstrap)</td></tr><tr><td>master1</td><td>RHCOS</td><td>8</td><td>16 GB</td><td>120 GB</td><td>10.226.45.251</td><td>master1.openshit4.example.com</td><td>也叫控制节点(Control plane)</td></tr><tr><td>master2</td><td>RHCOS</td><td>8</td><td>16 GB</td><td>120 GB</td><td>10.226.45.252</td><td>master2.openshit4.example.com</td><td>也叫控制节点(Control plane)</td></tr><tr><td>master3</td><td>RHCOS</td><td>8</td><td>16 GB</td><td>120 GB</td><td>10.226.45.222</td><td>master3.openshit4.example.com</td><td>也叫控制节点(Control plane)</td></tr><tr><td>worker1</td><td>RHCOS</td><td>4</td><td>16 GB</td><td>120 GB</td><td>10.226.45.223</td><td>worker1.openshit4.example.com</td><td>也叫计算节点(Compute)</td></tr></tbody></table><ul><li>这里 253 ip被抢占了，所以 master3 的ip选 222</li><li><code>openshift4</code> 是集群名，<code>example.com</code>是 basedomain</li><li>ocp内部的 kubernetes api、router、etcd通信都是使用域名，所以这里我们也给主机规定下<code>FQDN</code>，所有节点主机名都要采用三级域名格式，如 <code>master1.aa.bb.com</code>。后面会在 dns server里写入记录</li></ul><h4 id="防火墙端口"><a href="#防火墙端口" class="headerlink" title="防火墙端口"></a>防火墙端口</h4><p>接下来看一下每个节点的端口号分配。</p><p>所有节点（计算节点和控制平面）之间需要开放的端口：</p><table><thead><tr><th align="center">协议</th><th align="center">端口</th><th align="center">作用</th></tr></thead><tbody><tr><td align="center">ICMP</td><td align="center">N/A</td><td align="center">测试网络连通性</td></tr><tr><td align="center">TCP</td><td align="center"><code>9000-9999</code></td><td align="center">节点的服务端口，包括 node exporter 使用的 <code>9100-9101</code> 端口和 Cluster Version Operator 使用的 <code>9099</code> 端口</td></tr><tr><td align="center"></td><td align="center"><code>10250</code>-<code>10259</code></td><td align="center">Kubernetes 预留的默认端口</td></tr><tr><td align="center"></td><td align="center"><code>10256</code></td><td align="center">openshift-sdn</td></tr><tr><td align="center">UDP</td><td align="center"><code>4789</code></td><td align="center">VXLAN 协议或 GENEVE 协议的通信端口</td></tr><tr><td align="center"></td><td align="center"><code>6081</code></td><td align="center">VXLAN 协议或 GENEVE 协议的通信端口</td></tr><tr><td align="center"></td><td align="center"><code>9000</code>-<code>9999</code></td><td align="center">节点的服务端口，包括 node exporter 使用的 <code>9100-9101</code> 端口</td></tr><tr><td align="center"></td><td align="center"><code>30000</code>-<code>32767</code></td><td align="center">Kubernetes NodePort range</td></tr></tbody></table><p>控制平面需要向其他节点开放的端口：</p><table><thead><tr><th align="center">协议</th><th align="center">端口</th><th align="center">作用</th></tr></thead><tbody><tr><td align="center">TCP</td><td align="center"><code>2379</code>-<code>2380</code></td><td align="center">Etcd 服务端口</td></tr><tr><td align="center">TCP</td><td align="center"><code>6443</code></td><td align="center">Kubernetes API</td></tr></tbody></table><p>除此之外，还要配置两个四层负载均衡器，一个用来暴露集群 API，一个用来暴露 Ingress：</p><table><thead><tr><th align="center">端口</th><th align="center">作用</th><th align="center">内部</th><th align="center">外部</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center"><code>6443</code></td><td align="center">引导主机和控制平面使用。在引导主机初始化集群控制平面后，需从负载均衡器中手动删除引导主机</td><td align="center">x</td><td align="center">x</td><td align="center">Kubernetes API server</td></tr><tr><td align="center"><code>22623</code></td><td align="center">引导主机和控制平面使用。在引导主机初始化集群控制平面后，需从负载均衡器中手动删除引导主机</td><td align="center"></td><td align="center">x</td><td align="center">Machine Config server</td></tr><tr><td align="center"><code>443</code></td><td align="center">Ingress Controller 或 Router 使用</td><td align="center">x</td><td align="center">x</td><td align="center">HTTPS 流量</td></tr><tr><td align="center"><code>80</code></td><td align="center">Ingress Controller 或 Router 使用</td><td align="center">x</td><td align="center">x</td><td align="center">HTTP 流量</td></tr></tbody></table><h4 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h4><p>按照官方文档，使用 UPI 基础架构的 OCP 集群需要以下的 DNS 记录。在每条记录中，<code>&lt;cluster_name&gt;</code> 是集群名称，<code>&lt;base_domain&gt;</code> 是在 <code>install-config.yaml</code> 文件中指定的集群基本域，如下表所示：</p><table><thead><tr><th align="center">组件</th><th align="center">DNS记录</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">Kubernetes API</td><td align="center"><code>api.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">此 DNS 记录必须指向控制平面节点的负载均衡器。此记录必须可由集群外部的客户端和集群中的所有节点解析。</td></tr><tr><td align="center"></td><td align="center"><code>api-int.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">此 DNS 记录必须指向控制平面节点的负载均衡器。此记录必须可由集群外部的客户端和集群中的所有节点解析。</td></tr><tr><td align="center">Routes</td><td align="center"><code>*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">DNS 通配符记录，指向负载均衡器。这个负载均衡器的后端是 Ingress router 所在的节点，默认是计算节点。此记录必须可由集群外部的客户端和集群中的所有节点解析。</td></tr><tr><td align="center">etcd</td><td align="center"><code>etcd-&lt;index&gt;.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">OCP 要求每个 etcd 实例的 DNS 记录指向运行实例的控制平面节点。etcd 实例由 <index> 值区分，它们以 <code>0</code> 开头，以 <code>n-1</code> 结束，其中 <code>n</code> 是集群中控制平面节点的数量。集群中的所有节点必须都可以解析此记录。</td></tr><tr><td align="center"></td><td align="center"><code>_etcd-server-ssl._tcp.&lt;cluster_name&gt;.&lt;base_domain&gt;.</code></td><td align="center">因为 etcd 使用端口 <code>2380</code> 对外服务，因此需要建立对应每台 etcd 节点的 SRV DNS 记录，优先级 0，权重 10 和端口 2380</td></tr></tbody></table><h3 id="镜像准备"><a href="#镜像准备" class="headerlink" title="镜像准备"></a>镜像准备</h3><p>因为镜像都是在<code>quay.io</code>上，国内很难拉取下来，所以参考官方文档 <a href="https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html">Creating a mirror registry for installation in a restricted network</a> 创建个镜像仓库。**要求支持 version 2 schema 2 (manifest list)**，我这里选择的是 <code>Quay 3.3</code>。quay 镜像仓库需要部署在另外一台节点，因为它需要用到 <code>443</code> 端口，与后面的负载均衡 https 端口冲突。同时因为镜像是需要翻墙拉取，所以需要自备一台能翻墙的节点处于网络边界上。</p><h4 id="本地镜像仓库-registry"><a href="#本地镜像仓库-registry" class="headerlink" title="本地镜像仓库 - registry"></a>本地镜像仓库 - registry</h4><p>仓库没必要开始搭建，在下文的 oc 同步镜像之前搭建好即可</p><p>这里使用<code>docker-compose</code>搭建 quay 仓库，自行安装 <code>docker</code> 和 <code>docker-compose</code> 设置好系统和内核参数，如果你也使用容器搭建，可以不用和我一样的操作系统。容器已经能通过<code>alias</code>互联，所以没必要的端口不用映射到宿主机上，你也可以使用 <code>podman</code> 或者其他容器工具起一个环境，甚至<code>docker run</code>起来这些容器。</p><p>设置机器的 hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname registry.openshit4.example.com</span><br></pre></td></tr></table></figure><h5 id="docker-compose-for-registry"><a href="#docker-compose-for-registry" class="headerlink" title="docker-compose for registry"></a>docker-compose for registry</h5><p>这里数据都存放在<code>/data/quay/xxx</code>，创建目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/quay/lib/mysql \</span><br><span class="line">  /data/quay/lib/redis \</span><br><span class="line">  /data/quay/config \</span><br><span class="line">  /data/quay/storage</span><br><span class="line"><span class="meta">#</span><span class="bash"> 容器权限问题，容器的user都在root组下，但是默认<span class="built_in">umask</span>下组没有w权限，所以这里加下</span></span><br><span class="line">chmod g+w /data/quay/lib/mysql/ \</span><br><span class="line">  /data/quay/lib/redis/ \</span><br><span class="line">  /data/quay/config \</span><br><span class="line">  /data/quay/storage</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>创建quay仓库的<code>docker-compose.yml</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cat&gt;</span><span class="bash">/data/quay/docker-compose.yml &lt;&lt; <span class="string">EOF</span></span></span><br><span class="line">version: &#x27;3.2&#x27;</span><br><span class="line">services:</span><br><span class="line">  quay:</span><br><span class="line">    #image: quay.io/redhat/quay:v3.3.1</span><br><span class="line">    image: registry.aliyuncs.com/quayx/redhat-quay:v3.3.1</span><br><span class="line">    container_name: quay</span><br><span class="line">    restart: always</span><br><span class="line">    privileged: true</span><br><span class="line">    sysctls:</span><br><span class="line">      - net.core.somaxconn=1024</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/quay/config:/conf/stack:Z</span><br><span class="line">      - /data/quay/storage:/datastorage:Z</span><br><span class="line">    ports:</span><br><span class="line">      - 443:8443</span><br><span class="line">    command: [&quot;config&quot;, &quot;redhat&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - mysql</span><br><span class="line">      - redis</span><br><span class="line">    networks:</span><br><span class="line">      quay:</span><br><span class="line">        aliases:</span><br><span class="line">          - config</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">  mysql:</span><br><span class="line">    image: registry.access.redhat.com/rhscl/mysql-57-rhel7</span><br><span class="line">    container_name: quay-mysql</span><br><span class="line">    restart: always</span><br><span class="line">    privileged: true</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/quay/lib/mysql:/var/lib/mysql/data:Z</span><br><span class="line">    # ports:</span><br><span class="line">    #   - 3306:3306</span><br><span class="line">    environment:</span><br><span class="line">      - MYSQL_ROOT_PASSWORD=redhat</span><br><span class="line">      - MYSQL_DATABASE=enterpriseregistrydb</span><br><span class="line">      - MYSQL_USER=quayuser</span><br><span class="line">      - MYSQL_PASSWORD=redhat</span><br><span class="line">    networks:</span><br><span class="line">      quay:</span><br><span class="line">        aliases:</span><br><span class="line">          - mysql</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">  redis:</span><br><span class="line">    image: registry.access.redhat.com/rhscl/redis-32-rhel7</span><br><span class="line">    container_name: quay-redis</span><br><span class="line">    restart: always</span><br><span class="line">    privileged: true</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/quay/lib/redis:/var/lib/redis/data:Z</span><br><span class="line">    networks:</span><br><span class="line">      quay:</span><br><span class="line">        aliases:</span><br><span class="line">          - redis</span><br><span class="line">    # ports:</span><br><span class="line">    #   - 6379:6379</span><br><span class="line">    depends_on:</span><br><span class="line">      - mysql</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">networks:</span><br><span class="line">  quay:</span><br><span class="line">    name: quay</span><br><span class="line">    external: false</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><strong>数据库</strong> : 主要存放镜像仓库的元数据（非镜像存储)</li><li><strong>Redis</strong> : 存放构建日志和Quay的向导</li><li><strong>Quay</strong> : 作为镜像仓库</li><li><strong>Clair</strong> : 提供镜像扫描功能</li></ul><p>注意，其中的镜像<code>quay.io/redhat/quay:v3.3.1</code>是无法拉取的，参考 <a href="https://access.redhat.com/solutions/3533201">官方链接</a> 获取Red Hat Quay v3 镜像的访问权才可以拉取</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u=&quot;redhat+quay&quot; -p=&quot;O81WSHRSJR14UAZBK54GQHJS0P1V4CLWAJV1X2C4SD7KO59CQ9N3RE12612XU1HR&quot; quay.io</span><br></pre></td></tr></table></figure><p>这个镜像我已经同步到阿里云上的镜像仓库上，也方便拉取，另外这个镜像的运行命令 <code>config redhat</code>的 <code>redhat</code> 是 quay 仓库起来后的 web 里用到的密码，这里我们先拉取上面所需要的镜像。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay</span><br><span class="line">docker-compose pull</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>后续还会部署的话推荐这里使用 docker 把这三个镜像<code>docker save -o</code>成一个tar包方便以后<code>docker load -i</code>导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay</span><br><span class="line">docker save \</span><br><span class="line">  registry.access.redhat.com/rhscl/redis-32-rhel7 \</span><br><span class="line">  registry.access.redhat.com/rhscl/mysql-57-rhel7 \</span><br><span class="line">  registry.aliyuncs.com/quayx/redhat-quay:v3.3.1 | gzip - &gt; quay-img.tar.gz</span><br></pre></td></tr></table></figure><h5 id="setup-for-registry"><a href="#setup-for-registry" class="headerlink" title="setup for registry"></a>setup for registry</h5><p>起来后访问<code>https://ip</code> basic auth 信息为<code>quayconfig/redhat</code>，选择<code>Start New Registry Setup</code></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531150958.png" alt="choose an option"></p><p>选择新建配置，然后设置数据库：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531151305.png" alt="setup for rds"></p><p>连接信息按照上面的<code>docker-compose</code>里的环境变量写，<code>ssl certificate</code>先别管。</p><p>设置超级管理员，记住密码，然后下一步</p><p>下一步然后页面往下滑动，在<code>Server Configuration</code>段里设置<code>Server Hostname</code>，例如为<code>registry.openshift4.example.com</code>，往下滑动，配置redis信息</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531152811.png" alt="Server Configuration"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531152931.png" alt="redis"></p><p>点击左下角的Save，弹出的<code>Checking</code>全绿后点击<code>Next</code></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531153820.png" alt="checking your settings"></p><p>配置检查通过后，就可以保存下载下来：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200531154244.png" alt="Download Configuration"></p><p>最后会导出一个 <code>quay-config.tar.gz</code>，将其上传到 Quay 所在的服务器，解压到配置文件目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp quay-config.tar.gz /data/quay/config/</span><br><span class="line">cd /data/quay/config/</span><br><span class="line">tar zxvf quay-config.tar.gz</span><br></pre></td></tr></table></figure><h5 id="ssl-for-registry"><a href="#ssl-for-registry" class="headerlink" title="ssl for registry"></a>ssl for registry</h5><p>接下来为仓库生成域名自签名证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay/config/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成私钥</span></span><br><span class="line">openssl genrsa -out ssl.key 1024</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 生成证书，最好使用通配符</span></span><br><span class="line">openssl req \</span><br><span class="line">  -newkey rsa:2048 -nodes -keyout ssl.key \</span><br><span class="line">  -x509 -days 36500 -out ssl.cert -subj \</span><br><span class="line">  &quot;/C=CN/ST=Wuhan/L=Wuhan/O=WPS/OU=WPS/CN=*.openshift4.example.com&quot;</span><br></pre></td></tr></table></figure><p>证书搞定后<code>PREFERRED_URL_SCHEME: http</code>修改成 https</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -ri &#x27;/^PREFERRED_URL_SCHEME:/s#\S+$#https#&#x27; config.yaml</span><br></pre></td></tr></table></figure><p>然后停掉服务，注释掉 command 后再启动，web 打开看看是不是镜像仓库，是的话本机添加下 hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /data/quay/</span><br><span class="line">docker-compose down</span><br><span class="line">sed -ri &#x27;/^\s*command: \[&quot;config&quot;/s@^@#@&#x27; docker-compose.yml</span><br><span class="line">docker-compose up -d</span><br><span class="line">grep -qw &#x27;registry.openshift4.example.com&#x27; /etc/hosts || </span><br><span class="line">    echo &#x27;127.0.0.1 registry.openshift4.example.com&#x27; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>去浏览器上web登录下镜像仓库，添加一个<code>Organization</code>，名字为<code>ocp4</code>用于存放镜像，然后添加一个<code>Repository</code>名字为<code>openshift4</code></p><h3 id="纵云梯节点配置"><a href="#纵云梯节点配置" class="headerlink" title="纵云梯节点配置"></a>纵云梯节点配置</h3><h4 id="配置registry-的-hosts"><a href="#配置registry-的-hosts" class="headerlink" title="配置registry 的 hosts"></a>配置registry 的 hosts</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -qw &#x27;registry.openshift4.example.com&#x27; /etc/hosts || </span><br><span class="line">    echo &#x27;10.226.45.226 registry.openshift4.example.com&#x27; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h4 id="二进制文件和-secret-json-准备"><a href="#二进制文件和-secret-json-准备" class="headerlink" title="二进制文件和 secret.json 准备"></a>二进制文件和 secret.json 准备</h4><p>拉取镜像是在翻墙的节点上使用的<code>openshift client</code>的 oc 二进制 cli ，该命令大部分的子命令在操作集群的时候和 kubectl 是一致的。此节需要我们在纵云梯节点上执行oc 命令执行去把镜像拉取推送到本地的 registry 上。我们先准备拉取镜像用到的一些前置文件</p><p>首先是 oc 下载，很多机器都需要，官方说 这个<a href="https://cloud.redhat.com/openshift/install">页面</a>下载，我们也可以去<a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-4.5/">mirror页面</a>  下载。解压后放到系统的 <code>$PATH</code> 里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-4.5/openshift-client-linux-4.5.9.tar.gz</span><br><span class="line">tar zxvf openshift-client-linux-4.5.9.tar.gz</span><br><span class="line">mv oc kubectl /usr/local/bin</span><br></pre></td></tr></table></figure><p>安装一些基础小工具，jq 用来格式化 json 文件，chrony 用于时间同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release &amp;&amp; \</span><br><span class="line">  yum install \</span><br><span class="line">  jq \</span><br><span class="line">  bind-utils \</span><br><span class="line">  tcpdump \</span><br><span class="line">  chrony \</span><br><span class="line">  httpd-tools \</span><br><span class="line">  dos2unix \</span><br><span class="line">  strace</span><br></pre></td></tr></table></figure><p>准备拉取镜像权限认证文件，很多镜像在好几个镜像仓库上，有授权信息才能拉取。 从 Red Hat OpenShift Cluster Manager 站点的 Pull Secret 页面下载 <a href="https://cloud.redhat.com/openshift/install/pull-secret">registry.redhat.io</a> 的 pull secret</p><p>把文件或者内容整上去后，格式化下json</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jq . pull-secret.txt &gt; pull-secret.json</span><br></pre></td></tr></table></figure><p>大致下面的内容</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;auths&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;cloud.openshift.com&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;b3BlbnNo...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;quay.io&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;b3BlbnNo...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;registry.connect.redhat.com&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;NTE3Njg5Nj...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;registry.redhat.io&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;NTE3Njg5Nj...&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;email&quot;</span>: <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>把前面 quay 的用户名和密码按照<code>user:pass</code> base64 加密了，例如<code>echo -n admin:openshift | base64</code>， 然后符合 json 的格式要求下把镜像仓库和 auth 信息追加到<code>pull-secret.json</code>里</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    &quot;registry.openshift4.example.com&quot;: &#123;</span><br><span class="line">      &quot;auth&quot;: &quot;......==&quot;</span><br><span class="line">    &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>增加后用 <code>jq . pull-secret.json</code> 检验下 json 格式是否正确</p><p>下面利用变量拼接一些镜像tag来同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9</span></span><br><span class="line">OCP_VERSION=$(oc version -o json | jq -r .releaseClientVersion)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9-x86_64</span></span><br><span class="line">OCP_RELEASE=$OCP_VERSION-$(arch)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4</span></span><br><span class="line">OCP_MAJOR=$&#123;OCP_VERSION%%.*&#125;</span><br><span class="line"></span><br><span class="line">LOCAL_REGISTRY=&#x27;registry.openshift4.example.com&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 对应前面的 ocp4/openshift4</span></span><br><span class="line">LOCAL_REPOSITORY=&quot;ocp$&#123;OCP_MAJOR&#125;/openshift$&#123;OCP_MAJOR&#125;&quot;</span><br><span class="line"></span><br><span class="line">PRODUCT_REPO=&#x27;openshift-release-dev&#x27;</span><br><span class="line">RELEASE_NAME=&quot;ocp-release&quot;</span><br><span class="line"></span><br><span class="line">LOCAL_SECRET_JSON=&#x27;pull-secret.json&#x27;</span><br></pre></td></tr></table></figure><ul><li><strong>OCP_RELEASE</strong> : OCP 版本，可以在<a href="https://quay.io/repository/openshift-release-dev/ocp-release?tab=tags">这个页面</a>查看。如果版本不对，下面执行 <code>oc adm</code> 时会提示 <code>image does not exist</code>。</li><li><strong>LOCAL_REGISTRY</strong> : 本地仓库的域名和端口。</li><li><strong>LOCAL_REPOSITORY</strong> : 镜像存储库名称，使用 <code>ocp4/openshift4</code>。</li><li><code>PRODUCT_REPO</code> 和 <code>RELEASE_NAME</code> 都不需要改，这些都是一些版本特征，保持不变即可。</li><li><strong>LOCAL_SECRET_JSON</strong> : 密钥路径，就是上面 <code>pull-secret.json</code> 的存放路径。</li></ul><h4 id="同步镜像"><a href="#同步镜像" class="headerlink" title="同步镜像"></a>同步镜像</h4><p>同步镜像分为两种方式，一种是实时使用 oc 命令把镜像转发到 <code>LOCAL_REGISTRY</code>，一种是离线存储为文件，然后把文件拿到内网去用 oc 命令推送到内网的仓库。</p><p>一些后续可研究的东西:<br>如果后续有需求研究同步镜像到国内，每个版本的镜像列表可以查看<a href="https://github.com/openshift/cluster-version-operator/blob/master/docs/user/reconciliation.md">官方这个文档</a>里的命令解开镜像去查看镜像列表：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看镜像info</span></span><br><span class="line">oc adm release info -a $&#123;LOCAL_SECRET_JSON&#125;\</span><br><span class="line"> quay.io/openshift-release-dev/ocp-release:4.5.9-x86_64</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解开到本地查看</span></span><br><span class="line">oc image extract -a $&#123;LOCAL_SECRET_JSON&#125; \</span><br><span class="line">  quay.io/openshift-release-dev/ocp-release:4.5.9-x86_64 \</span><br><span class="line">  --path /:/tmp/release</span><br></pre></td></tr></table></figure><p>如果想解开这个镜像研究的话，我已经把这个镜像同步到阿里上了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">skopeo copy  docker://quay.io/openshift-release-dev/ocp-release:4.5.9-x86_64 \</span><br><span class="line">  docker://registry.aliyuncs.com/openshift-release-dev/ocp-release:4.5.9-x86_64   \</span><br><span class="line">  --insecure-policy</span><br></pre></td></tr></table></figure><h5 id="实时转发到-LOCAL-REGISTRY"><a href="#实时转发到-LOCAL-REGISTRY" class="headerlink" title="实时转发到 LOCAL_REGISTRY"></a>实时转发到 LOCAL_REGISTRY</h5><p>添加registry的hosts到同步的机器上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -qw &#x27;registry.openshift4.example.com&#x27; /etc/hosts || </span><br><span class="line">    echo &#x27;10.226.45.226 registry.openshift4.example.com&#x27; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>执行同步命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这里之前打算同步到阿里云仓库上，但是一直报错，以后有空的时候再研究下看怎么同步到阿里的镜像仓库上</span></span><br><span class="line">oc adm -a $&#123;LOCAL_SECRET_JSON&#125; release mirror \</span><br><span class="line">  --from=quay.io/$&#123;PRODUCT_REPO&#125;/$&#123;RELEASE_NAME&#125;:$&#123;OCP_RELEASE&#125; \</span><br><span class="line">  --to=$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125; \</span><br><span class="line">  --to-release-image=$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125;:$&#123;OCP_RELEASE&#125; \</span><br><span class="line">  --insecure # 镜像仓库是自签名证书，所以加这个选项</span><br></pre></td></tr></table></figure><p>下面是输出，可以临时保存下:</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">info: Mirroring 110 images to registry.openshift4.example.com/ocp4/openshift4 ...</span><br><span class="line">registry.openshift4.example.com/</span><br><span class="line">  ocp4/openshift4</span><br><span class="line">    manifests:</span><br><span class="line">      sha256:00edb6c1dae03e1870e1819b4a8d29b655fb6fc40a396a0db2d7c8a20bd8ab8d -&gt; 4.5.9-local-storage-static-provisioner</span><br><span class="line">      sha256:0259aa5845ce43114c63d59cedeb71c9aa5781c0a6154fe5af8e3cb7bfcfa304 -&gt; 4.5.9-machine-api-operator</span><br><span class="line">      sha256:07f11763953a2293bac5d662b6bd49c883111ba324599c6b6b28e9f9f74112be -&gt; 4.5.9-cluster-kube-storage-version-migrator-operator</span><br><span class="line">      sha256:08068503746a2dfd4dcaf61dca44680a9a066960cac7b1e81ea83e1f157e7e8c -&gt; 4.5.9-service-ca-operator</span><br><span class="line">      sha256:09a7dea10cd584c6048f8df3dcec67dd9a8432eb44051353e180dfeb350c6310 -&gt; 4.5.9-cluster-node-tuning-operator</span><br><span class="line">      sha256:09c763b2ad50711c2b1b5f26c9c70875f821f460e7214f1b11c4be8374424f42 -&gt; 4.5.9-kube-client-agent</span><br><span class="line">      sha256:1140eff8442d5ac679a3a85655723cd51f12a63c0c8d61251214ff952cc40b06 -&gt; 4.5.9-container-networking-plugins</span><br><span class="line">      sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd -&gt; 4.5.9-cluster-etcd-operator</span><br><span class="line">      sha256:14f97b5e9195c8f9b682523dc1febbb1f75fd60d408ac2731cdfa047aee0f43d -&gt; 4.5.9-must-gather</span><br><span class="line">      sha256:15be0e6de6e0d7bec726611f1dcecd162325ee57b993e0d886e70c25a1faacc3 -&gt; 4.5.9-openshift-controller-manager</span><br><span class="line">      sha256:167c14d56714982360fbaa72b3eb3f8d4fb6c5ad10bbbb7ea396606d8412b7eb -&gt; 4.5.9-sdn</span><br><span class="line">      sha256:1960c492ce80345e03d50c6cada3d37341ca604d891c48b03109de525e99997a -&gt; 4.5.9-tests</span><br><span class="line">      sha256:1c24c01e807700d30430ce4ff2f77b54d55910cfbf9ee0cbe5c1c64979547463 -&gt; 4.5.9-prometheus-operator</span><br><span class="line">      sha256:1cf4b3ce90933a7dbef9d01ebcb00a7f490dce7c44de6ddb16ddebad07de8eda -&gt; 4.5.9-k8s-prometheus-adapter</span><br><span class="line">      sha256:1d73207be4fbba735dbb53facd23c2359886a9194df3267fd91a843a7ba10316 -&gt; 4.5.9-console-operator</span><br><span class="line">      sha256:203ab5955dbcbc5f3334e1d7f2ba9fac5f7a1e187370e88c08ad2996a8507711 -&gt; 4.5.9-aws-pod-identity-webhook</span><br><span class="line">      sha256:21fdd7329f0d9d85acb935223915fbb3af07d4ea051678fc8ec85e47ab95edb1 -&gt; 4.5.9-mdns-publisher</span><br><span class="line">      sha256:22e47de98549ac9e792f06d4083b5d99db3ecc8dc4c94c8950dba9756a75dc28 -&gt; 4.5.9-grafana</span><br><span class="line">      sha256:2527b9f6712b8551cbcec637fc87bb9640973e9f9f4972d489c5308ef25eeed0 -&gt; 4.5.9-jenkins</span><br><span class="line">      sha256:273585703f8f56c980ff77f9a71e6e380d8caa645c4101525572906e0820e8fc -&gt; 4.5.9-docker-builder</span><br><span class="line">      sha256:27774942e605c4d4b13da7721c75135aac27db8409862625b76f5b31eb2e0d63 -&gt; 4.5.9-keepalived-ipfailover</span><br><span class="line">      sha256:2899afae1d0d9baa0309944dd55975a195a2cbd211f9ecf574e39287827fa684 -&gt; 4.5.9-ovirt-machine-controllers</span><br><span class="line">      sha256:289d792eada5d1460d69a927dff31328b5fcb1f1d2e31eb7a3fd69afd9a118fc -&gt; 4.5.9-kuryr-cni</span><br><span class="line">      sha256:2b40c4a5cb2a9586ec6c8e43e3013e6ee97781399b2fa836cf8d6c181ff8430c -&gt; 4.5.9-jenkins-agent-maven</span><br><span class="line">      sha256:2be07ebc1d005decbb8e624e9beb4d282900f42eb4f8ec0b76e31daedd8874cc -&gt; 4.5.9-installer-artifacts</span><br><span class="line">      sha256:3161e52e7bbbf445170c4992d5a47ed87c1d026a7f94b8d0cd30c4508fb48643 -&gt; 4.5.9-oauth-server</span><br><span class="line">      sha256:36e7c99acba49c22c7b84a339bfdab820fae17dec07cb4a17db18617b1d46a37 -&gt; 4.5.9-kube-etcd-signer-server</span><br><span class="line">      sha256:39f8b615e82b3a3a087d6f7d301303d8a0862e4ca13e8f99e6ad968a04985f80 -&gt; 4.5.9-cli</span><br><span class="line">      sha256:3b893950a9db5aa3653d80b05b236e72c168ff368d1c2e0192dd6036486eb715 -&gt; 4.5.9-cluster-version-operator</span><br><span class="line">      sha256:3bbdc631a9271d45960e9db061bde9bc87c6b1df3ec58a17f81dd7326065b4dd -&gt; 4.5.9-prom-label-proxy</span><br><span class="line">      sha256:3c45fb79851e8498666470739c452535d16fde408f54461cc3e239608ae55943 -&gt; 4.5.9-ironic-ipa-downloader</span><br><span class="line">      sha256:3ec062eef4a908c7ce7ee01222a641505f9ccb45e4864f4d3c40a2c3160928f2 -&gt; 4.5.9-azure-machine-controllers</span><br><span class="line">      sha256:416ad8f3ddd49adae4f7db66f8a130319084604296c076c2a6d22264a5688d65 -&gt; 4.5.9-cluster-bootstrap</span><br><span class="line">      sha256:43fdf850ddbbad7b727eff2850d1886a69b6efca837ff8a35d097c1f15f0922c -&gt; 4.5.9-aws-machine-controllers</span><br><span class="line">      sha256:45ec998707309535437f03a2f361ea4c660744c41926258f6a42b566228fe59a -&gt; 4.5.9-jenkins-agent-nodejs</span><br><span class="line">      sha256:486ed094bac19bd3be37eff728596319f73e5648cfb49742ef8c1a859db15e55 -&gt; 4.5.9-operator-lifecycle-manager</span><br><span class="line">      sha256:4a547f79252b06c0fd9fa112d4bca0cd2e5958f471e9a872e33f4e917c1ebccd -&gt; 4.5.9-cluster-update-keys</span><br><span class="line">      sha256:4b15cf622173dedc8ab30dd1d81c7a4ffe2317fce9fd179bb26b04167604bfab -&gt; 4.5.9-kube-state-metrics</span><br><span class="line">      sha256:4d10b546ad2eb8bf67b50217e441d3b01d219ceb96bdf31855a5199fc23c29c7 -&gt; 4.5.9-cluster-autoscaler-operator</span><br><span class="line">      sha256:4d4f72c1556d2085c55691bddf5fadea7bb14ba980fdc28f02146b090134f3a6 -&gt; 4.5.9-cluster-svcat-apiserver-operator</span><br><span class="line">      sha256:4f99f1a5d8a5b7789016316ab33319e5d8b1b27b42ccfb974eb98b27968683b5 -&gt; 4.5.9-prometheus-alertmanager</span><br><span class="line">      sha256:521721a7d0d298eb361d550c08f682942323a68d07e580450e2a64cbfab54880 -&gt; 4.5.9-baremetal-machine-controllers</span><br><span class="line">      sha256:52a566dabf19f82f3fba485807784bd34b2b6a03539d0a2c471ea283ee601e62 -&gt; 4.5.9-openstack-machine-controllers</span><br><span class="line">      sha256:5f77b35d6095068e685cd389f5aeea4b3fad82e771957663839baaa331859eee -&gt; 4.5.9-libvirt-machine-controllers</span><br><span class="line">      sha256:62b44f524d9b820855629cb01bc8c7fff79a039a72f3b421761e1a65ea621b13 -&gt; 4.5.9-baremetal-runtimecfg</span><br><span class="line">      sha256:65dc7fc90223061deec270ecdfa365d509950a1efa4b9025c9355d02b8c22f9b -&gt; 4.5.9-cluster-machine-approver</span><br><span class="line">      sha256:66e8675a73707d519467b3598259587de325315e186a839c233f3659f58be534 -&gt; 4.5.9-ironic</span><br><span class="line">      sha256:6b4bc1c8fa3e762d70a837f63f660e4ff3e129015d35c82a7b6da0c6fb7919da -&gt; 4.5.9-kube-rbac-proxy</span><br><span class="line">      sha256:708de132d6a5a6c15c0b7f04f572cd85ff67b1733c5235d74b74f3a690a98ce7 -&gt; 4.5.9-ironic-machine-os-downloader</span><br><span class="line">      sha256:70c3c46df383aa123ffd0a09f18afcc92e894b01cb9edc4e42cdac9b4a092fa1 -&gt; 4.5.9-cluster-svcat-controller-manager-operator</span><br><span class="line">      sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4 -&gt; 4.5.9-cluster-ingress-operator</span><br><span class="line">      sha256:755c6cd68730ad7f72950be4110c79b971403864cb9bf8f211edf4d35858b2e6 -&gt; 4.5.9-multus-cni</span><br><span class="line">      sha256:792a91df9bb7c4fb49f01c0a70479cc356aa324082ad869bf141a7ff98f51f5a -&gt; 4.5.9-deployer</span><br><span class="line">      sha256:7a56d91d1e0aeddc46dce7fcfda56d4430321e14563e10207893a360a87d3111 -&gt; 4.5.9-ironic-inspector</span><br><span class="line">      sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425 -&gt; 4.5.9-x86_64</span><br><span class="line">      sha256:80cb8dd15ae02c81ba09387560eb3edb8a12645b3a3179620038df340f51bd54 -&gt; 4.5.9-thanos</span><br><span class="line">      sha256:84cb2378e2115c1fba70f6cb7ccf053ad848a2038a38c2f5ab45a2e5f21d871c -&gt; 4.5.9-console</span><br><span class="line">      sha256:87037e2988ca6fee330729017c4ecb164f1a6f68e9eb4edacc976c3efb9515fd -&gt; 4.5.9-insights-operator</span><br><span class="line">      sha256:8bd68e0f18cd6ba0a6f12848f3edb630fcca9090a272aa58e3818a6382067e52 -&gt; 4.5.9-kuryr-controller</span><br><span class="line">      sha256:8c25f463d04079a8a93791c307ff893264855d60bbb8ba7a54179d9d44fc2f9f -&gt; 4.5.9-openshift-state-metrics</span><br><span class="line">      sha256:8ee14d942d7a971f52147b15fbd706844e250bd6c72c42150d92fc53cc826111 -&gt; 4.5.9-prometheus-config-reloader</span><br><span class="line">      sha256:95e63453871b0aca40375be741597363cb8b4393a2d59b2924bb4a4123b4835e -&gt; 4.5.9-csi-snapshot-controller</span><br><span class="line">      sha256:9bf5e6781444c43939ccf52f4d69fa163dd0faf874bff3c0adb2f259780f2b47 -&gt; 4.5.9-ovn-kubernetes</span><br><span class="line">      sha256:9d7cf54cc50ab837ec797954ff49c0d6c9989e1b3079431199026203f512daf9 -&gt; 4.5.9-cluster-openshift-controller-manager-operator</span><br><span class="line">      sha256:a14dc3297e6ea3098118c920275fc54aeb29d2f490e5023f9cbb37b6af05be81 -&gt; 4.5.9-cluster-dns-operator</span><br><span class="line">      sha256:a25790af4a004de3183a44b240dd3a3138749fae3ffa4ac41cca94319f614213 -&gt; 4.5.9-openshift-apiserver</span><br><span class="line">      sha256:a48d986d1731609226d8f6876c7cef21e8cefccd4e04c000f255b1f6f908a5ea -&gt; 4.5.9-cluster-csi-snapshot-controller-operator</span><br><span class="line">      sha256:a51adfc033493814df1a193b0de547749e052cb0811edacc05b878e1fef167b8 -&gt; 4.5.9-etcd</span><br><span class="line">      sha256:a7433426912aa9ff24b63105c5799d47614978ee80a4a241f5e9f31e8e588710 -&gt; 4.5.9-cluster-network-operator</span><br><span class="line">      sha256:aa73c07c74838b11b6150c6907ab2c40c86a40eb63bf2c59db881815ab3553c5 -&gt; 4.5.9-coredns</span><br><span class="line">      sha256:adc3e7c9ab16095165261f0feef990bb829ccf57ae506dadf3734cee0801057a -&gt; 4.5.9-cluster-image-registry-operator</span><br><span class="line">      sha256:af0f67519dbd7ffe2732d89cfa342ee55557f0dc5e8ee8c674eed5ff209bb15a -&gt; 4.5.9-machine-os-content</span><br><span class="line">      sha256:b05f9e685b3f20f96fa952c7c31b2bfcf96643e141ae961ed355684d2d209310 -&gt; 4.5.9-baremetal-installer</span><br><span class="line">      sha256:b1f97ac926075c15a45aac7249e03ff583159b6e03c429132c3111ed5302727b -&gt; 4.5.9-multus-admission-controller</span><br><span class="line">      sha256:b7261a317a4bdd681f8812c2022ff7f391a606f2b80a1c068363c5675abd4363 -&gt; 4.5.9-cluster-samples-operator</span><br><span class="line">      sha256:bb64c463c4b999fbd124a772e3db56f27c74b1e033052c737b69510fc1b9e590 -&gt; 4.5.9-pod</span><br><span class="line">      sha256:bc6c8fd4358d3a46f8df4d81cd424e8778b344c368e6855ed45492815c581438 -&gt; 4.5.9-hyperkube</span><br><span class="line">      sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597 -&gt; 4.5.9-machine-config-operator</span><br><span class="line">      sha256:bcd799425dbdbc7361a44c8bfdc7e6cc3f6f31d59ac18cbea14f3dce8de12d62 -&gt; 4.5.9-cluster-policy-controller</span><br><span class="line">      sha256:bf0d5fd64ab53dfdd477c90a293f3ec90379a22e8b356044082e807565699863 -&gt; 4.5.9-cloud-credential-operator</span><br><span class="line">      sha256:c011c86ee352f377bbcaa46521b731677e81195240cdd4faba7a25a679f00530 -&gt; 4.5.9-cluster-autoscaler</span><br><span class="line">      sha256:c3fc77b6c7ab6e1e2593043532d2a9076831915f2da031fa10aa62bdfbbd7ba8 -&gt; 4.5.9-prometheus-node-exporter</span><br><span class="line">      sha256:c492193a82adfb303b1ac27e48a12cc248c13d120909c98ea19088debe47fd8d -&gt; 4.5.9-kube-storage-version-migrator</span><br><span class="line">      sha256:c4fee4a9d551caa8bfa8cca0a1fb919408a86451bccb3649965c09dcab068a88 -&gt; 4.5.9-operator-marketplace</span><br><span class="line">      sha256:c516f6fa1dd2bf3cca769a32eae08fd027250d56361abb790910c9a18f9fcf07 -&gt; 4.5.9-cluster-node-tuned</span><br><span class="line">      sha256:c93a0de1e4cb3f04e37d547c1b81a2a22c4d9d01c013374c466ed3fd0416215a -&gt; 4.5.9-cluster-config-operator</span><br><span class="line">      sha256:ca556d4515818e3e10d2e771d986784460509146ea9dd188fb8ba6f6ac694132 -&gt; 4.5.9-cluster-kube-controller-manager-operator</span><br><span class="line">      sha256:cca683b844f3e73bb8160e27b986762e77601831eca77e2eea6d94a499959869 -&gt; 4.5.9-multus-route-override-cni</span><br><span class="line">      sha256:cd1f7e40de6a170946faea98f94375f5aae2d9868c049f9ed00fb4f07b32d775 -&gt; 4.5.9-kube-proxy</span><br><span class="line">      sha256:d014f98f1d9a5a6f7e70294830583670d1b17892d38bc8c009ec974f12599bff -&gt; 4.5.9-tools</span><br><span class="line">      sha256:d0d21ae3e27140e1fa13b49d6b2883a0f1466d8e47a2a4839f22de80668d5c95 -&gt; 4.5.9-haproxy-router</span><br><span class="line">      sha256:d11eff4148b733de49e588cfe4c002b5fdd3dea5caea4a7a3a1087390782e56a -&gt; 4.5.9-cluster-openshift-apiserver-operator</span><br><span class="line">      sha256:d72cbba07de1dacfe3669e11fa4f4efa2c7b65664899923622b0ca0715573671 -&gt; 4.5.9-telemeter</span><br><span class="line">      sha256:d8725a2f93d0df184617a41995ee068914ca5a155cdf11c304553abcc4c3100a -&gt; 4.5.9-cli-artifacts</span><br><span class="line">      sha256:db0794d279028179c45791b0dc7ff22f2172d2267d85e0a37ef2a26ffda9c642 -&gt; 4.5.9-cluster-storage-operator</span><br><span class="line">      sha256:dbca81e9b4055763f422c22df01a77efba1dca499686a24210795f2ffddf20c9 -&gt; 4.5.9-docker-registry</span><br><span class="line">      sha256:ddb26e047d0e0d7b11bdb625bd7a941ab66f7e1ef5a5f7455d8694e7ba48989d -&gt; 4.5.9-cluster-kube-scheduler-operator</span><br><span class="line">      sha256:e1d1cea1cf52358b3e91ce8176b83ad36b6b28a226ee6f356fa11496396d93ec -&gt; 4.5.9-cluster-authentication-operator</span><br><span class="line">      sha256:e34e4f3b0097db265b40d627c2cc7b4ddd953a30b7dd283e2ec4bbe5c257a49e -&gt; 4.5.9-configmap-reloader</span><br><span class="line">      sha256:e4758039391099dc1b0265099474113dcd8bcce84a1c92d02c1ef760793079e6 -&gt; 4.5.9-cluster-kube-apiserver-operator</span><br><span class="line">      sha256:e4aae960b36e292b9807f9dc6f3feb57b62cc6a91f9349f983468a1102dc01a7 -&gt; 4.5.9-multus-whereabouts-ipam-cni</span><br><span class="line">      sha256:e8e8da1a4d743770940708c84408dc6188e4df2104c014160e051ef4a8c51b04 -&gt; 4.5.9-baremetal-operator</span><br><span class="line">      sha256:ea61c3e635bbaacd3b0833cf1983f25d3fc6eddc306230bf2703334ba61c1c26 -&gt; 4.5.9-gcp-machine-controllers</span><br><span class="line">      sha256:ec29896354cb1f651142e1c3793890fb59e5252955cb5706cca0e6917e888a61 -&gt; 4.5.9-oauth-proxy</span><br><span class="line">      sha256:ec3a914142ee8dfa01b7a407f7909ff766c210c44a33d4fa731f5547fdb65796 -&gt; 4.5.9-ironic-static-ip-manager</span><br><span class="line">      sha256:ee00cecd8084aac097d0b0ee2033d7931467d92f00643c6434ce6620d643cf35 -&gt; 4.5.9-operator-registry</span><br><span class="line">      sha256:eee108b303058e74c75b5cf16aeefaeb9d0972cd25cf09e0d33ef3213dcd50d0 -&gt; 4.5.9-cluster-monitoring-operator</span><br><span class="line">      sha256:f6b9d00a5dbde8a772b0709aa4ae7d686aa2b645a2a6fbee85eebe15925f8fbb -&gt; 4.5.9-prometheus</span><br><span class="line">      sha256:f70fdff00e09230af556265af72a87fbc22de2cf0f6447ebc8205baaca02fae2 -&gt; 4.5.9-installer</span><br><span class="line">      sha256:fbc84979fec952728014a8551f6cb0deb436f19aa288b8e03803ae370fc3c911 -&gt; 4.5.9-ironic-hardware-inventory-recorder</span><br><span class="line">  stats: shared=0 unique=0 size=0B</span><br><span class="line"></span><br><span class="line">phase 0:</span><br><span class="line">  registry.openshift4.example.com ocp4/openshift4 blobs=0 mounts=0 manifests=110 shared=0</span><br><span class="line"></span><br><span class="line">info: Planning completed in 22.31s</span><br><span class="line">sha256:ec3a914142ee8dfa01b7a407f7909ff766c210c44a33d4fa731f5547fdb65796 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-static-ip-manager</span><br><span class="line">sha256:f6b9d00a5dbde8a772b0709aa4ae7d686aa2b645a2a6fbee85eebe15925f8fbb registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus</span><br><span class="line">sha256:65dc7fc90223061deec270ecdfa365d509950a1efa4b9025c9355d02b8c22f9b registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-machine-approver</span><br><span class="line">sha256:9d7cf54cc50ab837ec797954ff49c0d6c9989e1b3079431199026203f512daf9 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-openshift-controller-manager-operator</span><br><span class="line">sha256:07f11763953a2293bac5d662b6bd49c883111ba324599c6b6b28e9f9f74112be registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-storage-version-migrator-operator</span><br><span class="line">sha256:3bbdc631a9271d45960e9db061bde9bc87c6b1df3ec58a17f81dd7326065b4dd registry.openshift4.example.com/ocp4/openshift4:4.5.9-prom-label-proxy</span><br><span class="line">sha256:bc6c8fd4358d3a46f8df4d81cd424e8778b344c368e6855ed45492815c581438 registry.openshift4.example.com/ocp4/openshift4:4.5.9-hyperkube</span><br><span class="line">sha256:167c14d56714982360fbaa72b3eb3f8d4fb6c5ad10bbbb7ea396606d8412b7eb registry.openshift4.example.com/ocp4/openshift4:4.5.9-sdn</span><br><span class="line">sha256:486ed094bac19bd3be37eff728596319f73e5648cfb49742ef8c1a859db15e55 registry.openshift4.example.com/ocp4/openshift4:4.5.9-operator-lifecycle-manager</span><br><span class="line">sha256:c011c86ee352f377bbcaa46521b731677e81195240cdd4faba7a25a679f00530 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-autoscaler</span><br><span class="line">sha256:e4aae960b36e292b9807f9dc6f3feb57b62cc6a91f9349f983468a1102dc01a7 registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-whereabouts-ipam-cni</span><br><span class="line">sha256:ea61c3e635bbaacd3b0833cf1983f25d3fc6eddc306230bf2703334ba61c1c26 registry.openshift4.example.com/ocp4/openshift4:4.5.9-gcp-machine-controllers</span><br><span class="line">sha256:bf0d5fd64ab53dfdd477c90a293f3ec90379a22e8b356044082e807565699863 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cloud-credential-operator</span><br><span class="line">sha256:ddb26e047d0e0d7b11bdb625bd7a941ab66f7e1ef5a5f7455d8694e7ba48989d registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-scheduler-operator</span><br><span class="line">sha256:cd1f7e40de6a170946faea98f94375f5aae2d9868c049f9ed00fb4f07b32d775 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-proxy</span><br><span class="line">sha256:adc3e7c9ab16095165261f0feef990bb829ccf57ae506dadf3734cee0801057a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-image-registry-operator</span><br><span class="line">sha256:eee108b303058e74c75b5cf16aeefaeb9d0972cd25cf09e0d33ef3213dcd50d0 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-monitoring-operator</span><br><span class="line">sha256:09c763b2ad50711c2b1b5f26c9c70875f821f460e7214f1b11c4be8374424f42 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-client-agent</span><br><span class="line">sha256:4d10b546ad2eb8bf67b50217e441d3b01d219ceb96bdf31855a5199fc23c29c7 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-autoscaler-operator</span><br><span class="line">sha256:80cb8dd15ae02c81ba09387560eb3edb8a12645b3a3179620038df340f51bd54 registry.openshift4.example.com/ocp4/openshift4:4.5.9-thanos</span><br><span class="line">sha256:21fdd7329f0d9d85acb935223915fbb3af07d4ea051678fc8ec85e47ab95edb1 registry.openshift4.example.com/ocp4/openshift4:4.5.9-mdns-publisher</span><br><span class="line">sha256:87037e2988ca6fee330729017c4ecb164f1a6f68e9eb4edacc976c3efb9515fd registry.openshift4.example.com/ocp4/openshift4:4.5.9-insights-operator</span><br><span class="line">sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-ingress-operator</span><br><span class="line">sha256:d8725a2f93d0df184617a41995ee068914ca5a155cdf11c304553abcc4c3100a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cli-artifacts</span><br><span class="line">sha256:a25790af4a004de3183a44b240dd3a3138749fae3ffa4ac41cca94319f614213 registry.openshift4.example.com/ocp4/openshift4:4.5.9-openshift-apiserver</span><br><span class="line">sha256:e8e8da1a4d743770940708c84408dc6188e4df2104c014160e051ef4a8c51b04 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-operator</span><br><span class="line">sha256:521721a7d0d298eb361d550c08f682942323a68d07e580450e2a64cbfab54880 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-machine-controllers</span><br><span class="line">sha256:d11eff4148b733de49e588cfe4c002b5fdd3dea5caea4a7a3a1087390782e56a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-openshift-apiserver-operator</span><br><span class="line">sha256:4b15cf622173dedc8ab30dd1d81c7a4ffe2317fce9fd179bb26b04167604bfab registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-state-metrics</span><br><span class="line">sha256:cca683b844f3e73bb8160e27b986762e77601831eca77e2eea6d94a499959869 registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-route-override-cni</span><br><span class="line">sha256:70c3c46df383aa123ffd0a09f18afcc92e894b01cb9edc4e42cdac9b4a092fa1 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-svcat-controller-manager-operator</span><br><span class="line">sha256:a48d986d1731609226d8f6876c7cef21e8cefccd4e04c000f255b1f6f908a5ea registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-csi-snapshot-controller-operator</span><br><span class="line">sha256:5f77b35d6095068e685cd389f5aeea4b3fad82e771957663839baaa331859eee registry.openshift4.example.com/ocp4/openshift4:4.5.9-libvirt-machine-controllers</span><br><span class="line">sha256:203ab5955dbcbc5f3334e1d7f2ba9fac5f7a1e187370e88c08ad2996a8507711 registry.openshift4.example.com/ocp4/openshift4:4.5.9-aws-pod-identity-webhook</span><br><span class="line">sha256:aa73c07c74838b11b6150c6907ab2c40c86a40eb63bf2c59db881815ab3553c5 registry.openshift4.example.com/ocp4/openshift4:4.5.9-coredns</span><br><span class="line">sha256:ee00cecd8084aac097d0b0ee2033d7931467d92f00643c6434ce6620d643cf35 registry.openshift4.example.com/ocp4/openshift4:4.5.9-operator-registry</span><br><span class="line">sha256:e1d1cea1cf52358b3e91ce8176b83ad36b6b28a226ee6f356fa11496396d93ec registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-authentication-operator</span><br><span class="line">sha256:c4fee4a9d551caa8bfa8cca0a1fb919408a86451bccb3649965c09dcab068a88 registry.openshift4.example.com/ocp4/openshift4:4.5.9-operator-marketplace</span><br><span class="line">sha256:08068503746a2dfd4dcaf61dca44680a9a066960cac7b1e81ea83e1f157e7e8c registry.openshift4.example.com/ocp4/openshift4:4.5.9-service-ca-operator</span><br><span class="line">sha256:ec29896354cb1f651142e1c3793890fb59e5252955cb5706cca0e6917e888a61 registry.openshift4.example.com/ocp4/openshift4:4.5.9-oauth-proxy</span><br><span class="line">sha256:4f99f1a5d8a5b7789016316ab33319e5d8b1b27b42ccfb974eb98b27968683b5 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-alertmanager</span><br><span class="line">sha256:e4758039391099dc1b0265099474113dcd8bcce84a1c92d02c1ef760793079e6 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-apiserver-operator</span><br><span class="line">sha256:ca556d4515818e3e10d2e771d986784460509146ea9dd188fb8ba6f6ac694132 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-kube-controller-manager-operator</span><br><span class="line">sha256:b05f9e685b3f20f96fa952c7c31b2bfcf96643e141ae961ed355684d2d209310 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-installer</span><br><span class="line">sha256:22e47de98549ac9e792f06d4083b5d99db3ecc8dc4c94c8950dba9756a75dc28 registry.openshift4.example.com/ocp4/openshift4:4.5.9-grafana</span><br><span class="line">sha256:84cb2378e2115c1fba70f6cb7ccf053ad848a2038a38c2f5ab45a2e5f21d871c registry.openshift4.example.com/ocp4/openshift4:4.5.9-console</span><br><span class="line">sha256:0259aa5845ce43114c63d59cedeb71c9aa5781c0a6154fe5af8e3cb7bfcfa304 registry.openshift4.example.com/ocp4/openshift4:4.5.9-machine-api-operator</span><br><span class="line">sha256:1d73207be4fbba735dbb53facd23c2359886a9194df3267fd91a843a7ba10316 registry.openshift4.example.com/ocp4/openshift4:4.5.9-console-operator</span><br><span class="line">sha256:27774942e605c4d4b13da7721c75135aac27db8409862625b76f5b31eb2e0d63 registry.openshift4.example.com/ocp4/openshift4:4.5.9-keepalived-ipfailover</span><br><span class="line">sha256:2be07ebc1d005decbb8e624e9beb4d282900f42eb4f8ec0b76e31daedd8874cc registry.openshift4.example.com/ocp4/openshift4:4.5.9-installer-artifacts</span><br><span class="line">sha256:15be0e6de6e0d7bec726611f1dcecd162325ee57b993e0d886e70c25a1faacc3 registry.openshift4.example.com/ocp4/openshift4:4.5.9-openshift-controller-manager</span><br><span class="line">sha256:c3fc77b6c7ab6e1e2593043532d2a9076831915f2da031fa10aa62bdfbbd7ba8 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-node-exporter</span><br><span class="line">sha256:36e7c99acba49c22c7b84a339bfdab820fae17dec07cb4a17db18617b1d46a37 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-etcd-signer-server</span><br><span class="line">sha256:3c45fb79851e8498666470739c452535d16fde408f54461cc3e239608ae55943 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-ipa-downloader</span><br><span class="line">sha256:b7261a317a4bdd681f8812c2022ff7f391a606f2b80a1c068363c5675abd4363 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-samples-operator</span><br><span class="line">sha256:2899afae1d0d9baa0309944dd55975a195a2cbd211f9ecf574e39287827fa684 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ovirt-machine-controllers</span><br><span class="line">sha256:95e63453871b0aca40375be741597363cb8b4393a2d59b2924bb4a4123b4835e registry.openshift4.example.com/ocp4/openshift4:4.5.9-csi-snapshot-controller</span><br><span class="line">sha256:e34e4f3b0097db265b40d627c2cc7b4ddd953a30b7dd283e2ec4bbe5c257a49e registry.openshift4.example.com/ocp4/openshift4:4.5.9-configmap-reloader</span><br><span class="line">sha256:45ec998707309535437f03a2f361ea4c660744c41926258f6a42b566228fe59a registry.openshift4.example.com/ocp4/openshift4:4.5.9-jenkins-agent-nodejs</span><br><span class="line">sha256:1c24c01e807700d30430ce4ff2f77b54d55910cfbf9ee0cbe5c1c64979547463 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-operator</span><br><span class="line">sha256:d0d21ae3e27140e1fa13b49d6b2883a0f1466d8e47a2a4839f22de80668d5c95 registry.openshift4.example.com/ocp4/openshift4:4.5.9-haproxy-router</span><br><span class="line">sha256:3161e52e7bbbf445170c4992d5a47ed87c1d026a7f94b8d0cd30c4508fb48643 registry.openshift4.example.com/ocp4/openshift4:4.5.9-oauth-server</span><br><span class="line">sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-etcd-operator</span><br><span class="line">sha256:d014f98f1d9a5a6f7e70294830583670d1b17892d38bc8c009ec974f12599bff registry.openshift4.example.com/ocp4/openshift4:4.5.9-tools</span><br><span class="line">sha256:792a91df9bb7c4fb49f01c0a70479cc356aa324082ad869bf141a7ff98f51f5a registry.openshift4.example.com/ocp4/openshift4:4.5.9-deployer</span><br><span class="line">sha256:8bd68e0f18cd6ba0a6f12848f3edb630fcca9090a272aa58e3818a6382067e52 registry.openshift4.example.com/ocp4/openshift4:4.5.9-kuryr-controller</span><br><span class="line">sha256:62b44f524d9b820855629cb01bc8c7fff79a039a72f3b421761e1a65ea621b13 registry.openshift4.example.com/ocp4/openshift4:4.5.9-baremetal-runtimecfg</span><br><span class="line">sha256:dbca81e9b4055763f422c22df01a77efba1dca499686a24210795f2ffddf20c9 registry.openshift4.example.com/ocp4/openshift4:4.5.9-docker-registry</span><br><span class="line">sha256:52a566dabf19f82f3fba485807784bd34b2b6a03539d0a2c471ea283ee601e62 registry.openshift4.example.com/ocp4/openshift4:4.5.9-openstack-machine-controllers</span><br><span class="line">sha256:8c25f463d04079a8a93791c307ff893264855d60bbb8ba7a54179d9d44fc2f9f registry.openshift4.example.com/ocp4/openshift4:4.5.9-openshift-state-metrics</span><br><span class="line">sha256:43fdf850ddbbad7b727eff2850d1886a69b6efca837ff8a35d097c1f15f0922c registry.openshift4.example.com/ocp4/openshift4:4.5.9-aws-machine-controllers</span><br><span class="line">sha256:708de132d6a5a6c15c0b7f04f572cd85ff67b1733c5235d74b74f3a690a98ce7 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-machine-os-downloader</span><br><span class="line">sha256:f70fdff00e09230af556265af72a87fbc22de2cf0f6447ebc8205baaca02fae2 registry.openshift4.example.com/ocp4/openshift4:4.5.9-installer</span><br><span class="line">sha256:c516f6fa1dd2bf3cca769a32eae08fd027250d56361abb790910c9a18f9fcf07 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-node-tuned</span><br><span class="line">sha256:bcd799425dbdbc7361a44c8bfdc7e6cc3f6f31d59ac18cbea14f3dce8de12d62 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-policy-controller</span><br><span class="line">sha256:8ee14d942d7a971f52147b15fbd706844e250bd6c72c42150d92fc53cc826111 registry.openshift4.example.com/ocp4/openshift4:4.5.9-prometheus-config-reloader</span><br><span class="line">sha256:09a7dea10cd584c6048f8df3dcec67dd9a8432eb44051353e180dfeb350c6310 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-node-tuning-operator</span><br><span class="line">sha256:3b893950a9db5aa3653d80b05b236e72c168ff368d1c2e0192dd6036486eb715 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-version-operator</span><br><span class="line">sha256:4a547f79252b06c0fd9fa112d4bca0cd2e5958f471e9a872e33f4e917c1ebccd registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-update-keys</span><br><span class="line">sha256:2527b9f6712b8551cbcec637fc87bb9640973e9f9f4972d489c5308ef25eeed0 registry.openshift4.example.com/ocp4/openshift4:4.5.9-jenkins</span><br><span class="line">sha256:1cf4b3ce90933a7dbef9d01ebcb00a7f490dce7c44de6ddb16ddebad07de8eda registry.openshift4.example.com/ocp4/openshift4:4.5.9-k8s-prometheus-adapter</span><br><span class="line">sha256:416ad8f3ddd49adae4f7db66f8a130319084604296c076c2a6d22264a5688d65 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-bootstrap</span><br><span class="line">sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425 registry.openshift4.example.com/ocp4/openshift4:4.5.9-x86_64</span><br><span class="line">sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597 registry.openshift4.example.com/ocp4/openshift4:4.5.9-machine-config-operator</span><br><span class="line">sha256:6b4bc1c8fa3e762d70a837f63f660e4ff3e129015d35c82a7b6da0c6fb7919da registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-rbac-proxy</span><br><span class="line">sha256:a51adfc033493814df1a193b0de547749e052cb0811edacc05b878e1fef167b8 registry.openshift4.example.com/ocp4/openshift4:4.5.9-etcd</span><br><span class="line">sha256:1960c492ce80345e03d50c6cada3d37341ca604d891c48b03109de525e99997a registry.openshift4.example.com/ocp4/openshift4:4.5.9-tests</span><br><span class="line">sha256:66e8675a73707d519467b3598259587de325315e186a839c233f3659f58be534 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic</span><br><span class="line">sha256:755c6cd68730ad7f72950be4110c79b971403864cb9bf8f211edf4d35858b2e6 registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-cni</span><br><span class="line">sha256:a7433426912aa9ff24b63105c5799d47614978ee80a4a241f5e9f31e8e588710 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-network-operator</span><br><span class="line">sha256:a14dc3297e6ea3098118c920275fc54aeb29d2f490e5023f9cbb37b6af05be81 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-dns-operator</span><br><span class="line">sha256:d72cbba07de1dacfe3669e11fa4f4efa2c7b65664899923622b0ca0715573671 registry.openshift4.example.com/ocp4/openshift4:4.5.9-telemeter</span><br><span class="line">sha256:7a56d91d1e0aeddc46dce7fcfda56d4430321e14563e10207893a360a87d3111 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-inspector</span><br><span class="line">sha256:db0794d279028179c45791b0dc7ff22f2172d2267d85e0a37ef2a26ffda9c642 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-storage-operator</span><br><span class="line">sha256:3ec062eef4a908c7ce7ee01222a641505f9ccb45e4864f4d3c40a2c3160928f2 registry.openshift4.example.com/ocp4/openshift4:4.5.9-azure-machine-controllers</span><br><span class="line">sha256:273585703f8f56c980ff77f9a71e6e380d8caa645c4101525572906e0820e8fc registry.openshift4.example.com/ocp4/openshift4:4.5.9-docker-builder</span><br><span class="line">sha256:4d4f72c1556d2085c55691bddf5fadea7bb14ba980fdc28f02146b090134f3a6 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-svcat-apiserver-operator</span><br><span class="line">sha256:9bf5e6781444c43939ccf52f4d69fa163dd0faf874bff3c0adb2f259780f2b47 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ovn-kubernetes</span><br><span class="line">sha256:289d792eada5d1460d69a927dff31328b5fcb1f1d2e31eb7a3fd69afd9a118fc registry.openshift4.example.com/ocp4/openshift4:4.5.9-kuryr-cni</span><br><span class="line">sha256:af0f67519dbd7ffe2732d89cfa342ee55557f0dc5e8ee8c674eed5ff209bb15a registry.openshift4.example.com/ocp4/openshift4:4.5.9-machine-os-content</span><br><span class="line">sha256:14f97b5e9195c8f9b682523dc1febbb1f75fd60d408ac2731cdfa047aee0f43d registry.openshift4.example.com/ocp4/openshift4:4.5.9-must-gather</span><br><span class="line">sha256:c93a0de1e4cb3f04e37d547c1b81a2a22c4d9d01c013374c466ed3fd0416215a registry.openshift4.example.com/ocp4/openshift4:4.5.9-cluster-config-operator</span><br><span class="line">sha256:b1f97ac926075c15a45aac7249e03ff583159b6e03c429132c3111ed5302727b registry.openshift4.example.com/ocp4/openshift4:4.5.9-multus-admission-controller</span><br><span class="line">sha256:bb64c463c4b999fbd124a772e3db56f27c74b1e033052c737b69510fc1b9e590 registry.openshift4.example.com/ocp4/openshift4:4.5.9-pod</span><br><span class="line">sha256:39f8b615e82b3a3a087d6f7d301303d8a0862e4ca13e8f99e6ad968a04985f80 registry.openshift4.example.com/ocp4/openshift4:4.5.9-cli</span><br><span class="line">sha256:c492193a82adfb303b1ac27e48a12cc248c13d120909c98ea19088debe47fd8d registry.openshift4.example.com/ocp4/openshift4:4.5.9-kube-storage-version-migrator</span><br><span class="line">sha256:2b40c4a5cb2a9586ec6c8e43e3013e6ee97781399b2fa836cf8d6c181ff8430c registry.openshift4.example.com/ocp4/openshift4:4.5.9-jenkins-agent-maven</span><br><span class="line">sha256:00edb6c1dae03e1870e1819b4a8d29b655fb6fc40a396a0db2d7c8a20bd8ab8d registry.openshift4.example.com/ocp4/openshift4:4.5.9-local-storage-static-provisioner</span><br><span class="line">sha256:1140eff8442d5ac679a3a85655723cd51f12a63c0c8d61251214ff952cc40b06 registry.openshift4.example.com/ocp4/openshift4:4.5.9-container-networking-plugins</span><br><span class="line">sha256:fbc84979fec952728014a8551f6cb0deb436f19aa288b8e03803ae370fc3c911 registry.openshift4.example.com/ocp4/openshift4:4.5.9-ironic-hardware-inventory-recorder</span><br><span class="line">info: Mirroring completed in 2.27s (0B/s)</span><br><span class="line"></span><br><span class="line">Success</span><br><span class="line">Update image:  registry.openshift4.example.com/ocp4/openshift4:4.5.9-x86_64</span><br><span class="line">Mirror prefix: registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line"></span><br><span class="line">To use the new mirrored repository to install, add the following section to the install-config.yaml:</span><br><span class="line"></span><br><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-release</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy:</span><br><span class="line"></span><br><span class="line">apiVersion: operator.openshift.io/v1alpha1</span><br><span class="line">kind: ImageContentSourcePolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: example</span><br><span class="line">spec:</span><br><span class="line">  repositoryDigestMirrors:</span><br><span class="line">  - mirrors:</span><br><span class="line">    - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">  - mirrors:</span><br><span class="line">    - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">    source: quay.io/openshift-release-dev/ocp-release</span><br></pre></td></tr></table></figure><p>梯子不稳定的话多执行几次，<code>oc adm release mirror</code> 命令执行完成后会输出下面类似的信息，保存下来，将来会用在 <code>install-config.yaml</code> 文件中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">imageContentSources:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">registry.openshift4.example.com/ocp4/openshift4</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-release</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mirrors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">registry.openshift4.example.com/ocp4/openshift4</span></span><br><span class="line">  <span class="attr">source:</span> <span class="string">quay.io/openshift-release-dev/ocp-v4.0-art-dev</span></span><br></pre></td></tr></table></figure><h5 id="同步到本地目录"><a href="#同步到本地目录" class="headerlink" title="同步到本地目录"></a>同步到本地目录</h5><p>这里说下同步到本地，假如云梯的节点和内网是不通的，所以我们需要先在纵云梯节点上把镜像存为文件，后面文件拷贝内网里去推送到内网仓库上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">mkdir mirror</span><br><span class="line">oc adm -a $&#123;LOCAL_SECRET_JSON&#125; release mirror \</span><br><span class="line">  --from=quay.io/$&#123;PRODUCT_REPO&#125;/$&#123;RELEASE_NAME&#125;:$&#123;OCP_RELEASE&#125; \</span><br><span class="line">  --to-dir=mirror/</span><br></pre></td></tr></table></figure><p>梯子不稳定的话多执行几次，这个命令支持继续上次的位置下载，下载完成后会有下面类似内容</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">phase 0:</span><br><span class="line">   openshift/release blobs=238 mounts=0 manifests=110 shared=5</span><br><span class="line"></span><br><span class="line">info: Planning completed in 22.61s</span><br><span class="line">uploading: file://openshift/release sha256:740e93718e387c4611372639d025bcde65fa084f10cd64b2fea59ac3edc3b5c2 823.3MiB</span><br><span class="line">uploading: file://openshift/release sha256:4908e3220585a526b87e77f88ee7ddd06c502447269792ea4013e1b2f414f41e 383.1MiB</span><br><span class="line">uploading: file://openshift/release sha256:b9cb5a1468a5d3b235df159d5f795d2f44bc14fbe6c3d36b28ce8a69eb545771 515.5MiB</span><br><span class="line">sha256:15be0e6de6e0d7bec726611f1dcecd162325ee57b993e0d886e70c25a1faacc3 file://openshift/release:4.5.9-openshift-controller-manager</span><br><span class="line">sha256:bc6c8fd4358d3a46f8df4d81cd424e8778b344c368e6855ed45492815c581438 file://openshift/release:4.5.9-hyperkube</span><br><span class="line">sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4 file://openshift/release:4.5.9-cluster-ingress-operator</span><br><span class="line">sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597 file://openshift/release:4.5.9-machine-config-operator</span><br><span class="line">sha256:9d7cf54cc50ab837ec797954ff49c0d6c9989e1b3079431199026203f512daf9 file://openshift/release:4.5.9-cluster-openshift-controller-manager-operator</span><br><span class="line">sha256:c492193a82adfb303b1ac27e48a12cc248c13d120909c98ea19088debe47fd8d file://openshift/release:4.5.9-kube-storage-version-migrator</span><br><span class="line">sha256:2527b9f6712b8551cbcec637fc87bb9640973e9f9f4972d489c5308ef25eeed0 file://openshift/release:4.5.9-jenkins</span><br><span class="line">....</span><br><span class="line">info: Mirroring completed in 1h27m3.41s (345.7kB/s)</span><br><span class="line"></span><br><span class="line">Success</span><br><span class="line">Update image:  openshift/release:4.5.9</span><br><span class="line"></span><br><span class="line">To upload local images to a registry, run:</span><br><span class="line"></span><br><span class="line">    oc image mirror --from-dir=mirror/ &#x27;file://openshift/release:4.5.9*&#x27; REGISTRY/REPOSITORY</span><br><span class="line"></span><br><span class="line">Configmap signature file mirror/config/signature-sha256-7ad540594e2a6673.yaml created</span><br></pre></td></tr></table></figure><p>结尾输出了把 mirror 目录下文件推送到镜像仓库的命令 <code>oc image mirror --from-dir=mirror/ &#39;file://openshift/release:4.5.9*&#39; REGISTRY/REPOSITORY</code>，后面会用到这个命令，目录为下面情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tree -L 4 mirror/</span></span><br><span class="line">mirror/</span><br><span class="line">├── config</span><br><span class="line">│   └── signature-sha256-7ad540594e2a6673.yaml</span><br><span class="line">└── v2</span><br><span class="line">    └── openshift</span><br><span class="line">        └── release</span><br><span class="line">            ├── blobs</span><br><span class="line">            └── manifests</span><br></pre></td></tr></table></figure><p>把这些镜像目录打包成压缩包，压缩后的大概5-6G大小</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zcvf mirror.tar.gz mirror/</span><br></pre></td></tr></table></figure><p>把前面的<code>mirror.tar.gz</code>压缩包传过来，这里传到内网的<code>LOCAL_REGISTRY</code>机器上，还有<code>pull-secret.json</code>文件和<code>oc</code>命令也记得拷贝过来。文件准备好后设置下变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9</span></span><br><span class="line">OCP_VERSION=$(oc version -o json | jq -r .releaseClientVersion)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.5.9-x86_64</span></span><br><span class="line">OCP_RELEASE=$OCP_VERSION-$(arch)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4</span></span><br><span class="line">OCP_MAJOR=$&#123;OCP_VERSION%%.*&#125;</span><br><span class="line"></span><br><span class="line">LOCAL_REGISTRY=&#x27;registry.openshift4.example.com&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 对应前面的 ocp4/openshift4</span></span><br><span class="line">LOCAL_REPOSITORY=&quot;ocp$&#123;OCP_MAJOR&#125;/openshift$&#123;OCP_MAJOR&#125;&quot;</span><br><span class="line"></span><br><span class="line">PRODUCT_REPO=&#x27;openshift-release-dev&#x27;</span><br><span class="line">RELEASE_NAME=&quot;ocp-release&quot;</span><br><span class="line"></span><br><span class="line">LOCAL_SECRET_JSON=&#x27;pull-secret.json&#x27;</span><br></pre></td></tr></table></figure><p>然后用之前下载镜像到目录的时候结尾输出的命令导入下，因为证书是非权威机构ca签署的，所以加<code>--insecure</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf mirror.tar.gz</span><br><span class="line"></span><br><span class="line">oc image mirror -a $&#123;LOCAL_SECRET_JSON&#125; \</span><br><span class="line">  --from-dir=mirror/ &#x27;file://openshift/release:4.5.9*&#x27; \</span><br><span class="line"><span class="meta">  $</span><span class="bash">&#123;LOCAL_REGISTRY&#125;/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span> \</span></span><br><span class="line"><span class="bash">  --insecure</span></span><br></pre></td></tr></table></figure><p>导入后去镜像仓库的 web 上查看有没有下面名字的镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> <span class="variable">$&#123;LOCAL_REGISTRY&#125;</span>/<span class="variable">$&#123;LOCAL_REPOSITORY&#125;</span>:<span class="variable">$&#123;OCP_RELEASE&#125;</span></span></span><br><span class="line">registry.openshift4.example.com/ocp4/openshift4:4.5.9-x86_64</span><br></pre></td></tr></table></figure><h3 id="bastion"><a href="#bastion" class="headerlink" title="bastion"></a>bastion</h3><p>bastion 提供了下列功能：</p><ul><li>dns server: 由 coredns + etcd 提供</li><li>负载均衡: haproxy</li><li>http file download: nginx</li><li>openshift-install 二进制文件: 用它转换部署清单成 ignition 文件</li><li>oc: ocp 的 client cli，和 kubectl 一样操作 ocp 集群</li></ul><p>如果你机器数量多，或者内网有 dns server 和负载均衡，上面这些服务没必要耦合部署在一台上，同时这些实现手段看自己掌握的工具，没必要和我用一模一样</p><h4 id="cert-for-registry"><a href="#cert-for-registry" class="headerlink" title="cert for registry"></a>cert for registry</h4><p>bastion 节点推送镜像由于非权威 ca 签署证书会报错 <code>x509: certificate signed by unknown authority</code>，把 registry 机器上 quay 的 <code>ssl.cert</code> 复制到 bastion 上，执行下面操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp ssl.cert /etc/pki/ca-trust/source/anchors/ssl.crt</span><br><span class="line">update-ca-trust extract</span><br></pre></td></tr></table></figure><p>如果使用 Docker 登录，需要将证书复制到 docker 的信任证书路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker/certs.d/registry.openshift4.example.com</span><br><span class="line">cp ssl.cert /etc/docker/certs.d/registry.openshift4.example.com/ssl.crt</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h4 id="oc-amp-amp-openshift-install"><a href="#oc-amp-amp-openshift-install" class="headerlink" title="oc &amp;&amp; openshift-install"></a>oc &amp;&amp; openshift-install</h4><p>把 registry 上的 oc 命令拷贝过来。为了保证安装版本一致性，需要从镜像库中提取 <code>openshift-install</code> 二进制文件，不能直接从 <a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/">https://mirror.openshift.com/pub/openshift-v4/clients/ocp/</a> 下载，不然后面会有 <code>sha256</code> 匹配不上的问题。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 这一步需要用到上面的 <span class="built_in">export</span> 变量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 或者提前在 纵云梯节点执行然后拷贝过来</span></span><br><span class="line">oc adm release extract \</span><br><span class="line">  -a $&#123;LOCAL_SECRET_JSON&#125; \</span><br><span class="line">  --command=openshift-install \</span><br><span class="line">  &quot;$&#123;LOCAL_REGISTRY&#125;/$&#123;LOCAL_REPOSITORY&#125;:$&#123;OCP_RELEASE&#125;&quot;</span><br></pre></td></tr></table></figure><p>如果提示 <code>error: image dose not exist</code>，说明拉取的镜像不全，或者版本不对。</p><p>把文件移动到 <code>$PATH</code> 并确认版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">chmod +x openshift-install</span><br><span class="line">mv openshift-install /usr/local/bin/</span><br><span class="line"></span><br><span class="line">openshift-install version</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面是输出</span></span><br><span class="line"></span><br><span class="line">openshift-install 4.5.9</span><br><span class="line">built from commit 0d5c871ce7d03f3d03ab4371dc39916a5415cf5c</span><br><span class="line">release image registry.openshift4.example.com/ocp4/openshift4@sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425</span><br></pre></td></tr></table></figure><h4 id="dns-server"><a href="#dns-server" class="headerlink" title="dns server"></a>dns server</h4><p>按照官方文档要求，集群里大部分组件之间通信都是用 dns ，所以我们这里得部署一个 dns server，官方以前 3.x.x 的时候使用 ansible 部署的<code>named</code>提供，网上也有人用 dnsmasq，这里使用 <code>docker-compose</code> 起 coredns 作为 dns server，由于这里需要添加 <code>SRV</code> 记录，所以需要 CoreDNS 结合 etcd 插件使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/coredns</span><br><span class="line">cd /data/coredns</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">docker-compose.yml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">version: &#x27;3.2&#x27;</span><br><span class="line">services:</span><br><span class="line">  coredns:</span><br><span class="line">    image: coredns/coredns:1.7.0</span><br><span class="line">    container_name: coredns</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;53:53/udp&quot;</span><br><span class="line">      - &quot;53:53/tcp&quot;</span><br><span class="line">      - &quot;9153:9153/tcp&quot;</span><br><span class="line">    cap_drop:</span><br><span class="line">      - ALL</span><br><span class="line">    cap_add:</span><br><span class="line">      - NET_BIND_SERVICE</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/coredns/config/:/etc/coredns/</span><br><span class="line">      - /etc/localtime:/etc/localtime:ro</span><br><span class="line">    command: [&quot;-conf&quot;, &quot;/etc/coredns/Corefile&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - coredns-etcd</span><br><span class="line">    networks:</span><br><span class="line">      coredns:</span><br><span class="line">        aliases:</span><br><span class="line">          - coredns</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">  coredns-etcd:</span><br><span class="line">    #image: quay.io/coreos/etcd:v3.4.13</span><br><span class="line">    image: registry.aliyuncs.com/k8sxio/etcd:3.4.13-0</span><br><span class="line">    container_name: coredns-etcd</span><br><span class="line">    restart: always</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/coredns/etcd/data:/var/lib/etcd:Z</span><br><span class="line">      - /data/coredns/etcd/conf:/etc/etcd:Z</span><br><span class="line">      - /etc/localtime:/etc/localtime:ro</span><br><span class="line">    command: [&quot;/usr/local/bin/etcd&quot;, &quot;--config-file=/etc/etcd/etcd.config.yml&quot;]</span><br><span class="line">    networks:</span><br><span class="line">      coredns:</span><br><span class="line">        aliases:</span><br><span class="line">          - etcd</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">networks:</span><br><span class="line">  coredns:</span><br><span class="line">    name: coredns</span><br><span class="line">    external: false</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建相关目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/coredns/config/ \</span><br><span class="line">    /data/coredns/etcd/data \</span><br><span class="line">    /data/coredns/etcd/conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> etcd 3.4.10后data目录权限必须是0700</span></span><br><span class="line">chmod 0700 /data/coredns/etcd/data</span><br></pre></td></tr></table></figure><h5 id="coredns"><a href="#coredns" class="headerlink" title="coredns"></a>coredns</h5><p>创建 coredns 的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /data/coredns/config/Corefile &lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">.:53 &#123;  # 监听 TCP 和 UDP 的 53 端口</span><br><span class="line">    template IN A apps.openshift4.example.com &#123;</span><br><span class="line">    match .*apps\.openshift4\.example\.com # 匹配请求 DNS 名称的正则表达式</span><br><span class="line">    answer &quot;&#123;&#123; .Name &#125;&#125; 60 IN A 10.226.45.250&quot; # DNS 应答</span><br><span class="line">    fallthrough</span><br><span class="line">    &#125;</span><br><span class="line">    etcd &#123;   # 配置启用 etcd 插件,后面可以指定域名,例如 etcd test.com &#123;</span><br><span class="line">        path /skydns # etcd 里面的路径 默认为 /skydns，以后所有的 dns 记录都存储在该路径下</span><br><span class="line">        endpoint http://etcd:2379 # etcd 访问地址，这里是容器里，所以写alias域名，多个则空格分开</span><br><span class="line">        fallthrough # 如果区域匹配但不能生成记录，则将请求传递给下一个插件</span><br><span class="line">        # tls CERT KEY CACERT # 可选参数，etcd 认证证书设置</span><br><span class="line">    &#125;</span><br><span class="line">    prometheus  # 监控插件，开启metrics</span><br><span class="line">    cache 160</span><br><span class="line">    reload</span><br><span class="line">    loadbalance   # 负载均衡，开启 DNS 记录轮询策略</span><br><span class="line">    forward . 114.114.114.114 #上游 dns server，多个的话空格隔开</span><br><span class="line">    log # 打印日志</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>这里配置了一个通配符解析就是给 router 使用的（ocp的 ingress controller），有条件可以硬件F5，real server 则写所有 router 所在的 node ip，然后 vhost的通配符域名解析的 ip 写 F5 的 ip。也可以 keepalived 漂个 VIP 做，这里偷懒使用 基础节点，因为基础节点上有 haproxy 作为负载均衡，会反向代理 集群里 hostNetwork 的 router-default 的端口</li><li>forward 的上游可以看宿主机 <code>/etc/resolv.conf</code> 上的 nameserver 段填写</li></ul><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>创建 etcd 的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /data/coredns/etcd/conf/etcd.config.yml &lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">name: coredns-etcd</span><br><span class="line">data-dir: /var/lib/etcd</span><br><span class="line">wal-dir: /var/lib/etcd/wal</span><br><span class="line">auto-compaction-mode: periodic</span><br><span class="line">auto-compaction-retention: &quot;1&quot;</span><br><span class="line">snapshot-count: 5000</span><br><span class="line">heartbeat-interval: 100</span><br><span class="line">election-timeout: 1000</span><br><span class="line">quota-backend-bytes: 0</span><br><span class="line">listen-peer-urls: &#x27;http://127.0.0.1:2380&#x27;</span><br><span class="line">listen-client-urls: &#x27;http://0.0.0.0:2379&#x27;</span><br><span class="line">max-snapshots: 3</span><br><span class="line">max-wals: 5</span><br><span class="line">cors:</span><br><span class="line">initial-advertise-peer-urls: &#x27;http://127.0.0.1:2380&#x27;</span><br><span class="line">advertise-client-urls: &#x27;http://0.0.0.0:2379&#x27;</span><br><span class="line">discovery:</span><br><span class="line">discovery-fallback: &#x27;proxy&#x27;</span><br><span class="line">discovery-proxy:</span><br><span class="line">discovery-srv:</span><br><span class="line">initial-cluster: &#x27;coredns-etcd=http://127.0.0.1:2380&#x27; #和上面的name一致</span><br><span class="line">initial-cluster-token: &#x27;etcd-coredns&#x27;</span><br><span class="line">initial-cluster-state: &#x27;new&#x27;</span><br><span class="line">strict-reconfig-check: false</span><br><span class="line">enable-v2: false</span><br><span class="line">enable-pprof: true</span><br><span class="line">proxy: &#x27;off&#x27;</span><br><span class="line">proxy-failure-wait: 5000</span><br><span class="line">proxy-refresh-interval: 30000</span><br><span class="line">proxy-dial-timeout: 1000</span><br><span class="line">proxy-write-timeout: 5000</span><br><span class="line">proxy-read-timeout: 0</span><br><span class="line">force-new-cluster: false</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h5 id="config-for-dns"><a href="#config-for-dns" class="headerlink" title="config for dns"></a>config for dns</h5><p>确保宿主机上的53没有其他 dns server 进程使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>验证下解析</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> dig +short apps.openshift4.example.com @127.0.0.1</span></span><br><span class="line">10.226.45.250</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> dig +short x.apps.openshift4.example.com @127.0.0.1</span></span><br><span class="line">10.226.45.250</span><br></pre></td></tr></table></figure><p>然后根据集群的 ip 添加解析记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">alias etcdctlv3=&#x27;docker exec -e ETCDCTL_API=3 coredns-etcd etcdctl&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/api &#x27;&#123;&quot;host&quot;:&quot;10.226.45.250&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/api-int &#x27;&#123;&quot;host&quot;:&quot;10.226.45.250&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里默认etcd是部署在master节点上</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/etcd-0 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.251&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/etcd-1 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.252&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/etcd-2 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.222&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 加密的etcd域名SRV记录，这里我是只有一个master，所以这里只写一个</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/_tcp/_etcd-server-ssl/x1 &#x27;&#123;&quot;host&quot;:&quot;etcd-0.openshift4.example.com&quot;,&quot;ttl&quot;:60,&quot;priority&quot;:0,&quot;weight&quot;:10,&quot;port&quot;:2380&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/_tcp/_etcd-server-ssl/x2 &#x27;&#123;&quot;host&quot;:&quot;etcd-1.openshift4.example.com&quot;,&quot;ttl&quot;:60,&quot;priority&quot;:0,&quot;weight&quot;:10,&quot;port&quot;:2380&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/_tcp/_etcd-server-ssl/x3 &#x27;&#123;&quot;host&quot;:&quot;etcd-2.openshift4.example.com&quot;,&quot;ttl&quot;:60,&quot;priority&quot;:0,&quot;weight&quot;:10,&quot;port&quot;:2380&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 除此之外再添加各节点主机名记录</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/bootstrap &#x27;&#123;&quot;host&quot;:&quot;10.226.45.223&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/master1 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.251&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/master2 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.252&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/master3 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.222&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里bootstrap后面用来当作worker用，所以ip写bootstrap</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/worker1 &#x27;&#123;&quot;host&quot;:&quot;10.226.45.223&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 镜像节点的域名</span></span><br><span class="line">etcdctlv3 put /skydns/com/example/openshift4/registry &#x27;&#123;&quot;host&quot;:&quot;10.226.45.226&quot;,&quot;ttl&quot;:60&#125;&#x27;</span><br></pre></td></tr></table></figure><p>查看所有记录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">etcdctlv3 get /skydns --prefix</span><br></pre></td></tr></table></figure><h5 id="validate-for-dns"><a href="#validate-for-dns" class="headerlink" title="validate for dns"></a>validate for dns</h5><p>验证dns，自己比对输出看ip是否正确</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">yum install -y bind-utils</span><br><span class="line"></span><br><span class="line">dig +short api.openshift4.example.com @127.0.0.1</span><br><span class="line"></span><br><span class="line">dig +short api-int.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short etcd-0.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short etcd-1.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short etcd-2.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short bootstrap.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short master1.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short master2.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short master3.openshift4.example.com @127.0.0.1</span><br><span class="line">dig +short worker1.openshift4.example.com @127.0.0.1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dig +short -t SRV _etcd-server-ssl._tcp.openshift4.example.com @127.0.0.1</span><br><span class="line">10 33 2380 etcd-0.openshift4.example.com.</span><br><span class="line">10 33 2380 etcd-1.openshift4.example.com.</span><br><span class="line">10 33 2380 etcd-2.openshift4.example.com.</span><br></pre></td></tr></table></figure><p>然后我们改下系统的<code>resolv.conf</code>，使用 coredns 作为dns，这样我们不用去写 hosts</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp -a /etc/resolv.conf /etc/resolv.conf.bak</span><br><span class="line">cat &gt;/etc/resolv.conf&lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">search openshift4.example.com</span><br><span class="line">nameserver 10.226.45.250</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h4><p>这里因为安装流程是 bootstrap 机器起来后，bootstrap 运行 machine-config 进程，安装 master 和 worker 的时候会访问 bootstrap 上的 <code>mcahine-config</code> 下面的 http 接口（后面有兴趣在 bootstrap 起来后去执行试试）:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sk https://api-int.openshift4.example.com:22623/config/master</span><br></pre></td></tr></table></figure><p>安装完成后 bootstrap 会交出控制平面到 master 上（machine-config 和 kubernetes api），所以无论 master 有几个，都必须要配置负载均衡。使用工具不限，nginx，envoy 啥的均可。nginx 配置四层 mode 七层 check rs 的时候要安装插件来7层 healthz check，挺麻烦的，所以这里我使用 haproxy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/haproxy</span><br><span class="line">cd /data/haproxy</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">docker-compose.yml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">version: &#x27;3.2&#x27;</span><br><span class="line">services:</span><br><span class="line">  haproxy:</span><br><span class="line">    image: haproxy:lts</span><br><span class="line">    container_name: haproxy</span><br><span class="line">    restart: always</span><br><span class="line">    network_mode: host</span><br><span class="line">    sysctls:</span><br><span class="line">      - net.core.somaxconn=2000</span><br><span class="line">    cap_drop:</span><br><span class="line">      - ALL</span><br><span class="line">    cap_add:</span><br><span class="line">      - NET_BIND_SERVICE</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/haproxy/config/:/etc/haproxy/</span><br><span class="line">      - /etc/localtime:/etc/localtime:ro</span><br><span class="line">      - /etc/hosts:/etc/hosts:ro</span><br><span class="line">    command: [&quot;-f&quot;, &quot;/etc/haproxy/haproxy.cfg&quot;]</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h5 id="config-for-haproxy"><a href="#config-for-haproxy" class="headerlink" title="config for haproxy"></a>config for haproxy</h5><p>配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/haproxy/config/</span><br><span class="line">cat &gt;/data/haproxy/config/haproxy.cfg&lt;&lt;&#x27;EOF&#x27;</span><br><span class="line">global</span><br><span class="line">  maxconn  2000</span><br><span class="line">  ulimit-n  16384</span><br><span class="line">  log  127.0.0.1 local0 err</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  log global</span><br><span class="line">  mode  http</span><br><span class="line">  option  httplog</span><br><span class="line">  timeout connect 5000</span><br><span class="line">  timeout client  50000</span><br><span class="line">  timeout server  50000</span><br><span class="line">  timeout http-request 15s</span><br><span class="line">  timeout http-keep-alive 15s</span><br><span class="line"></span><br><span class="line">listen stats</span><br><span class="line">    bind         :9000</span><br><span class="line">    mode         http</span><br><span class="line">    stats        enable</span><br><span class="line">    stats        uri /</span><br><span class="line">    stats        refresh   30s</span><br><span class="line">    stats        auth      admin:openshift #web页面登录</span><br><span class="line">    monitor-uri  /healthz</span><br><span class="line"></span><br><span class="line">frontend openshift-api-server</span><br><span class="line">    bind :6443</span><br><span class="line">    default_backend openshift-api-server</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend openshift-api-server</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    option httpchk GET /healthz</span><br><span class="line">    http-check expect string ok</span><br><span class="line">    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100</span><br><span class="line">    server bootstrap 10.226.45.223:6443 check check-ssl verify none #安装结束后删掉此行</span><br><span class="line">    server master1 10.226.45.251:6443 check check-ssl verify none</span><br><span class="line">    server master2 10.226.45.252:6443 check check-ssl verify none</span><br><span class="line">    server master3 10.226.45.222:6443 check check-ssl verify none</span><br><span class="line"></span><br><span class="line">frontend machine-config-server</span><br><span class="line">    bind :22623</span><br><span class="line">    default_backend machine-config-server</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend machine-config-server</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    server bootstrap 10.226.45.223:22623 check #安装结束后删掉此行</span><br><span class="line">    server master1 10.226.45.251:22623 check</span><br><span class="line">    server master2 10.226.45.252:22623 check</span><br><span class="line">    server master3 10.226.45.222:22623 check</span><br><span class="line"></span><br><span class="line">frontend ingress-http</span><br><span class="line">    bind :80</span><br><span class="line">    default_backend ingress-http</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend ingress-http</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    server master1 10.226.45.251:80 check</span><br><span class="line">    server master2 10.226.45.252:80 check</span><br><span class="line">    server master3 10.226.45.222:80 check</span><br><span class="line"></span><br><span class="line">frontend ingress-https</span><br><span class="line">    bind :443</span><br><span class="line">    default_backend ingress-https</span><br><span class="line">    mode tcp</span><br><span class="line">    option tcplog</span><br><span class="line"></span><br><span class="line">backend ingress-https</span><br><span class="line">    balance roundrobin</span><br><span class="line">    mode tcp</span><br><span class="line">    server master1 10.226.45.251:443 check</span><br><span class="line">    server master2 10.226.45.252:443 check</span><br><span class="line">    server master3 10.226.45.222:443 check</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>因为前文使用的 quay 仓库无法更改 443 端口，这会和 haproxy 代理的 ingress-https 的 443 冲突，所以 quay 仓库单独部署。启动 haproxy</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/haproxy</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>可以访问 <a href="http://ip:9000/">http://ip:9000</a> 查看 haproxy 的状态，basic auth 为上面 haproxy 配置文件里的<code>admin:openshift</code></p><h4 id="http-file-download"><a href="#http-file-download" class="headerlink" title="http file download"></a>http file download</h4><p>由于这里不是 pxe 安装，我们还得提供 http server，让 master 和 bootstrap 从 installer iso 启动后下载 raw.gz 写入到硬盘里。这里使用 nginx 提供</p><p>如果有配置 dhcp 的网络环境，这里需要起 dhcp server 和 tftp 之类的服务让 pxe 自动安装，但是我这里没这个网络条件，所以是开机器后挂载 installer iso，然后开机的时候按 tab 按键输入一堆 boot 选项参数，让它从指定 http 的地方下载和安装（也就是前面我门下载的rhcos的raw.gz文件）。这里 http 用容器 nginx 提供</p><p>创建相关目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/install-nginx/&#123;install/ignition,conf.d&#125;</span><br></pre></td></tr></table></figure><p>创建 docker-compose 文件和配置文件，因为宿主机有负载均衡 80 端口在运行，所以这里 nginx 使用 8080 端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cd /data/install-nginx</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">docker-compose.yml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">version: &#x27;3.4&#x27;</span><br><span class="line">services:</span><br><span class="line">  nginx:</span><br><span class="line">    image: nginx:alpine</span><br><span class="line">    container_name: install-nginx</span><br><span class="line">    hostname: install-nginx</span><br><span class="line">    volumes:</span><br><span class="line">      - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro</span><br><span class="line">      - /data/install-nginx/install:/usr/share/nginx/html</span><br><span class="line">      - /data/install-nginx/conf.d/:/etc/nginx/conf.d/</span><br><span class="line">    network_mode: &quot;host&quot;</span><br><span class="line">    logging:</span><br><span class="line">      driver: json-file</span><br><span class="line">      options:</span><br><span class="line">        max-file: &#x27;3&#x27;</span><br><span class="line">        max-size: 100m</span><br><span class="line">EOF</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash"> /data/install-nginx/conf.d/default.conf &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">server &#123;</span><br><span class="line">    listen       8080;</span><br><span class="line">    server_name  localhost;</span><br><span class="line">    location / &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">        autoindex    on;</span><br><span class="line">    &#125;</span><br><span class="line">    error_page   500 502 503 504  /50x.html;</span><br><span class="line">    location = /50x.html &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>启动<code>install-nginx</code>，起来后访问 <a href="http://ip:8080/">http://ip:8080</a> 看到目录就正常</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><h5 id="rhcos-iso"><a href="#rhcos-iso" class="headerlink" title="rhcos iso"></a>rhcos iso</h5><p>下载 rhcos 的 ISO ， <a href="https://cloud.redhat.com/openshift/install/metal/user-provisioned">下载链接地址</a> 或者 <a href="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/">这里下载</a> 。我们下载下面俩个即可，<strong>版本号必须小于等于ocp的版本号</strong>，<code>installer</code>是给机器挂载，会在内存里运行，根据用户输入的 boot cmdline 去从 http server 下载 <code>metal.x86_64.raw.gz</code> 和 ignition 文件安装和配置</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rhcos-4.5.6-x86_64-installer.x86_64.iso</span><br><span class="line">rhcos-4.5.6-x86_64-metal.x86_64.raw.gz</span><br></pre></td></tr></table></figure><p>installer是给机器挂载从光驱启动的，raw.gz存放在 nginx的http目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /data/install-nginx/install/</span><br><span class="line">wget https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/latest/latest/rhcos-4.5.6-x86_64-metal.x86_64.raw.gz</span><br></pre></td></tr></table></figure><p>installer 的 iso 如果是物理机或者虚机的话不要在 bmc 或者类似 vshpere 上远程挂载你自己pc上的 iso，否则会遇到 bootstrap 和 master 在 logo 的安装界面输入boot cmdline 后回车无响应的情况，建议传到远端上，相对于机器近的存储上。例如是 exsi 和 vshpere 则上传到上面的数据存储上。</p><h4 id="安装准备"><a href="#安装准备" class="headerlink" title="安装准备"></a>安装准备</h4><h5 id="生成ssh密钥对"><a href="#生成ssh密钥对" class="headerlink" title="生成ssh密钥对"></a>生成ssh密钥对</h5><p>在安装过程中，我们会在基础节点上执行 OCP 安装调试和灾难恢复，因此必须在基础节点上配置 SSH key，ssh-agent 将会用它来执行安装程序。</p><p>基础节点上的 core 用户可以使用该私钥登录到 Master 节点。同时部署集群时，该私钥会被添加到 core 用户的 <code>~/.ssh/authorized_keys</code> 列表中。</p><p>创建无密码验证的 SSH key：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -N &#x27;&#x27; -f ~/.ssh/new_rsa</span><br></pre></td></tr></table></figure><p>后续 ssh 到 master 和 node 使用下面命令即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -i ~/.ssh/new_rsa core@10.226.45.251</span><br></pre></td></tr></table></figure><h5 id="创建安装配置文件"><a href="#创建安装配置文件" class="headerlink" title="创建安装配置文件"></a>创建安装配置文件</h5><p>ocp的一些 yaml 和安装文件存放在<code>/data/ocpinstall</code>下，自定义 <code>install-config.yaml</code> 配置文件必须命名为 <code>install-config.yaml</code>。配置文件内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/ocpinstall</span><br><span class="line">cd /data/ocpinstall</span><br><span class="line"><span class="meta">cat&gt;</span><span class="bash">install-config.yaml&lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">apiVersion: v1</span><br><span class="line">baseDomain: example.com</span><br><span class="line">compute:</span><br><span class="line">- hyperthreading: Enabled</span><br><span class="line">  name: worker</span><br><span class="line">  replicas: 0</span><br><span class="line">controlPlane:</span><br><span class="line">  hyperthreading: Enabled</span><br><span class="line">  name: master</span><br><span class="line">  replicas: 3</span><br><span class="line">metadata:</span><br><span class="line">  name: openshift4</span><br><span class="line">networking:</span><br><span class="line">  clusterNetwork:</span><br><span class="line">  - cidr: 10.128.0.0/14</span><br><span class="line">    hostPrefix: 23</span><br><span class="line">  networkType: OpenShiftSDN</span><br><span class="line">  serviceNetwork:</span><br><span class="line">  - 172.30.0.0/16</span><br><span class="line">platform:</span><br><span class="line">  none: &#123;&#125;</span><br><span class="line">fips: false</span><br><span class="line">pullSecret: &#x27;&#123;&quot;auths&quot;: ...&#125;&#x27;</span><br><span class="line">sshKey: &#x27;ssh-rsa ...&#x27;</span><br><span class="line">additionalTrustBundle: |</span><br><span class="line">  -----BEGIN CERTIFICATE-----</span><br><span class="line">  注意这里要前面空个两格用于yaml对齐</span><br><span class="line">  -----END CERTIFICATE-----</span><br><span class="line">imageContentSources:</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-release</span><br><span class="line">- mirrors:</span><br><span class="line">  - registry.openshift4.example.com/ocp4/openshift4</span><br><span class="line">  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><strong>baseDomain</strong> : 所有 Openshift 内部的 DNS 记录必须是此基础的子域，并包含集群名称。</li><li><strong>compute</strong> : 计算节点配置。这是一个数组，每一个元素必须以连字符 - 开头。</li><li><strong>hyperthreading</strong> : Enabled 表示启用同步多线程或超线程。默认启用同步多线程，可以提高机器内核的性能。如果要禁用，则控制平面和计算节点都要禁用。</li><li><strong>compute.replicas</strong> : 计算节点数量。因为我们要手动创建计算节点，所以这里要设置为 0。</li><li><strong>controlPlane.replicas</strong> : 控制平面节点数量。控制平面节点数量必须和 etcd 节点数量一致，为了实现高可用，所以设置为 3。</li><li><strong>metadata.name</strong> : 集群名称。即前面 DNS 记录中的 <code>&lt;cluster_name&gt;</code>。</li><li><strong>cidr</strong> : 定义了分配 Pod IP 的 IP 地址段，不能和物理网络重叠。</li><li><strong>hostPrefix</strong> : 分配给每个节点的子网前缀长度，等同于 k8s 的 node-max-mask。例如，如果将 <code>hostPrefix</code> 设置为 <code>23</code>，则为每一个节点分配一个给定 cidr 的 <code>/23</code> 子网，允许510个 Pod IP 地址，24位的话每个 node 的 pod 最多254个ip后期可能会太少了。</li><li><strong>serviceNetwork</strong> : Service IP 的地址池，只能设置一个。</li><li><strong>pullSecret</strong> : 上篇文章使用的 pull secret，可通过命令 <code>jq . /root/pull-secret.json</code> 来压缩成一行。</li><li><strong>sshKey</strong> : 上面创建的公钥，可通过命令 <code>cat ~/.ssh/new_rsa.pub</code> 查看。</li><li><strong>additionalTrustBundle</strong> : 私有镜像仓库 Quay 的信任证书，可在镜像节点上通过命令 <code>cat /data/quay/config/ssl.cert</code> 查看。</li><li><strong>imageContentSources</strong> : 来自前面 <code>oc adm release mirror</code> 的输出结果。</li></ul><p><strong>platform</strong> 实际上支持 gcp，aws，vshpere，bmc之类的自动创建机器，有兴趣可以去去研究下</p><p>修改好上面的文件后接着生成 Ignition 配置文件，创建后<code>install-config.yaml</code>会被删除，所以备份下，同时该文件实际上会被生成 configmap 的 yaml 放在<code>manifests/cluster-config.yaml</code>)里，在集群起来后会存在<code>kube-system cm/cluster-config-v1</code>里</p><h5 id="生成部署配置文件"><a href="#生成部署配置文件" class="headerlink" title="生成部署配置文件"></a>生成部署配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp install-config.yaml  install-config.yaml.$(date +%Y%m%d)</span><br></pre></td></tr></table></figure><p>创建部署清单manifests</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openshift-install create manifests</span><br></pre></td></tr></table></figure><p>如果你的 worker 节点很多，不希望 master 节点上有pod被调度，修改 <code>manifests/cluster-scheduler-02-config.yml</code> 文件，将 <code>mastersSchedulable</code> 的值设为 flase，以防止 Pod 调度到控制节点。</p><p>后面的install create会转换manifest目录下所有文件，这里备份下manifest目录，有兴趣后面可以研究下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp -a manifests manifests-bak</span><br><span class="line"><span class="meta">#</span><span class="bash"> 把部署清单转换成 Ignition 配置文件</span></span><br><span class="line">openshift-install create ignition-configs</span><br></pre></td></tr></table></figure><p><code>ll manifests/</code>目录是一堆yaml</p><p>Ignition 生成的文件如下，也有隐藏的文件，有兴趣可以去看看</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── auth</span><br><span class="line">│   ├── kubeadmin-password</span><br><span class="line">│   └── kubeconfig</span><br><span class="line">├── bootstrap.ign</span><br><span class="line">├── master.ign</span><br><span class="line">├── metadata.json</span><br><span class="line">└── worker.ign</span><br></pre></td></tr></table></figure><p>把所有 ignition 文件复制到 http server 的目录里</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\cp -a /data/ocpinstall/*.ign /data/install-nginx/install/ignition/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 由于nginx容器用户是nginx，上面的ign文件的o是0，增加下o的r的权限</span></span><br><span class="line">chmod o+r /data/install-nginx/install/ignition/*.ign</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 改名下raw.gz的名字，以便在 boot cmdline的时候少输入url</span></span><br><span class="line">cd /data/install-nginx/install/</span><br><span class="line">mv rhcos-4.*-x86_64-metal.x86_64.raw.gz rhcos.raw.gz</span><br></pre></td></tr></table></figure><h3 id="安装集群"><a href="#安装集群" class="headerlink" title="安装集群"></a>安装集群</h3><p>先安装 bootstrap 机器，是因为 master 机器起来的时候会请求<code>curl -sk https://api-int.openshift4.example.com:22623/config/master</code>，如果 bootstrap 机器起来后上面的 容器和 pod 服务没就绪。master 节点安装完系统开机后会一直 retry 这个接口</p><h4 id="bootstrap"><a href="#bootstrap" class="headerlink" title="bootstrap"></a>bootstrap</h4><p>机器创建好后给机器的光驱挂载上前面的 installer 的 iso 文件，开机后会停留在菜单选择界面</p><ol><li>在 RHCOS Installer 安装界面按 <code>Tab</code> 键进入引导参数配置选项</li><li>在默认选项 <code>coreos.inst=yes</code> 之后添加（由于无法拷贝粘贴，请输入<strong>仔细核对</strong>后再回车进行）：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=10.226.45.223::10.226.45.254:255.255.255.0:bootstrap.openshift4.example.com:ens192:none nameserver=10.226.45.250 coreos.inst.install_dev=sda coreos.inst.image_url=http://10.226.45.250:8080/rhcos.raw.gz coreos.inst.ignition_url=http://10.226.45.250:8080/ignition/bootstrap.ign</span><br></pre></td></tr></table></figure><p>参数说明，参数为<code>key=value key1=value1</code> 每个得空格隔开</p><table><thead><tr><th align="center">key</th><th align="center">value</th><th align="center">注意事项</th></tr></thead><tbody><tr><td align="center"><code>ip</code></td><td align="center"><code>$IPADDRESS::$DEFAULTGW:$NETMASK:$HOSTNAMEFQDN:$IFACE:none</code></td><td align="center"><code>IFACE</code>在rhcos里似乎是固定的<code>ens192</code>，我试过了可以不写</td></tr><tr><td align="center"><code>nameserver</code></td><td align="center"><code>$&#123;bastion_ip&#125;</code></td><td align="center">集群的dns server，这里是<code>bastion</code></td></tr><tr><td align="center"><code>coreos.inst.install_dev</code></td><td align="center"><code>sda</code></td><td align="center">rhcos 安装在哪块儿硬盘上，测试过只有一块硬盘也不会自动识别，所以不能省略，我的环境是 vsphere，硬盘是 sda，没有在 openstack 的虚机下试过(这样可能写vda？)</td></tr><tr><td align="center"><code>coreos.inst.image_url</code></td><td align="center"><code>http://$&#123;bastion_ip&#125;:8080/rhcos-metal.raw.gz</code></td><td align="center"><code>rhcos-metal</code>的 url，根据自己http server的实际 url 填写</td></tr><tr><td align="center"><code>coreos.inst.ignition_url</code></td><td align="center"><code>http://$&#123;bastion_ip&#125;:8080/ignition/$&#123;type&#125;.ign</code></td><td align="center">ignition 的 url 链接，bootstrap 则结尾是 <code>bootstrap.ign</code>，master 则是 <code>master.gin</code></td></tr></tbody></table><p><img src="https://cdn.jsdelivr.net/gh/zhangguanzhang/Image-Hosting/picgo/rhcos_install_boot_cmdline.png" alt="boot_cmdline"></p><ol start="3"><li>如果安装有问题会进入 <code>emergency shell</code>，检查网络、域名解析是否正常，如果不正常一般是以上参数输入有误，reboot 退出 shell 回到第一步重新开始。</li></ol><p>安装成功后从基础节点通过命令 <code>ssh -i ~/.ssh/new_rsa core@10.226.45.250</code> 登录 bootstrap 节点，然后<code>sudo su</code>切换到 root 后验证：</p><ul><li>网络配置是否符合自己的设定：<ul><li><code>hostname</code></li><li><code>ip route</code></li><li><code>cat /etc/resolv.conf</code></li></ul></li><li>验证是否成功启动 bootstrap 相应服务：<ul><li><code>podman ps -a</code> 查看服务是否以容器方式运行</li><li>使用 <code>ss -tulnp</code> 查看 6443 和 22623 端口是否启用。</li></ul></li></ul><p>这里简单介绍一下 bootstrap 节点的启动流程，它启动后运行<code>bootkube.service</code>，systemd 的 <code>ConditionPathExists</code> 可以看到用文件作为防止成功后再次运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@bootstrap core]# systemctl cat bootkube</span><br><span class="line"><span class="meta">#</span><span class="bash"> /etc/systemd/system/bootkube.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Bootstrap a Kubernetes cluster</span><br><span class="line">Requires=crio-configure.service</span><br><span class="line">Wants=kubelet.service</span><br><span class="line">After=kubelet.service crio-configure.service</span><br><span class="line">ConditionPathExists=!/opt/openshift/.bootkube.done</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/opt/openshift</span><br><span class="line">ExecStart=/usr/local/bin/bootkube.sh</span><br></pre></td></tr></table></figure><p>查看上面的脚本内容，我们会发现它会先通过 <code>podman</code> 跑一些容器，然后在容器里面启动临时控制平面，这个临时控制平面是通过 <code>CRIO</code> 跑在容器里的，有点绕。。看下面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@bootstrap core]# podman ps -a</span><br><span class="line">CONTAINER ID  IMAGE                                                                                                                    COMMAND               CREATED         STATUS                     PORTS  NAMES</span><br><span class="line">7a9a800c6819  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:416ad8f3ddd49adae4f7db66f8a130319084604296c076c2a6d22264a5688d65   start --tear-down...  12 minutes ago  Up 12 minutes ago                 strange_yonath</span><br><span class="line">8997b001e324  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bf0d5fd64ab53dfdd477c90a293f3ec90379a22e8b356044082e807565699863   render --dest-dir...  12 minutes ago  Exited (0) 12 minutes ago         stoic_williamson</span><br><span class="line">7c45d60ecd39  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bcd6cd1559b62e4a8031cf0e1676e25585845022d240ac3d927ea47a93469597   bootstrap --etcd-...  12 minutes ago  Exited (0) 12 minutes ago         lucid_heisenberg</span><br><span class="line">fe300339df9b  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:74c4a3c93c7fba691195dec0190a47cf194759b381d41045a52b6c86aa4169c4   render --prefix=c...  12 minutes ago  Exited (0) 12 minutes ago         suspicious_kepler</span><br><span class="line">102c20d247d7  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ddb26e047d0e0d7b11bdb625bd7a941ab66f7e1ef5a5f7455d8694e7ba48989d   /usr/bin/cluster-...  12 minutes ago  Exited (0) 12 minutes ago         optimistic_euler</span><br><span class="line">1b7c759792c0  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:ca556d4515818e3e10d2e771d986784460509146ea9dd188fb8ba6f6ac694132   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         nice_lehmann</span><br><span class="line">8dab8ea23ebd  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e4758039391099dc1b0265099474113dcd8bcce84a1c92d02c1ef760793079e6   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         clever_meitner</span><br><span class="line">8bd804fb88bf  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c93a0de1e4cb3f04e37d547c1b81a2a22c4d9d01c013374c466ed3fd0416215a   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         recursing_ramanujan</span><br><span class="line">11dc7e222bf9  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd   /usr/bin/cluster-...  13 minutes ago  Exited (0) 13 minutes ago         wizardly_easley</span><br><span class="line">b60208539287  registry.openshift4.example.com/ocp4/openshift4@sha256:7ad540594e2a667300dd2584fe2ede2c1a0b814ee6a62f60809d87ab564f4425  render --output-d...  13 minutes ago  Exited (0) 13 minutes ago         goofy_bassi</span><br><span class="line">428dec4624e1  quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:13179789b2e15ecf749f5ab51cf11756e2831bc019c02ed0659182e805e725dd   /usr/bin/grep -oP...  13 minutes ago  Exited (0) 13 minutes ago         pedantic_northcutt</span><br><span class="line">[root@bootstrap core]# crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                                                                  NAMESPACE                             ATTEMPT</span><br><span class="line">ec6155d87b866       13 minutes ago      Ready               bootstrap-kube-scheduler-bootstrap.openshift4.example.com             kube-system                           0</span><br><span class="line">6808217a146b7       13 minutes ago      Ready               bootstrap-kube-controller-manager-bootstrap.openshift4.example.com    kube-system                           0</span><br><span class="line">7f1ae27edd3fe       13 minutes ago      Ready               bootstrap-kube-apiserver-bootstrap.openshift4.example.com             kube-system                           0</span><br><span class="line">cc92475639870       13 minutes ago      Ready               cloud-credential-operator-bootstrap.openshift4.example.com            openshift-cloud-credential-operator   0</span><br><span class="line">08c45b635d5c1       13 minutes ago      Ready               bootstrap-cluster-version-operator-bootstrap.openshift4.example.com   openshift-cluster-version             0</span><br><span class="line">69fc31911a89f       14 minutes ago      Ready               bootstrap-machine-config-operator-bootstrap.openshift4.example.com    default                               0</span><br><span class="line">7d2598ac710e6       14 minutes ago      Ready               etcd-bootstrap-member-bootstrap.openshift4.example.com                openshift-etcd                        0</span><br></pre></td></tr></table></figure><p>如果你快速查看上面的可能一时半会儿没起来，可以通过下面命令持续观察脚本运行的日志，<code>bootkube.sh</code> 正常运行完（所有master也上来后）会出现<code>bootkube.service complete</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> journalctl -b -f -u bootkube.service</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Jun 05 00:24:12 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:12.108179       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:21 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:21.595680       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:26 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:26.250214       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:26 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:26.306421       1 waitforceo.go:67] waiting on condition EtcdRunningInCluster in etcd CR /cluster to be True.</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:29.097072       1 waitforceo.go:64] Cluster etcd operator bootstrapped successfully</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com bootkube.sh[12571]: I0605 00:24:29.097306       1 waitforceo.go:58] cluster-etcd-operator bootstrap etcd</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com podman[16531]: 2020-06-05 00:24:29.120864426 +0000 UTC m=+17.965364064 container died 77971b6ca31755a89b279fab6f9c04828c4614161c2e678c7cba48348e684517 (image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9f7a02df3a5d91326d95e444e2e249f8205632ae986d6dccc7f007ec65c8af77, name=recursing_cerf)</span><br><span class="line">Jun 05 00:24:29 bootstrap.openshift4.example.com bootkube.sh[12571]: bootkube.service complete</span><br></pre></td></tr></table></figure><h4 id="master"><a href="#master" class="headerlink" title="master"></a>master</h4><p>在 bootstrap 的 crictl pods 没问题后，我们来启动 master节点，参考 bootstrap，同样配置 boot cmdline 启动，注意 ip，hostname，ign结尾的<code>类型.ign</code>，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=10.226.45.251::10.226.45.254:255.255.255.0:master1.openshift4.example.com:ens192:none nameserver=10.226.45.250 coreos.inst.install_dev=sda coreos.inst.image_url=http://10.226.45.250:8080/rhcos.raw.gz coreos.inst.ignition_url=http://10.226.45.250:8080/ignition/master.ign</span><br></pre></td></tr></table></figure><h4 id="worker"><a href="#worker" class="headerlink" title="worker"></a>worker</h4><p>此时你worker也可以按照相关参数启动，下面是参考，我这里是后面 bootstrap 完成后重装它去启动作为worker的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=10.226.45.223::10.226.45.254:255.255.255.0:worker1.openshift4.example.com:ens192:none nameserver=10.226.45.250 coreos.inst.install_dev=sda coreos.inst.image_url=http://10.226.45.250:8080/rhcos.raw.gz coreos.inst.ignition_url=http://10.226.45.250:8080/ignition/worker.ign</span><br></pre></td></tr></table></figure><p>一样的安装，注意<code>ingition_url</code>结尾是<code>worker.ign</code>，起来后 node 的 csr 得批准</p><p>查看挂起的证书签名请求（CSR），并确保添加到集群的每台节点都能看到具有 <code>Pending</code> 或 <code>Approved</code> 状态的客户端和服务端请求。针对 Pending 状态的 CSR 批准请求：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc get csr</span><br><span class="line">oc adm certificate approve xxx</span><br></pre></td></tr></table></figure><p>或者执行以下命令批准所有 CSR：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CSR_NAMES=`oc get csr -o json | jq -r <span class="string">&#x27;.items[] | select(.status == &#123;&#125; ) | .metadata.name&#x27;</span>`</span><br><span class="line">[ -n <span class="string">&quot;<span class="variable">$CSR_NAMES</span>&quot;</span> ] &amp;&amp;  oc adm certificate approve <span class="variable">$CSR_NAMES</span></span><br></pre></td></tr></table></figure><p>后续添加 worker 重复此步骤即可</p><h4 id="验证集群"><a href="#验证集群" class="headerlink" title="验证集群"></a>验证集群</h4><h5 id="配置-oc-kubeconfig"><a href="#配置-oc-kubeconfig" class="headerlink" title="配置 oc kubeconfig"></a>配置 oc kubeconfig</h5><p>在 bastion 节点上,我们生成部署清单的时候会产生一个<code>kubeconfig</code>，目录<code>/data/ocpinstall/auth</code></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">├── auth</span><br><span class="line">│   ├── kubeadmin-password</span><br><span class="line">│   └── kubeconfig</span><br></pre></td></tr></table></figure><p>像 k8s 那样，我们需要拷贝到默认读取目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.kube</span><br><span class="line">\cp /data/ocpinstall/auth/kubeconfig ~/.kube/config</span><br><span class="line">oc whoami</span><br></pre></td></tr></table></figure><p>配置下 oc 的自动补全</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cat&gt;</span><span class="bash">/etc/bash_completion.d/oc &lt;&lt;<span class="string">&#x27;EOF&#x27;</span></span></span><br><span class="line">source &lt;(oc completion bash)</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h5 id="验证集群节点"><a href="#验证集群节点" class="headerlink" title="验证集群节点"></a>验证集群节点</h5><p>查看集群节点是否上来</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oc  get node</span></span><br><span class="line">NAME                             STATUS   ROLES           AGE     VERSION</span><br><span class="line">master1.openshift4.example.com   Ready    master,worker   3d14h   v1.18.3+6c42de8</span><br><span class="line">master2.openshift4.example.com   Ready    master,worker   3d14h   v1.18.3+6c42de8</span><br><span class="line">master3.openshift4.example.com   Ready    master,worker   3d14h   v1.18.3+6c42de8</span><br></pre></td></tr></table></figure><p>查看集群的 pod，如果很久之后某些 pod 还是一直 crash 的话看下面<code>troubleshooting</code>的，因为<a href="https://github.com/openshift/cluster-bootstrap/blob/master/pkg/start/status.go#L46-L106">源码里会等待所有pod不再crash</a>  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc get po -A</span><br></pre></td></tr></table></figure><p>使用 openshift-install 命令查看 bootstrap 完成否，会需要等很久，有问题看下文的 troubleshooting</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> openshift-install --dir=/data/ocpinstall wait-for bootstrap-complete --log-level=debug</span></span><br><span class="line">DEBUG OpenShift Installer 4.5.9</span><br><span class="line">DEBUG Built from commit 0d5c871ce7d03f3d03ab4371dc39916a5415cf5c</span><br><span class="line">INFO Waiting up to 20m0s for the Kubernetes API at https://api.openshift4.example.com:6443...</span><br><span class="line">INFO API v1.18.3+6c42de8 up</span><br><span class="line">INFO Waiting up to 40m0s for bootstrapping to complete...</span><br><span class="line">DEBUG Bootstrap status: complete</span><br><span class="line">INFO It is now safe to remove the bootstrap resources</span><br><span class="line">DEBUG Time slapsed per state:</span><br><span class="line">DEBUG Bootstrap Complete: 24m13s</span><br><span class="line">INFO Time elapsed: 24m13s</span><br></pre></td></tr></table></figure><p>这里<a href="https://github.com/openshift/cluster-bootstrap/blob/master/pkg/start/start.go#L105-L129">查看源码</a>，实际上 bootkube.sh 最后阶段会向集群里注入一个 cm <code>kube-system/bootstrap</code>存放状态。而上面的<code>wait-for bootstrap-complete</code>实际上就是等待这个cm创建，可以<a href="https://github.com/openshift/installer/blob/master/cmd/openshift-install/create.go#L312">查看源码里的waitForBootstrapConfigMap方法</a></p><h4 id="web-console"><a href="#web-console" class="headerlink" title="web console"></a>web console</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /data/ocpinstall</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> openshift-install wait-for install-complete</span></span><br><span class="line">INFO Waiting up to 30m0s for the cluster at https://api.openshift4.example.com:6443 to initialize...</span><br><span class="line">INFO Waiting up to 10m0s for the openshift-console route to be created...</span><br><span class="line">INFO Install complete!</span><br><span class="line">INFO To access the cluster as the system:admin user when using &#x27;oc&#x27;, run &#x27;export KUBECONFIG=/data/ocpinstall/auth/kubeconfig&#x27;</span><br><span class="line">INFO Access the OpenShift web-console here: https://console-openshift-console.apps.openshift4.example.com</span><br><span class="line">INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;xxxx-xxxx-zKu8J-W7QRi&quot;</span><br><span class="line">INFO Time elapsed: 1s  </span><br></pre></td></tr></table></figure><p>注意最后提示访问 <code>Web Console</code> 的网址及用户密码。如果密码忘了也没关系，可以查看文件 <code>/data/ocpinstall/auth/kubeadmin-password</code> 来获得密码。</p><p>本地访问 Web Console，因为集群里已经有 ingress controller 了，并且我们的入口是 10.226.45.250 ，所以本地需要添加下列 hosts：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.226.45.250 console-openshift-console.apps.openshift4.example.com</span><br><span class="line">10.226.45.250 oauth-openshift.apps.openshift4.example.com</span><br></pre></td></tr></table></figure><p>浏览器访问 <code>https://console-openshift-console.apps.openshift4.example.com</code>，输入上面输出的用户名(kubeadm)和密码登录。首次登录后顶部会提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">You are logged <span class="keyword">in</span> as a temporary administrative user. Update the Cluster OAuth configuration to allow others to <span class="built_in">log</span> <span class="keyword">in</span>.</span><br></pre></td></tr></table></figure><p>我们可以通过 htpasswd 自定义管理员账号，步骤如下：</p><p>1 <code>cd /data/ocpinstall/auth &amp;&amp; htpasswd -c -B -b users.htpasswd admin xxxxx</code></p><p>2 将 <code>users.htpasswd</code> 文件下载到本地。</p><p>3 在 Web Console 页面打开 <code>Global Configuration</code>：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605150947.png" alt="Global Configuration"></p><p>然后找到 <code>OAuth</code>，点击进入，然后添加 <code>HTPasswd</code> 类型的 <code>Identity Providers</code>，并上传 <code>users.htpasswd</code> 文件。</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605151307.png" alt="htpasswd"></p><p>4 退出当前用户，要注意退出到如下界面：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605151646.png" alt="login"></p><p>选择 <code>htpasswd</code>，然后输入之前创建的用户名密码登录。</p><p>如果退出后出现的就是用户密码输入窗口，实际还是 <code>kube:admin</code> 的校验，如果未出现如上提示，可以手动输入 Web Console 地址来自动跳转。</p><p>5 登录后貌似能看到 <code>Administrator</code> 菜单项，但访问如 <code>OAuth Details</code> 仍然提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oauths.config.openshift.io <span class="string">&quot;cluster&quot;</span> is forbidden: User <span class="string">&quot;admin&quot;</span> cannot get resource <span class="string">&quot;oauths&quot;</span> <span class="keyword">in</span> API group <span class="string">&quot;config.openshift.io&quot;</span> at the cluster scope</span><br></pre></td></tr></table></figure><p>因此需要授予集群管理员权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc adm policy add-cluster-role-to-user cluster-admin admin</span><br></pre></td></tr></table></figure><p>Web Console 部分截图：</p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605152528.png" alt="operatorhub"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605152729.png" alt="routes"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605152911.png" alt="dashboards"></p><p><img src="https://cdn.jsdelivr.net/gh/yangchuansheng/imghosting/img/20200605153048.png" alt="metrics"></p><p>如果想删除默认账号，可以执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc -n kube-system delete secrets kubeadmin</span><br></pre></td></tr></table></figure><h4 id="集群善后的一些配置"><a href="#集群善后的一些配置" class="headerlink" title="集群善后的一些配置"></a>集群善后的一些配置</h4><h5 id="ingress-controller"><a href="#ingress-controller" class="headerlink" title="ingress controller"></a>ingress controller</h5><p>github 地址为<a href="https://github.com/openshift/cluster-ingress-operator">cluster-ingress-operator</a><br>这里我集群是3个 master，发现ingress controller的一些属性没配置对，自行先<code>get deploy router-default -o yaml</code>查看，可能后续修复了。根据推测排查到是 operator 部署的，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ~]# oc get ingresscontrollers -A</span><br><span class="line">NAMESPACE                    NAME      AGE</span><br><span class="line">openshift-ingress-operator   default   3d15h</span><br></pre></td></tr></table></figure><p>但是看了下属性<code>oc explain ingresscontrollers.spec.endpointPublishingStrategy.hostNetwork</code>发现不支持配置互斥和<code>dnsPolicy</code>，这会导致无法解析集群内的svc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@master2 ~]# crictl pods --name router-default-fb744fb7f-hmmn5 -q</span><br><span class="line">0a0c7cc6d1ad6815f7613fd758c5329c4265ddb6607f568b69e30fdafdfc0a52</span><br><span class="line">[root@master2 ~]# crictl ps --pod=0a0c7cc6d1ad6815f7613fd758c5329c4265ddb6607f568b69e30fdafdfc0a52</span><br><span class="line">CONTAINER           IMAGE                                                              CREATED             STATE               NAME                ATTEMPT             POD ID</span><br><span class="line">b04c17fb1c58d       dd7aaceb9081f88c9ba418708f32a66f5de4e527a00c7f6ede50d55c93eb04ed   3 days ago          Running             router              1                   0a0c7cc6d1ad6</span><br><span class="line">[root@master2 ~]# crictl exec b04 cat /etc/resolv.conf</span><br><span class="line">search openshift4.example.com</span><br><span class="line">nameserver 10.226.45.250</span><br><span class="line">[root@master2 ~]# crictl exec b04 curl  kubernetes.default.svc.cluster.local</span><br><span class="line"><span class="meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span></span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: kubernetes.default.svc.cluster.local; Unknown error</span><br><span class="line">FATA[0000] execing command in container failed: command terminated with exit code 6</span><br></pre></td></tr></table></figure><p>这个我已经提<a href="https://github.com/openshift/cluster-ingress-operator/issues/464">issue</a> 了</p><p>调整下 ingress controller 的数量，因为 deployment 由 crd 纳管，最好不要直接去操作 deploy 的属性，同时这里我们是把 ingress controller 部署在 master 上，所以得用 nodeSelector 固定，先打 label</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">oc label node master1.openshift4.example.com ingressControllerDeploy=true</span><br><span class="line">oc label node master2.openshift4.example.com ingressControllerDeploy=true</span><br><span class="line">oc label node master3.openshift4.example.com ingressControllerDeploy=true</span><br><span class="line"></span><br><span class="line">oc -n openshift-ingress-operator patch ingresscontroller default --type=&#x27;merge&#x27; -p &quot;$(cat &lt;&lt;- EOF</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  nodePlacement:</span><br><span class="line">    nodeSelector:</span><br><span class="line">      matchLabels:</span><br><span class="line">        ingressControllerDeploy: &quot;true&quot;</span><br><span class="line">EOF</span><br><span class="line">)&quot;</span><br></pre></td></tr></table></figure><h5 id="移除haproxy-里-bootstrap-的配置"><a href="#移除haproxy-里-bootstrap-的配置" class="headerlink" title="移除haproxy 里 bootstrap 的配置"></a>移除haproxy 里 bootstrap 的配置</h5><p>自行操作，移除负载均衡里的无用配置</p><h5 id="Local-Storage-Operator"><a href="#Local-Storage-Operator" class="headerlink" title="Local Storage Operator"></a>Local Storage Operator</h5><p>参考 <a href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.5/html/storage/persistent-storage-using-local-volume#local-storage-install_persistent-storage-local">Installing the Local Storage Operator</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oc oc new-project local-storage</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="clusteroperator"><a href="#clusteroperator" class="headerlink" title="clusteroperator"></a>clusteroperator</h5><p>查看 clusteroperator，如果有个别有问题，可以等worker部署完了慢慢排查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ~]# oc get clusteroperator</span><br><span class="line">NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE</span><br><span class="line">authentication                             4.5.9     True        False         False      118m</span><br><span class="line">cloud-credential                           4.5.9     True        False         False      3d19h</span><br><span class="line">cluster-autoscaler                         4.5.9     True        False         False      3d18h</span><br><span class="line">config-operator                            4.5.9     True        False         False      3d18h</span><br><span class="line">console                                    4.5.9     True        False         False      8m22s</span><br><span class="line">csi-snapshot-controller                    4.5.9     True        False         False      8m27s</span><br><span class="line">dns                                        4.5.9     True        False         False      8m22s</span><br><span class="line">etcd                                       4.5.9     True        False         False      3d19h</span><br><span class="line">image-registry                             4.5.9     True        False         False      3d19h</span><br><span class="line">ingress                                    4.5.9     True        False         False      8m24s</span><br><span class="line">insights                                   4.5.9     True        False         False      3d19h</span><br><span class="line">kube-apiserver                             4.5.9     True        False         False      3d19h</span><br><span class="line">kube-controller-manager                    4.5.9     True        False         False      3d19h</span><br><span class="line">kube-scheduler                             4.5.9     True        False         False      3d19h</span><br><span class="line">kube-storage-version-migrator              4.5.9     True        False         False      8m23s</span><br><span class="line">machine-api                                4.5.9     True        False         False      3d19h</span><br><span class="line">machine-approver                           4.5.9     True        False         False      3d19h</span><br><span class="line">machine-config                             4.5.9     True        False         False      3d19h</span><br><span class="line">marketplace                                4.5.9     True        False         False      3d19h</span><br><span class="line">monitoring                                 4.5.9     True        False         False      3d18h</span><br><span class="line">network                                    4.5.9     True        False         False      3d19h</span><br><span class="line">node-tuning                                4.5.9     True        False         False      8m27s</span><br><span class="line">openshift-apiserver                        4.5.9     True        False         False      8m20s</span><br><span class="line">openshift-controller-manager               4.5.9     True        False         False      8m5s</span><br><span class="line">openshift-samples                          4.5.9     True        False         False      3d18h</span><br><span class="line">operator-lifecycle-manager                 4.5.9     True        False         False      3d19h</span><br><span class="line">operator-lifecycle-manager-catalog         4.5.9     True        False         False      3d19h</span><br><span class="line">operator-lifecycle-manager-packageserver   4.5.9     True        False         False      7m56s</span><br><span class="line">service-ca                                 4.5.9     True        False         False      3d19h</span><br><span class="line">storage                                    4.5.9     True        False         False      3d19h</span><br></pre></td></tr></table></figure><h4 id="troubleshooting"><a href="#troubleshooting" class="headerlink" title="troubleshooting"></a>troubleshooting</h4><p>参考<a href="https://github.com/openshift/installer/blob/master/docs/user/troubleshooting.md">官方troubleshooting文档</a>，很多理念还是 k8s，所以多用 oc 命令排查</p><p>这里是我单 master 时候的错误，可以参考下排查思路</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ocpinstall]# oc get po -A</span><br><span class="line">NAMESPACE                                          NAME                                                      READY   STATUS             RESTARTS   AGE</span><br><span class="line">openshift-apiserver-operator                       openshift-apiserver-operator-6ddb679b87-478mq             1/1     Running            1          65m</span><br><span class="line">openshift-authentication-operator                  authentication-operator-f9c8c4d9-gqkhl                    1/1     Running            6          65m</span><br><span class="line">openshift-cluster-machine-approver                 machine-approver-55ccb88c4-cfxtm                          2/2     Running            0          65m</span><br><span class="line">openshift-cluster-node-tuning-operator             cluster-node-tuning-operator-576df548b-mbzcz              1/1     Running            0          65m</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-8vv2c                                               1/1     Running            0          57m</span><br><span class="line">openshift-cluster-storage-operator                 csi-snapshot-controller-operator-59cfdb9dc6-8qsm2         1/1     Running            0          65m</span><br><span class="line">openshift-cluster-version                          cluster-version-operator-6fc5bf7855-bjdbg                 1/1     Running            0          65m</span><br><span class="line">openshift-controller-manager-operator              openshift-controller-manager-operator-6d85b6f94-zxdc4     1/1     Running            1          65m</span><br><span class="line">openshift-dns-operator                             dns-operator-b47fff57d-4vlbs                              2/2     Running            0          65m</span><br><span class="line">openshift-dns                                      dns-default-qq6lt                                         3/3     Running            0          56m</span><br><span class="line">openshift-etcd-operator                            etcd-operator-5bcff88f49-mqvdh                            1/1     Running            1          65m</span><br><span class="line">openshift-kube-apiserver-operator                  kube-apiserver-operator-79b99c5564-c24z8                  1/1     Running            1          65m</span><br><span class="line">openshift-kube-apiserver                           installer-2-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master1.openshift4.example.com             3/4     CrashLoopBackOff   14         55m</span><br><span class="line">openshift-kube-controller-manager-operator         kube-controller-manager-operator-656db94888-gxlh8         1/1     Running            1          65m</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-controller-manager                  installer-4-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master1.openshift4.example.com    4/4     Running            0          55m</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master1.openshift4.example.com          0/1     Completed          0          55m</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-4-master1.openshift4.example.com          0/1     Completed          0          54m</span><br><span class="line">openshift-kube-scheduler-operator                  openshift-kube-scheduler-operator-5db8ddbd86-zgc2j        1/1     Running            1          65m</span><br><span class="line">openshift-kube-scheduler                           installer-2-master1.openshift4.example.com                0/1     Completed          0          57m</span><br><span class="line">openshift-kube-scheduler                           installer-3-master1.openshift4.example.com                0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           installer-4-master1.openshift4.example.com                0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           installer-5-master1.openshift4.example.com                0/1     Completed          0          55m</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master1.openshift4.example.com   2/2     Running            0          55m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-2-master1.openshift4.example.com          0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-3-master1.openshift4.example.com          0/1     Completed          0          56m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master1.openshift4.example.com          0/1     Completed          0          55m</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master1.openshift4.example.com          0/1     Completed          0          54m</span><br><span class="line">openshift-kube-storage-version-migrator-operator   kube-storage-version-migrator-operator-57c5ccbffc-rsfgm   1/1     Running            1          65m</span><br><span class="line">openshift-kube-storage-version-migrator            migrator-84f6d56959-7fjrb                                 1/1     Running            0          57m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-5gx99                         0/1     Pending            0          56m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-dpjf8                         0/1     Pending            0          56m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-whsdq                         0/1     Running            0          56m</span><br><span class="line">openshift-machine-config-operator                  machine-config-controller-c5f979c7c-jrr7h                 1/1     Running            0          56m</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-hqfns                               2/2     Running            0          57m</span><br><span class="line">openshift-machine-config-operator                  machine-config-operator-7f9fc9fb9-6zh9j                   1/1     Running            0          65m</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-w84sq                               1/1     Running            0          56m</span><br><span class="line">openshift-multus                                   multus-admission-controller-z9hc4                         2/2     Running            0          57m</span><br><span class="line">openshift-multus                                   multus-jfwbz                                              1/1     Running            0          58m</span><br><span class="line">openshift-network-operator                         network-operator-84bb7765bc-7c2jm                         1/1     Running            0          65m</span><br><span class="line">openshift-operator-lifecycle-manager               catalog-operator-5ff486f6c8-tplsr                         1/1     Running            0          65m</span><br><span class="line">openshift-operator-lifecycle-manager               olm-operator-b4fd47678-wmpxt                              1/1     Running            0          65m</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-b68fc7b76-g8fxb                             1/1     Running            0          72s</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-b68fc7b76-z9t72                             1/1     Running            0          79s</span><br><span class="line">openshift-sdn                                      ovs-r69fg                                                 1/1     Running            0          57m</span><br><span class="line">openshift-sdn                                      sdn-6js24                                                 1/1     Running            0          57m</span><br><span class="line">openshift-sdn                                      sdn-controller-sk2f9                                      1/1     Running            0          57m</span><br><span class="line">openshift-service-ca-operator                      service-ca-operator-57cd4bc97f-t2mnq                      1/1     Running            1          65m</span><br><span class="line">openshift-service-ca                               service-ca-c75f9fb85-grcsm                                1/1     Running            0          57m</span><br></pre></td></tr></table></figure><p>发现<code>kube-apiserver-master1.openshift4.example.com</code>的 pod 有个容器一直 crash，从 bastion 上 ssh 到 master1 上去后查看日志</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 apiserver 的容器组</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> crictl ps --pod=$(crictl pods --name=kube-apiserver-master1.openshift4.example.com --quiet)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到失败的容器，查看<span class="built_in">log</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> crictl logs xxx</span></span><br><span class="line">...</span><br><span class="line">W0917 06:31:23.240165    1 clientconn.go:1208] grpc: addrConn.createTransport failed to connect to &#123;https://10.226.45.251:2379 &lt;nil&gt; 0 &lt;nil&gt;&#125;. Err :connection error: desc = &quot;transport: Error while dialing dial tcp 10.226.45.251:2379: connect: connection refused&quot;. Reconnecting...</span><br></pre></td></tr></table></figure><p>查看发现没有etcd的容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crictl ps -a | grep etcd</span><br></pre></td></tr></table></figure><p>回到 bastion 上，查看 etcd 来源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> oc get po -A | grep etcd</span></span><br><span class="line">openshift-etcd-operator                            etcd-operator-5bcff88f49-jf4rk                            1/1     Running            1          100m</span><br><span class="line"><span class="meta">#</span><span class="bash"> 有operator，查看deloy</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> oc get deployment -A | grep etcd</span></span><br><span class="line">openshift-etcd-operator                            etcd-operator                            1/1     1            1           100m</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard                        0/1     1            0           91m</span><br></pre></td></tr></table></figure><p>从时间上看，operator 先部署，然后operator 部署了 etcd-quorum-guard 的pod，但是没就绪。这个 etcd 来源于 crd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> oc -n openshift-etcd get etcd</span></span><br><span class="line">NAME     AGE</span><br><span class="line">cluster  138m</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> oc -n openshift-etcd get etcd cluster -o yaml</span></span><br><span class="line">...</span><br><span class="line">- lastTransitionTime: &quot;2020-09-17T05:33:17Z&quot;</span><br><span class="line">  message: still wating for trhee healthz etcd members</span><br><span class="line">  reason：NotEnoughEtcdMembers</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h5 id="单-master-的无法部署的处理手段"><a href="#单-master-的无法部署的处理手段" class="headerlink" title="单 master 的无法部署的处理手段"></a>单 master 的无法部署的处理手段</h5><p>看输出是期待 3 个 etcd member，我们需要更改成非HA的，执行下面命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">oc patch etcd cluster -p=&#x27;</span><br><span class="line">&#123;&quot;spec&quot;: &#123;&quot;unsupportedConfigOverrides&quot;: &#123;&quot;useUnsupportedUnsafeNonHANonProductionUnstableEtcd&quot;: true&#125;&#125;&#125;</span><br><span class="line">&#x27; </span><br><span class="line">--type=merge</span><br></pre></td></tr></table></figure><p>ocp4.6.+ 除了要应用以上<code>etcd</code>配置更新外，还需要配置更改<code>oauth</code>对于APIServer HA 的要求，参<a href="https://github.com/openshift/okd/issues/465">https://github.com/openshift/okd/issues/465</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oc patch authentication cluster -p=&#x27;&#123;&quot;spec&quot;:&#123;&quot;unsupportedConfigOverrides&quot;:&#123;&quot;useUnsupportedUnsafeNonHANonProductionUnstableOAuthServer&quot;:true&#125;&#125;&#125;&#x27; --type=merge</span><br></pre></td></tr></table></figure><h5 id="一些参考信息"><a href="#一些参考信息" class="headerlink" title="一些参考信息"></a>一些参考信息</h5><p>master 上 manifests 目录正常参考如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 core]# ll /etc/kubernetes/manifests/</span><br><span class="line">total 48</span><br><span class="line">-rw-r--r--. 1 root root 22092 Sep 17 11:54 etcd-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root  5431 Sep 18 10:17 kube-apiserver-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root  5903 Sep 17 12:01 kube-controller-manager-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root  3531 Sep 17 12:00 kube-scheduler-pod.yaml</span><br><span class="line">-rw-r--r--. 1 root root   697 Sep 17 11:19 recycler-pod.yaml</span><br></pre></td></tr></table></figure><p>具体哪一步骤有问题可以看下面目录的done文件结合<code>/usr/local/bin/bootkube.sh</code>脚本查看，正常完成后是下面的</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@bootstrap kubernetes]# ll /opt/openshift/</span><br><span class="line">total 28</span><br><span class="line">drwxr-xr-x. 2 root root   77 Sep 17 05:21 auth</span><br><span class="line">drwxr-xr-x. 2 root root  176 Sep 17 05:22 bootstrap-manifests</span><br><span class="line">drwxr-xr-x. 4 root root   50 Sep 17 05:22 cco-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 cco-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 config-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 config-bootstrap.done</span><br><span class="line">drw-r--r--. 4 root root   40 Sep 17 05:21 cvo-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:21 cvo-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:21 etcd-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:21 etcd-bootstrap.done</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 ingress-operator-bootstrap.done</span><br><span class="line">drwxr-x---. 2 root root  105 Sep 17 05:22 ingress-operator-manifests</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 kube-apiserver-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 kube-apiserver-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 kube-controller-manager-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 kube-controller-manager-bootstrap.done</span><br><span class="line">drwxr-xr-x. 4 root root   64 Sep 17 05:22 kube-scheduler-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 kube-scheduler-bootstrap.done</span><br><span class="line">drwxr-xr-x. 2 root root 8192 Sep 17 05:22 manifests</span><br><span class="line">drw-r-xr-x. 4 root root   40 Sep 17 05:22 mco-bootstrap</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:22 mco-bootstrap.done</span><br><span class="line">drwxr-xr-x. 2 root root 4096 Sep 17 05:21 openshift</span><br><span class="line">-rw-r--r--. 1 root root    0 Sep 17 05:21 openshift-manifests.done</span><br><span class="line">drwxr-xr-x. 2 root root 8192 Sep 17 05:21 tls</span><br></pre></td></tr></table></figure><p>3个 master 和 1个 node的所有pod情况参考:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion ocpinstall]# oc get po -A</span><br><span class="line">NAMESPACE                                          NAME                                                         READY   STATUS      RESTARTS   AGE</span><br><span class="line">openshift-apiserver-operator                       openshift-apiserver-operator-6ddb679b87-48rfk                1/1     Running     2          3d21h</span><br><span class="line">openshift-apiserver                                apiserver-559b848b77-2x22v                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-apiserver                                apiserver-559b848b77-bctpw                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-apiserver                                apiserver-559b848b77-fwks6                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-authentication-operator                  authentication-operator-f9c8c4d9-n94nf                       1/1     Running     4          3d21h</span><br><span class="line">openshift-authentication                           oauth-openshift-8555c79cc8-hksfc                             1/1     Running     0          145m</span><br><span class="line">openshift-authentication                           oauth-openshift-8555c79cc8-nlmxb                             1/1     Running     0          145m</span><br><span class="line">openshift-cloud-credential-operator                cloud-credential-operator-5b67677b7d-h5kfg                   1/1     Running     0          3d20h</span><br><span class="line">openshift-cluster-machine-approver                 machine-approver-55ccb88c4-xdnkl                             2/2     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             cluster-node-tuning-operator-576df548b-7xb6r                 1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-bw8ql                                                  1/1     Running     0          47m</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-mxpz2                                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-pblld                                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-node-tuning-operator             tuned-w5pct                                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-cluster-samples-operator                 cluster-samples-operator-6bdbc4ffc5-jq7zn                    2/2     Running     0          3d20h</span><br><span class="line">openshift-cluster-storage-operator                 cluster-storage-operator-56f4c88949-844js                    1/1     Running     0          3d20h</span><br><span class="line">openshift-cluster-storage-operator                 csi-snapshot-controller-7d764ffdf8-wnp7h                     1/1     Running     0          3d20h</span><br><span class="line">openshift-cluster-storage-operator                 csi-snapshot-controller-operator-59cfdb9dc6-58ntn            1/1     Running     1          3d21h</span><br><span class="line">openshift-cluster-version                          cluster-version-operator-6fc5bf7855-96p2p                    1/1     Running     0          3d21h</span><br><span class="line">openshift-config-operator                          openshift-config-operator-b79c8b76-5x8c5                     1/1     Running     0          3d20h</span><br><span class="line">openshift-console-operator                         console-operator-887dd965b-28cj2                             1/1     Running     0          3d20h</span><br><span class="line">openshift-console                                  console-7cbd7657d-56bkb                                      1/1     Running     910        3d20h</span><br><span class="line">openshift-console                                  console-7cbd7657d-dqvjz                                      1/1     Running     0          3h28m</span><br><span class="line">openshift-console                                  downloads-54fc476767-59qbt                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-console                                  downloads-54fc476767-hjxhq                                   1/1     Running     0          3d20h</span><br><span class="line">openshift-controller-manager-operator              openshift-controller-manager-operator-6d85b6f94-4t2f7        1/1     Running     2          3d21h</span><br><span class="line">openshift-controller-manager                       controller-manager-2wbpf                                     1/1     Running     0          2d22h</span><br><span class="line">openshift-controller-manager                       controller-manager-pnnk9                                     1/1     Running     0          2d22h</span><br><span class="line">openshift-controller-manager                       controller-manager-qrtg9                                     1/1     Running     0          2d22h</span><br><span class="line">openshift-dns-operator                             dns-operator-b47fff57d-rmnl5                                 2/2     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-h7scc                                            3/3     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-mdsq5                                            3/3     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-r24lg                                            3/3     Running     0          3d21h</span><br><span class="line">openshift-dns                                      dns-default-sm5zl                                            3/3     Running     0          47m</span><br><span class="line">openshift-etcd-operator                            etcd-operator-5bcff88f49-jf4rk                               1/1     Running     1          3d21h</span><br><span class="line">openshift-etcd                                     etcd-master1.openshift4.example.com                          4/4     Running     0          3d20h</span><br><span class="line">openshift-etcd                                     etcd-master2.openshift4.example.com                          4/4     Running     0          3d20h</span><br><span class="line">openshift-etcd                                     etcd-master3.openshift4.example.com                          4/4     Running     0          3d20h</span><br><span class="line">openshift-etcd                                     installer-2-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-etcd                                     installer-2-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-etcd                                     installer-2-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-etcd                                     installer-3-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     installer-3-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     installer-3-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-2-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-2-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-2-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-3-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-3-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-etcd                                     revision-pruner-3-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-image-registry                           cluster-image-registry-operator-574467db97-hgcl6             2/2     Running     0          3d20h</span><br><span class="line">openshift-image-registry                           image-pruner-1600473600-ps7rq                                0/1     Completed   0          2d8h</span><br><span class="line">openshift-image-registry                           image-pruner-1600560000-pzqzt                                0/1     Completed   0          32h</span><br><span class="line">openshift-image-registry                           image-pruner-1600646400-bbghl                                0/1     Completed   0          8h</span><br><span class="line">openshift-image-registry                           node-ca-5s66m                                                1/1     Running     0          3d20h</span><br><span class="line">openshift-image-registry                           node-ca-mn8wr                                                1/1     Running     0          3d20h</span><br><span class="line">openshift-image-registry                           node-ca-w22br                                                1/1     Running     0          47m</span><br><span class="line">openshift-image-registry                           node-ca-wcj74                                                1/1     Running     0          3d20h</span><br><span class="line">openshift-ingress-operator                         ingress-operator-69cc5476dc-scjjx                            2/2     Running     0          3d20h</span><br><span class="line">openshift-ingress                                  router-default-5f5d6bf574-67bbd                              1/1     Running     0          134m</span><br><span class="line">openshift-ingress                                  router-default-5f5d6bf574-d6z84                              1/1     Running     0          133m</span><br><span class="line">openshift-ingress                                  router-default-5f5d6bf574-kcf7v                              1/1     Running     0          135m</span><br><span class="line">openshift-insights                                 insights-operator-6cd58d5859-cf9lg                           1/1     Running     0          3d20h</span><br><span class="line">openshift-kube-apiserver-operator                  kube-apiserver-operator-79b99c5564-tlr67                     1/1     Running     2          3d21h</span><br><span class="line">openshift-kube-apiserver                           installer-10-master1.openshift4.example.com                  0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           installer-10-master2.openshift4.example.com                  0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           installer-10-master3.openshift4.example.com                  0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           installer-11-master1.openshift4.example.com                  0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           installer-11-master2.openshift4.example.com                  0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           installer-11-master3.openshift4.example.com                  0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           installer-3-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-apiserver                           installer-4-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-apiserver                           installer-5-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-5-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-5-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-6-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-7-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-8-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-8-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-8-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           installer-9-master1.openshift4.example.com                   0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           installer-9-master2.openshift4.example.com                   0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           installer-9-master3.openshift4.example.com                   0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master1.openshift4.example.com                4/4     Running     0          2d22h</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master2.openshift4.example.com                4/4     Running     0          2d22h</span><br><span class="line">openshift-kube-apiserver                           kube-apiserver-master3.openshift4.example.com                4/4     Running     0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-10-master1.openshift4.example.com            0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-10-master2.openshift4.example.com            0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-10-master3.openshift4.example.com            0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-11-master1.openshift4.example.com            0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-11-master2.openshift4.example.com            0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-11-master3.openshift4.example.com            0/1     Completed   0          2d22h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-3-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-4-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-5-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-5-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-5-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-6-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-7-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-8-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-8-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-8-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-9-master1.openshift4.example.com             0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-9-master2.openshift4.example.com             0/1     Completed   0          3d3h</span><br><span class="line">openshift-kube-apiserver                           revision-pruner-9-master3.openshift4.example.com             0/1     Completed   0          3d2h</span><br><span class="line">openshift-kube-controller-manager-operator         kube-controller-manager-operator-656db94888-9twxm            1/1     Running     2          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-2-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-3-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  installer-5-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-5-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-5-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-6-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-6-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-6-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-7-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-7-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  installer-7-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master1.openshift4.example.com       4/4     Running     0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master2.openshift4.example.com       4/4     Running     1          3d20h</span><br><span class="line">openshift-kube-controller-manager                  kube-controller-manager-master3.openshift4.example.com       4/4     Running     0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-2-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master2.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-3-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-5-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-5-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-5-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-6-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-6-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-6-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-7-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-7-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-controller-manager                  revision-pruner-7-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler-operator                  openshift-kube-scheduler-operator-5db8ddbd86-b4npw           1/1     Running     1          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-3-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-4-master1.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-4-master2.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-4-master3.openshift4.example.com                   0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           installer-5-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-5-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-5-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-6-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-6-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-7-master1.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-7-master2.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           installer-7-master3.openshift4.example.com                   0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master1.openshift4.example.com      2/2     Running     1          3d20h</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master2.openshift4.example.com      2/2     Running     0          3d20h</span><br><span class="line">openshift-kube-scheduler                           openshift-kube-scheduler-master3.openshift4.example.com      2/2     Running     2          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-3-master2.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master1.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master2.openshift4.example.com             0/1     Completed   0          3d21h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-4-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-5-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-6-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-6-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-7-master1.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-7-master2.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-scheduler                           revision-pruner-7-master3.openshift4.example.com             0/1     Completed   0          3d20h</span><br><span class="line">openshift-kube-storage-version-migrator-operator   kube-storage-version-migrator-operator-57c5ccbffc-7x2tv      1/1     Running     1          3d21h</span><br><span class="line">openshift-kube-storage-version-migrator            migrator-84f6d56959-hnhlb                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-api                              cluster-autoscaler-operator-568855ff87-mxpfm                 2/2     Running     0          3d20h</span><br><span class="line">openshift-machine-api                              machine-api-operator-7744b46cbc-frbmx                        2/2     Running     0          3d20h</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-dgk9t                            1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-g495s                            1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  etcd-quorum-guard-6868b6549-pbwq9                            1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-controller-c5f979c7c-vkmdd                    1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-79k88                                  2/2     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-h6fxj                                  2/2     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-qgdvd                                  2/2     Running     0          47m</span><br><span class="line">openshift-machine-config-operator                  machine-config-daemon-t9gsq                                  2/2     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-operator-7f9fc9fb9-9whkf                      1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-66vdl                                  1/1     Running     0          3d20h</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-x7zw6                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-machine-config-operator                  machine-config-server-xfs4q                                  1/1     Running     0          3d21h</span><br><span class="line">openshift-marketplace                              certified-operators-77d7996cd8-59qhh                         1/1     Running     82         3d20h</span><br><span class="line">openshift-marketplace                              community-operators-7c466b6dc4-87q7n                         1/1     Running     104        3d20h</span><br><span class="line">openshift-marketplace                              marketplace-operator-66f9cd98-c5psh                          1/1     Running     0          3d20h</span><br><span class="line">openshift-marketplace                              redhat-marketplace-799fdd5f6c-pwhb6                          1/1     Running     36         3d20h</span><br><span class="line">openshift-marketplace                              redhat-operators-6c568f687f-t494g                            1/1     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               alertmanager-main-0                                          5/5     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               alertmanager-main-1                                          5/5     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               alertmanager-main-2                                          5/5     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               cluster-monitoring-operator-db8666945-pvsdq                  2/2     Running     2          3d20h</span><br><span class="line">openshift-monitoring                               grafana-6db66b6b79-glzvk                                     2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               kube-state-metrics-6495bd567c-cp47h                          3/3     Running     2          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-6bc5z                                          2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-hv2kb                                          2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-lh4rs                                          2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               node-exporter-sgrh8                                          2/2     Running     0          47m</span><br><span class="line">openshift-monitoring                               openshift-state-metrics-84c7687db5-6dkcq                     3/3     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               prometheus-adapter-674b6c66dc-hmwwt                          1/1     Running     0          2d22h</span><br><span class="line">openshift-monitoring                               prometheus-adapter-674b6c66dc-vwvcq                          1/1     Running     0          2d22h</span><br><span class="line">openshift-monitoring                               prometheus-k8s-0                                             7/7     Running     1          3d20h</span><br><span class="line">openshift-monitoring                               prometheus-k8s-1                                             7/7     Running     1          3d20h</span><br><span class="line">openshift-monitoring                               prometheus-operator-78769c95bb-2w2sc                         2/2     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               telemeter-client-654456d57f-grq8w                            3/3     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               thanos-querier-657fff7b4-p2zb7                               4/4     Running     0          3d20h</span><br><span class="line">openshift-monitoring                               thanos-querier-657fff7b4-tnkxm                               4/4     Running     0          3d20h</span><br><span class="line">openshift-multus                                   multus-9ljk9                                                 1/1     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-admission-controller-2wxgw                            2/2     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-admission-controller-srz97                            2/2     Running     0          3d20h</span><br><span class="line">openshift-multus                                   multus-admission-controller-wh6g8                            2/2     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-nf2vz                                                 1/1     Running     0          47m</span><br><span class="line">openshift-multus                                   multus-pnvbh                                                 1/1     Running     0          3d21h</span><br><span class="line">openshift-multus                                   multus-qw5j4                                                 1/1     Running     0          3d20h</span><br><span class="line">openshift-network-operator                         network-operator-84bb7765bc-92p6g                            1/1     Running     0          3d21h</span><br><span class="line">openshift-operator-lifecycle-manager               catalog-operator-5ff486f6c8-2pjdd                            1/1     Running     0          3d21h</span><br><span class="line">openshift-operator-lifecycle-manager               olm-operator-b4fd47678-c5kvt                                 1/1     Running     0          3d21h</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-c8c74bbcb-64wf2                                1/1     Running     0          102m</span><br><span class="line">openshift-operator-lifecycle-manager               packageserver-c8c74bbcb-skbpd                                1/1     Running     0          100m</span><br><span class="line">openshift-sdn                                      ovs-cnmcx                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      ovs-kftfb                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      ovs-qglgb                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      ovs-tczlx                                                    1/1     Running     0          47m</span><br><span class="line">openshift-sdn                                      sdn-7dxts                                                    1/1     Running     0          3d20h</span><br><span class="line">openshift-sdn                                      sdn-controller-9xkqp                                         1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      sdn-controller-bwcc2                                         1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      sdn-controller-pfq7c                                         1/1     Running     2          3d21h</span><br><span class="line">openshift-sdn                                      sdn-crgmz                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-sdn                                      sdn-kwtss                                                    1/1     Running     0          47m</span><br><span class="line">openshift-sdn                                      sdn-vqzp9                                                    1/1     Running     0          3d21h</span><br><span class="line">openshift-service-ca-operator                      service-ca-operator-57cd4bc97f-9hll7                         1/1     Running     2          3d21h</span><br><span class="line">openshift-service-ca                               service-ca-c75f9fb85-xs454                                   1/1     Running     0          3d21h</span><br><span class="line">openshift-service-catalog-removed                  openshift-service-catalog-apiserver-remover-j7nft            0/1     Completed   0          3h28m</span><br><span class="line">openshift-service-catalog-removed                  openshift-service-catalog-controller-manager-remover-tp4ff   0/1     Completed   0          3h28m</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.5/html/installing_on_bare_metal/installing-restricted-networks-bare-metal#installation-about-restricted-networks_installing-restricted-networks-bare-metal">官方文档 [zh]在受限网络中的裸机上安装集群</a></li><li><a href="https://fuckcloudnative.io/posts/openshift4.4-install-offline-static-1-requirement/">4.4.9部署</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/welcome/index.html">学习文档</a></li><li><a href="https://access.redhat.com/documentation/zh-cn/openshift_container_platform/4.5/pdf/architecture/OpenShift_Container_Platform-4.5-Architecture-zh-CN.pdf">pdf</a></li><li><a href="https://docs.openshift.com/container-platform/4.5/architecture/architecture-rhcos.html">rhcos</a></li></ul><p>其他的 单master 参考:</p><ul><li><a href="https://gist.github.com/williamcaban/7d4fa16c91cf597517e5778428e74658">https://gist.github.com/williamcaban/7d4fa16c91cf597517e5778428e74658</a></li><li><a href="https://misa.gitbook.io/k8s-ocp-yaml/openshift-docs/2020-02-25-openshift4.4-install-online-staticip-allinone">https://misa.gitbook.io/k8s-ocp-yaml/openshift-docs/2020-02-25-openshift4.4-install-online-staticip-allinone</a></li><li><a href="https://cgruver.github.io/okd4-single-node-cluster/">https://cgruver.github.io/okd4-single-node-cluster/</a></li><li><a href="https://github.com/cai11745/k8s-ocp-yaml/blob/master/ocp4/2020-02-25-openshift4.4-install-online-staticIP-allinone.md#%E5%AE%89%E8%A3%85%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6">https://github.com/cai11745/k8s-ocp-yaml/blob/master/ocp4/2020-02-25-openshift4.4-install-online-staticIP-allinone.md#%E5%AE%89%E8%A3%85%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文是全部离线安装，也就是 UPI (User Provisioned Infrastructure) 模式安装，假设机器只能配置静态ip不能有网络配置权限和配置 dhcp 和 pxe。机器可以是物理机和虚拟机。&lt;/p&gt;
&lt;h2 id=&quot;前言介绍&quot;&gt;&lt;a href=&quot;#前言</summary>
      
    
    
    
    
    <category term="openshift" scheme="http://zhangguanzhang.github.io/tags/openshift/"/>
    
    <category term="ocp" scheme="http://zhangguanzhang.github.io/tags/ocp/"/>
    
  </entry>
  
  <entry>
    <title>个人办公用 wireguard 组网笔记</title>
    <link href="http://zhangguanzhang.github.io/2020/08/05/wireguard-for-personal/"/>
    <id>http://zhangguanzhang.github.io/2020/08/05/wireguard-for-personal/</id>
    <published>2020-08-05T18:29:08.000Z</published>
    <updated>2020-08-05T18:29:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>作为 IT 人员，经常需要连到办公网工作，并不是每个公司都有 vpn，自己搭建的话 openvpn 之类的配置麻烦啰嗦。这里写下 wireguard 的简单搭建。它比 IPSec 更快，更简单，更精简，更有用。它比 OpenVPN 更高效。WireGuard 设计为通用 VPN，适用于多种不同情况。它是跨平台的，可大规模部署。</p><p>通常如下图的部署: 一台 ECS 主机，得有公网 IP，下图就是 <code>pc ----&gt; ECS &lt;------ 公司的 pc</code></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">            +----------+</span><br><span class="line">            |          |</span><br><span class="line">   +-------&gt;+ ECS      +&lt;-----+</span><br><span class="line">   |        +----------+      |</span><br><span class="line">   |                          |</span><br><span class="line">   |                          |</span><br><span class="line">   |                          |</span><br><span class="line">   |                          |          company</span><br><span class="line">   |                      +---+------------------+</span><br><span class="line">+--++                     |                      |</span><br><span class="line">|PC |                     |            +---+     |</span><br><span class="line">+---+                     |            |PC |     |</span><br><span class="line">                          |            +---+     |</span><br><span class="line">                          |                      |</span><br><span class="line">                          +----------------------+</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当然，如果你会折腾的话 pc 可以是软路由，有兴趣和条件的可以看我博客 <a href="https://zhangguanzhang.github.io/2020/05/13/x86-router-flash/">proxmox x86软路由笔记</a>。</p><h2 id="登录到-ECS-上"><a href="#登录到-ECS-上" class="headerlink" title="登录到 ECS 上"></a>登录到 ECS 上</h2><p>得益于 wireguard 中没有 client/server 的概念，只要所有 nat 中的某台机器能够和 gateway 主机建立连接，即可实现共享所有节点的网络资源。这里 ECS 有公网ip，所以担当 gateway</p><h3 id="安装-wireguard"><a href="#安装-wireguard" class="headerlink" title="安装 wireguard"></a>安装 wireguard</h3><p><a href="https://www.wireguard.com/install/">官方安装文档</a> ，或者查看 <a href="https://mp.weixin.qq.com/s?__biz=MzU1MzY4NzQ1OA==&mid=2247488853&idx=1&sn=38acb5689db9d9d69ab1ebc78248e0ed&chksm=fbee5598cc99dc8ee81dc6e2a6ed12bb1fd61efd19f152c75e6e41aadb79a15562d7a6c9cb81&mpshare=1&scene=1&srcid=1118udSysN19LYkQxZEVWFTY&sharer_sharetime=1605681632258&sharer_shareid=8eaca72194dae7b3d51d5c708436eee4&key=8236791ccb71351070dff27fe2ad7a9f146455609c8c7a4bc57532e008e6e2a92c27e10b673a090dd88e54740c3391dbc2623a4128ba12f4ebfc9f83dbaf4ec0e6f01195f693765eb5690757359f4eaecfd37a78bb722773f7c6fa6a83cfbe73fa5273902c5aa16b765ece15a9130e8b12a3496d7bf2ae684ac9200cc5f39a31&ascene=1&uin=MzA1MzI4OTMzMQ==&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=AREByymqoZ6jfJZckbtVD7I=&pass_ticket=Jm9uDmvylBr7yM4ArNVQwkHhP3TB921kMFgCmo8A4uq+xezPGCG3aYKbPKyDMclJ&wx_header=0">如何在五分钟内装好 WireGuard？</a> ECS是 linux 系统的话内核要5.x以上，没有就升级下内核，其他个人 pc 电脑则下载客户端，当然软路由的话则去找个带 wireguard的固件。</p><h4 id="centos7"><a href="#centos7" class="headerlink" title="centos7"></a>centos7</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">yum -y install https:&#x2F;&#x2F;www.elrepo.org&#x2F;elrepo-release-7.0-4.el7.elrepo.noarch.rpm</span><br><span class="line">rpm --import https:&#x2F;&#x2F;www.elrepo.org&#x2F;RPM-GPG-KEY-elrepo.org</span><br><span class="line">yum --enablerepo&#x3D;elrepo-kernel install -y kernel-lt</span><br><span class="line"></span><br><span class="line">yum -y --enablerepo&#x3D;elrepo-kernel install kernel-lt-&#123;devel,headers,perf&#125;</span><br><span class="line"></span><br><span class="line">awk -F\&#39; &#39;$1&#x3D;&#x3D;&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#39; &#x2F;etc&#x2F;grub2.cfg</span><br><span class="line"></span><br><span class="line">#看数字</span><br><span class="line">grub2-set-default 2</span><br><span class="line">grub2-mkconfig -o &#x2F;etc&#x2F;grub2.cfg</span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line">yum install dkms kmod-wireguard wireguard-tools</span><br><span class="line"></span><br><span class="line">reboot</span><br><span class="line"></span><br><span class="line">modprobe wireguard</span><br></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>开启转发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.ip_forward=1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/wireguard</span><br></pre></td></tr></table></figure><h3 id="生成密钥对"><a href="#生成密钥对" class="headerlink" title="生成密钥对"></a>生成密钥对</h3><p>wg 的每个互相之间要一对密钥，例如 A 连 gateway， A 需要 gateway的公钥，gateway 需要 A 的公钥，不能共用一套密钥对。</p><p>生成 gateway 的密钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wg genkey | tee gw-privatekey | wg pubkey &gt; gw-publickey</span><br></pre></td></tr></table></figure><p>生成个人电脑的密钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wg genkey | tee pc-privatekey | wg pubkey &gt; pc-publickey</span><br></pre></td></tr></table></figure><p>生成公司电脑的密钥对</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wg genkey | tee cm-pc-privatekey | wg pubkey &gt; cm-pc-publickey</span><br></pre></td></tr></table></figure><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>wg 的组网得定义一个网段，这个网段和你所有运行了wg的局域网的ip不能一样，例如我定义的是 <code>10.1.0.1/24</code>，ecs 上配置文件为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; wg0.conf &lt;&lt;EOF</span><br><span class="line">[Interface]</span><br><span class="line">ListenPort = 16000 # 客户端连过来填写的端口，安全组的tcp和udp都要放行</span><br><span class="line">Address = 10.1.0.1/24  #wg之前通信组网的内网ip和段</span><br><span class="line">PrivateKey = $(cat gw-privatekey)   # 使用 shell 读取gateway的私钥到这里</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面两条是放行的iptables和MASQUERADE</span></span><br><span class="line">PostUp   = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span><br><span class="line">PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> pc</span></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat pc-publickey)</span><br><span class="line">AllowedIPs = 10.1.0.2/32</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> company router</span></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat cm-pc-publickey)</span><br><span class="line">AllowedIPs = 10.1.0.3/32, 192.168.2.0/24, 10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后是每个客户端的配置文件，下面是我笔记本 wg 的客户端软件配置文件内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; pc.conf &lt;&lt;EOF</span><br><span class="line">[Interface]</span><br><span class="line">PrivateKey = $(cat pc-privatekey)</span><br><span class="line">Address = 10.1.0.2/24 #wg之前通信组网的内网ip和段，主机位每个得不一样</span><br><span class="line"><span class="meta">#</span><span class="bash"> DNS = 192.168.2.3</span></span><br><span class="line"></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat gw-publickey)   # gateway的公钥</span><br><span class="line">AllowedIPs = 10.1.0.0/24, 192.168.2.0/24, 10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</span><br><span class="line">Endpoint = $(curl -s ip.sb):16000 #gateway 公网ip和端口</span><br><span class="line">PersistentKeepalive = 10 # 心跳时间</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>公司的电脑 wg 配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; cm-pc.conf &lt;&lt;EOF</span><br><span class="line">[Interface]</span><br><span class="line">PrivateKey = $(cat cm-pc-privatekey)</span><br><span class="line">Address = 10.1.0.3/24 #wg之前通信组网的内网ip和段，主机位每个得不一样</span><br><span class="line"></span><br><span class="line">[Peer]</span><br><span class="line">PublicKey = $(cat gw-publickey)   # gateway的公钥</span><br><span class="line">AllowedIPs = 10.1.0.0/24</span><br><span class="line">Endpoint = $(curl -s ip.sb):16000  # gateway 公网ip和端口</span><br><span class="line">PersistentKeepalive = 10 # 心跳时间</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>然后把 <code>pc.conf</code> 和 <code>cm-pc.conf</code> 的内容拷贝到对应的 wg 客户端软件里。</p><p>讲解下配置文件，我办公室是台式机proxmox里的软路由，并不是上面我说的办公室pc，这样我接入的设备也可以访问。我个人是推荐办公室搞个 proxmox 整虚拟机和软路由。</p><p>办公网的路由器网段是 <code>192.168.2.0/24</code>，<code>192.168.2.3</code>是软路由，主要是上面有dns server（adguard home），办公网内添加 hosts 我是直接在dns server上添加的。所以我个人PC那里写了 <code>DNS = 192.168.2.3</code>，这样 dns 解析都走到办公网的软路由上，家里不需要本地配置 hosts。</p><p><code>10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</code>的网段都是办公网的内网网段。<code>AllowedIPs</code>意思就是把请求目的IP是这些网段的，都发到 wg0 这个接口上，也就是添加路由表。这样我在家里，我个人 pc 打开 wg后，就能访问办公网了。</p><p>ECS上启动 wg 和停止 wg</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wg-quick up wg0 #默认取 /etc/wireguard/$name.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定配置文件启动 wg-quick up /etc/wireguard/wg0.conf</span></span><br><span class="line">wg-quick down wg0</span><br></pre></td></tr></table></figure><p>PostUp 和 PostDown 就是启动后和停止后的命令，是 Linux 的话就推荐写 iptables 放行转发和做 NAT。</p><p>查看组网状态，shell 上 wg 回车即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wg</span></span><br><span class="line">interface: wg0</span><br><span class="line">  public key: FZcFhf0eq2yFgXPNBqYnpoZHnzmgFI7JCLp/5vn1DG0=</span><br><span class="line">  private key: (hidden)</span><br><span class="line">  listening port: 16000</span><br><span class="line"></span><br><span class="line">peer: OtydRPJDt+H8upZDz5zJueRjUQ0tS4tr9P6w4BL2+w0=</span><br><span class="line">  endpoint: xxxxxxxxxxx:53956</span><br><span class="line">  allowed ips: 10.1.0.3/32, 192.168.2.0/24, 10.243.0.0/16, 10.0.6.0/24, 172.13.0.0/16</span><br><span class="line">  latest handshake: 1 minute, 2 seconds ago</span><br><span class="line">  transfer: 485.48 MiB received, 55.88 MiB sent</span><br><span class="line"></span><br><span class="line">peer: VkhLdmaPS2KmhlSOrPk1XS1MWZrhb+00BdsC0swUBhk=</span><br><span class="line">  endpoint: xxxxxxxxxx:13545</span><br><span class="line">  allowed ips: 10.1.0.2/32</span><br><span class="line">  latest handshake: 21 minutes, 25 seconds ago</span><br><span class="line">  transfer: 56.11 MiB received, 476.83 MiB sent</span><br></pre></td></tr></table></figure><h4 id="一些注意点"><a href="#一些注意点" class="headerlink" title="一些注意点"></a>一些注意点</h4><ul><li>如果是软路由，开了 passwall 代理之类的，记得把 ECS 的 公网IP 设置为不走代理。</li><li>openwrt 的 wireguard 多次配置可能会有问题，gateway上无法看到它上线，重启下后再试试。也可能需要下面规则<ul><li><code>网络</code> – <code>防火墙</code> – <code>自定义规则</code>：<code>iptables -A FORWARD -i wg0 -j ACCEPT</code> <code>iptables -A FORWARD -o wg0 -j ACCEPT</code> <code>iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</code></li></ul></li></ul><h2 id="udp被-qos-下配合-udp2raw-使用"><a href="#udp被-qos-下配合-udp2raw-使用" class="headerlink" title="udp被 qos 下配合 udp2raw 使用"></a>udp被 qos 下配合 udp2raw 使用</h2><p>实际使用中很大几率遇到 udp 被 qos 了，导致连接经常断开，这里使用 <a href="https://github.com/wangyu-/udp2raw-tunnel">udp2raw</a> 把 udp 报文伪装成 tcp 避免被 qos。这里我使用 docker 部署的，实体进程和相关文档见 <a href="https://github.com/wangyu-/udp2raw-tunnel/blob/master/doc/README.zh-cn.md#%E8%BF%90%E8%A1%8C">udp2raw运行</a></p><p>Linux server端 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 监听86的tcp端口，把86端口收到的伪装成tcp的udp报文转发到 127.0.0.1:16000 上</span></span><br><span class="line">docker run \</span><br><span class="line">    -d --name udp2raw  \</span><br><span class="line">    --restart always \</span><br><span class="line">    --net host \</span><br><span class="line">    --cap-add NET_RAW \</span><br><span class="line">    --cap-add NET_ADMIN  \</span><br><span class="line">    zhangguanzhang/udp2raw \</span><br><span class="line">    -s -l 0.0.0.0:86 \</span><br><span class="line">    -r 127.0.0.1:16000 \</span><br><span class="line">    -k passwd123 \</span><br><span class="line">    --raw-mode faketcp  \</span><br><span class="line">    --cipher-mode xor  -a</span><br></pre></td></tr></table></figure><p>Linux或者软路由系统 client 端 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 监听16000的 udp 端口，把16000端口收到的udp报文伪装成tcp发到 &lt;public_ip&gt;:86 上</span></span><br><span class="line">docker run --net host \</span><br><span class="line">    -d --name udp2raw  \</span><br><span class="line">    --restart always \</span><br><span class="line">    --cap-add NET_RAW \</span><br><span class="line">    --cap-add NET_ADMIN    \</span><br><span class="line">    zhangguanzhang/udp2raw \</span><br><span class="line">    -c -l 0.0.0.0:16000 \</span><br><span class="line">    -r &lt;public_ip&gt;:86 \</span><br><span class="line">    -k passwd123 \</span><br><span class="line">    --raw-mode faketcp   \</span><br><span class="line">    --cipher-mode xor  -a</span><br></pre></td></tr></table></figure><p><a href="https://github.com/wangyu-/udp2raw-multiplatform">windows 客户端下载</a>，运行命令参考:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./udp2raw_mp.exe -c -l 0.0.0.0:16000 -r &lt;public_ip&gt;:86 -k passwd123 --raw-mode faketcp --cipher-mode xor</span><br></pre></td></tr></table></figure><p>所有 client 端的 <code>[peer]</code> 部分里之前连云主机的 ip 都写成<code>127.0.0.1:16000</code>，这样 wg 客户端是先向本地的 udp2raw 客户端发 udp 报文，然后报文被封装成 tcp 发往云主机上的 udp2raw server，再到 wg server 上。</p><p>** 客户端和云主机上 ** 的 wg 的 mtu 设置成 <code>1280</code>(网上有写1200的，但是 windows 的 wg 客户端无法启动，邮件询问作者说最小 <code>1280</code> 才能启动)。例如我路由器配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[Interface]</span><br><span class="line">...</span><br><span class="line">MTU = 1280</span><br><span class="line">[Peer]</span><br><span class="line">...</span><br><span class="line">Endpoint = 127.0.0.1:16000</span><br><span class="line">PersistentKeepalive = 10</span><br></pre></td></tr></table></figure><p>windows的 wg 目前 <code>Endpoint</code>必须写本机的 ip（ipconfig命令查看），不能写<code>127.0.0.1</code>，否则无法连 peer（日志会一直刷<code>Failed to send handshake initiation write udp4 0.0.0.0:xxx-&gt;127.0.0.1:16000: wsasendto: The requested address is not valid in its context</code>），这个 bug 已经反馈给作者了。</p><p>udp2raw 的 client 连上 server 后，双方都会打印下面日志:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># server</span><br><span class="line">changed state to server_ready</span><br><span class="line"># client</span><br><span class="line">changed state from to client_handshake2 to client_ready</span><br></pre></td></tr></table></figure><h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ul><li><a href="https://anyisalin.github.io/2018/11/21/fast-flexible-nat-to-nat-vpn-wireguard/">https://anyisalin.github.io/2018/11/21/fast-flexible-nat-to-nat-vpn-wireguard/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为 IT 人员，经常需要连到办公网工作，并不是每个公司都有 vpn，自己搭建的话 openvpn 之类的配置麻烦啰嗦。这里写下 wireguard 的简单搭建。它比 IPSec 更快，更简单，更精简，更有用。它比 OpenVPN 更高效。WireGuard 设计为通用 V</summary>
      
    
    
    
    <category term="wireguard" scheme="http://zhangguanzhang.github.io/categories/wireguard/"/>
    
    
    <category term="wireguard" scheme="http://zhangguanzhang.github.io/tags/wireguard/"/>
    
  </entry>
  
  <entry>
    <title>prometheus的rate与irate内部是如何计算的</title>
    <link href="http://zhangguanzhang.github.io/2020/07/30/prometheus-rate-and-irate/"/>
    <id>http://zhangguanzhang.github.io/2020/07/30/prometheus-rate-and-irate/</id>
    <published>2020-07-30T10:58:10.000Z</published>
    <updated>2020-08-21T09:58:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>市面上的翻译误导人，压根不是啥<code>平均增长率</code>，看了下源码和实际算下来让大家好理解</p><h3 id="rate"><a href="#rate" class="headerlink" title="rate"></a>rate</h3><p>主要代码是在 <a href="https://github.com/prometheus/prometheus/blob/master/promql/functions.go" target="_blank" rel="noopener">https://github.com/prometheus/prometheus/blob/master/promql/functions.go</a> 的<code>extrapolatedRate</code> 和 <code>funcRate</code>，funcRate为</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">funcRate</span><span class="params">(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper)</span> <span class="title">Vector</span></span> &#123;</span><br><span class="line">  <span class="keyword">return</span> extrapolatedRate(vals, args, enh, <span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它的前后还有<code>funcDelta</code>和<code>funcIncrease</code>对应promql的<code>delta</code>和<code>increase</code>，这俩函数内部都是调用的<code>extrapolatedRate</code>，主要区别是通过向<code>extrapolatedRate</code>函数传递最后的两个布尔标志位的差异，来在<code>extrapolatedRate</code>内部进行差异化计算，也就是说<code>rate</code>、<code>delta</code>和<code>increase</code>的部分数学计算逻辑是一样的。</p><p><code>funcRate</code>里<code>extrapolatedRate</code>最后俩实参格式为<code>isCounter bool, isRate bool</code>，所以<code>rate</code>只能用在<code>counter</code>的 metrics 类型上进行计算。</p><h3 id="数据点的选取"><a href="#数据点的选取" class="headerlink" title="数据点的选取"></a>数据点的选取</h3><p>先看这段代码</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> (</span><br><span class="line">  counterCorrection <span class="keyword">float64</span></span><br><span class="line">  lastValue         <span class="keyword">float64</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> _, sample := <span class="keyword">range</span> samples.Points &#123;</span><br><span class="line">  <span class="keyword">if</span> isCounter &amp;&amp; sample.V &lt; lastValue &#123;</span><br><span class="line">    counterCorrection += lastValue</span><br><span class="line">  &#125;</span><br><span class="line">  lastValue = sample.V</span><br><span class="line">&#125;</span><br><span class="line">resultValue := lastValue - samples.Points[<span class="number">0</span>].V + counterCorrection</span><br></pre></td></tr></table></figure><p>counterCorrection是字面意思修正数值，counter会reset，例如exporter重启了。例如60秒内有下面6数值，在第四个数字后面发生了重置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2 4 6 8 2 4</span><br></pre></td></tr></table></figure><p>2小于lastValue 8，所以<code>counterCorrection = 8</code></p><p>最后的 <code>resultValue = 4 + 8 - 2</code>，当然，重置的情况很少，这里如果不重置用数据<code>2 4 6 8 10 12</code>算就是最后一个值减去第一个值<code>resultValue = 12 - 2 + 0</code>和重置算得一样</p><h3 id="计算的算式"><a href="#计算的算式" class="headerlink" title="计算的算式"></a>计算的算式</h3><p>是结果除以时间的秒数</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> isRate &#123;</span><br><span class="line">  resultValue = resultValue / ms.Range.Seconds()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="对比下irate"><a href="#对比下irate" class="headerlink" title="对比下irate"></a>对比下irate</h3><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 取最后一个数据点</span></span><br><span class="line">lastSample := samples.Points[<span class="built_in">len</span>(samples.Points)<span class="number">-1</span>]</span><br><span class="line"><span class="comment">// 取倒数第二个数据点</span></span><br><span class="line">previousSample := samples.Points[<span class="built_in">len</span>(samples.Points)<span class="number">-2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> resultValue <span class="keyword">float64</span></span><br><span class="line"><span class="keyword">if</span> isRate &amp;&amp; lastSample.V &lt; previousSample.V &#123;</span><br><span class="line">  <span class="comment">// counter重置则取最后一个值.</span></span><br><span class="line">  resultValue = lastSample.V</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="comment">// 最后一个点数值 - 倒数第二个数值</span></span><br><span class="line">  resultValue = lastSample.V - previousSample.V</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 最后两个点的时间间隔</span></span><br><span class="line">sampledInterval := lastSample.T - previousSample.T</span><br><span class="line"><span class="keyword">if</span> sampledInterval == <span class="number">0</span> &#123;</span><br><span class="line">  <span class="comment">// Avoid dividing by 0.</span></span><br><span class="line">  <span class="keyword">return</span> out</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> isRate &#123;</span><br><span class="line">    <span class="comment">// 转换成秒，然后结果除以秒数</span></span><br><span class="line">  resultValue /= <span class="keyword">float64</span>(sampledInterval) / <span class="number">1000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>官方文档和市面上的 gitbook 都是把<code>rate</code>翻译成<code>增长率</code>是错误的，应该是<code>平均每秒增长了多少数值</code>。按照实践来算下，同时查询<code>node_time_seconds[1m]</code>和<code>rate(node_time_seconds[1m])</code>。我们手动计算下看看是否和rate的结果一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ node_time_seconds[1m]</span><br><span class="line">node_time_seconds&#123;instance&#x3D;&quot;exporter:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;</span><br><span class="line">1596077182.3093214 @1596077182.307 &#x2F;&#x2F; 第一个点</span><br><span class="line">1596077192.3132203 @1596077192.307</span><br><span class="line">1596077202.311446 @1596077202.307</span><br><span class="line">1596077212.309673 @1596077212.307</span><br><span class="line">1596077222.316771 @1596077222.307</span><br><span class="line">1596077232.3151288 @1596077232.307 &#x2F;&#x2F; 最后一个点</span><br><span class="line">node_time_seconds&#123;instance&#x3D;&quot;10.0.23.29:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;</span><br><span class="line">1596077178.6314309 @1596077178.633  &#x2F;&#x2F; 第一个点</span><br><span class="line">1596077188.6312084 @1596077188.633</span><br><span class="line">1596077198.633293 @1596077198.634</span><br><span class="line">1596077208.6332283 @1596077208.634</span><br><span class="line">1596077218.6320524 @1596077218.633</span><br><span class="line">1596077228.635078 @1596077228.633  &#x2F;&#x2F; 最后一个点</span><br><span class="line"></span><br><span class="line">$ rate(node_time_seconds[1m])</span><br><span class="line">&#123;instance&#x3D;&quot;exporter:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;    1.0001161479949952</span><br><span class="line">&#123;instance&#x3D;&quot;10.0.23.29:9100&quot;,job&#x3D;&quot;node-resources&quot;&#125;  1.0000729417800902</span><br></pre></td></tr></table></figure><p>先用<code>10.0.23.29</code>这个 instance 算，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1596077228.635078 - 1596077178.6314309) &#x2F; (1596077228.633 - 1596077178.633)</span><br><span class="line">&#x2F;&#x2F; web上的时间是秒数的，go的time是多了三个单位，所以代码里&#x2F;1000转换成秒这里不需要除以1000</span><br><span class="line">上面式子左边和右边算是下面结果:</span><br><span class="line">50.003647089       &#x2F;  50    &#x3D;  1.00007294178</span><br></pre></td></tr></table></figure><p>谷歌搜的在线计算器算的(比windows的calc精度高一些)，由于是float64，所以精度丢失了一些。结果一样。再算下另一个 instance</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1596077232.3151288 - 1596077182.3093214) &#x2F;</span><br><span class="line">    (1596077232.307 - 1596077182.307)</span><br><span class="line">            50.0058073997     &#x2F;    50 &#x3D; 1.00011614799</span><br></pre></td></tr></table></figure><p><code>increase</code>是最后一个点减去第一个点，不除以秒数。所以在 counter 没发生重置情况下，下面两个是相等的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">increase(node_time_seconds[1m]) &#x2F; 60 &#x3D;&#x3D; rate(node_time_seconds[1m])</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;市面上的翻译误导人，压根不是啥&lt;code&gt;平均增长率&lt;/code&gt;，看了下源码和实际算下来让大家好理解&lt;/p&gt;
&lt;h3 id=&quot;rate&quot;&gt;</summary>
      
    
    
    
    <category term="Prometheus" scheme="http://zhangguanzhang.github.io/categories/Prometheus/"/>
    
    
    <category term="Prometheus" scheme="http://zhangguanzhang.github.io/tags/Prometheus/"/>
    
  </entry>
  
  <entry>
    <title>k8s master机器文件系统故障的一次恢复过程</title>
    <link href="http://zhangguanzhang.github.io/2020/07/23/fs-error-fix-k8s-master/"/>
    <id>http://zhangguanzhang.github.io/2020/07/23/fs-error-fix-k8s-master/</id>
    <published>2020-07-23T13:58:10.000Z</published>
    <updated>2020-08-01T13:59:21.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>研发反馈他们那边一套集群有台master文件系统损坏无法开机，他们是三台openstack上的虚机，是虚拟化宿主机故障导致的虚机文件系统损坏。三台机器是master+node，指导他修复后开机，修复过程和我之前文章<a href="https://zhangguanzhang.github.io/2019/12/05/suse-fix-data-but-device-busy/">opensuse的一次救援</a>步骤一样</p><p>起来后我上去看，因为做了 HA 的，所以只有这个node有问题，集群没影响</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# kubectl get node -o wide</span><br><span class="line">NAME             STATUS     ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">10.252.146.104   NotReady   &lt;none&gt;   30d   v1.16.9   10.252.146.104   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker:&#x2F;&#x2F;19.3.11</span><br><span class="line">10.252.146.105   Ready      &lt;none&gt;   30d   v1.16.9   10.252.146.105   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker:&#x2F;&#x2F;19.3.11</span><br><span class="line">10.252.146.106   Ready      &lt;none&gt;   30d   v1.16.9   10.252.146.106   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker:&#x2F;&#x2F;19.3.11</span><br></pre></td></tr></table></figure><p>启动docker试试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl start docker</span><br><span class="line">Job for docker.service canceled.</span><br></pre></td></tr></table></figure><p>无法启动，查看下启动失败的服务</p><h3 id="containerd"><a href="#containerd" class="headerlink" title="containerd"></a>containerd</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl --failed</span><br><span class="line">  UNIT               LOAD   ACTIVE SUB    DESCRIPTION                 </span><br><span class="line">● containerd.service loaded failed failed containerd container runtime</span><br></pre></td></tr></table></figure><p>查看下<code>containerd</code>的日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# journalctl -xe -u containerd</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481459735+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.service.v1.snapshots-service&quot;...&quot; type&#x3D;io.containerd.service.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481472223+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.runtime.v1.linux&quot;...&quot; type&#x3D;io.containerd.runtime.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481517630+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.runtime.v2.task&quot;...&quot; type&#x3D;io.containerd.runtime.v2</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481562176+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.monitor.v1.cgroups&quot;...&quot; type&#x3D;io.containerd.monitor.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481964349+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.service.v1.tasks-service&quot;...&quot; type&#x3D;io.containerd.service.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.481996158+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.internal.v1.restart&quot;...&quot; type&#x3D;io.containerd.internal.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482048208+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.containers&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482081110+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.content&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482096598+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.diff&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482112263+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.events&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482123307+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.healthcheck&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482133477+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.images&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482142943+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.leases&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482151644+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482160741+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.internal.v1.opt&quot;...&quot; type&#x3D;io.containerd.internal.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482184201+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482194643+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482206871+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.version&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482215454+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482365838+08:00&quot; level&#x3D;info msg&#x3D;serving... address&#x3D;&quot;&#x2F;run&#x2F;containerd&#x2F;containerd.sock&quot;</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: time&#x3D;&quot;2020-07-23T11:20:11.482404139+08:00&quot; level&#x3D;info msg&#x3D;&quot;containerd successfully booted in 0.003611s&quot;</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: panic: runtime error: invalid memory address or nil pointer dereference</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: [signal SIGSEGV: segmentation violation code&#x3D;0x1 addr&#x3D;0x8 pc&#x3D;0x5626b983c259]</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: goroutine 55 [running]:</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*Bucket).Cursor(...)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;bucket.go:84</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*Bucket).Get(0x0, 0x5626bb7e3f10, 0xb, 0xb, 0x0, 0x2, 0x4)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;bucket.go:260 +0x39</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.scanRoots.func6(0x7fe557c63020, 0x2, 0x2, 0x0, 0x0, 0x0, 0x0, 0x5626b95eec72)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;gc.go:222 +0xcb</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*Bucket).ForEach(0xc0003d1780, 0xc00057b640, 0xa, 0xa)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;bucket.go:388 +0x100</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.scanRoots(0x5626bacedde0, 0xc0003d1680, 0xc0002ee2a0, 0xc00031a3c0, 0xc000527a60, 0x7fe586a43fff)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;gc.go:216 +0x4df</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).getMarked.func1(0xc0002ee2a0, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:359 +0x165</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt.(*DB).View(0xc00000c1e0, 0xc00008b860, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;vendor&#x2F;go.etcd.io&#x2F;bbolt&#x2F;db.go:701 +0x92</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).getMarked(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x203000, 0x203000, 0x400)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:342 +0x7e</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).GarbageCollect(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:257 +0xa3</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.(*gcScheduler).run(0xc0000a0b40, 0x5626bacede20, 0xc0000d6010)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:310 +0x511</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: created by github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.init.0.func1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:132 +0x462</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENT</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Failed with result &#39;exit-code&#39;.</span><br></pre></td></tr></table></figure><p>这个问题从panic抛出的堆栈信息看和我之前文章<a href="https://zhangguanzhang.github.io/2020/01/08/docker-panic-invalid-page-type/">docker启动panic</a>很类似，都是 boltdb 文件出错，找下 git 信息去看看代码路径在哪</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl cat containerd | grep ExecStart</span><br><span class="line">ExecStartPre&#x3D;-&#x2F;sbin&#x2F;modprobe overlay</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line"></span><br><span class="line">[root@k8s-m1 ~]# &#x2F;usr&#x2F;bin&#x2F;containerd --version</span><br><span class="line">containerd containerd.io 1.2.13 7ad184331fa3e55e52b890ea95e65ba581ae3429</span><br></pre></td></tr></table></figure><p>按照这个blob去用github的url访问是404，只有去按照tag版本查看了，根据相关代码找到了 boltdb 的文件名是<code>meta.db</code><br><a href="https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L257" target="_blank" rel="noopener">https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L257</a><br><a href="https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L79" target="_blank" rel="noopener">https://github.com/containerd/containerd/blob/v1.2.13/metadata/db.go#L79</a><br><a href="https://github.com/containerd/containerd/blob/v1.2.13/services/server/server.go#L261-L268" target="_blank" rel="noopener">https://github.com/containerd/containerd/blob/v1.2.13/services/server/server.go#L261-L268</a></p><p>查找下<code>ic.Root</code>路径是多少</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# &#x2F;usr&#x2F;bin&#x2F;containerd --help | grep config</span><br><span class="line">     config    information on the containerd config</span><br><span class="line">   --config value, -c value     path to the configuration file (default: &quot;&#x2F;etc&#x2F;containerd&#x2F;config.toml&quot;)</span><br><span class="line"></span><br><span class="line">[root@k8s-m1 ~]# grep root &#x2F;etc&#x2F;containerd&#x2F;config.toml</span><br><span class="line">#root &#x3D; &quot;&#x2F;var&#x2F;lib&#x2F;containerd&quot;</span><br><span class="line">[root@k8s-m1 ~]]# find &#x2F;var&#x2F;lib&#x2F;containerd -type f -name meta.db</span><br><span class="line">&#x2F;var&#x2F;lib&#x2F;containerd&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db</span><br></pre></td></tr></table></figure><p>找到boltdb文件，改名启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]]# mv &#x2F;var&#x2F;lib&#x2F;containerd&#x2F;io.containerd.metadata.v1.bolt&#x2F;meta.db&#123;,.bak&#125;</span><br><span class="line">[root@k8s-m1 ~]# systemctl status containerd.service</span><br><span class="line">● containerd.service - containerd container runtime</span><br><span class="line">   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;containerd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: failed (Result: exit-code) since Thu 2020-07-23 11:20:11 CST; 17min ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;containerd.io</span><br><span class="line">  Process: 9186 ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;containerd (code&#x3D;exited, status&#x3D;2)</span><br><span class="line">  Process: 9182 ExecStartPre&#x3D;&#x2F;sbin&#x2F;modprobe overlay (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 9186 (code&#x3D;exited, status&#x3D;2)</span><br><span class="line"></span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).getMarked(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x203000, 0x203000, 0x400)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:342 +0x7e</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata.(*DB).GarbageCollect(0xc0000a0a80, 0x5626bacede20, 0xc0000d6010, 0x0, 0x1, 0x0, 0x0)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;metadata&#x2F;db.go:257 +0xa3</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.(*gcScheduler).run(0xc0000a0b40, 0x5626bacede20, 0xc0000d6010)</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:310 +0x511</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]: created by github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler.init.0.func1</span><br><span class="line">Jul 23 11:20:11 k8s-m1 containerd[9186]:         &#x2F;go&#x2F;src&#x2F;github.com&#x2F;containerd&#x2F;containerd&#x2F;gc&#x2F;scheduler&#x2F;scheduler.go:132 +0x462</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Main process exited, code&#x3D;exited, status&#x3D;2&#x2F;INVALIDARGUMENT</span><br><span class="line">Jul 23 11:20:11 k8s-m1 systemd[1]: containerd.service: Failed with result &#39;exit-code&#39;.</span><br><span class="line">[root@k8s-m1 ~]# systemctl restart containerd.service</span><br><span class="line">[root@k8s-m1 ~]# systemctl status containerd.service</span><br><span class="line">● containerd.service - containerd container runtime</span><br><span class="line">   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;containerd.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu 2020-07-23 11:25:37 CST; 1s ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;containerd.io</span><br><span class="line">  Process: 15661 ExecStartPre&#x3D;&#x2F;sbin&#x2F;modprobe overlay (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 15663 (containerd)</span><br><span class="line">    Tasks: 16</span><br><span class="line">   Memory: 28.6M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;containerd.service</span><br><span class="line">           └─15663 &#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line"></span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496725460+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.images&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496734129+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.leases&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496742793+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.namespaces&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496751740+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.internal.v1.opt&quot;...&quot; type&#x3D;io.containerd.internal.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496775185+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.snapshots&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496785498+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.tasks&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496794873+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.version&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496803178+08:00&quot; level&#x3D;info msg&#x3D;&quot;loading plugin &quot;io.containerd.grpc.v1.introspection&quot;...&quot; type&#x3D;io.containerd.grpc.v1</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496944458+08:00&quot; level&#x3D;info msg&#x3D;serving... address&#x3D;&quot;&#x2F;run&#x2F;containerd&#x2F;containerd.sock&quot;</span><br><span class="line">Jul 23 11:25:37 k8s-m1 containerd[15663]: time&#x3D;&quot;2020-07-23T11:25:37.496958031+08:00&quot; level&#x3D;info msg&#x3D;&quot;containerd successfully booted in 0.003994s&quot;</span><br></pre></td></tr></table></figure><h3 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h3><p>containerd 起来后，启动下 docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d</span><br><span class="line">           └─10-docker.conf</span><br><span class="line">   Active: inactive (dead) since Thu 2020-07-23 11:20:13 CST; 18min ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;docs.docker.com</span><br><span class="line">  Process: 9398 ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#x2F;sbin&#x2F;iptables -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || : (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line">  Process: 9187 ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 9187 (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"></span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.956503485+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 68860c8d16b9ce7e74e8efd9db00e70a57eef1b752c2e6c703073c0bce5517d3 with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.954347116+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 5ec9922beed1276989f1866c3fd911f37cc26aae4e4b27c7ce78183a9a4725cc with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.953615411+08:00&quot; level&#x3D;info msg&#x3D;&quot;Container failed to stop after sending signal 15 to the process, force killing&quot;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.956557179+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 6d0096fbcd4055f8bafb6b38f502a0186cd1dfca34219e9dd6050f512971aef5 with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.954601191+08:00&quot; level&#x3D;info msg&#x3D;&quot;Container failed to stop after sending signal 15 to the process, force killing&quot;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.956600790+08:00&quot; level&#x3D;error msg&#x3D;&quot;Stop container error: Failed to stop container 6d1175ba6c55cb05ad89f4134ba8e9d3495c5acb5f07938dc16339b7cca013bf with error: Cannot kill c&gt;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.957188989+08:00&quot; level&#x3D;info msg&#x3D;&quot;Daemon shutdown complete&quot;</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.957212655+08:00&quot; level&#x3D;info msg&#x3D;&quot;stopping event stream following graceful shutdown&quot; error&#x3D;&quot;context canceled&quot; module&#x3D;libcontainerd namespace&#x3D;plugins.moby</span><br><span class="line">Jul 23 11:20:13 k8s-m1 dockerd[9187]: time&#x3D;&quot;2020-07-23T11:20:13.957209679+08:00&quot; level&#x3D;info msg&#x3D;&quot;stopping event stream following graceful shutdown&quot; error&#x3D;&quot;context canceled&quot; module&#x3D;libcontainerd namespace&#x3D;moby</span><br><span class="line">Jul 23 11:20:13 k8s-m1 systemd[1]: Stopped Docker Application Container Engine.</span><br><span class="line">[root@k8s-m1 ~]# systemctl start docker</span><br><span class="line">[root@k8s-m1 ~]# systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;docker.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service.d</span><br><span class="line">           └─10-docker.conf</span><br><span class="line">   Active: active (running) since Thu 2020-07-23 11:26:11 CST; 1s ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;docs.docker.com</span><br><span class="line">  Process: 9398 ExecStopPost&#x3D;&#x2F;bin&#x2F;bash -c &#x2F;sbin&#x2F;iptables -D FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT &amp;&gt; &#x2F;dev&#x2F;null || : (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line">  Process: 16156 ExecStartPost&#x3D;&#x2F;sbin&#x2F;iptables -I FORWARD -s 0.0.0.0&#x2F;0 -j ACCEPT (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 15974 (dockerd)</span><br><span class="line">    Tasks: 62</span><br><span class="line">   Memory: 89.1M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;docker.service</span><br><span class="line">           └─15974 &#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line"></span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.851106564+08:00&quot; level&#x3D;error msg&#x3D;&quot;cb4e16249cd8eac48ed734c71237195f04d63c56c55c0199b3cdf3d49461903d cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.860456898+08:00&quot; level&#x3D;error msg&#x3D;&quot;d9bbcab186ccb59f96c95fc886ec1b66a52aa96e45b117cf7d12e3ff9b95db9f cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.872405757+08:00&quot; level&#x3D;error msg&#x3D;&quot;07eb7a09bc8589abcb4d79af4b46798327bfb00624a7b9ceea457de392ad8f3d cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:10 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:10.877896618+08:00&quot; level&#x3D;error msg&#x3D;&quot;f5867657025bd7c3951cbd3e08ad97338cf69df2a97967a419e0e78eda869b73 cleanup: failed to delete container from containerd: no such container&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.143661583+08:00&quot; level&#x3D;info msg&#x3D;&quot;Default bridge (docker0) is assigned with an IP address 172.17.0.0&#x2F;16. Daemon option --bip can be used to set a preferred IP address&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.198200760+08:00&quot; level&#x3D;info msg&#x3D;&quot;Loading containers: done.&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.219959208+08:00&quot; level&#x3D;info msg&#x3D;&quot;Docker daemon&quot; commit&#x3D;42e35e61f3 graphdriver(s)&#x3D;overlay2 version&#x3D;19.03.11</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.220049865+08:00&quot; level&#x3D;info msg&#x3D;&quot;Daemon has completed initialization&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 dockerd[15974]: time&#x3D;&quot;2020-07-23T11:26:11.232373131+08:00&quot; level&#x3D;info msg&#x3D;&quot;API listen on &#x2F;var&#x2F;run&#x2F;docker.sock&quot;</span><br><span class="line">Jul 23 11:26:11 k8s-m1 systemd[1]: Started Docker Application Container Engine.</span><br></pre></td></tr></table></figure><h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><p>etcd启动也失败，journal 查看下 etcd 状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# journalctl -xe -u etcd</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Loading server configuration from &quot;&#x2F;etc&#x2F;etcd&#x2F;etcd.config.yml&quot;</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: etcd Version: 3.3.20</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Git SHA: 9fd7e2b80</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Go Version: go1.12.17</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: setting maximum number of CPUs to 16, total number of available CPUs is 16</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: found invalid file&#x2F;dir wal under data dir &#x2F;var&#x2F;lib&#x2F;etcd (Ignore this if you are upgrading etcd)</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: ignoring peer auto TLS since certs given</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: peerTLS: cert &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt, key &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key, ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, trusted-ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, client-cert-auth &#x3D; fals&gt;</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for peers on https:&#x2F;&#x2F;10.252.146.104:2380</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: ignoring client auto TLS since certs given</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: pprof is enabled under &#x2F;debug&#x2F;pprof</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for client requests on 127.0.0.1:2379</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: listening for client requests on 10.252.146.104:2379</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: skipped unexpected non snapshot file 000000000000002e-000000000052f2be.snap.broken</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: recovered store from snapshot at index 5426092</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: restore compact to 3967425</span><br><span class="line">Jul 23 11:26:15 k8s-m1 etcd[18129]: cannot unmarshal event: proto: KeyValue: illegal tag 0 (wire type 0)</span><br><span class="line">Jul 23 11:26:15 k8s-m1 systemd[1]: etcd.service: Main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Jul 23 11:26:15 k8s-m1 systemd[1]: etcd.service: Failed with result &#39;exit-code&#39;.</span><br><span class="line">Jul 23 11:26:15 k8s-m1 systemd[1]: Failed to start Etcd Service.</span><br><span class="line">[root@k8s-m1 ~]# ll &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;member&#x2F;snap&#x2F;</span><br><span class="line">total 8560</span><br><span class="line">-rw-r--r-- 1 root root   13499 Jul 20 13:36 000000000000002e-000000000052cbac.snap</span><br><span class="line">-rw-r--r-- 2 root root  128360 Jul 20 13:01 000000000000002e-000000000052f2be.snap.broken</span><br><span class="line">-rw------- 1 root root 8617984 Jul 23 11:26 db</span><br></pre></td></tr></table></figure><p>这套集群是使用我的<a href="https://github.com/zhangguanzhang/Kubernetes-ansible" target="_blank" rel="noopener">ansible部署的，求star</a>，自带了<a href="https://github.com/zhangguanzhang/Kubernetes-ansible-base/tree/roles/etcd/templates" target="_blank" rel="noopener">备份脚本</a>，但是是三天前坏的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# ll &#x2F;opt&#x2F;etcd_bak&#x2F;</span><br><span class="line">total 41524</span><br><span class="line">-rw-r--r-- 1 root root 8618016 Jul 17 02:00 etcd-2020-07-17-02:00:01.db</span><br><span class="line">-rw-r--r-- 1 root root 8618016 Jul 18 02:00 etcd-2020-07-18-02:00:01.db</span><br><span class="line">-rw-r--r-- 1 root root 8323104 Jul 19 02:00 etcd-2020-07-19-02:00:01.db</span><br><span class="line">-rw-r--r-- 1 root root 8618016 Jul 20 02:00 etcd-2020-07-20-02:00:01.db</span><br></pre></td></tr></table></figure><h4 id="etcd备份文件恢复"><a href="#etcd备份文件恢复" class="headerlink" title="etcd备份文件恢复"></a>etcd备份文件恢复</h4><p>有恢复剧本，但是前提是etcd的v2和v3不能共存，否则无法恢复备份，我们线上都是把v2的存储关闭了的。主要是<a href="https://github.com/zhangguanzhang/Kubernetes-ansible-base/blob/roles/restoreETCD/tasks/main.yml" target="_blank" rel="noopener">这个tasks里的26到42行步骤</a>，这里复制了其他机器master上的 07/23 号的etcd备份文件，然后改了下host跑了下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 ~]# cp -a &#x2F;var&#x2F;lib&#x2F;etcd&#123;,.bak&#125; #先备份下etcd目录</span><br><span class="line">[root@k8s-m1 ~]# cd Kubernetes-ansible</span><br><span class="line">[root@k8s-m1 Kubernetes-ansible]# ansible-playbook restoreETCD.yml -e &#39;db&#x3D;&#x2F;opt&#x2F;etcd_bak&#x2F;etcd-bak.db&#39;</span><br><span class="line"></span><br><span class="line">PLAY [10.252.146.104] **********************************************************************************************************************************************************************************************************************</span><br><span class="line"></span><br><span class="line">TASK [Gathering Facts] *********************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : fail] ******************************************************************************************************************************************************************************************************************</span><br><span class="line">skipping: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 检测备份文件存在否] *************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : fail] ******************************************************************************************************************************************************************************************************************</span><br><span class="line">skipping: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : set_fact] **************************************************************************************************************************************************************************************************************</span><br><span class="line">skipping: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : set_fact] **************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 停止etcd] ****************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 删除etcd数据目录] ************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104] &#x3D;&gt; (item&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd)</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 分发备份文件] ****************************************************************************************************************************************************************************************************************</span><br><span class="line">ok: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 恢复备份] ******************************************************************************************************************************************************************************************************************</span><br><span class="line">changed: [10.252.146.104]</span><br><span class="line"></span><br><span class="line">TASK [restoreETCD : 启动etcd] ****************************************************************************************************************************************************************************************************************</span><br><span class="line">fatal: [10.252.146.104]: FAILED! &#x3D;&gt; &#123;&quot;changed&quot;: false, &quot;msg&quot;: &quot;Unable to start service etcd: Job for etcd.service failed because the control process exited with error code.\nSee \&quot;systemctl status etcd.service\&quot; and \&quot;journalctl -xe\&quot; for details.\n&quot;&#125;</span><br><span class="line"></span><br><span class="line">PLAY RECAP *********************************************************************************************************************************************************************************************************************************</span><br><span class="line">10.252.146.104             : ok&#x3D;7    changed&#x3D;1    unreachable&#x3D;0    failed&#x3D;1    skipped&#x3D;3    rescued&#x3D;0    ignored&#x3D;0</span><br></pre></td></tr></table></figure><p>查看下日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# journalctl -xe -u etcd</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Loading server configuration from &quot;&#x2F;etc&#x2F;etcd&#x2F;etcd.config.yml&quot;</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: etcd Version: 3.3.20</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Git SHA: 9fd7e2b80</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Go Version: go1.12.17</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: setting maximum number of CPUs to 16, total number of available CPUs is 16</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: ignoring peer auto TLS since certs given</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: peerTLS: cert &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt, key &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key, ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, trusted-ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, client-cert-auth &#x3D; fals&gt;</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for peers on https:&#x2F;&#x2F;10.252.146.104:2380</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: ignoring client auto TLS since certs given</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: pprof is enabled under &#x2F;debug&#x2F;pprof</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for client requests on 127.0.0.1:2379</span><br><span class="line">Jul 23 11:27:46 k8s-m1 etcd[58954]: listening for client requests on 10.252.146.104:2379</span><br><span class="line">Jul 23 11:27:47 k8s-m1 etcd[58954]: member ac2dcf6aed12e8f1 has already been bootstrapped</span><br><span class="line">Jul 23 11:27:47 k8s-m1 systemd[1]: etcd.service: Main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Jul 23 11:27:47 k8s-m1 systemd[1]: etcd.service: Failed with result &#39;exit-code&#39;.</span><br><span class="line">Jul 23 11:27:47 k8s-m1 systemd[1]: Failed to start Etcd Service.</span><br></pre></td></tr></table></figure><p>这个<code>member xxxx has already been bootstrapped</code>解决办法就是把配置文件的下面修改，后面启动完记得改回来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initial-cluster-state: &#39;new&#39; 改成 initial-cluster-state: &#39;existing&#39;</span><br></pre></td></tr></table></figure><p>然后成功启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# systemctl start etcd</span><br><span class="line">[root@k8s-m1 Kubernetes-ansible]# journalctl -xe -u etcd</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Loading server configuration from &quot;&#x2F;etc&#x2F;etcd&#x2F;etcd.config.yml&quot;</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: etcd Version: 3.3.20</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Git SHA: 9fd7e2b80</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Go Version: go1.12.17</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: setting maximum number of CPUs to 16, total number of available CPUs is 16</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: found invalid file&#x2F;dir wal under data dir &#x2F;var&#x2F;lib&#x2F;etcd (Ignore this if you are upgrading etcd)</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: ignoring peer auto TLS since certs given</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: peerTLS: cert &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.crt, key &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;peer.key, ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, trusted-ca &#x3D; &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt, client-cert-auth &#x3D; fals&gt;</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for peers on https:&#x2F;&#x2F;10.252.146.104:2380</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: ignoring client auto TLS since certs given</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: pprof is enabled under &#x2F;debug&#x2F;pprof</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for client requests on 127.0.0.1:2379</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: listening for client requests on 10.252.146.104:2379</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: recovered store from snapshot at index 5952463</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: restore compact to 4369703</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: name &#x3D; etcd-001</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: data dir &#x3D; &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: member dir &#x3D; &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;member</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: dedicated WAL dir &#x3D; &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;wal</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: heartbeat &#x3D; 100ms</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: election &#x3D; 1000ms</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: snapshot count &#x3D; 5000</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: advertise client URLs &#x3D; https:&#x2F;&#x2F;10.252.146.104:2379</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: restarting member ac2dcf6aed12e8f1 in cluster 367e2aebc6430cbe at commit index 5952491</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: ac2dcf6aed12e8f1 became follower at term 47</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: newRaft ac2dcf6aed12e8f1 [peers: [1e713be314744d53,8b1621b475555fd9,ac2dcf6aed12e8f1], term: 47, commit: 5952491, applied: 5952463, lastindex: 5952491, lastterm: 47]</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: enabled capabilities for version 3.3</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: added member 1e713be314744d53 [https:&#x2F;&#x2F;10.252.146.105:2380] to cluster 367e2aebc6430cbe from store</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: added member 8b1621b475555fd9 [https:&#x2F;&#x2F;10.252.146.106:2380] to cluster 367e2aebc6430cbe from store</span><br><span class="line">Jul 23 11:27:55 k8s-m1 etcd[59889]: added member ac2dcf6aed12e8f1 [https:&#x2F;&#x2F;10.252.146.104:2380] to cluster 367e2aebc6430cbe from store</span><br></pre></td></tr></table></figure><p>查看集群状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# etcd-ha</span><br><span class="line">+-----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|          ENDPOINT           |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |</span><br><span class="line">+-----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">| https:&#x2F;&#x2F;10.252.146.104:2379 | ac2dcf6aed12e8f1 |  3.3.20 |  8.3 MB |     false |        47 |    5953557 |</span><br><span class="line">| https:&#x2F;&#x2F;10.252.146.105:2379 | 1e713be314744d53 |  3.3.20 |  8.6 MB |     false |        47 |    5953557 |</span><br><span class="line">| https:&#x2F;&#x2F;10.252.146.106:2379 | 8b1621b475555fd9 |  3.3.20 |  8.3 MB |      true |        47 |    5953557 |</span><br><span class="line">+-----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br></pre></td></tr></table></figure><p>然后给kube-apiserver三个组件和kubelet起来后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m1 Kubernetes-ansible]# kubectl get node -o wide</span><br><span class="line">NAME             STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME</span><br><span class="line">10.252.146.104   Ready    &lt;none&gt;   30d   v1.16.9   10.252.146.104   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker:&#x2F;&#x2F;19.3.11</span><br><span class="line">10.252.146.105   Ready    &lt;none&gt;   30d   v1.16.9   10.252.146.105   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker:&#x2F;&#x2F;19.3.11</span><br><span class="line">10.252.146.106   Ready    &lt;none&gt;   30d   v1.16.9   10.252.146.106   &lt;none&gt;        CentOS Linux 8 (Core)   4.18.0-193.6.3.el8_2.x86_64   docker:&#x2F;&#x2F;19.3.11</span><br></pre></td></tr></table></figure><p>pod也在慢慢自愈了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;研发反馈他们那边一套集群有台master文件系统损坏无法开机，他们是三台openstack上的虚机，是虚拟化宿主机故障导致的虚机文件系统损坏</summary>
      
    
    
    
    <category term="kubernetes" scheme="http://zhangguanzhang.github.io/categories/kubernetes/"/>
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/tags/k8s/"/>
    
    <category term="etcd" scheme="http://zhangguanzhang.github.io/tags/etcd/"/>
    
  </entry>
  
  <entry>
    <title>阿里云上使用flannel host-gw跨节点pod不通的解决</title>
    <link href="http://zhangguanzhang.github.io/2020/06/23/host-gw-in-aliyun/"/>
    <id>http://zhangguanzhang.github.io/2020/06/23/host-gw-in-aliyun/</id>
    <published>2020-06-23T12:36:53.000Z</published>
    <updated>2020-07-15T07:45:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>挺多人问的，写下解决办法</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先主机得在同一个专有vpc下，跨vpc或者经典vpc就扯淡了。还有flannel别把网络信息存储在etcd里，目前flannel只支持v2的etcd api。但是现在k8s都是使用v3 etcd存储了。如果v2和v3共存会导致无法恢复etcd的备份</p><p>所以网上那些etcdctl set flannel的cidr的文章别看了，都没做过备份恢复实验就到处发k8s搭建教程。flannel v3的话看我<a href="https://zhangguanzhang.github.io/2019/03/15/flannel-bin/">过往文章</a></p><h2 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h2><p>先来说下host-gw的包走向，假设两台机器</p><table><thead><tr><th align="left">IP</th><th align="center">pod cidr</th></tr></thead><tbody><tr><td align="left">10.0.6.166</td><td align="center">10.200.1.0/24</td></tr><tr><td align="left">10.0.6.167</td><td align="center">10.200.2.0/24</td></tr></tbody></table><p>pod1 <code>10.200.1.2</code> ping node2上的<code>10.200.2.2</code>，出去的包源目IP是<code>10.200.1.2</code> 和 <code>10.200.2.2</code>，但是包的mac是宿主机和目的主机的mac地址。也就是走二层转发直接到目的的宿主机上，同时这也是host-gw无法跨三层的原因。另外openstack默认会检查出去的包的源ip和源mac是否对的上网卡，收包也一样。这会导致host-gw无法使用，见<a href="https://zhangguanzhang.github.io/2018/07/18/ecs-vip/">ip和mac解绑</a></p><p>帮人排查不通，源主机上抓包发现包发出去了，目的主机上抓包发现没收到包。可以断定阿里的vpc实际上有一些类似过滤的行为，理解为包从宿主机出去还没发到目的宿主机的路上经过了一个东西(ovs，SDN)。实际上阿里云vpc下host-gw跨节点不通就是包经过这个东西被路由到外面去了，然后因为是包的源目IP是私网ip会被SDN丢掉。我们得在专有vpc上配置路由让包不出公网而是到目标ecs上</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>几台主机写几条，为了把包发送到指定的宿主机上</p><p>登录阿里云控制台，<code>专有vpc</code> –&gt; <code>路由表</code> –&gt; <code>自定义</code><br>例如上面我举例的环境则是两条:</p><ul><li><code>10.200.1.0/24</code> 下一跳类型是ecs实例，资源组全部，ECS实例选择<code>10.200.1.0/24</code>所在ECS</li><li><code>10.200.2.0/24</code> 下一跳类型是ecs实例，资源组全部，ECS实例选择<code>10.200.2.0/24</code>所在ECS</li></ul><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>阿里的vpc内部有些cidr是预留的不给客户使用。可用的cidr有:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A类地址：10.0.0.0～10.255.255.255</span><br><span class="line">B类地址：172.16.0.0 ～172.31.255.255</span><br><span class="line">C类地址：192.168.0.0～192.168.255.255</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;挺多人问的，写下解决办法&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;首先主机得在同一个专有vpc下，跨vpc或者经典vpc就扯淡了。还有flannel别把网络信息存储在</summary>
      
    
    
    
    <category term="openstack" scheme="http://zhangguanzhang.github.io/categories/openstack/"/>
    
    
    <category term="host-gw" scheme="http://zhangguanzhang.github.io/tags/host-gw/"/>
    
  </entry>
  
  <entry>
    <title>[未写完]使用go开发一个Prometheus的exporter</title>
    <link href="http://zhangguanzhang.github.io/2020/06/19/go-prom-exporter/"/>
    <id>http://zhangguanzhang.github.io/2020/06/19/go-prom-exporter/</id>
    <published>2020-06-19T20:24:09.000Z</published>
    <updated>2020-07-23T06:41:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>市面上的例子太多都太简单了，这里详细写下我知道的</p><a id="more"></a><h2 id="基本概念和前提"><a href="#基本概念和前提" class="headerlink" title="基本概念和前提"></a>基本概念和前提</h2><p>这里使用go mod开发，别问包怎么拉取</p><p>Prometheus将所有数据存储为时间序列，这里先来了解一下prometheus中的一些基本概念</p><h3 id="指标名和标签"><a href="#指标名和标签" class="headerlink" title="指标名和标签"></a>指标名和标签</h3><p>每个时间序列都由<code>指标名</code>和一组<code>键值对</code>（也称为标签）唯一标识。</p><p>metric的格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;metric name&gt;&#123;&lt;label name&gt;&#x3D;&lt;label value&gt;, ...&#125; metrics_value</span><br></pre></td></tr></table></figure><ul><li><code>metrics_value</code>的值只能是<code>float64</code>，<strong>那些想着收集日志的就别想了</strong></li></ul><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http_requests_total&#123;host&#x3D;&quot;192.10.0.1&quot;, method&#x3D;&quot;POST&quot;, handler&#x3D;&quot;&#x2F;messages&quot;&#125; 278</span><br></pre></td></tr></table></figure><ul><li><code>http_requests_total</code>是指标名；</li><li><code>host</code>、<code>method</code>、<code>handler</code>是三个标签(label)，也就是三个维度；</li><li>值278，根据 metrics 的名字总体就是这个接口POST的次数是278；</li><li>查询语句可以基于这些标签or维度进行过滤和聚合；</li></ul><p>prometheus的监控架构是server向提供了metrics信息的http(s)接口发起GET请求，目标进程或者exporter必须在web路由(例如<code>/metrics</code>)上暴漏metrics的指标。例如下面有三个指标:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># HELP harbor_exporter_collector_duration_seconds Collector time duration.</span><br><span class="line"># TYPE harbor_exporter_collector_duration_seconds gauge</span><br><span class="line">harbor_exporter_collector_duration_seconds&#123;collector&#x3D;&quot;logs&quot;&#125; 0.04826962</span><br><span class="line">harbor_exporter_collector_duration_seconds&#123;collector&#x3D;&quot;projects&quot;&#125; 0.174844256</span><br><span class="line">harbor_exporter_collector_duration_seconds&#123;collector&#x3D;&quot;reach&quot;&#125; 0.011827241</span><br><span class="line">harbor_exporter_collector_duration_seconds&#123;collector&#x3D;&quot;statistics&quot;&#125; 0.056164916</span><br><span class="line">harbor_exporter_collector_duration_seconds&#123;collector&#x3D;&quot;systeminfo&quot;&#125; 0.032053573</span><br><span class="line">harbor_exporter_collector_duration_seconds&#123;collector&#x3D;&quot;systeminfoVolumes&quot;&#125; 0.030168302</span><br><span class="line"># HELP harbor_exporter_last_scrape_error Whether the last scrape of metrics from harbor resulted in an error (1 for error, 0 for success).</span><br><span class="line"># TYPE harbor_exporter_last_scrape_error gauge</span><br><span class="line">harbor_exporter_last_scrape_error 0</span><br><span class="line"># HELP harbor_exporter_scrapes_total Total number of times harbor was scraped for metrics.</span><br><span class="line"># TYPE harbor_exporter_scrapes_total counter</span><br><span class="line">harbor_exporter_scrapes_total 697</span><br></pre></td></tr></table></figure><p>单独一个指标在web上的格式为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># HELP &lt;metric name&gt; &lt;help_msg&gt;</span><br><span class="line"># TYPE &lt;metric name&gt; &lt;metrics type&gt;</span><br><span class="line">&lt;metric name&gt;&#123;&lt;label1&gt;&#x3D;value1,label2&#x3D;value2&#125; &lt;metrics value1&gt;</span><br><span class="line">&lt;metric name&gt;&#123;&lt;label1&gt;&#x3D;value3,label2&#x3D;value4&#125; &lt;metrics value2&gt;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>prometheus的client库已经封装好了这些，我们直接使用即可</p><h3 id="指标类型-metrics-type"><a href="#指标类型-metrics-type" class="headerlink" title="指标类型(metrics type)"></a>指标类型(metrics type)</h3><p>Prometheus client库提供四种核心度量标准类型。注意是客户端。Prometheus服务端没有区分类型，将所有数据展平为无类型时间序列。</p><p><strong>1、 Counter：只增不减的累加指标</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># HELP node_cpu_seconds_total Seconds the cpus spent in each mode.</span><br><span class="line"># TYPE node_cpu_seconds_total counter</span><br><span class="line">node_cpu_seconds_total&#123;cpu&#x3D;&quot;0&quot;,mode&#x3D;&quot;idle&quot;&#125; 380090.49</span><br><span class="line">node_cpu_seconds_total&#123;cpu&#x3D;&quot;0&quot;,mode&#x3D;&quot;iowait&quot;&#125; 114.2</span><br><span class="line">node_cpu_seconds_total&#123;cpu&#x3D;&quot;0&quot;,mode&#x3D;&quot;irq&quot;&#125; 0</span><br><span class="line">node_cpu_seconds_total&#123;cpu&#x3D;&quot;0&quot;,mode&#x3D;&quot;nice&quot;&#125; 0.05</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Counter就是一个计数器，表示一种累积型指标，该指标只能<strong>单调递增</strong>或在重新启动时重置为零，例如，您可以使用计数器来表示所服务的请求数，已完成的任务或错误。</p><p><strong>2、 Gauge：可增可减的测量指标</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># HELP node_filesystem_avail_bytes Filesystem space available to non-root users in bytes.</span><br><span class="line"># TYPE node_filesystem_avail_bytes gauge</span><br><span class="line">node_filesystem_avail_bytes&#123;device&#x3D;&quot;&#x2F;dev&#x2F;mapper&#x2F;centos-home&quot;,fstype&#x3D;&quot;xfs&quot;,mountpoint&#x3D;&quot;&#x2F;home&quot;&#125; 1.300291584e+10</span><br><span class="line">node_filesystem_avail_bytes&#123;device&#x3D;&quot;&#x2F;dev&#x2F;mapper&#x2F;centos-root&quot;,fstype&#x3D;&quot;xfs&quot;,mountpoint&#x3D;&quot;&#x2F;&quot;&#125; 1.300291584e+10</span><br><span class="line">node_filesystem_avail_bytes&#123;device&#x3D;&quot;rootfs&quot;,fstype&#x3D;&quot;rootfs&quot;,mountpoint&#x3D;&quot;&#x2F;&quot;&#125; 1.300291584e+10</span><br></pre></td></tr></table></figure><p>Gauge是最简单的度量类型，只有一个简单的返回值，可增可减，也可以 set 为指定的值，例如是否down了，可以在1和0之间set。</p><p>所以 Gauge 通常用于反映当前状态，比如当前温度或当前内存使用情况；是一种“可增加可减少”的计数指标。</p><p><strong>3、Histogram：自带buckets区间用于统计分布的直方图</strong></p><p>Histogram主要用于在设定的分布范围内(Buckets)记录个数，而不是值。</p><p>例如http请求响应时间：0-100ms、100-200ms、200-300ms、&gt;300ms 的分布情况，Histogram会自动创建3个指标，分别为：</p><ul><li>事件发送的总次数<code>&lt;basename&gt;_count</code>：比如当前一共发生了2次http请求</li><li>所有事件产生值的大小的总和<code>&lt;basename&gt;_sum</code>：比如发生的2次http请求总的响应时间为150ms</li><li>事件产生的值分布在bucket中的次数<code>&lt;basename&gt;_bucket{le=&quot;上限&quot;}</code>：比如响应时间0-100ms的请求1次，100-200ms的请求1次，是累计的直方图</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># HELP rest_client_request_latency_seconds Request latency in seconds. Broken down by verb and URL.</span><br><span class="line"># TYPE rest_client_request_latency_seconds histogram</span><br><span class="line">rest_client_request_latency_seconds_bucket&#123;path&#x3D;&quot;&#x2F;&quot;,method&#x3D;&quot;GET&quot;,code&#x3D;&quot;200&quot;,le&#x3D;&quot;0.1&quot;&#125; 1.0</span><br><span class="line">rest_client_request_latency_seconds_bucket&#123;path&#x3D;&quot;&#x2F;&quot;,method&#x3D;&quot;GET&quot;,code&#x3D;&quot;200&quot;,le&#x3D;&quot;0.2&quot;&#125; 2.0</span><br><span class="line">rest_client_request_latency_seconds_bucket&#123;path&#x3D;&quot;&#x2F;&quot;,method&#x3D;&quot;GET&quot;,code&#x3D;&quot;200&quot;,le&#x3D;&quot;0.3&quot;&#125; 2.0</span><br><span class="line">rest_client_request_latency_seconds_bucket&#123;path&#x3D;&quot;&#x2F;&quot;,method&#x3D;&quot;GET&quot;,code&#x3D;&quot;200&quot;,le&#x3D;&quot;+Inf&quot;&#125; 2.0</span><br><span class="line">rest_client_request_latency_seconds_sum&#123;path&#x3D;&quot;&#x2F;&quot;,method&#x3D;&quot;GET&quot;,code&#x3D;&quot;200&quot;&#125; 0.150</span><br><span class="line">rest_client_request_latency_seconds_count&#123;path&#x3D;&quot;&#x2F;&quot;,method&#x3D;&quot;GET&quot;,code&#x3D;&quot;200&quot;&#125; 2.0</span><br></pre></td></tr></table></figure><p><strong>4、Summary：数据分布统计图</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.</span><br><span class="line"># TYPE go_gc_duration_seconds summary</span><br><span class="line">go_gc_duration_seconds&#123;quantile&#x3D;&quot;0&quot;&#125; 1.4846e-05</span><br><span class="line">go_gc_duration_seconds&#123;quantile&#x3D;&quot;0.25&quot;&#125; 1.8948e-05</span><br><span class="line">go_gc_duration_seconds&#123;quantile&#x3D;&quot;0.5&quot;&#125; 3.9602e-05</span><br><span class="line">go_gc_duration_seconds&#123;quantile&#x3D;&quot;0.75&quot;&#125; 5.8061e-05</span><br><span class="line">go_gc_duration_seconds&#123;quantile&#x3D;&quot;1&quot;&#125; 9.6987e-05</span><br><span class="line">go_gc_duration_seconds_sum 0.000772525</span><br><span class="line">go_gc_duration_seconds_count 18</span><br></pre></td></tr></table></figure><p>其中 quantile 的0和1表示最小和最大值，其余例如 <code>go_gc_duration_seconds{quantile=&quot;0.75&quot;} 5.8061e-05</code>，有百分之75的值是<code>5.8061e-05</code></p><p>Summary和Histogram类似，都可以统计事件发生的次数或者大小，以及其分布情况。</p><p>如果需要聚合（aggregate），选择histograms。</p><p>如果比较清楚要观测的指标的范围和分布情况，选择histograms。如果需要精确的分位数选择summary</p><h3 id="作业和实例"><a href="#作业和实例" class="headerlink" title="作业和实例"></a>作业和实例</h3><p>在Prometheus中，一个可以拉取数据的端点<code>IP:Port</code>叫做一个实例（instance），而具有多个相同类型实例的集合称作一个作业（job）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job:</span> <span class="string">api-server</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">instance 1:</span> <span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span><span class="string">:5670</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">instance 2:</span> <span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span><span class="string">:5671</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">instance 3:</span> <span class="number">5.6</span><span class="number">.7</span><span class="number">.8</span><span class="string">:5670</span></span><br><span class="line">     <span class="bullet">-</span> <span class="attr">instance 4:</span> <span class="number">5.6</span><span class="number">.7</span><span class="number">.8</span><span class="string">:5671</span></span><br></pre></td></tr></table></figure><p>当Prometheus拉取指标数据时，会自动生成一些标签（label）用于区别抓取的来源：</p><ul><li><code>job</code>：配置的作业名；</li><li><code>instance</code>：配置的实例名，若没有实例名，则是抓取的<code>IP:Port</code>。</li></ul><p>对于每一个实例（instance）的抓取，Prometheus会默认保存以下数据：</p><ul><li><code>up{job=&quot;&lt;job&gt;&quot;, instance=&quot;&lt;instance&gt;&quot;}</code>：如果实例是健康的，即可达，值为1，否则为0；</li><li><code>scrape_duration_seconds{job=&quot;&lt;job&gt;&quot;, instance=&quot;&lt;instance&gt;&quot;}</code>：抓取耗时；</li><li><code>scrape_samples_post_metric_relabeling{job=&quot;&lt;job&gt;&quot;, instance=&quot;&lt;instance&gt;&quot;}</code>：指标重新标记后剩余的样本数。</li><li><code>scrape_samples_scraped{job=&quot;&lt;job&gt;&quot;, instance=&quot;&lt;instance&gt;&quot;}</code>：实例暴露的样本数</li></ul><p>该<code>up</code>指标对于监控实例健康状态很有用。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考:"></a>参考:</h2><ul><li><a href="https://github.com/SongLee24/prometheus-exporter" target="_blank" rel="noopener">https://github.com/SongLee24/prometheus-exporter</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;市面上的例子太多都太简单了，这里详细写下我知道的&lt;/p&gt;</summary>
    
    
    
    <category term="go" scheme="http://zhangguanzhang.github.io/categories/go/"/>
    
    <category term="Prometheus" scheme="http://zhangguanzhang.github.io/categories/go/Prometheus/"/>
    
    
    <category term="exporter" scheme="http://zhangguanzhang.github.io/tags/exporter/"/>
    
  </entry>
  
  <entry>
    <title>v1.17+ k8s集群下CNI使用VXLAN模式SVC有63秒延迟的触发原因定位</title>
    <link href="http://zhangguanzhang.github.io/2020/05/23/k8s-vxlan-63-timeout/"/>
    <id>http://zhangguanzhang.github.io/2020/05/23/k8s-vxlan-63-timeout/</id>
    <published>2020-05-23T19:01:01.000Z</published>
    <updated>2021-01-12T03:57:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>这个问题 flannel 和 calico 的 VXLAN 模式下都会发生，部分人的现象是集群的A记录 UDP 下查询可能有问题（也有人在 azure 上在宿主机上访问 svc 的 clusterIP 10%几率才能通），原因是v1.17+的k8s会引起内核的某个 UDP 相关 bug而不是cni的软件层面，weave没有，后面说。</p><p>写这篇文章的日期是<code>05/28</code>，发现是上周五也就是<code>05/23</code>号，文章从时间线写起(因为很多时候想发文章但是没空，所以文章的发布日期是<code>05/23</code>)</p><p>2020-07-19更新，版本<code>v1.18.6, v1.16.13, v1.17.9</code>+已经修复这个问题，可以同版本内升级，或者只切kube-proxy版本</p><h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>上周五我经过同事的工位看到同事的桌面是k8s的get po的输出，问他咋开始学k8s了，他说跟着视频学下，看了下用的kubeadm部署了一套<code>1.18.2</code>的集群。1.18的kube-proxy的ipvs包的parseIP有bug，我推荐他换<code>v1.17.5</code>。他当时在部署一个入门的svc实验，换了后还是无法解析域名(实际上后面对照组<code>v1.18.2</code>也解析不了)。使用dig命令排查了下，下面是对照:</p><ul><li><code>dig @&lt;podIP&gt; +short kubernetes.default.svc.cluster.local</code> 能解析</li><li><code>dig @10.96.0.10 +short kubernetes.default.svc.cluster.local</code> 超时</li></ul><p>很多市面上的kubeadm部署教程都是直接kubeadm init，所以我推荐同事去按照我文章的kubeadm部署一套后再试试，叫他用v1.17的最新版本<code>v1.17.5</code>，结果还是上面一样。coredns实际上还有metrics的http接口，从http层测了下：</p><ul><li><code>curl -I 10.96.0.10:9153/metrics</code> 超时，很久之后才有返回</li><li><code>curl -I &lt;podIP&gt;:9153/metrics</code> 能直接返回</li></ul><p>涉及到本次排查的信息为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node -o wide</span><br><span class="line">NAME     STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME</span><br><span class="line">master   Ready    master   7d8h   v1.18.2   10.0.100.3    &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.el7.x86_64   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line">node1    Ready    &lt;none&gt;   7d7h   v1.18.2   10.0.100.4    &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.el7.x86_64   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line">node2    Ready    &lt;none&gt;   7d7h   v1.18.2   10.0.100.15   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.el7.x86_64   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line"></span><br><span class="line">$ kubectl get po -o wide -n kube-system -l k8s-app&#x3D;kube-dns</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-546565776c-v5wwg   1&#x2F;1     Running   2          25h   10.244.2.73   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>多次尝试发现很久的时间都是一样，用time命令观察了下一直是63秒返回。包括其他任何svc都是这样</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ time    curl -I 10.96.0.10:9153&#x2F;metrics</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Content-Type: text&#x2F;plain; version&#x3D;0.0.4; charset&#x3D;utf-8</span><br><span class="line">Date: Wed, 25 May 2020 08:39:35 GMT</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real1m3.091s</span><br><span class="line">user0m0.002s</span><br><span class="line">sys0m0.007s</span><br></pre></td></tr></table></figure><p>proxyMode是ipvs，用ipvsadm看下超时的时候的状态，一直是<code>SYN_RECV</code>，也就是发送了SYN，没收到回包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -lnc |&amp; grep 9153</span><br><span class="line">TCP 00:59  SYN_RECV    10.96.0.10:41282   10.96.0.10:9153    10.244.2.73:9153</span><br></pre></td></tr></table></figure><h3 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h3><p>因为cni使用的flannel，用的vxlan模式。master上抓9153和flannel.1的8472端口，coredns的pod所在node上抓flannel的vxlan包，下面三个是对应的:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@master &#x2F;root]# tcpdump -nn -i flannel.1 port 9153</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:30:56.705696 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17148909 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:30:57.708489 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17149912 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:30:59.712458 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17151916 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:03.716441 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17155920 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:11.732562 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17163936 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:27.764498 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17179968 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:59.828493 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17212032 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:59.829565 IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [S.], seq 435819916, ack 911217172, win 27960, options [mss 1410,sackOK,TS val 17212067 ecr 17212032,nop,wscale 7], length 0</span><br><span class="line">16:31:59.829611 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 0</span><br><span class="line">16:31:59.829714 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [P.], seq 1:88, ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 87</span><br><span class="line">16:31:59.829897 IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [.], ack 88, win 219, options [nop,nop,TS val 17212067 ecr 17212033], length 0</span><br><span class="line">16:31:59.831300 IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [P.], seq 1:113, ack 88, win 219, options [nop,nop,TS val 17212069 ecr 17212033], length 112</span><br><span class="line">16:31:59.831322 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 113, win 342, options [nop,nop,TS val 17212034 ecr 17212069], length 0</span><br><span class="line">16:31:59.831435 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [F.], seq 88, ack 113, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0</span><br><span class="line">16:31:59.831633 IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [F.], seq 113, ack 89, win 219, options [nop,nop,TS val 17212069 ecr 17212035], length 0</span><br><span class="line">16:31:59.831660 IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 114, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@master &#x2F;root]# tcpdump -nn -i eth0 port 8472</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:30:56.705718 IP 10.0.100.3.48683 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17148909 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:30:57.708523 IP 10.0.100.3.48683 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17149912 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:30:59.712478 IP 10.0.100.3.48683 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17151916 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:03.716452 IP 10.0.100.3.48683 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17155920 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:11.732590 IP 10.0.100.3.48683 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17163936 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:27.764513 IP 10.0.100.3.48683 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17179968 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:59.828541 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17212032 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:59.829521 IP 10.0.100.15.56771 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [S.], seq 435819916, ack 911217172, win 27960, options [mss 1410,sackOK,TS val 17212067 ecr 17212032,nop,wscale 7], length 0</span><br><span class="line">16:31:59.829617 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 0</span><br><span class="line">16:31:59.829729 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [P.], seq 1:88, ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 87</span><br><span class="line">16:31:59.829883 IP 10.0.100.15.34571 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [.], ack 88, win 219, options [nop,nop,TS val 17212067 ecr 17212033], length 0</span><br><span class="line">16:31:59.831292 IP 10.0.100.15.34571 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [P.], seq 1:113, ack 88, win 219, options [nop,nop,TS val 17212069 ecr 17212033], length 112</span><br><span class="line">16:31:59.831327 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 113, win 342, options [nop,nop,TS val 17212034 ecr 17212069], length 0</span><br><span class="line">16:31:59.831448 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [F.], seq 88, ack 113, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0</span><br><span class="line">16:31:59.831612 IP 10.0.100.15.34571 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [F.], seq 113, ack 89, win 219, options [nop,nop,TS val 17212069 ecr 17212035], length 0</span><br><span class="line">16:31:59.831665 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 114, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@node2 &#x2F;root]# tcpdump -nn  -i eth0 port 8472</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:31:59.836137 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [S], seq 911217171, win 43690, options [mss 65495,sackOK,TS val 17212032 ecr 0,nop,wscale 7], length 0</span><br><span class="line">16:31:59.836328 IP 10.0.100.15.56771 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [S.], seq 435819916, ack 911217172, win 27960, options [mss 1410,sackOK,TS val 17212067 ecr 17212032,nop,wscale 7], length 0</span><br><span class="line">16:31:59.836811 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 0</span><br><span class="line">16:31:59.836910 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [P.], seq 1:88, ack 1, win 342, options [nop,nop,TS val 17212033 ecr 17212067], length 87</span><br><span class="line">16:31:59.836951 IP 10.0.100.15.34571 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [.], ack 88, win 219, options [nop,nop,TS val 17212067 ecr 17212033], length 0</span><br><span class="line">16:31:59.838385 IP 10.0.100.15.34571 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [P.], seq 1:113, ack 88, win 219, options [nop,nop,TS val 17212069 ecr 17212033], length 112</span><br><span class="line">16:31:59.838522 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 113, win 342, options [nop,nop,TS val 17212034 ecr 17212069], length 0</span><br><span class="line">16:31:59.838621 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [F.], seq 88, ack 113, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0</span><br><span class="line">16:31:59.838703 IP 10.0.100.15.34571 &gt; 10.0.100.3.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.2.73.9153 &gt; 10.244.0.0.2201: Flags [F.], seq 113, ack 89, win 219, options [nop,nop,TS val 17212069 ecr 17212035], length 0</span><br><span class="line">16:31:59.838836 IP 10.0.100.3.56618 &gt; 10.0.100.15.8472: OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">IP 10.244.0.0.2201 &gt; 10.244.2.73.9153: Flags [.], ack 114, win 342, options [nop,nop,TS val 17212035 ecr 17212069], length 0</span><br></pre></td></tr></table></figure><p>先看上面的第一部分，搜了下资料 <a href="https://blog.csdn.net/u010039418/article/details/78234570">https://blog.csdn.net/u010039418/article/details/78234570</a></p><p>得知tcp默认SYN报文最大retry 5次，每次超时了翻倍，<code>1s -&gt; 3s -&gt; 7s -&gt; 15s -&gt; 31s -&gt; 63s</code>。只有63秒的时候node的机器上才收到了vxlan的报文。说明pod所在node压根没收到63秒之前的。</p><p>一般lvs的dr模式下tcp的时间戳混乱或者其他几个arp的内核参数不对下<code>SYN</code>是一直收不到的而不是63秒后有结果，所以和内核相关参数无关。于是同样上面的步骤tcpdump抓包，加上-w选项把抓的包导出下来导入到wireshark里准备看看</p><h3 id="报文分析"><a href="#报文分析" class="headerlink" title="报文分析"></a>报文分析</h3><p>9153的包wireshark里看63秒前面都是tcp的SYN重传，看到了master上向外发送的vxlan报文的时候有了发现。下面的截图是放github上的，网络不好会看不到截图</p><p>可以看到udp的checksum是<code>0xffff</code>，我对udp报文不太熟悉，udp的header的checksum没记错的话crc32校验的，不可能是这种两个字节都置1的0xffff，明显就是udp的header的校验出错了。后面几个正常包的checksum都是missing的</p><p><img src="https://github.com/zhangguanzhang/Image-Hosting/blob/master/k8s/vxlan-udp-csum1.png?raw=true" alt="vxlan1"></p><p>wireshark的<code>编辑</code>-&gt;<code>首选项</code>-&gt;<code>Protocols</code>-&gt;<code>UDP</code>的<code>Validate the UDP checksum if possible</code>勾上更直观看</p><p><img src="https://github.com/zhangguanzhang/Image-Hosting/blob/master/k8s/vxlan-udp-csum2.png?raw=true" alt="vxlan1"></p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>搜了下<code>wireshark linux udp checksum incorrect</code>，都是推荐把checksum offload disable掉就行了，例如我这里是flannel，则是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;sbin&#x2F;ethtool -K flannel.1 tx-checksum-ip-generic off</span><br><span class="line">Actual changes:</span><br><span class="line">tx-checksumming: off</span><br><span class="line">tx-checksum-ip-generic: off</span><br><span class="line">tcp-segmentation-offload: off</span><br><span class="line">tx-tcp-segmentation: off [requested on]</span><br><span class="line">tx-tcp-ecn-segmentation: off [requested on]</span><br><span class="line">tx-tcp6-segmentation: off [requested on]</span><br><span class="line">tx-tcp-mangleid-segmentation: off [requested on]</span><br><span class="line">udp-fragmentation-offload: off [requested on]</span><br></pre></td></tr></table></figure><p>再测下正常，而weave他们也用的vxlan模式，但是他们在创建网卡的时候把这个已经off掉了，所以他的vxlan模式在<code>v1.17+</code>集群没出现这个问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time curl -I 10.96.0.10:9153</span><br><span class="line">HTTP&#x2F;1.1 404 Not Found</span><br><span class="line">Content-Type: text&#x2F;plain; charset&#x3D;utf-8</span><br><span class="line">X-Content-Type-Options: nosniff</span><br><span class="line">Date: Wed, 27 May 2020 02:14:04 GMT</span><br><span class="line">Content-Length: 19</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real0m0.009s</span><br><span class="line">user0m0.005s</span><br><span class="line">sys0m0.003s</span><br></pre></td></tr></table></figure><p>你以为这样就完了？其实并没有，因为我自己维护了一套ansible部署k8s的方案，每次新版本发布我都会实际测下。并且同事反映了他同样云主机开出来用我ansible部署<code>v1.17.5</code>没有这个问题。这就很奇怪了，原因后面说，请接着继续看</p><h3 id="什么是checksum-offload"><a href="#什么是checksum-offload" class="headerlink" title="什么是checksum offload"></a>什么是checksum offload</h3><p>资料:</p><ul><li><a href="https://wiki.wireshark.org/CaptureSetup/Offloading">https://wiki.wireshark.org/CaptureSetup/Offloading</a></li><li><a href="https://zh.wikipedia.org/wiki/TCP%E6%A0%A1%E9%AA%8C%E5%92%8C%E5%8D%B8%E8%BD%BD">https://zh.wikipedia.org/wiki/TCP%E6%A0%A1%E9%AA%8C%E5%92%8C%E5%8D%B8%E8%BD%BD</a></li><li>Checksum Offload 是网卡的一个功能选项。如果该选项开启，则网卡会负责计算需要发送或者接收到的TCP消息的校验和，从而节省CPU的计算开销。此时，在需要发送的消息到达网卡前，系统会在TCP报头的校验和字段填充一个随机值。<br>但是，尽管Checksum Offload能够降低 CPU 的计算开销，但受到计算能力的限制，某些环境下的一些网卡计算速度不如主频超过400MHz的CPU快</li></ul><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="对照组"><a href="#对照组" class="headerlink" title="对照组"></a>对照组</h3><p>很奇怪的就是为啥就是我的ansible部署的二进制就正常没这个问题，而kubeadm部署的就不正常，后面我花时间整了以下几个对照组(期间同事也帮我做了几个条件下的测试，但是不是系统用错了就是版本整错了。。。)，终于找到了问题的范围，下面是我自己统计的对照组信息，kubeadm和ansible版本均为<code>1.17.5</code>测试。os不重要，因为最终排查出和os无关</p><table><thead><tr><th align="left">os</th><th align="center">type(kubeadm or ansible)</th><th align="center">flannel version</th><th align="center">flannel is running in pod?</th><th align="center">will 63 sec</th></tr></thead><tbody><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v0.11.0</td><td align="center">yes</td><td align="center">yes</td></tr><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v0.12.0</td><td align="center">yes</td><td align="center">yes</td></tr><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v0.11.0</td><td align="center">no</td><td align="center">yes</td></tr><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v0.12.0</td><td align="center">no</td><td align="center">yes</td></tr><tr><td align="left">7.6</td><td align="center">ansible</td><td align="center">v0.11.0</td><td align="center">yes</td><td align="center">no</td></tr><tr><td align="left">7.6</td><td align="center">ansible</td><td align="center">v0.12.0</td><td align="center">yes</td><td align="center">no</td></tr><tr><td align="left">7.6</td><td align="center">ansible</td><td align="center">v0.11.0</td><td align="center">no</td><td align="center">no</td></tr><tr><td align="left">7.6</td><td align="center">ansible</td><td align="center">v0.12.0</td><td align="center">no</td><td align="center">no</td></tr></tbody></table><p>这就看起来很迷了。但是排查出和flannel无关，感觉kube-proxy有关系，然后今天<code>05/28</code>针对kube-proxy做了个对照组</p><table><thead><tr><th align="left">os</th><th align="center">type(kubeadm or ansible)</th><th align="center">kube-proxy version</th><th align="center">kube-proxy is running in pod?</th><th align="center">will 63 sec</th></tr></thead><tbody><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v1.17.5</td><td align="center">yes</td><td align="center">yes</td></tr><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v1.17.5</td><td align="center">no</td><td align="center">no</td></tr><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v1.16.9</td><td align="center">yes</td><td align="center">no</td></tr><tr><td align="left">7.6</td><td align="center">kubeadm</td><td align="center">v1.16.9</td><td align="center">no</td><td align="center">no</td></tr><tr><td align="left">7.6</td><td align="center">ansible</td><td align="center">v1.17.5</td><td align="center">yes</td><td align="center">yes</td></tr><tr><td align="left">7.6</td><td align="center">ansible</td><td align="center">v1.17.5</td><td align="center">no</td><td align="center">no</td></tr></tbody></table><p>可以看出就是1.17以上的版本如果使用pod则会有这个问题，而非pod则不会，github上compare了下<code>v1.17.0</code>和<code>v1.16.3</code> <a href="https://github.com/kubernetes/kubernetes/compare/v1.16.3...v1.17.0">https://github.com/kubernetes/kubernetes/compare/v1.16.3...v1.17.0</a></p><p>发现了Dockerfile的改动 <a href="https://github.com/kubernetes/kubernetes/commit/fed582333f639dc22e879f4bbb258e403c210c30">https://github.com/kubernetes/kubernetes/commit/fed582333f639dc22e879f4bbb258e403c210c30</a></p><p>见上面的commit <code>1.17.0</code>里的 Dockerfile 的BASEIMAGE是用  <a href="https://github.com/coreos/flannel/pull/1282#issuecomment-635273081">指定了一个源安装了最新的iptables</a>，然后利用<code>update-alternatives</code>把脚本<code>/usr/sbin/iptables-wrapper</code>去替代<code>iptables</code> 来检测应该使用<code>nft</code>还是<code>legacy</code>， hack 下镜像回自带源里的 iptables 验证下</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> registry.aliyuncs.com/google_containers/kube-proxy:v1.<span class="number">17.5</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> rm -f /usr/sbin/iptables &amp;&amp; </span></span><br><span class="line">    clean-install iptables</span><br></pre></td></tr></table></figure><p>构建的镜像推送到了dockerhub上<code>zhangguanzhang/hack-kube-proxy:v1.17.5</code>，更改下集群kube-proxy ds的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get ds kube-proxy -o yaml | grep image:</span><br><span class="line">        image: zhangguanzhang&#x2F;hack-kube-proxy:v1.17.5</span><br></pre></td></tr></table></figure><p>测试访问成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> time curl -I 10.96.0.10:9153</span><br><span class="line">HTTP&#x2F;1.1 404 Not Found</span><br><span class="line">Content-Type: text&#x2F;plain; charset&#x3D;utf-8</span><br><span class="line">X-Content-Type-Options: nosniff</span><br><span class="line">Date: Thu, 28 May 2020 04:47:21 GMT</span><br><span class="line">Content-Length: 19</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">real0m0.008s</span><br><span class="line">user0m0.003s</span><br><span class="line">sys0m0.003s</span><br></pre></td></tr></table></figure><p>对于这个问题我在flannel的pr下面也参与了回复，<a href="https://github.com/coreos/flannel/pull/1282">https://github.com/coreos/flannel/pull/1282</a> 官方github上提了一个issue <a href="https://github.com/kubernetes/kubernetes/issues/91519">https://github.com/kubernetes/kubernetes/issues/91519</a></p><p>这个问题的触发是由于<code>v1.17+</code>的 kube-proxy 的 docker 镜像里安装了最新的 iptables ， <code>--random-fully</code>选项<a href="https://github.com/kubernetes/kubernetes/issues/88986#issuecomment-635640143">会触发内核vxlan的bug</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>解决问题三种办法:</p><ul><li>关闭vxlan的网卡的checksum offload</li><li>更改Docker镜像</li><li>升级到新内核，具体版本就不知道了，只要在这个内核pr并后出的内核版本都行 <a href="https://github.com/torvalds/linux/commit/ea64d8d6c675c0bb712689b13810301de9d8f77a">https://github.com/torvalds/linux/commit/ea64d8d6c675c0bb712689b13810301de9d8f77a</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这个问题 flannel 和 calico 的 VXLAN 模式下都会发生，部分人的现象是集群的A记录 UDP 下查询可能有问题（也有人在 azure 上在宿主机上访问 svc 的 clusterIP 10%几率才能通），原因是v1.17+的k8s会引起内核的某个 UDP </summary>
      
    
    
    
    <category term="k8s" scheme="http://zhangguanzhang.github.io/categories/k8s/"/>
    
    <category term="vxlan" scheme="http://zhangguanzhang.github.io/categories/k8s/vxlan/"/>
    
    
    <category term="63s" scheme="http://zhangguanzhang.github.io/tags/63s/"/>
    
    <category term="timeout" scheme="http://zhangguanzhang.github.io/tags/timeout/"/>
    
  </entry>
  
  <entry>
    <title>proxmox x86软路由笔记</title>
    <link href="http://zhangguanzhang.github.io/2020/05/13/x86-router-flash/"/>
    <id>http://zhangguanzhang.github.io/2020/05/13/x86-router-flash/</id>
    <published>2020-05-13T19:14:06.000Z</published>
    <updated>2021-02-12T08:05:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>办公室有台式机，不想整天带着N1上下班，整下x86的软路由，exsi性能会比pve好，但是pve毕竟Linux，兼容性和对接很多场景方便，这里我使用pve开机器安装openwrt。</p><p>pve已经安装好系统(如果你还没安装且打算安装，安装完后可以看看我这个文章<a href="https://zhangguanzhang.github.io/2019/01/22/proxmox-cloud-init/#%E5%AE%89%E8%A3%85%E5%AE%8Cproxmox%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%BE%E7%BD%AE">安装完proxmox的一些设置</a>)，并且台式机的口子接主路由的lan口。主路由是我的路由器，它wan口接办公室的网口，台式机只有一个口子，所以软路由作为旁路由使用。</p><h2 id="前置工作"><a href="#前置工作" class="headerlink" title="前置工作"></a>前置工作</h2><h3 id="虚机准备"><a href="#虚机准备" class="headerlink" title="虚机准备"></a>虚机准备</h3><p>先去<a href="https://www.right.com.cn/forum/forum-169-1.html">恩山论坛x86版块</a>下一个固件，或者下载我的 <a href="https://github.com/zhangguanzhang/Actions-OpenWrt/actions">action 编译的固件</a>找 <code>Build OpenWrt</code>的 step 点进去，然后最下面的<code>OpenWrt_firmware_generic_202xxxx</code>下载。如果你有办公室—&gt; ecs &lt;— 个人家里组网的要求的话，推荐你去找个带wireguard的固件，我的固件也带 wireguard。<br>pve上开台机器</p><ul><li><code>一般</code>-<code>高级</code>-<code>开机自启动</code>勾上，有必要的话手动设置下vmID，后面有用</li><li><code>操作系统</code>不适用任何介质</li><li><code>系统</code>默认，下一步</li><li><code>硬盘</code>随便设置，后面会删除</li><li>cpu按照实际，我给2核，内存我给的2g</li><li><code>网络</code>，模型选<code>intel E1000</code>，<code>防火墙</code>的勾去掉</li><li>完成</li><li>选中虚机，<code>硬件</code>-<code>选中硬盘，点击分离，删除</code></li></ul><h3 id="导入img"><a href="#导入img" class="headerlink" title="导入img"></a>导入img</h3><p>把固件上传到pve的机器上，一般是gz，解压成img后用命令转成qcow2文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img convert -f raw  -O qcow2 openwrt-x86-64-generic-squashfs-combined-efi.img op.qcow2</span><br></pre></td></tr></table></figure><p>检查下，应该输出<code>No errors</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img check op.qcow2</span><br></pre></td></tr></table></figure><p>导入成硬盘，这里vm的id是对应前面的vmid，前面没设置的话web控制台上看下openwrt的虚机的vmid</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qm importdisk 200 op.qcow2 local-lvm</span><br></pre></td></tr></table></figure><p>导入后在界面上，选中创建机器带的硬盘，点击上面的分离，然后点击这个硬盘，点击删除。双击我们导入的硬盘，点击右下角的<code>添加</code>。</p><h2 id="旁路由的配置"><a href="#旁路由的配置" class="headerlink" title="旁路由的配置"></a>旁路由的配置</h2><h3 id="路由静态ip配置"><a href="#路由静态ip配置" class="headerlink" title="路由静态ip配置"></a>路由静态ip配置</h3><p>这里我是主路由<code>192.168.2.1/24</code>作为二级路由接办公网的口子上的，还提供wifi，openwrt的虚机作为旁路由，ip规划为<code>192.168.2.3</code>，主路由不开DHCP，旁路由开DHCP(有的路由器不支持dhcp设置网关的ip，所以我这里旁路由作为DHCP server)</p><p>开机后大概36秒后按回车进入终端。更改旁路由的网络配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/config/network /etc/config/network.bak</span><br><span class="line">vi /etc/config/network</span><br></pre></td></tr></table></figure><p>把<code>192.168.1.1</code>改为预期的配置，没网关的话就也加上预期的主路由的ip</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config interface &#x27;eth0&#x27;</span><br><span class="line">option ifname &#x27;eth0&#x27;</span><br><span class="line">option proto &#x27;static&#x27;</span><br><span class="line">option ipaddr &#x27;192.168.2.3&#x27;</span><br><span class="line">option netmask &#x27;255.255.255.0&#x27;</span><br><span class="line">option gateway &#x27;192.168.2.1&#x27;</span><br></pre></td></tr></table></figure><p>重启网卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network restart</span><br></pre></td></tr></table></figure><p>然后按下回车，ping下114测下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping 114.114.114.114</span><br></pre></td></tr></table></figure><p>然后浏览器进<code>192.168.2.3</code>，默认密码admin password啥的试试</p><h3 id="web配置"><a href="#web配置" class="headerlink" title="web配置"></a>web配置</h3><h4 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h4><p>我的固件只有一个LAN接口，不确定其他的是不是这样(推荐此处到最后都先看一遍完后再跟着操作)<br><code>网络</code>-<code>接口</code>，进入LAN修改，下面的DHCP，开启动了。然后高级设置，动态DHCP+强制，写上掩码，下面的DHCP选项两行</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3,192.168.2.3                    # 配置dhcp的网关，指向旁路由自己</span><br><span class="line">6,192.168.2.3    # 配置dhcp获取到的dns，如果稳定则旁路由自己的IP</span><br></pre></td></tr></table></figure><p><code>IPv6设置</code>里前三个全部选禁用<br>保存应用</p><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><p>这里分为使用<code>dnsmasq</code>和<code>adguard home</code>，我个人是使用<code>adguard home</code>的，我遇到过了dnsmasq经常在加配置的时候卡死，而且解析不稳定</p><h4 id="dnsmasq作为dns-server"><a href="#dnsmasq作为dns-server" class="headerlink" title="dnsmasq作为dns server"></a>dnsmasq作为dns server</h4><p><code>网络</code>-<code>DHCP/DNS</code>-<code>常规设置</code>，有必要的话配置下DNS转发，屏蔽一些激活码请求域名啥的，<code>丢弃 RFC1918 上行响应数据</code>这个取消了，我这儿是不然某些上游dns的域名无法访问到<br>最下面的写hosts列表，例如单独的指定公网ip下载jetbrains家的插件和软件，绕过前面配置的屏蔽</p><h4 id="adguard-home-作为dns-server"><a href="#adguard-home-作为dns-server" class="headerlink" title="adguard home 作为dns server"></a>adguard home 作为dns server</h4><p>如果固件没有的话去下载ipk文件  <a href="https://github.com/rufengsuixing/luci-app-adguardhome/releases">https://github.com/rufengsuixing/luci-app-adguardhome/releases</a> 下载后在 <code>web-系统-文件传输</code>传上去， 然后ssh执行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opkg install /tmp/upload/luci-app-adguardhome_*</span><br></pre></td></tr></table></figure><p>如果你当前的网络是已经连在这个旁路由上，那这步的关闭先别做，因为关闭了 dnsmasq 的 dns 就会无法解析域名导致无法下载后面的 adguard home 二进制文件。</p><p>大多数固件都不会自带 adgurad home 的二进制文件的，我们需要在自己pc上下载 adguardhome 的二进制文件在<code>web-系统-文件传输</code>传上去。<a href="https://github.com/AdguardTeam/AdGuardHome/wiki/Getting-Started">下载网页，记得下载linux-amd64的</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩包带了目录，所以直接解压到/usr/bin</span></span><br><span class="line">tar zxf /tmp/upload/AdGuardHome_linux_amd64.tar.gz -C /usr/bin/</span><br><span class="line">rm -f /tmp/upload/AdGuardHome_linux_amd64.tar.gz</span><br><span class="line">touch /etc/config/AdGuardHome.yaml</span><br></pre></td></tr></table></figure><p><code>服务</code>-<code>AdGuard Home</code>-<code>配置文件路径</code> 改为 <code>/etc/config/AdGuardHome.yaml</code></p><p><code>服务</code>-<code>AdGuard Home</code>-<code>手动设置</code>，下面是我用的配置文件，web登录的密码是root，配置也可以参考 <a href="https://p3terx.com/archives/use-adguard-home-to-build-dns-to-prevent-pollution-and-remove-ads-2.html">使用 AdGuard Home 自建 DNS 防污染、去广告 #2 - 优化增强设置详解教程</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">bind_host:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">bind_port:</span> <span class="number">3000</span></span><br><span class="line"><span class="attr">users:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">root</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">$2y$05$8h.LpbIR7U50.qbV7ynCtOvS9szcqu2lFk6J86Oabnz1J5BtLpVni</span></span><br><span class="line"><span class="attr">http_proxy:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">rlimit_nofile:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">debug_pprof:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">web_session_ttl:</span> <span class="number">720</span></span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line">  <span class="attr">bind_host:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">  <span class="attr">statistics_interval:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">querylog_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">querylog_file_enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">querylog_interval:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">querylog_size_memory:</span> <span class="number">1000</span></span><br><span class="line">  <span class="attr">anonymize_client_ip:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">protection_enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">blocking_mode:</span> <span class="string">nxdomain</span></span><br><span class="line">  <span class="attr">blocking_ipv4:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">blocking_ipv6:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">blocked_response_ttl:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">parental_block_host:</span> <span class="string">family-block.dns.adguard.com</span></span><br><span class="line">  <span class="attr">safebrowsing_block_host:</span> <span class="string">standard-block.dns.adguard.com</span></span><br><span class="line">  <span class="attr">ratelimit:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">ratelimit_whitelist:</span> []</span><br><span class="line">  <span class="attr">refuse_any:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">upstream_dns:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://dns.google/dns-query</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">tcp://223.5.5.5</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://dns.alidns.com/dns-query</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://doh.pub/dns-query</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://dns.adguard.com/dns-query</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://dns-family.adguard.com/dns-query</span></span><br><span class="line">  <span class="comment">#- sdns://AwAAAAAAAAAAAAANdGxzOi8vOC44LjguOA</span></span><br><span class="line">  <span class="comment">#- sdns://AAAAAAAAAAAACTc3Ljg4LjguOA</span></span><br><span class="line">  <span class="comment">#- sdns://AAAAAAAAAAAACjc3Ljg4LjguODg</span></span><br><span class="line">  <span class="comment">#- sdns://AQMAAAAAAAAAFDE4NS4yMjguMTY4LjE2ODo4NDQzILysMvrVQ2kXHwgy1gdQJ8MgjO7w6OmflBjcd2Bl1I8pEWNsZWFuYnJvd3Npbmcub3Jn</span></span><br><span class="line">  <span class="comment">#- sdns://AgcAAAAAAAAABzEuMC4wLjGgENk8mGSlIfMGXMOlIlCcKvq7AVgcrZxtjon911-ep0cg63Ul-I8NlFj4GplQGb_TTLiczclX57DvMV8Q-JdjgRgSZG5zLmNsb3VkZmxhcmUuY29tCi9kbnMtcXVlcnk</span></span><br><span class="line">  <span class="comment">#- sdns://AwAAAAAAAAAAAAANdGxzOi8vOS45LjkuOQ</span></span><br><span class="line">  <span class="attr">bootstrap_dns:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">223.5</span><span class="number">.5</span><span class="number">.5</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">223.6</span><span class="number">.6</span><span class="number">.6</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">9.9</span><span class="number">.9</span><span class="number">.10</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">149.112</span><span class="number">.112</span><span class="number">.10</span></span><br><span class="line">  <span class="attr">all_servers:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">fastest_addr:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">allowed_clients:</span> []</span><br><span class="line">  <span class="attr">disallowed_clients:</span> []</span><br><span class="line">  <span class="attr">blocked_hosts:</span> []</span><br><span class="line">  <span class="attr">cache_size:</span> <span class="number">4194304</span></span><br><span class="line">  <span class="attr">cache_ttl_min:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">cache_ttl_max:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">bogus_nxdomain:</span> []</span><br><span class="line">  <span class="attr">aaaa_disabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">enable_dnssec:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">edns_client_subnet:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">filtering_enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">filters_update_interval:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">parental_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">safesearch_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">safebrowsing_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">safebrowsing_cache_size:</span> <span class="number">1048576</span></span><br><span class="line">  <span class="attr">safesearch_cache_size:</span> <span class="number">1048576</span></span><br><span class="line">  <span class="attr">parental_cache_size:</span> <span class="number">1048576</span></span><br><span class="line">  <span class="attr">cache_time:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">rewrites:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">plugins.jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">13.32</span><span class="number">.53</span><span class="number">.109</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">download.jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">52.30</span><span class="number">.174</span><span class="number">.243</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">harbor.zhangguanzhang.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">192.168</span><span class="number">.2</span><span class="number">.111</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">www.jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">  <span class="attr">blocked_services:</span> []</span><br><span class="line"><span class="attr">tls:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">server_name:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">force_https:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">port_https:</span> <span class="number">443</span></span><br><span class="line">  <span class="attr">port_dns_over_tls:</span> <span class="number">853</span></span><br><span class="line">  <span class="attr">allow_unencrypted_doh:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">strict_sni_check:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">certificate_chain:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">private_key:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">certificate_path:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">private_key_path:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">filters:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://adguardteam.github.io/AdGuardSDNSFilter/Filters/filter.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">AdGuard</span> <span class="string">Simplified</span> <span class="string">Domain</span> <span class="string">Names</span> <span class="string">filter</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://adaway.org/hosts.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">AdAway</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">2</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://hosts-file.net/ad_servers.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hpHosts</span> <span class="bullet">-</span> <span class="string">Ad</span> <span class="string">and</span> <span class="string">Tracking</span> <span class="string">servers</span> <span class="string">only</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">3</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://www.malwaredomainlist.com/hostslist/hosts.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">MalwareDomainList.com</span> <span class="string">Hosts</span> <span class="string">List</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">4</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://raw.githubusercontent.com/vokins/yhosts/master/data/tvbox.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tvbox</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575018007</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://hosts.nfz.moe/full/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">neoHosts</span> <span class="string">full</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618240</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://hosts.nfz.moe/basic/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">neoHosts</span> <span class="string">basic</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618241</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">http://sbc.io/hosts/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">StevenBlack</span> <span class="string">host</span> <span class="string">basic</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618242</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">http://sbc.io/hosts/alternates/fakenews-gambling-porn-social/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">StevenBlack</span> <span class="string">host+fakenews</span> <span class="string">+</span> <span class="string">gambling</span> <span class="string">+</span> <span class="string">porn</span> <span class="string">+</span> <span class="string">social</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618243</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://cdn.jsdelivr.net/gh/privacy-protection-tools/anti-AD/anti-ad-easylist.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">anti-AD(Adblock+neohosts+yhosts+cjxlist+adhlist)</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1577113202</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://zhangguanzhang.github.io/adguard/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hosts</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1608901996</span></span><br><span class="line"><span class="attr">whitelist_filters:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://zhangguanzhang.github.io/adguard/whitelist.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">white</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1599614146</span></span><br><span class="line"><span class="attr">user_rules:</span> []</span><br><span class="line"><span class="attr">dhcp:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">interface_name:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">gateway_ip:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">subnet_mask:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">range_start:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">range_end:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">lease_duration:</span> <span class="number">86400</span></span><br><span class="line">  <span class="attr">icmp_timeout_msec:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">clients:</span> []</span><br><span class="line"><span class="attr">log_compress:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">log_localtime:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">log_max_backups:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">log_max_size:</span> <span class="number">100</span></span><br><span class="line"><span class="attr">log_max_age:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">log_file:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">verbose:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">schema_version:</span> <span class="number">6</span></span><br></pre></td></tr></table></figure><p>保存，然后<code>网络</code>-<code>DHCP/DNS</code>-<code>高级设置</code>-<code>DNS 服务器端口</code>写<code>0</code>，不使用 dnsmasq 的 dns 功能，回到 web 上保存应用。确保关闭了dnsamsq的dns server功能后，在adguardhome保存应用后打开web:3000就可以看到了，默认root/root<br>在<code>设置</code>-<code>常规设置</code>里保留时间之类的不要设置成30天90天之类的，特别是你接入设备多，日志记录了可能把路由器容量撑满，配置成24小时就够了</p><h3 id="放行转发"><a href="#放行转发" class="headerlink" title="放行转发"></a>放行转发</h3><p>设备连上wifi后无法访问外网，看了下到旁路由上能通，旁路由上也能ping公网，猜测iptables缺少放行，<code>网络</code>-<code>防火墙</code>-<code>自定义规则</code>，添加下面内容，cidr根据自己实际情况写</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -I forwarding_rule --src 192.168.2.0/24 -j ACCEPT</span><br></pre></td></tr></table></figure><h3 id="网络不通"><a href="#网络不通" class="headerlink" title="网络不通"></a>网络不通</h3><p>我笔记本经常宿舍到公司，经常遇到到了公司后连不上局域网下面其他机器，抓包发现arp无响应，查看arp表项错误，cmd管理员arp -d删除，例如<code>192.168.2.111</code>不通</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arp -d 192.168.2.111</span><br></pre></td></tr></table></figure><h3 id="配置源"><a href="#配置源" class="headerlink" title="配置源"></a>配置源</h3><p>web上<code>系统</code>-<code>软件包</code>-<code>配置</code>里， <code>发行版软件源</code>全部注释了，最下面追加</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">src/gz openwrt_core http://mirrors.ustc.edu.cn/lede/releases/19.07.4/targets/x86/64/packages</span><br><span class="line">src/gz openwrt_base http://mirrors.ustc.edu.cn/lede/releases/19.07.4/packages/x86_64/base</span><br><span class="line">src/gz openwrt_luci http://mirrors.ustc.edu.cn/lede/releases/19.07.4/packages/x86_64/luci</span><br><span class="line">src/gz openwrt_packages http://mirrors.ustc.edu.cn/lede/releases/19.07.4/packages/x86_64/packages</span><br><span class="line">src/gz openwrt_routing http://mirrors.ustc.edu.cn/lede/releases/19.07.4/packages/x86_64/routing</span><br><span class="line">src/gz openwrt_telephony http://mirrors.ustc.edu.cn/lede/releases/19.07.4/packages/x86_64/telephony</span><br></pre></td></tr></table></figure><p>然后保存</p><h3 id="安装tcpdump"><a href="#安装tcpdump" class="headerlink" title="安装tcpdump"></a>安装tcpdump</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://archive.openwrt.org/releases/packages-18.06/aarch64_generic/base/</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.myxzy.com/post-487.html">https://www.myxzy.com/post-487.html</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;办公室有台式机，不想整天带着N1上下班，整下x86的软路由，exsi性能会比pve好，但是pve毕竟Linux，兼容性和对接很多场景方便，这</summary>
      
    
    
    
    
    <category term="openwrt" scheme="http://zhangguanzhang.github.io/tags/openwrt/"/>
    
    <category term="x86" scheme="http://zhangguanzhang.github.io/tags/x86/"/>
    
  </entry>
  
  <entry>
    <title>斐讯N1刷机和旁路由的设置</title>
    <link href="http://zhangguanzhang.github.io/2020/05/12/N1-flash/"/>
    <id>http://zhangguanzhang.github.io/2020/05/12/N1-flash/</id>
    <published>2020-05-12T19:14:06.000Z</published>
    <updated>2021-02-12T08:03:11.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由来"><a href="#由来" class="headerlink" title="由来"></a>由来</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前一直是使用<code>k2p</code>刷<code>openwrt</code>或者<code>pandora</code>跑v2用，前几天看到了个v2的订阅地址给写到openwrt上了。结果运行了几天发现web上任何配置都无法更改，ssh上去发现touch报错没有容量(而且重装后感觉还是有问题，github访问也不稳定了)，意识到k2p容量实在太小了，于是去恩山论坛逛了下准备买个N1盒子玩玩旁路由。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;N1有8G的rom 2G的ram，cpu是armv8的。询问了一番pdd上有卖的，132买的<code>刷机版+刷机线</code>套餐(部分固件说好些白色的稳定，如果店铺能选颜色可以选白色的试试？，刷机线是双公头的USB，<strong>后期也可以用来救砖</strong>)。到手是安卓电视盒子系统，如果不刷机则是<code>天天链</code>系统。推荐买刷机版本的，省去解锁boot分区啥的（但不是意味着就啥都不用做，毕竟还是要刷openwrt），如果没解锁可以去看 <a href="https://zhuanlan.zhihu.com/p/129414399">https://zhuanlan.zhihu.com/p/129414399</a> 步骤跟着降级。下面步骤是我个人使用的，有些是需要键盘啥的，你没有键盘则应该思考我步骤的目的是干啥的。我步骤全部不带图，毕竟读者都懂Linux</p><h3 id="刷机"><a href="#刷机" class="headerlink" title="刷机"></a>刷机</h3><ul><li>网线一根</li><li>一个显示屏或者家里电视，一根HDMI线</li><li>一个1G以上的USB刻录iso</li><li>一个键盘和一个鼠标</li></ul><p>我的环境是一个路由器(主路由)，我笔记本和N1盒子网口都连路由器的lan，CIDR是<code>192.168.2.1/24</code>, 我盒子作为旁路由，后续准备用<code>192.168.2.2</code>静态ip</p><h4 id="adb"><a href="#adb" class="headerlink" title="adb"></a>adb</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;盒子插上电，然后接上网线和笔记本同一个局域网，盒子HDMI线接个显示屏或者电视。接上usb鼠标点两下设置的齿轮，最下面进去点击版本号多点几次开启开发者选项。鼠标随意右击返回，进开发者选项确认adb状态开启，进法律信息里看局域网的ip</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;笔记本windows如果没adb的话自行下载 <a href="https://dl.google.com/android/repository/platform-tools-latest-windows.zip">adb下载连接</a> 解压到目录，把目录加到环境变量 <code>PATH</code> 里</p><h4 id="准备镜像文件"><a href="#准备镜像文件" class="headerlink" title="准备镜像文件"></a>准备镜像文件</h4><p>去恩山找个N1的openwrt镜像 <a href="https://www.right.com.cn/forum/thread-3160780-1-1.html">我是使用的这个</a>，下载了后准备一个大于1G的usb刻录，使用软件<code>balenaEtcher</code>刻录，下载地址 <a href="https://www.balena.io/etcher/">https://www.balena.io/etcher/</a> ，下载Portable免安装的。刻录后usb插到盒子上</p><h4 id="刷机-1"><a href="#刷机-1" class="headerlink" title="刷机"></a>刷机</h4><p>笔记本git bash或者cmd里执行adb</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adb connect &lt;盒子局域网ip&gt; 5555</span><br></pre></td></tr></table></figure><p>连上之后确认下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adb shell ls</span><br></pre></td></tr></table></figure><p>有安卓文件则继续走</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adb shell reboot update</span><br></pre></td></tr></table></figure><p>不出意外重启会进emcc系统，等待很久后把鼠标拔了换键盘。开机很久后按回车进入终端，下面是把系统写到emcc里，否则是u盘的系统.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /root</span><br><span class="line"><span class="meta">#</span><span class="bash"> 有些固件不一定是这个脚本名，可能是 install.sh 啥的，没有下面脚本就 ls -l 看看</span></span><br><span class="line">./inst-to-emmc.sh</span><br></pre></td></tr></table></figure><p>跟着交互走，不出意外看到成功就可以pweroff关机了，然后拔掉U盘。</p><p>后续u盘刷新系统固件后，插上开机，觉得好用想把u盘的系统刷到emcc里的话</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /root</span><br><span class="line">./update-to-emmc.sh</span><br></pre></td></tr></table></figure><h3 id="旁路由的配置"><a href="#旁路由的配置" class="headerlink" title="旁路由的配置"></a>旁路由的配置</h3><h4 id="路由静态ip配置"><a href="#路由静态ip配置" class="headerlink" title="路由静态ip配置"></a>路由静态ip配置</h4><p>开机后继续键盘。更改旁路由的网络配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/config/network /etc/config/network.bak</span><br><span class="line">vi /etc/config/network</span><br></pre></td></tr></table></figure><p>把<code>192.168.1.1</code>改为预期的配置，没网关的话就也加上预期的主路由的ip</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">option ipaddr <span class="string">&#x27;192.168.2.2&#x27;</span></span><br><span class="line">option netmask <span class="string">&#x27;255.255.255.0&#x27;</span></span><br><span class="line">option gateway <span class="string">&#x27;192.168.2.1&#x27;</span></span><br></pre></td></tr></table></figure><p>重启网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network restart</span><br></pre></td></tr></table></figure><p>然后浏览器进<code>192.168.2.2</code>，默认密码admin password啥的试试</p><h4 id="web配置"><a href="#web配置" class="headerlink" title="web配置"></a>web配置</h4><h5 id="time"><a href="#time" class="headerlink" title="time"></a>time</h5><p>有些固件断电后openwrt的时间不对，可能会遇到旁路由设置好后无法上网，抓包发现tcp重传，最后发现旁路由断电后ntp没同步，如果断电后打开网页提示连接被重置就进<code>系统</code>-<code>系统</code> 点击同步浏览器时间.。有些固件在这样操作后还是会在访问网页的时候发生连接已重置，看后面的丢包的iptables命令处理下后再试试</p><h5 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h5><p>我的固件只有一个LAN接口，不确定其他的是不是这样(推荐此处到最后都先看一遍完后再跟着操作)<br><code>网络</code>-<code>接口</code>，进入LAN修改，下面的DHCP，开启动了。然后高级设置，动态DHCP+强制，写上掩码，下面的DHCP选项两行</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3,192.168.2.2                    # 配置dhcp的网关，指向旁路由自己</span><br><span class="line">6,192.168.2.2,8.8.4.4,8.8.8.8    # 配置dhcp获取到的dns，如果稳定则旁路由自己的IP,建议只设置旁路由的ip</span><br></pre></td></tr></table></figure><p><code>IPv6设置</code>里前三个全部选禁用<br>保存应用</p><h5 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h5><p><code>网络</code>-<code>DHCP/DNS</code>-<code>常规设置</code>，有必要的话配置下DNS转发，屏蔽一些激活码请求域名啥的，<code>丢弃 RFC1918 上行响应数据</code>这个取消了，我这儿是不然某些上游dns的域名无法访问到<br>最下面的写hosts列表，例如单独的指定公网ip下载jetbrains家的插件和软件，绕过前面配置的屏蔽</p><p><code>网络</code>-<code>DHCP/DNS</code>-<code>高级设置</code>-<code>DNS 服务器端口</code>写0，不使用dnsmasq的dns功能，因为dnsmasq经常在加配置的时候卡死，而且解析不稳定，回到web上保存应用。ssh上去<code>netstat -nlptu | grep &#39;:53&#39;</code>看看是不是dnsmasq没有监听53，有些固件web上端口写0还是会监听，此时得手动改配置文件<code>echo port=0 &gt;&gt;/etc/dnsmasq.conf</code> 然后web dnsmasq那重新保存应用</p><p>因为这里关闭了dnsmasq的dns解析，我们打算使用adguardhome来做路由器的dns服务。我们需要在自己pc上下载adguardhome的二进制文件上传到路由器的<code>/usr/bin/AdGuardHome/</code>路径下，权限为755，或者去github下载stable版本的arm64的release，或者下载下面的解压后scp或者<code>web-系统-文件传输</code>传上去,(下载网页)[<a href="https://github.com/AdguardTeam/AdGuardHome/wiki/Getting-Started]">https://github.com/AdguardTeam/AdGuardHome/wiki/Getting-Started]</a></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://static.adguard.com/adguardhome/release/AdGuardHome_linux_arm64.tar.gz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/bin/AdGuardHome</span><br><span class="line">cp -a /tmp/upload/AdGuardHome .</span><br><span class="line">chmod 755 AdGuardHome</span><br></pre></td></tr></table></figure><p><code>服务</code>-<code>AdGuard Home</code>-<code>手动设置</code>，下面是我用的配置文件，web登录的密码是root</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">bind_host:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="attr">bind_port:</span> <span class="number">3000</span></span><br><span class="line"><span class="attr">users:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">root</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">$2y$05$8h.LpbIR7U50.qbV7ynCtOvS9szcqu2lFk6J86Oabnz1J5BtLpVni</span></span><br><span class="line"><span class="attr">http_proxy:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">rlimit_nofile:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">debug_pprof:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">web_session_ttl:</span> <span class="number">720</span></span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line">  <span class="attr">bind_host:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">  <span class="attr">statistics_interval:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">querylog_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">querylog_interval:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">querylog_size_memory:</span> <span class="number">1000</span></span><br><span class="line">  <span class="attr">anonymize_client_ip:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">protection_enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">blocking_mode:</span> <span class="string">nxdomain</span></span><br><span class="line">  <span class="attr">blocking_ipv4:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">blocking_ipv6:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">blocked_response_ttl:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">parental_block_host:</span> <span class="string">family-block.dns.adguard.com</span></span><br><span class="line">  <span class="attr">safebrowsing_block_host:</span> <span class="string">standard-block.dns.adguard.com</span></span><br><span class="line">  <span class="attr">ratelimit:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">ratelimit_whitelist:</span> []</span><br><span class="line">  <span class="attr">refuse_any:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">upstream_dns:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://dns.alidns.com/dns-query</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://dns-family.adguard.com/dns-query</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">tcp://223.5.5.5</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">https://doh.pub/dns-query</span></span><br><span class="line">  <span class="attr">bootstrap_dns:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">223.5</span><span class="number">.5</span><span class="number">.5</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">223.6</span><span class="number">.6</span><span class="number">.6</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">9.9</span><span class="number">.9</span><span class="number">.10</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">149.112</span><span class="number">.112</span><span class="number">.10</span></span><br><span class="line">  <span class="attr">all_servers:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">fastest_addr:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">allowed_clients:</span> []</span><br><span class="line">  <span class="attr">disallowed_clients:</span> []</span><br><span class="line">  <span class="attr">blocked_hosts:</span> []</span><br><span class="line">  <span class="attr">cache_size:</span> <span class="number">4194304</span></span><br><span class="line">  <span class="attr">cache_ttl_min:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">cache_ttl_max:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">bogus_nxdomain:</span> []</span><br><span class="line">  <span class="attr">aaaa_disabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">enable_dnssec:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">edns_client_subnet:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">filtering_enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">filters_update_interval:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">parental_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">safesearch_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">safebrowsing_enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">safebrowsing_cache_size:</span> <span class="number">1048576</span></span><br><span class="line">  <span class="attr">safesearch_cache_size:</span> <span class="number">1048576</span></span><br><span class="line">  <span class="attr">parental_cache_size:</span> <span class="number">1048576</span></span><br><span class="line">  <span class="attr">cache_time:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">rewrites:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">plugins.jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">13.32</span><span class="number">.53</span><span class="number">.109</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">download.jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">52.30</span><span class="number">.174</span><span class="number">.243</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">www.jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">domain:</span> <span class="string">jetbrains.com</span></span><br><span class="line">    <span class="attr">answer:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">  <span class="attr">blocked_services:</span> []</span><br><span class="line"><span class="attr">tls:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">server_name:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">force_https:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">port_https:</span> <span class="number">443</span></span><br><span class="line">  <span class="attr">port_dns_over_tls:</span> <span class="number">853</span></span><br><span class="line">  <span class="attr">allow_unencrypted_doh:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">strict_sni_check:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">certificate_chain:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">private_key:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">certificate_path:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">private_key_path:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">filters:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://adguardteam.github.io/AdGuardSDNSFilter/Filters/filter.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">AdGuard</span> <span class="string">Simplified</span> <span class="string">Domain</span> <span class="string">Names</span> <span class="string">filter</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://adaway.org/hosts.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">AdAway</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">2</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://hosts-file.net/ad_servers.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hpHosts</span> <span class="bullet">-</span> <span class="string">Ad</span> <span class="string">and</span> <span class="string">Tracking</span> <span class="string">servers</span> <span class="string">only</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">3</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://www.malwaredomainlist.com/hostslist/hosts.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">MalwareDomainList.com</span> <span class="string">Hosts</span> <span class="string">List</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">4</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://raw.githubusercontent.com/vokins/yhosts/master/data/tvbox.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tvbox</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575018007</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://hosts.nfz.moe/full/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">neoHosts</span> <span class="string">full</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618240</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://hosts.nfz.moe/basic/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">neoHosts</span> <span class="string">basic</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618241</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">http://sbc.io/hosts/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">StevenBlack</span> <span class="string">host</span> <span class="string">basic</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618242</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">http://sbc.io/hosts/alternates/fakenews-gambling-porn-social/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">StevenBlack</span> <span class="string">host+fakenews</span> <span class="string">+</span> <span class="string">gambling</span> <span class="string">+</span> <span class="string">porn</span> <span class="string">+</span> <span class="string">social</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1575618243</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://cdn.jsdelivr.net/gh/privacy-protection-tools/anti-AD/anti-ad-easylist.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">anti-AD(Adblock+neohosts+yhosts+cjxlist+adhlist)</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1577113202</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://zhangguanzhang.github.io/adguard/hosts</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hosts</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1608901996</span></span><br><span class="line"><span class="attr">whitelist_filters:</span> </span><br><span class="line"><span class="bullet">-</span> <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">https://zhangguanzhang.github.io/adguard/whitelist.txt</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">white</span></span><br><span class="line">  <span class="attr">id:</span> <span class="number">1599614146</span></span><br><span class="line"><span class="attr">user_rules:</span> []</span><br><span class="line"><span class="attr">dhcp:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">interface_name:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">gateway_ip:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">subnet_mask:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">range_start:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">range_end:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">lease_duration:</span> <span class="number">86400</span></span><br><span class="line">  <span class="attr">icmp_timeout_msec:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">clients:</span> []</span><br><span class="line"><span class="attr">log_file:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">verbose:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">schema_version:</span> <span class="number">6</span></span><br></pre></td></tr></table></figure><p>保存应用后打开web:3000就可以看到了，默认root/root<br>在<code>设置</code>-<code>常规设置</code>里保留时间之类的不要设置成30天90天之类的，特别是你接入设备多，日志记录了可能把路由器容量撑满，配置成24小时就够了</p><h4 id="主路由的lan配置"><a href="#主路由的lan配置" class="headerlink" title="主路由的lan配置"></a>主路由的lan配置</h4><p>关闭DHCP即可，因为有人的旁路由丢包说关闭主路由的DHCP然后重启主路由。如果你不能动主路由，可以旁路由上关闭dhcp，就是接入的设备需要设置下不适用dhcp的ip，使用静态ip配置网关和dns成旁路有的ip。有些主路由现在的wan口默认是智能选择，可能需要改成固定wan口模式后重启下</p><h3 id="后续的一些问题和解决办法"><a href="#后续的一些问题和解决办法" class="headerlink" title="后续的一些问题和解决办法"></a>后续的一些问题和解决办法</h3><h4 id="N1盒子旁路由丢包"><a href="#N1盒子旁路由丢包" class="headerlink" title="N1盒子旁路由丢包"></a>N1盒子旁路由丢包</h4><p>有人说缺少下面iptables命令，但是我前面加了后面配置完无法上网，我最后把它删除了就能上网了</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注：固件做旁路由的话不要忘了加自定义防火墙规则（网络-&gt;防火墙-&gt;自定义规则）：</span><br><span class="line">iptables -t nat -I POSTROUTING -o eth0 -j MASQUERADE</span><br><span class="line">也可以尝试（有桥接存在的情况下）</span><br><span class="line">iptables -t nat -I POSTROUTING -o br-lan -j MASQUERADE</span><br></pre></td></tr></table></figure><p><a href="https://koolshare.cn/thread-178371-1-1.html">https://koolshare.cn/thread-178371-1-1.html</a> 帖子下面有两个人说关闭主路由的DHCP并重启主路由后才行。<br><a href="https://www.right.com.cn/forum/forum.php?mod=viewthread&amp;tid=3592744&amp;page=1#pid8444520">https://www.right.com.cn/forum/forum.php?mod=viewthread&amp;tid=3592744&amp;page=1#pid8444520</a> 帖子下说主路由关闭硬件加速<br><a href="https://www.right.com.cn/forum/forum.php?mod=viewthread&amp;tid=3592744&amp;page=1#pid8444520">https://www.right.com.cn/forum/forum.php?mod=viewthread&amp;tid=3592744&amp;page=1#pid8444520</a> <code>网络</code>-<code>接口</code>-<code>LAN</code>桥接关掉 不要自定义 直接单选那个eth0</p><p><code>网络</code>-<code>接口</code>-<code>LAN</code>-<code>物理设置</code>-<code>桥接接口</code>关掉 不要自定义 直接单选那个eth0。我试了这个后，br-lan桥接的网卡没了，eth0静态ip，qq邮箱打开都更快了。观察了好几天都没丢包了。如果取消桥接后断网，可以接屏幕和键盘到盒子ssh，看文件<code>/etc/config/network.bak</code>里的桥接那些行给加到<code>/etc/config/network</code>里重启网络</p><h4 id="docker容器无法访问外网"><a href="#docker容器无法访问外网" class="headerlink" title="docker容器无法访问外网"></a>docker容器无法访问外网</h4><p>容器无法访问外网，查看了下nat表里缺少<code>masquerade</code>，<code>网络</code>-<code>防火墙</code>-<code>自定义规则</code>，添加下面内容，cidr是容器的cidr段，然后<code>网络</code>-<code>防火墙</code>-<code>常规设置</code>-<code>转发</code>把<code>拒绝</code>切到<code>接受</code>，然后点击重启防火墙</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -I POSTROUTING -s 172.31.0.0/24 ! -o docker0 -j MASQUERADE</span><br></pre></td></tr></table></figure><h4 id="安装tcpdump"><a href="#安装tcpdump" class="headerlink" title="安装tcpdump"></a>安装tcpdump</h4><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://archive.openwrt.org/releases/packages-18.06/aarch64_generic/base/</span><br></pre></td></tr></table></figure><h3 id="救砖-免拆机"><a href="#救砖-免拆机" class="headerlink" title="救砖(免拆机)"></a>救砖(免拆机)</h3><p>2020/09/21 我把新固件写到U盘里后插上盒子上重启，结果还是老系统。搜到官方文档说用dd写，上传到目录后改名。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd if=op.img of=/dev/sda</span><br></pre></td></tr></table></figure><p>结果重启还是老系统。然后 <code>dd的of=/dev/mmcblk1</code> 后开机一直不断重启。后面搜了下救砖姿势</p><h4 id="救砖开始"><a href="#救砖开始" class="headerlink" title="救砖开始"></a>救砖开始</h4><p>需要一根USB双公头的线，好些之前买的时候刷机线就是，USB线接电脑和N1，N1的电源插头插上插座，N1这头电源接口不插。</p><p>链接: <a href="https://pan.baidu.com/s/1y0IM3J46KTXr071jDUwidQ">https://pan.baidu.com/s/1y0IM3J46KTXr071jDUwidQ</a> 提取码: s2an， 下载 <code>刷机软件</code> 和 <code>N1_mod_by_webpad_v2.2_20180920.rar</code></p><p>电脑解开<code>刷机软件</code>安装<code>USB Burning tool</code>（好些很多电视盒子刷机都可以用它）并安装，然后打开，点击左上角文件，选择解压后的<code>webpad_v2.2_20180920</code>，然后取消右侧的<code>擦除bootloader</code>，点击<code>开始</code>后迅速差上N1的电源。会开始烧录，等待烧录完成后，点击停止关闭软件。盒子会自动重启进入<code>webpad</code>的系统，网线接到路由器的lan口下，会默认dhcp获取ip。或者你接显示屏和鼠标键盘配置ip。</p><p>然后盒子USB差上烧录固件的u盘，电脑 ADB connect上后和shell reboot update，然后笔记本配置和固件同一个二层的ip ssh上去后执行脚本写入emcc，或者接显示器和键盘把固件写入emcc</p><h3 id="刷回安卓盒子系统"><a href="#刷回安卓盒子系统" class="headerlink" title="刷回安卓盒子系统"></a>刷回安卓盒子系统</h3><p>来源 <a href="https://www.right.com.cn/forum/thread-2866816-1-1.html">https://www.right.com.cn/forum/thread-2866816-1-1.html</a></p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">1. U盘刷入M大的9.2.0 CoreELEC系统</span><br><span class="line"></span><br><span class="line">2. U盘插N1远离HDMI的那个USB口，对，是远离的那个口，然后加电开机</span><br><span class="line"></span><br><span class="line">3. 路由器看该系统IP，然后Xshell进入，用户名root ,密码coreelec</span><br><span class="line"></span><br><span class="line">4. 电脑端刷机工具加载IMG文件，我用的是R大的6.2.3，勾选“擦除flash”，然后点开始</span><br><span class="line"></span><br><span class="line">5. 双公头线连接电脑和N1近HDMI线的那个口，是的，此时就可以直接连上双公头线。</span><br><span class="line"></span><br><span class="line">6. Xshell端命令行处输入： reboot update</span><br><span class="line"></span><br><span class="line">7. 此时会重启，然后耐心等待刷入安卓系统即可，整个过程U盘也不用拔。</span><br><span class="line"></span><br><span class="line">8. 耐心等待刷完，点停止，然后拔出所有的U盘，网线啥的，再上电就可以了。</span><br><span class="line">------------------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">附：查看N1安卓系统有线和无线MAC办法，前提是如R大的盒子已有busybox</span><br><span class="line"></span><br><span class="line">1. 路由器查看N1安卓盒子IP</span><br><span class="line"></span><br><span class="line">2. Xshell进入，注意此时端口是：2223，用户名：rush，密码rush</span><br><span class="line"></span><br><span class="line">3. 进入后ifconfig即可查看有线和无线的MAC了</span><br><span class="line"></span><br><span class="line">测试结果，</span><br><span class="line"></span><br><span class="line">1. 有线的MAC重启后不会变，IP不会变</span><br><span class="line"></span><br><span class="line">2. 同一个安卓IMG刷入别的盒子后，别的盒子有线MAC地址和前一盒子不同，不用担心有线MAC重合问题</span><br><span class="line"></span><br><span class="line">3. 无线MAC固定就是00:90:4C:C5:12:38，刷2个盒子都一样，重合</span><br><span class="line"></span><br><span class="line">4. 蓝牙MAC不重合，刷了2个盒子做了测试。</span><br><span class="line">5. 干掉安卓，把armbian 5.77重新写入EMMC后，发现在armbian里每次重启有线MAC都会变，但IP不变，无线的MAC也不变。</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由来&quot;&gt;&lt;a href=&quot;#由来&quot; class=&quot;headerlink&quot; title=&quot;由来&quot;&gt;&lt;/a&gt;由来&lt;/h2&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;之前一直是使用&lt;code&gt;k2p&lt;/code</summary>
      
    
    
    
    
    <category term="N1" scheme="http://zhangguanzhang.github.io/tags/N1/"/>
    
    <category term="openwrt" scheme="http://zhangguanzhang.github.io/tags/openwrt/"/>
    
  </entry>
  
</feed>
